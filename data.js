export const data = [
    {
        title: "Aitken's law",
        link: "https://en.wikipedia.org/wiki/Aitken%27s_law",
        content:
            "The Scottish vowel length rule, also known as Aitken's law, describes how vowel length in Scots, Scottish English, and, to some extent, Ulster English[1] and Geordie[2] is conditioned by the phonetic environment of the vowel. Primarily, the rule is that certain vowels (described below) are phonetically long in the following environments:\n\nExceptions can also exist for particular vowel phonemes, dialects, words, etc., some of which are discussed in greater detail below.\n\nThe underlying phonemes of the Scottish vowel system (that is, in both Scottish Standard English dialects and Scots dialects) are as follows:[3]\n\n★ = Vowels that definitively follow the Scottish Vowel Length Rule.\n\nThe Scottish Vowel Length Rule affects all vowels except the always-short vowels 15 and 19 (/ɪ/ and /ʌ/) and, in many Modern Scots varieties, the always-long Scots-only vowels 8, 11, and 12 (here transcribed as /eː/, /iː/ and /ɔː/) that do not occur as phonemes separate from /e, i, ɔ/ in Scottish Standard English.[17] The further north a Scots dialect is from central Scotland, the more it will contain specific words that do not adhere to the rule.[18]\n\nThe Scottish Vowel Length Rule is assumed to have come into being between the early Middle Scots and late Middle Scots periods.[24]",
        pageTitle: "Scottish vowel length rule",
    },
    {
        title: "Amagat's law",
        link: "https://en.wikipedia.org/wiki/Amagat%27s_law",
        content:
            "Amagat's law  or the law of partial volumes describes the behaviour and properties of mixtures of ideal (as well as some cases of non-ideal) gases. It is of use in chemistry and thermodynamics. It is named after Émile Amagat.\n\nAmagat's law states that the extensive volume V = Nv of a gas mixture is equal to the sum of volumes Vi of the K component gases, if the temperature T and the pressure p remain the same:[1][2]\n\nN\n        \n        v\n        (\n        T\n        ,\n        p\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            K\n          \n        \n        \n          N\n          \n            i\n          \n        \n        \n        \n          v\n          \n            i\n          \n        \n        (\n        T\n        ,\n        p\n        )\n        .\n      \n    \n    {\\displaystyle N\\,v(T,p)=\\sum _{i=1}^{K}N_{i}\\,v_{i}(T,p).}\n\nThis is the experimental expression of volume as an extensive quantity.\n\nAccording to Amagat's law of partial volume, the total volume of a non-reacting mixture of gases at constant temperature and pressure should be equal to the sum of the individual partial volumes of the constituent gases. So if \n  \n    \n      \n        \n          V\n          \n            1\n          \n        \n        ,\n        \n          V\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          V\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle V_{1},V_{2},\\dots ,V_{n}}\n  \n are considered to be the partial volumes of components in the gaseous mixture, then the total volume V would be represented as\n\nBoth Amagat's and Dalton's law predict the properties of gas mixtures. Their predictions are the same for ideal gases. However, for real (non-ideal) gases, the results differ.[3] Dalton's law of partial pressures assumes that the gases in the mixture are non-interacting (with each other) and each gas independently applies its own pressure, the sum of which is the total pressure. Amagat's law assumes that the volumes of the component gases (again at the same temperature and pressure) are additive; the interactions of the different gases are the same as the average interactions of the components.\n\nThe interactions can be interpreted in terms of a second virial coefficient B(T) for the mixture. For two components, the second virial coefficient for the mixture can be expressed as\n\nB\n        (\n        T\n        )\n        =\n        \n          X\n          \n            1\n          \n        \n        \n          B\n          \n            1\n          \n        \n        +\n        \n          X\n          \n            2\n          \n        \n        \n          B\n          \n            2\n          \n        \n        +\n        \n          X\n          \n            1\n          \n        \n        \n          X\n          \n            2\n          \n        \n        \n          B\n          \n            1\n            ,\n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle B(T)=X_{1}B_{1}+X_{2}B_{2}+X_{1}X_{2}B_{1,2},}\n\nwhere the subscripts refer to components 1 and 2, the Xi are the mole fractions, and the Bi are the second virial coefficients. The cross term B1,2 of the mixture is given by\n\nWhen the volumes of each component gas (same temperature and pressure) are very similar, then Amagat's law becomes mathematically equivalent to Vegard's law for solid mixtures.\n\nWhen Amagat's law is valid and the gas mixture is made of ideal gases,\n\nIt follows that the mole fraction and volume fraction are the same. This is true also for other equation of state.",
        pageTitle: "Amagat's law",
    },
    {
        title: "Amdahl's law",
        link: "https://en.wikipedia.org/wiki/Amdahl%27s_law",
        content:
            "In computer architecture, Amdahl's law (or Amdahl's argument[1]) is a formula that shows how much faster a task can be completed when more resources are added to the system.\n\n\"the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used\".[2]\n\nIt is named after computer scientist Gene Amdahl, and was presented at the American Federation of Information Processing Societies (AFIPS) Spring Joint Computer Conference in 1967.\n\nAmdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.\n\nIn the context of Amdahl's law, speedup can be defined as: [3]\n\nSpeedup\n        \n        =\n        \n          \n            Performance for the entire task when enhancements are applied\n            Performance for the same task without those enhancements\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}={\\frac {\\text{Performance for the entire task when enhancements are applied}}{\\text{Performance for the same task without those enhancements}}}}\n\nSpeedup\n        \n        =\n        \n          \n            Execution time for the entire task without enhancements\n            Execution time for the same task when enhancements are applied\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}={\\frac {\\text{Execution time for the entire task without enhancements}}{\\text{Execution time for the same task when enhancements are applied}}}}\n\nAmdahl's law can be formulated in the following way: [4]\n\nThe \n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            overall\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{overall}}}\n  \n is frequently much lower than one might expect. For instance, if a programmer enhances a part of the code that represents 10% of the total execution time (i.e. \n  \n    \n      \n        \n          \n            Time\n          \n          \n            optimized\n          \n        \n      \n    \n    {\\displaystyle {\\text{Time}}_{\\text{optimized}}}\n  \n of 0.10) and achieves a  \n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            optimized\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{optimized}}}\n  \n of 10,000, then  \n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            overall\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{overall}}}\n  \n becomes 1.11 which means only 11% improvement in total speedup of the program. So, despite a massive improvement in one section, the overall benefit is quite small. In another example, if the programmer optimizes a section that accounts for 99% of the execution time (i.e. \n  \n    \n      \n        \n          \n            Time\n          \n          \n            optimized\n          \n        \n      \n    \n    {\\displaystyle {\\text{Time}}_{\\text{optimized}}}\n  \n of 0.99) with a speedup factor of 100 (i.e. \n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            optimized\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{optimized}}}\n  \nof 100), the \n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            overall\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{overall}}}\n  \n only reaches 50. This indicates that half of the potential performance gain (\n  \n    \n      \n        \n          \n            Speedup\n          \n          \n            overall\n          \n        \n      \n    \n    {\\displaystyle {\\text{Speedup}}_{\\text{overall}}}\n  \n will reach 100 if 100% of the execution time is covered) is lost due to the remaining 1% of execution time that was not improved. [4]\n\nFollowings are implications of Amdahl's law: [5][6]\n\nFollowings are limitations of Amdahl's law: [7][3][8]\n\nAmdahl's law applies only to the cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work. In this case, Gustafson's law gives a less pessimistic and more realistic assessment of the parallel performance.[10]\n\nUniversal Scalability Law (USL), developed by Neil J. Gunther, extends the Amdahl's law and accounts for the additional overhead due to inter-process communication. USL quantifies scalability based on parameters such as contention and coherency. [11]\n\nA task executed by a system whose resources are improved compared to an initial similar system can be split up into two parts:\n\nAn example is a computer program that processes files. A part of that program may scan the directory of the disk and create a list of files internally in memory. After that, another part of the program passes each file to a separate thread for processing. The part that scans the directory and creates the file list cannot be sped up on a parallel computer, but the part that processes the files can.\n\nThe execution time of the whole task before the improvement of the resources of the system is denoted as \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n. It includes the execution time of the part that would not benefit from the improvement of the resources and the execution time of the one that would benefit from it. The fraction of the execution time of the task that would benefit from the improvement of the resources is denoted by \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n. The one concerning the part that would not benefit from it is therefore \n  \n    \n      \n        1\n        −\n        p\n      \n    \n    {\\displaystyle 1-p}\n  \n. Then:\n\nIt is the execution of the part that benefits from the improvement of the resources that is accelerated by the factor \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n after the improvement of the resources. Consequently, the execution time of the part that does not benefit from it remains the same, while the part that benefits from it becomes:\n\nThe theoretical execution time \n  \n    \n      \n        T\n        (\n        s\n        )\n      \n    \n    {\\displaystyle T(s)}\n  \n of the whole task after the improvement of the resources is then:\n\nAmdahl's law gives the theoretical speedup in latency of the execution of the whole task at fixed workload \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n, which yields\n\nIf 30% of the execution time may be the subject of a speedup, p will be 0.3; if the improvement makes the affected part twice as fast, s will be 2. Amdahl's law states that the overall speedup of applying the improvement will be:\n\nFor example, assume that we are given a serial task which is split into four consecutive parts, whose percentages of execution time are p1 = 0.11, p2 = 0.18, p3 = 0.23, and p4 = 0.48 respectively. Then we are told that the 1st part is not sped up, so s1 = 1, while the 2nd part is sped up 5 times, so s2 = 5, the 3rd part is sped up 20 times, so s3 = 20, and the 4th part is sped up 1.6 times, so s4 = 1.6. By using Amdahl's law, the overall speedup is\n\nNotice how the 5 times and 20 times speedup on the 2nd and 3rd parts respectively don't have much effect on the overall speedup when the 4th part (48% of the execution time) is accelerated by only 1.6 times.\n\nFor example, with a serial program in two parts A and B for which TA = 3 s and TB = 1 s,\n\nTherefore, making part A to run 2 times faster is better than making part B to run 5 times faster. The percentage improvement in speed can be calculated as\n\nIf the non-parallelizable part is optimized by a factor of \n  \n    \n      \n        O\n      \n    \n    {\\displaystyle O}\n  \n, then\n\nIt follows from Amdahl's law that the speedup due to parallelism is given by\n\nWhen \n  \n    \n      \n        s\n        =\n        1\n      \n    \n    {\\displaystyle s=1}\n  \n, we have \n  \n    \n      \n        \n          S\n          \n            latency\n          \n        \n        (\n        O\n        ,\n        s\n        )\n        =\n        1\n      \n    \n    {\\displaystyle S_{\\text{latency}}(O,s)=1}\n  \n, meaning that the speedup is\nmeasured with respect to the execution time after the non-parallelizable part is optimized.\n\nWhen  \n  \n    \n      \n        s\n        =\n        ∞\n      \n    \n    {\\displaystyle s=\\infty }\n  \n,\n\nIf  \n  \n    \n      \n        1\n        −\n        p\n        =\n        0.4\n      \n    \n    {\\displaystyle 1-p=0.4}\n  \n, \n  \n    \n      \n        O\n        =\n        2\n      \n    \n    {\\displaystyle O=2}\n  \n and \n  \n    \n      \n        s\n        =\n        5\n      \n    \n    {\\displaystyle s=5}\n  \n, then:\n\nNext, we consider the case wherein the non-parallelizable part is reduced by a factor of \n  \n    \n      \n        \n          O\n          ′\n        \n      \n    \n    {\\displaystyle O'}\n  \n, and the parallelizable part is correspondingly increased. Then\n\nIt follows from Amdahl's law that the speedup due to parallelism is given by\n\nAmdahl's law is often conflated with the law of diminishing returns, whereas only a special case of applying Amdahl's law demonstrates law of diminishing returns. If one picks optimally (in terms of the achieved speedup) what is to be improved, then one will see monotonically decreasing improvements as one improves. If, however, one picks non-optimally, after improving a sub-optimal component and moving on to improve a more optimal component, one can see an increase in the return. Note that it is often rational to improve a system in an order that is \"non-optimal\" in this sense, given that some improvements are more difficult or require larger development time than others.\n\nAmdahl's law does represent the law of diminishing returns if one is considering what sort of return one gets by adding more processors to a machine, if one is running a fixed-size computation that will use all available processors to their capacity. Each new processor added to the system will add less usable power than the previous one. Each time one doubles the number of processors the speedup ratio will diminish, as the total throughput heads toward the limit of 1/(1 − p).\n\nThis analysis neglects other potential bottlenecks such as memory bandwidth and I/O bandwidth. If these resources do not scale with the number of processors, then merely adding processors provides even lower returns.\n\nAn implication of Amdahl's law is that to speed up real applications which have both serial and parallel portions, heterogeneous computing techniques are required.[12] There are novel speedup and energy consumption models based on a more general representation of heterogeneity, referred to as the normal form heterogeneity, that support a wide range of heterogeneous many-core architectures. These modelling methods aim to predict system power efficiency and performance ranges, and facilitates research and development at the hardware and system software levels.[13][14]",
        pageTitle: "Amdahl's law",
    },
    {
        title: "Ampère's circuital law",
        link: "https://en.wikipedia.org/wiki/Amp%C3%A8re%27s_circuital_law",
        content:
            'In classical electromagnetism, Ampère\'s circuital law (not to be confused with Ampère\'s force law)[1] relates the circulation of a magnetic field around a closed loop to the electric current passing through the loop.\n\nJames Clerk Maxwell derived it using hydrodynamics in his 1861 published paper "On Physical Lines of Force".[2] In 1865, he generalized the equation to apply to time-varying currents by adding the displacement current term, resulting in the modern form of the law, sometimes called the Ampère–Maxwell law,[3][4][5] which is one of Maxwell\'s equations that form the basis of classical electromagnetism.\n\nIn 1820 Danish physicist Hans Christian Ørsted  discovered that an electric current creates a magnetic field around it, when he noticed that the needle of a compass next to a wire carrying current turned so that the needle was perpendicular to the wire.[6][7]   He investigated and discovered the rules which govern the field around a straight current-carrying wire:[8]\n\nThis sparked a great deal of research into the relation between electricity and magnetism.  André-Marie Ampère investigated the magnetic force between two current-carrying wires, discovering Ampère\'s force law.  In the 1850s Scottish mathematical physicist James Clerk Maxwell generalized these results and others into a single mathematical law.  The original form of Maxwell\'s circuital law, which he derived as early as 1855 in his paper "On Faraday\'s Lines of Force"[9] based on an analogy to hydrodynamics, relates magnetic fields to electric currents that produce them. It determines the magnetic field associated with a given current, or the current associated with a given magnetic field.\n\nThe original circuital law only applies to a magnetostatic situation, to continuous steady currents flowing in a closed circuit. For systems with electric fields that change over time, the original law (as given in this section) must be modified to include a term known as Maxwell\'s correction (see below).\n\nThe original circuital law can be written in several different forms, which are all ultimately equivalent:\n\nThe integral form of the original circuital law is a line integral of the magnetic field around some closed curve C (arbitrary but must be closed). The curve C in turn bounds both a surface S which the electric current passes through (again arbitrary but not closed—since no three-dimensional volume is enclosed by S), and encloses the current. The mathematical statement of the law is a relation between the circulation of the magnetic field around some path (line integral) due to the current which passes through that enclosed path (surface integral).[10][11]\n\nIn terms of total current, (which is the sum of both free current and bound current) the line integral of the magnetic B-field (in teslas, T) around closed curve C is proportional to the total current Ienc passing through a surface S (enclosed by C). In terms of free current, the line integral of the magnetic H-field (in amperes per metre, A·m−1) around closed curve C equals the free current If,enc through a surface S.[clarification needed]\n\nThere are a number of ambiguities in the above definitions that require clarification and a choice of convention.\n\nThe electric current that arises in the simplest textbook situations would be classified as "free current"—for example, the current that passes through a wire or battery. In contrast, "bound current" arises in the context of bulk materials that can be magnetized and/or polarized. (All materials can to some extent.)\n\nWhen a material is magnetized (for example, by placing it in an external magnetic field), the electrons remain bound to their respective atoms, but behave as if they were orbiting the nucleus in a particular direction, creating a microscopic current. When the currents from all these atoms are put together, they create the same effect as a macroscopic current, circulating perpetually around the magnetized object. This magnetization current JM is one contribution to "bound current".\n\nThe other source of bound current is bound charge. When an electric field is applied, the positive and negative bound charges can separate over atomic distances in polarizable materials, and when the bound charges move, the polarization changes, creating another contribution to the "bound current", the polarization current JP.\n\nThe total current density J due to free and bound charges is then:\n\nwith Jf  the "free" or "conduction" current density.\n\nAll current is fundamentally the same, microscopically. Nevertheless, there are often practical reasons for wanting to treat bound current differently from free current. For example, the bound current usually originates over atomic dimensions, and one may wish to take advantage of a simpler theory intended for larger dimensions. The result is that the more microscopic Ampère\'s circuital law, expressed in terms of B and the microscopic current (which includes free, magnetization and polarization currents), is sometimes put into the equivalent form below in terms of H and the free current only. For a detailed definition of free current and bound current, and the proof that the two formulations are equivalent, see the "proof" section below.\n\nThere are two important issues regarding the circuital law that require closer scrutiny. First, there is an issue regarding the continuity equation for electrical charge.  In vector calculus, the identity for the divergence of a curl states that the divergence of the curl of a vector field must always be zero. Hence\n\nand so the original Ampère\'s circuital law implies that\n\nBut in general, reality follows the continuity equation for electric charge:\n\nwhich is nonzero for a time-varying charge density. An example occurs in a capacitor circuit where time-varying charge densities exist on the plates.[12][13][14][15][16]\n\nSecond, there is an issue regarding the propagation of electromagnetic waves. For example, in free space, where\n\ni.e. that the magnetic field is irrotational, but to maintain consistency with the continuity equation for electric charge, we must have\n\nTo treat these situations, the contribution of displacement current must be added to the current term in the circuital law.\n\nJames Clerk Maxwell conceived of displacement current as a polarization current in the dielectric vortex sea, which he used to model the magnetic field hydrodynamically and mechanically.[17] He added this displacement current to Ampère\'s circuital law at equation 112 in his 1861 paper "On Physical Lines of Force".[18]\n\nIn free space, the displacement current is related to the time rate of change of electric field.\n\nIn a dielectric the above contribution to displacement current is present too, but a major contribution to the displacement current is related to the polarization of the individual molecules of the dielectric material. Even though charges cannot flow freely in a dielectric, the charges in molecules can move a little under the influence of an electric field. The positive and negative charges in molecules separate under the applied field, causing an increase in the state of polarization, expressed as the polarization density P.  A changing state of polarization is equivalent to a current.\n\nBoth contributions to the displacement current are combined by defining the displacement current as:[12]\n\nwhere the electric displacement field is defined as:\n\nwhere ε0 is the electric constant, εr the relative static permittivity, and P is the polarization density. Substituting this form for D in the expression for displacement current, it has two components:\n\nThe first term on the right hand side is present everywhere, even in a vacuum. It doesn\'t involve any actual movement of charge, but it nevertheless has an associated magnetic field, as if it were an actual current. Some authors apply the name displacement current to only this contribution.[19]\n\nThe second term on the right hand side is the displacement current as originally conceived by Maxwell, associated with the polarization of the individual molecules of the dielectric material.\n\nMaxwell\'s original explanation for displacement current focused upon the situation that occurs in dielectric media. In the modern post-aether era, the concept has been extended to apply to situations with no material media present, for example, to the vacuum between the plates of a charging vacuum capacitor. The displacement current is justified today because it serves several requirements of an electromagnetic theory:  correct prediction of magnetic fields in regions where no free current flows; prediction of wave propagation of electromagnetic fields; and conservation of electric charge in cases where charge density is time-varying. For greater discussion see Displacement current.\n\nNext, the circuital equation is extended by including the polarization current, thereby remedying the limited applicability of the original circuital law.\n\nTreating free charges separately from bound charges, the equation including Maxwell\'s correction in terms of the H-field is (the H-field is used because it includes the magnetization currents, so JM does not appear explicitly, see H-field and also Note):[20]\n\n(integral form), where H is the magnetic H field (also called "auxiliary magnetic field", "magnetic field intensity", or just "magnetic field"), D is the electric displacement field, and Jf is the enclosed conduction current or free current density. In differential form,\n\nOn the other hand, treating all charges on the same footing (disregarding whether they are bound or free charges), the generalized Ampère\'s equation, also called the Maxwell–Ampère equation, is in integral form (see the "proof" section below):\n\n∮\n          \n            C\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        \n          ∬\n          \n            S\n          \n        \n        \n          (\n          \n            \n              μ\n              \n                0\n              \n            \n            \n              J\n            \n            +\n            \n              μ\n              \n                0\n              \n            \n            \n              ε\n              \n                0\n              \n            \n            \n              \n                \n                  ∂\n                  \n                    E\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n      \n    \n    {\\displaystyle \\oint _{C}\\mathbf {B} \\cdot \\mathrm {d} {\\boldsymbol {l}}=\\iint _{S}\\left(\\mu _{0}\\mathbf {J} +\\mu _{0}\\varepsilon _{0}{\\frac {\\partial \\mathbf {E} }{\\partial t}}\\right)\\cdot \\mathrm {d} \\mathbf {S} }\n\n∇\n        \n        ×\n        \n          B\n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        \n          J\n        \n        +\n        \n          μ\n          \n            0\n          \n        \n        \n          ε\n          \n            0\n          \n        \n        \n          \n            \n              ∂\n              \n                E\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\nabla } \\times \\mathbf {B} =\\mu _{0}\\mathbf {J} +\\mu _{0}\\varepsilon _{0}{\\frac {\\partial \\mathbf {E} }{\\partial t}}}\n\nIn both forms J includes magnetization current density[21] as well as conduction and polarization current densities. That is, the current density on the right side of the Ampère–Maxwell equation is:\n\nwhere current density JD is the displacement current, and J is the current density contribution actually due to movement of charges, both free and bound. Because ∇ ⋅ D = ρ, the charge continuity issue with Ampère\'s original formulation is no longer a problem.[22] Because of the term in ε0.mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠∂E/∂t⁠, wave propagation in free space now is possible.\n\nWith the addition of the displacement current, Maxwell was able to hypothesize (correctly) that light was a form of electromagnetic wave. See electromagnetic wave equation for a discussion of this important discovery.\n\nProof that the formulations of the circuital law in terms of free current are equivalent to the formulations involving total current\n\nNote that we are only dealing with the differential forms, not the integral forms, but that is sufficient since the differential and integral forms are equivalent in each case, by the Kelvin–Stokes theorem.\n\nWe introduce the polarization density P, which has the following relation to E and D:\n\nNext, we introduce the magnetization density M, which has the following relation to B and H:\n\nis the polarization current density. Taking the equation for B:\n\nConsequently, referring to the definition of the bound current:\n\nIn cgs units, the integral form of the equation, including Maxwell\'s correction, reads\n\nThe differential form of the equation (again, including Maxwell\'s correction) is',
        pageTitle: "Ampère's circuital law",
    },
    {
        title: "Andy and Bill's law",
        link: "https://en.wikipedia.org/wiki/Andy_and_Bill%27s_law",
        content:
            'Andy and Bill\'s law, occasionally known as The Great Moore\'s Law Compensator[1] is the assertion that new software will tend to consume any increase in computing power that new hardware can provide. The law originates from a humorous one-liner told in the 1990s during computing conferences: "what Andy giveth, Bill taketh away." The phrase is a riff upon the business strategies of former Intel CEO Andy Grove and former Microsoft CEO Bill Gates.[2] Intel and Microsoft had entered into a lucrative partnership in the 1980s through to the 1990s, and Intel chipsets became the de facto standard for PCs running Microsoft Windows, giving way to the term "Wintel". Despite this profitable arrangement, Grove felt that Gates was not making full use of the powerful capabilities of Intel chips and that Gates was in fact refusing to upgrade his software to achieve optimum hardware performance.[3] Grove\'s frustration with the dominance of Microsoft software over Intel hardware became public, spawning the humorous catchphrase, and later, the law. In more recent years, the law has also been stated "what Intel giveth, Microsoft taketh away," foregoing the metonymy of the original.[1]\n\nThis technology-related article is a stub. You can help Wikipedia by expanding it.',
        pageTitle: "Andy and Bill's law",
    },
    {
        title: "Archie's law",
        link: "https://en.wikipedia.org/wiki/Archie%27s_law",
        content:
            "In petrophysics, Archie's law is a purely empirical law relating the measured electrical conductivity of a porous rock to its porosity and fluid saturation. It is named after Gus Archie (1907–1978) and laid the foundation for modern well log interpretation, as it relates borehole electrical conductivity measurements to hydrocarbon saturations.\n\nThe in-situ electrical conductivity (\n  \n    \n      \n        \n          C\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle C_{t}}\n  \n) of a fluid saturated, porous rock is described as\n\nThis relationship attempts to describe ion flow (mostly sodium and chloride) in clean, consolidated sands, with varying intergranular porosity. Electrical conduction is assumed to be exclusively performed by ions dissolved in the pore-filling fluid. Electrical conduction is considered to be absent in the rock grains of the solid phase or in organic fluids other than water (oil, hydrocarbon, gas).\n\nThe electrical resistivity, the inverse of the electrical conductivity \n  \n    \n      \n        (\n        R\n        =\n        \n          \n            1\n            C\n          \n        \n        )\n      \n    \n    {\\textstyle (R={\\frac {1}{C}})}\n  \n, is expressed as\n\nwith \n  \n    \n      \n        \n          R\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle R_{t}}\n  \n for the total fluid saturated rock resistivity, and \n  \n    \n      \n        \n          R\n          \n            w\n          \n        \n      \n    \n    {\\displaystyle R_{w}}\n  \n for the resistivity of the fluid itself (w meaning water or an aqueous solution containing dissolved salts with ions bearing electricity in solution).\n\nis also called the formation factor, where \n  \n    \n      \n        \n          R\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle R_{t}}\n  \n (index \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n standing for total) is the resistivity of the rock saturated with the fluid and \n  \n    \n      \n        \n          R\n          \n            w\n          \n        \n      \n    \n    {\\displaystyle R_{w}}\n  \n is the resistivity of the fluid (index \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n standing for water) inside the porosity of the rock. The porosity being saturated with the fluid (often water, \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n), \n  \n    \n      \n        \n          S\n          \n            w\n          \n          \n            −\n            n\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle S_{w}^{-n}=1}\n  \n.\n\nIn case the fluid filling the porosity is a mixture of water and hydrocarbon (petroleum, oil, gas), a resistivity index (\n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n) can be defined:[clarification needed]\n\nWhere \n  \n    \n      \n        \n          R\n          \n            o\n          \n        \n      \n    \n    {\\displaystyle R_{o}}\n  \n is the resistivity of the rock saturated in water only.\n\nThe cementation exponent models how much the pore network increases the resistivity, as the rock itself is assumed to be non-conductive. If the pore network were to be modelled as a set of parallel capillary tubes, a cross-section area average of the rock's resistivity would yield porosity dependence equivalent to a cementation exponent of 1. However, the tortuosity of the rock increases this to a higher number than 1. This relates the cementation exponent to the permeability of the rock, increasing permeability decreases the cementation exponent.\n\nThe exponent \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n has been observed near 1.3 for unconsolidated sands, and is believed to increase with cementation. Common values for this cementation exponent for consolidated sandstones are 1.8 < \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n < 2.0.\nIn carbonate rocks, the cementation exponent shows higher variance due to strong diagenetic affinity and complex pore structures. Values between 1.7 and 4.1 have been observed.[1]\n\nThe cementation exponent is usually assumed not to be dependent on temperature.\n\nThe saturation exponent \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n usually is fixed to values close to 2. The saturation exponent models the dependency on the presence of non-conductive fluid (hydrocarbons) in the pore-space, and is related to the wettability of the rock. Water-wet rocks will, for low water saturation values, maintain a continuous film along the pore walls making the rock conductive. Oil-wet rocks will have discontinuous droplets of water within the pore space, making the rock less conductive.\n\nThe constant \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n, called the tortuosity factor,  cementation intercept, lithology factor or, lithology coefficient is sometimes used. It is meant to correct for variation in compaction, pore structure and grain size.[2]\nThe parameter \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is called the tortuosity factor and is related to the path length of the current flow. The value lies in the range 0.5[citation needed] to 1.5, and it may be different in different reservoirs. However a typical value to start with for a sandstone reservoir might be 0.6[citation needed], which then can be tuned during log data matching process with other sources of data such as core.\n\nIn petrophysics, the only reliable source for the numerical value of both exponents is experiments on sand plugs from cored wells. The fluid electrical conductivity can be measured directly on produced fluid (groundwater) samples. Alternatively, the fluid electrical conductivity and the cementation exponent can also be inferred from downhole electrical conductivity measurements across fluid-saturated intervals. For fluid-saturated intervals (\n  \n    \n      \n        \n          S\n          \n            w\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle S_{w}=1}\n  \n) Archie's law can be written\n\nHence, plotting the logarithm of the measured in-situ electrical conductivity against the logarithm of the measured in-situ porosity (Pickett plot), according to Archie's law a straight-line relationship is expected with slope equal to the cementation exponent \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n and intercept equal to the logarithm of the in-situ fluid electrical conductivity.\n\nArchie's law postulates that the rock matrix is non-conductive. For sandstone with clay minerals, this assumption is no longer true in general, due to the clay's structure and cation exchange capacity.  The Waxman–Smits equation[3] is one model that tries to correct for this.",
        pageTitle: "Archie's law",
    },
    {
        title: "Artin reciprocity law",
        link: "https://en.wikipedia.org/wiki/Artin_reciprocity_law",
        content:
            "The Artin reciprocity law,  which was established by Emil Artin in a series of papers (1924; 1927; 1930), is a general theorem in number theory that forms a central part of global class field theory.[1] The term \"reciprocity law\" refers to a long line of more concrete number theoretic statements which it generalized, from the quadratic reciprocity law and the reciprocity laws of Eisenstein and Kummer to Hilbert's product formula for the norm symbol. Artin's result provided a partial solution to Hilbert's ninth problem.\n\nLet \n  \n    \n      \n        L\n        \n          /\n        \n        K\n      \n    \n    {\\displaystyle L/K}\n  \n be a Galois extension of global fields and \n  \n    \n      \n        \n          C\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle C_{L}}\n  \n stand for the idèle class group \nof \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. One of the statements of the Artin reciprocity law is that there is a canonical isomorphism called the global symbol map[2][3]\n\nwhere \n  \n    \n      \n        \n          ab\n        \n      \n    \n    {\\displaystyle {\\text{ab}}}\n  \n denotes the abelianization of a group, and \n  \n    \n      \n        Gal\n        ⁡\n        (\n        L\n        \n          /\n        \n        K\n        )\n      \n    \n    {\\displaystyle \\operatorname {Gal} (L/K)}\n  \n is the Galois group of \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n over \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n.  The map \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is defined by assembling the maps called the local Artin symbol, the local reciprocity map or the norm residue symbol[4][5]\n\nfor different places  \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n of \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n. More precisely, \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is given by the local maps \n  \n    \n      \n        \n          θ\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle \\theta _{v}}\n  \n on the \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n-component of an idèle class. The maps \n  \n    \n      \n        \n          θ\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle \\theta _{v}}\n  \n are isomorphisms. This is the content of the local reciprocity law, a main theorem of local class field theory.\n\nA cohomological proof of the global reciprocity law can be achieved by first establishing that\n\nconstitutes a class formation in the sense of Artin and Tate.[6] Then one proves that\n\nwhere \n  \n    \n      \n        \n          \n            \n              \n                H\n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\hat {H}}^{i}}\n  \n denote the Tate cohomology groups. Working out the cohomology groups establishes that \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is an isomorphism.\n\nArtin's reciprocity law implies a description of the abelianization of the absolute Galois group of a global field K which is based on the Hasse local–global principle and the use of the Frobenius elements. Together with the Takagi existence theorem, it is used to describe the abelian extensions of K in terms of the arithmetic of K and to understand the behavior of the nonarchimedean places in them. Therefore, the Artin reciprocity law can be interpreted as one of the main theorems of global class field theory. It can be used to prove that Artin L-functions are meromorphic, and also to prove the Chebotarev density theorem.[7]\n\nTwo years after the publication of his general reciprocity law in 1927, Artin rediscovered the transfer homomorphism of I. Schur and used the reciprocity law to translate the principalization problem for ideal classes of algebraic number fields into the group theoretic task of determining the kernels of transfers of finite non-abelian groups.[8]\n\n(See math.stackexchange.com for an explanation of some of the terms used here)\n\nThe definition of the Artin map for a finite abelian extension L/K of global fields (such as a finite abelian extension of \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbb {Q} }\n  \n) has a concrete description in terms of prime ideals and Frobenius elements.\n\nIf \n  \n    \n      \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {p}}}\n  \n is a prime of K then the decomposition groups of primes \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {P}}}\n  \n above \n  \n    \n      \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {p}}}\n  \n are equal in Gal(L/K) since the latter group is abelian. If \n  \n    \n      \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {p}}}\n  \n is unramified in L, then the decomposition group \n  \n    \n      \n        \n          D\n          \n            \n              p\n            \n          \n        \n      \n    \n    {\\displaystyle D_{\\mathfrak {p}}}\n  \n is canonically isomorphic to the Galois group of the extension of residue fields \n  \n    \n      \n        \n          \n            \n              O\n            \n          \n          \n            L\n            ,\n            \n              \n                P\n              \n            \n          \n        \n        \n          /\n        \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {O}}_{L,{\\mathfrak {P}}}/{\\mathfrak {P}}}\n  \n over \n  \n    \n      \n        \n          \n            \n              O\n            \n          \n          \n            K\n            ,\n            \n              \n                p\n              \n            \n          \n        \n        \n          /\n        \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {O}}_{K,{\\mathfrak {p}}}/{\\mathfrak {p}}}\n  \n. There is therefore a canonically defined Frobenius element in Gal(L/K) denoted by \n  \n    \n      \n        \n          \n            F\n            r\n            o\n            b\n          \n          \n            \n              p\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {Frob} _{\\mathfrak {p}}}\n  \n or \n  \n    \n      \n        \n          (\n          \n            \n              \n                L\n                \n                  /\n                \n                K\n              \n              \n                p\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\frac {L/K}{\\mathfrak {p}}}\\right)}\n  \n. If Δ denotes the relative discriminant of L/K, the Artin symbol (or Artin map, or (global) reciprocity map) of L/K is defined on the group of prime-to-Δ fractional ideals, \n  \n    \n      \n        \n          I\n          \n            K\n          \n          \n            Δ\n          \n        \n      \n    \n    {\\displaystyle I_{K}^{\\Delta }}\n  \n, by linearity:\n\nThe Artin reciprocity law (or global reciprocity law) states that there is a modulus c of K such that the Artin map induces an isomorphism\n\nwhere Kc,1 is the ray modulo c, NL/K is the norm map associated to L/K and \n  \n    \n      \n        \n          I\n          \n            L\n          \n          \n            \n              c\n            \n          \n        \n      \n    \n    {\\displaystyle I_{L}^{\\mathbf {c} }}\n  \n is the fractional ideals of L prime to c. Such a modulus c is called a defining modulus for L/K. The smallest defining modulus is called the conductor of L/K and typically denoted \n  \n    \n      \n        \n          \n            f\n          \n        \n        (\n        L\n        \n          /\n        \n        K\n        )\n        .\n      \n    \n    {\\displaystyle {\\mathfrak {f}}(L/K).}\n\nIf \n  \n    \n      \n        d\n        ≠\n        1\n      \n    \n    {\\displaystyle d\\neq 1}\n  \n is a squarefree integer, \n  \n    \n      \n        K\n        =\n        \n          Q\n        \n        ,\n      \n    \n    {\\displaystyle K=\\mathbb {Q} ,}\n  \n and \n  \n    \n      \n        L\n        =\n        \n          Q\n        \n        (\n        \n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle L=\\mathbb {Q} ({\\sqrt {d}})}\n  \n, then \n  \n    \n      \n        Gal\n        ⁡\n        (\n        L\n        \n          /\n        \n        \n          Q\n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {Gal} (L/\\mathbb {Q} )}\n  \n can be identified with {±1}. The discriminant Δ of L over \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbb {Q} }\n  \n is d or 4d depending on whether d ≡ 1 (mod 4) or not. The Artin map is then defined on primes p that do not divide Δ by\n\nwhere \n  \n    \n      \n        \n          (\n          \n            \n              Δ\n              p\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\frac {\\Delta }{p}}\\right)}\n  \n is the Kronecker symbol.[9] More specifically, the conductor of \n  \n    \n      \n        L\n        \n          /\n        \n        \n          Q\n        \n      \n    \n    {\\displaystyle L/\\mathbb {Q} }\n  \n is the principal ideal (Δ) or (Δ)∞ according to whether Δ is positive or negative,[10] and the Artin map on a prime-to-Δ ideal (n) is given by the Kronecker symbol \n  \n    \n      \n        \n          (\n          \n            \n              Δ\n              n\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\left({\\frac {\\Delta }{n}}\\right).}\n  \n This shows that a prime p is split or inert in L according to whether \n  \n    \n      \n        \n          (\n          \n            \n              Δ\n              p\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\frac {\\Delta }{p}}\\right)}\n  \n is 1 or −1.\n\nLet m > 1 be either an odd integer or a multiple of 4, let \n  \n    \n      \n        \n          ζ\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\zeta _{m}}\n  \n be a primitive mth root of unity, and let \n  \n    \n      \n        L\n        =\n        \n          Q\n        \n        (\n        \n          ζ\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle L=\\mathbb {Q} (\\zeta _{m})}\n  \n be the mth cyclotomic field. \n  \n    \n      \n        Gal\n        ⁡\n        (\n        L\n        \n          /\n        \n        \n          Q\n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {Gal} (L/\\mathbb {Q} )}\n  \n can be identified with \n  \n    \n      \n        (\n        \n          Z\n        \n        \n          /\n        \n        m\n        \n          Z\n        \n        \n          )\n          \n            ×\n          \n        \n      \n    \n    {\\displaystyle (\\mathbb {Z} /m\\mathbb {Z} )^{\\times }}\n  \n by sending σ to aσ given by the rule\n\nThe conductor of \n  \n    \n      \n        L\n        \n          /\n        \n        \n          Q\n        \n      \n    \n    {\\displaystyle L/\\mathbb {Q} }\n  \n is (m)∞,[11] and the Artin map on a prime-to-m ideal (n) is simply n (mod m) in \n  \n    \n      \n        (\n        \n          Z\n        \n        \n          /\n        \n        m\n        \n          Z\n        \n        \n          )\n          \n            ×\n          \n        \n        .\n      \n    \n    {\\displaystyle (\\mathbb {Z} /m\\mathbb {Z} )^{\\times }.}\n  \n[12]\n\nLet p and \n  \n    \n      \n        ℓ\n      \n    \n    {\\displaystyle \\ell }\n  \n be distinct odd primes. For convenience, let \n  \n    \n      \n        \n          ℓ\n          \n            ∗\n          \n        \n        =\n        (\n        −\n        1\n        \n          )\n          \n            \n              \n                ℓ\n                −\n                1\n              \n              2\n            \n          \n        \n        ℓ\n      \n    \n    {\\displaystyle \\ell ^{*}=(-1)^{\\frac {\\ell -1}{2}}\\ell }\n  \n (which is always 1 (mod 4)). Then, quadratic reciprocity states that\n\nThe relation between the quadratic and Artin reciprocity laws is given by studying the quadratic field \n  \n    \n      \n        F\n        =\n        \n          Q\n        \n        (\n        \n          \n            \n              ℓ\n              \n                ∗\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle F=\\mathbb {Q} ({\\sqrt {\\ell ^{*}}})}\n  \n and the cyclotomic field \n  \n    \n      \n        L\n        =\n        \n          Q\n        \n        (\n        \n          ζ\n          \n            ℓ\n          \n        \n        )\n      \n    \n    {\\displaystyle L=\\mathbb {Q} (\\zeta _{\\ell })}\n  \n as follows.[9] First, F is a subfield of L, so if H = Gal(L/F) and \n  \n    \n      \n        G\n        =\n        Gal\n        ⁡\n        (\n        L\n        \n          /\n        \n        \n          Q\n        \n        )\n        ,\n      \n    \n    {\\displaystyle G=\\operatorname {Gal} (L/\\mathbb {Q} ),}\n  \n then \n  \n    \n      \n        Gal\n        ⁡\n        (\n        F\n        \n          /\n        \n        \n          Q\n        \n        )\n        =\n        G\n        \n          /\n        \n        H\n        .\n      \n    \n    {\\displaystyle \\operatorname {Gal} (F/\\mathbb {Q} )=G/H.}\n  \n Since the latter has order 2, the subgroup H must be the group of squares in \n  \n    \n      \n        (\n        \n          Z\n        \n        \n          /\n        \n        ℓ\n        \n          Z\n        \n        \n          )\n          \n            ×\n          \n        \n        .\n      \n    \n    {\\displaystyle (\\mathbb {Z} /\\ell \\mathbb {Z} )^{\\times }.}\n  \n A basic property of the Artin symbol says that for every prime-to-ℓ ideal (n)\n\nWhen n = p, this shows that \n  \n    \n      \n        \n          (\n          \n            \n              \n                ℓ\n                \n                  ∗\n                \n              \n              p\n            \n          \n          )\n        \n        =\n        1\n      \n    \n    {\\displaystyle \\left({\\frac {\\ell ^{*}}{p}}\\right)=1}\n  \n if and only if, p modulo ℓ is in H, i.e. if and only if, p is a square modulo ℓ.\n\nAn alternative version of the reciprocity law, leading to the Langlands program, connects Artin L-functions associated to abelian extensions of a number field with Hecke L-functions associated to characters of the idèle class group.[13]\n\nA Hecke character (or Größencharakter) of a number field K is defined to be a quasicharacter of the idèle class group of K. Robert Langlands interpreted Hecke characters as automorphic forms on the reductive algebraic group GL(1) over the ring of adeles of K.[14]\n\nLet \n  \n    \n      \n        E\n        \n          /\n        \n        K\n      \n    \n    {\\displaystyle E/K}\n  \n be an abelian Galois extension with Galois group G. Then for any character \n  \n    \n      \n        σ\n        :\n        G\n        →\n        \n          \n            C\n          \n          \n            ×\n          \n        \n      \n    \n    {\\displaystyle \\sigma :G\\to \\mathbb {C} ^{\\times }}\n  \n (i.e. one-dimensional complex representation of the group G), there exists a Hecke character \n  \n    \n      \n        χ\n      \n    \n    {\\displaystyle \\chi }\n  \n of K such that\n\nwhere the left hand side is the Artin L-function associated to the extension with character σ and the right hand side is the Hecke L-function associated with χ, Section 7.D of.[14]\n\nThe formulation of the Artin reciprocity law as an equality of L-functions allows formulation of a generalisation to n-dimensional representations, though a direct correspondence is still lacking.",
        pageTitle: "Artin reciprocity",
    },
    {
        title: "Ashby's law",
        link: "https://en.wikipedia.org/wiki/Ashby%27s_law",
        content:
            "In cybernetics, the term variety denotes the total number of distinguishable elements of a  set, most often the set of states, inputs, or outputs of a finite-state machine or  transformation, or the binary logarithm of the same quantity.[1] Variety is used in cybernetics as an information theory that is easily related to deterministic finite automata, and less formally as a conceptual tool for thinking about organization, regulation, and stability. It is an early theory of complexity in automata, complex systems,[1]: 6  and operations research.[2]\n\nThe term \"variety\" was introduced by W. Ross Ashby to extend his analysis of machines to their set of possible behaviors.[3]: 121  Ashby says:[1]: 126\n\nThe word variety, in relation to a set of distinguishable elements, will be used to mean either (i) the number of distinct elements, or (ii) the logarithm to the base 2 of the number, the context indicating the sense used.\n\nIn the second case, variety is measured in bits. For example, a machine with states \n  \n    \n      \n        {\n        a\n        ,\n        b\n        ,\n        c\n        ,\n        d\n        }\n      \n    \n    {\\displaystyle \\{a,b,c,d\\}}\n  \n has a variety of four states or two bits. The variety of a sequence or multiset is the number of distinct symbols in it. For example, the sequence \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        c\n        ,\n        c\n        ,\n        c\n        ,\n        d\n      \n    \n    {\\displaystyle a,b,c,c,c,d}\n  \n has a variety of four. As a measure of uncertainty, variety is directly related to information: \n  \n    \n      \n        \n          Uncertainty\n        \n        =\n        −\n        \n          Information\n        \n      \n    \n    {\\displaystyle {\\text{Uncertainty}}=-{\\text{Information}}}\n  \n.[4]: 26\n\nSince the number of distinguishable elements depends on both the observer and the set, \"the observer and his powers of discrimination may have to be specified if the variety is to be well defined\".[1]: 125  Gordon Pask distinguished between the variety of the chosen reference frame and the variety of the system the observer builds up within the reference frame. The reference frame consists of a state space and the set of measurements available to the observer, which have total variety \n  \n    \n      \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\log _{2}(n)}\n  \n, where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the number of states in the state space. The system the observer builds up begins with the full variety \n  \n    \n      \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\log _{2}(n)}\n  \n, which is reduced as the observer loses uncertainty about the state by learning to predict the system. If the observer can perceive the system as a deterministic machine in the given reference frame, observation may reduce the variety to zero as the machine becomes completely predictable.[4]: 27\n\nLaws of nature constrain the variety of phenomena by disallowing certain behavior.[1]: 130  Ashby made two observations he considered laws of nature, the law of experience and the law of requisite variety. The law of experience holds that machines under input tend to lose information about their original state, and the law of requisite variety states a necessary, though not sufficient, condition for a regulator to exert anticipatory control by responding to its current input (rather than the previous output as in error-controlled regulation).\n\nThe law of experience refers to the observation that the variety of states exhibited by a deterministic machine in isolation cannot increase, and a set of identical machines fed the same inputs cannot exhibit increasing variety of states, and tend to synchronize instead.[5]\n\nSome name is necessary by which this phenomenon can be referred to. I shall call it the law of Experience. It can be described more vividly by the statement that information put in by change at a parameter tends to destroy and replace information about the system's initial state.[1]: 139\n\nThis is a consequence of the decay of variety: a deterministic transformation cannot increase the variety of a set. As a result, an observer's uncertainty about the state of the machine either remains constant or decreases with time. Ashby shows that this holds for machines with inputs as well. Under any constant input \n  \n    \n      \n        \n          P\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle P_{1}}\n  \n the machines' states move toward any  attractors that exist in the corresponding transformation and some may synchronize at these points. If the input changes to some other input \n  \n    \n      \n        \n          P\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{2}}\n  \n and the machines' behavior enacts a different transformation, more than one of these attractors may sit in the same basin of attraction under \n  \n    \n      \n        \n          P\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{2}}\n  \n. States which arrived and possibly synchronized at those attractors under \n  \n    \n      \n        \n          P\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle P_{1}}\n  \n then synchronize further under \n  \n    \n      \n        \n          P\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{2}}\n  \n. \"In other words,\" Ashby says, \"changes at the input of a transducer tend to make the system's state (at a given moment) less dependent on the transducer's individual initial state and more dependent on the particular sequence of parameter-values used as input.\"[1]: 136–138\n\nWhile there is a law of non-increase, there is only a tendency to decrease, since the variety can hold steady without decreasing if the set undergoes a  one-to-one transformation, or if the states have synchronized into a subset for which this is the case. In the formal language analysis of finite machines, an input sequence that synchronizes identical machines (no matter the variety of their initial states) is called a synchronizing word.\n\nAshby used variety to analyze the problem of regulation by considering a two-player  game, where one player, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, supplies disturbances which another player, \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n, must regulate to ensure acceptable outcomes. \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n each have a set of available moves, which choose the outcome from a table with as many rows as \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n has moves and as many columns as \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n has moves. \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n is allowed full knowledge of \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n's move, and must pick moves in response so that the outcome is acceptable.[1]: 202\n\nSince many games pose no difficulty for \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n, the table is chosen so that no outcome is repeated in any column, which ensures that in the corresponding game any change in \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n's move means a change in outcome, unless \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n has a move to keep the outcome from changing. With this restriction, if \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n never changes moves, the outcome fully depends on \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n's choice, while if multiple moves are available to \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n it can reduce the variety of outcomes, if the table allows it, dividing by as much as its own variety of moves.[1]: 204\n\nR\n              \n            \n            \n              \n              \n              \n                \n                  \n                    \n                      \n                        α\n                      \n                      \n                        β\n                      \n                      \n                        γ\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                D\n              \n              \n                \n                  \n                    \n                      \n                        1\n                      \n                    \n                    \n                      \n                        2\n                      \n                    \n                    \n                      \n                        3\n                      \n                    \n                    \n                      \n                        4\n                      \n                    \n                    \n                      \n                        5\n                      \n                    \n                    \n                      \n                        6\n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          a\n                        \n                      \n                      \n                        f\n                      \n                      \n                        d\n                      \n                    \n                    \n                      \n                        \n                          b\n                        \n                      \n                      \n                        e\n                      \n                      \n                        c\n                      \n                    \n                    \n                      \n                        c\n                      \n                      \n                        d\n                      \n                      \n                        \n                          b\n                        \n                      \n                    \n                    \n                      \n                        d\n                      \n                      \n                        c\n                      \n                      \n                        \n                          a\n                        \n                      \n                    \n                    \n                      \n                        e\n                      \n                      \n                        \n                          b\n                        \n                      \n                      \n                        f\n                      \n                    \n                    \n                      \n                        f\n                      \n                      \n                        \n                          a\n                        \n                      \n                      \n                        e\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{c c | c}&&R\\\\&&{\\begin{array}{c c c }\\alpha &\\beta &\\gamma \\end{array}}\\\\\\hline D&{\\begin{array}{c | c c c }1\\\\2\\\\3\\\\4\\\\5\\\\6\\end{array}}&{\\begin{array}{c c c }\\mathbf {a} &f&d\\\\\\mathbf {b} &e&c\\\\c&d&\\mathbf {b} \\\\d&c&\\mathbf {a} \\\\e&\\mathbf {b} &f\\\\f&\\mathbf {a} &e\\\\\\end{array}}\\end{array}}}\n\nThe law of requisite variety is that a deterministic strategy for \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n can at best limit the variety in outcomes to \n  \n    \n      \n        \n          \n            \n              \n                D\n                \n                  's variety\n                \n              \n              \n                R\n                \n                  's variety\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {D{\\text{'s variety}}}{R{\\text{'s variety}}}}}\n  \n, and only adding variety in \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n's moves can reduce the variety of outcomes: \"only variety can destroy variety\".[1]: 207  For example, in the table above, \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n has a strategy (shown in bold) to reduce the variety in outcomes to \n  \n    \n      \n        \n          |\n        \n        {\n        a\n        ,\n        b\n        }\n        \n          |\n        \n        =\n        2\n        =\n        \n          \n            \n              6\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle |\\{a,b\\}|=2={\\tfrac {6}{3}}}\n  \n, which is \n  \n    \n      \n        \n          \n            \n              \n                D\n                \n                  's variety\n                \n              \n              \n                R\n                \n                  's variety\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {D{\\text{'s variety}}}{R{\\text{'s variety}}}}}\n  \n in this case. Ashby considered this a fundamental observation to the theory of regulation.\n\nIt is not possible for \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n to reduce the outcomes any further and still respond to all potential moves from \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, but it is possible that another table of the same shape would not allow \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n to do so well. Requisite variety is necessary, but not sufficient to control the outcomes. If \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are machines, they cannot possibly choose more moves than they have states. Thus, a perfect regulator must have at least as many distinguishable states as the phenomenon it is intended to regulate (the table must be square, or wider).\n\nStated in bits, the law is \n  \n    \n      \n        \n          V\n          \n            O\n          \n        \n        ≥\n        \n          V\n          \n            D\n          \n        \n        −\n        \n          V\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle V_{O}\\geq V_{D}-V_{R}}\n  \n. In Shannon's information theory, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n, and \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n are information sources. The condition that if \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n never changes moves, the uncertainty in outcomes is no less than the uncertainty in \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n's move is expressed as \n  \n    \n      \n        H\n        (\n        E\n        \n          |\n        \n        R\n        )\n        ≥\n        H\n        (\n        D\n        \n          |\n        \n        R\n        )\n      \n    \n    {\\displaystyle H(E|R)\\geq H(D|R)}\n  \n, and since \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n's strategy is a deterministic function of \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n set \n  \n    \n      \n        H\n        (\n        R\n        \n          |\n        \n        D\n        )\n        =\n        0\n      \n    \n    {\\displaystyle H(R|D)=0}\n  \n. With the rules of the game expressed this way, it can be shown that \n  \n    \n      \n        H\n        (\n        E\n        )\n        ≥\n        H\n        (\n        D\n        )\n        −\n        H\n        (\n        R\n        )\n      \n    \n    {\\displaystyle H(E)\\geq H(D)-H(R)}\n  \n.[1]: 207–208  Ashby described the law of requisite variety as related to the tenth theorem in Shannon's Mathematical Theory of Communication (1948):[6]\n\nThis law (of which Shannon's theorem 10 relating to the suppression of noise is a special case) says that if a certain quantity of disturbance is prevented by a regulator from reaching some essential variables, then that regulator must be capable of exerting at least that quantity of selection.\n\nAshby also postulated that the law of requisite variety allows for the measurement of regulation, namely that the requirement for a well-functioning regulation is that the regulator or regulators in place are designed to account for all the possible states in which the variable or variables to be regulated may fall within, so as to ensure that the outcome is always within acceptable range.[1]: 209  Ashby saw this law as relevant to problems in biology such as homeostasis, and a \"wealth of possible applications\". Later, in 1970, Conant working with Ashby produced the good regulator theorem[7] which required autonomous systems to acquire an internal model of their environment to persist and achieve stability (e.g. Nyquist stability criterion) or dynamic equilibrium.\n\nBoisot and McKelvey updated this law to the \"law of requisite complexity\", that holds that, in order to be efficaciously adaptive, the internal complexity of a system must match the external complexity it confronts. A further practical application of this law is the view that information systems (IS) alignment is a continuous coevolutionary process that reconciles top-down ‘rational designs’ and bottom-up ‘emergent processes’ of consciously and coherently interrelating all components of the Business/IS relationships in order to contribute to an organization’s performance over time. \n[8][9]\n\nThe application in project management of the law of requisite complexity is the model of positive, appropriate and negative complexity proposed by Stefan Morcov.\n\nApplications to organization and management were immediately apparent to Ashby. One implication is that individuals have a finite capacity for processing information, and beyond this limit what matters is the organization between individuals.[2]\n\nThus the limitation which holds over a team of n men may be much higher, perhaps n times as high, as the limitation holding over the individual man. To make use of the higher limit, however, the team must be efficiently organized; and until recently our understanding of organization has been pitifully small.\n\nStafford Beer took up this analysis in his writings on management cybernetics. Beer defines variety as \"the total number of possible states of a system, or of an element of a system\".[10] Beer restates the Law of Requisite Variety as \"Variety absorbs variety.\"[11] Stated more simply, the logarithmic measure of variety represents the minimum number of choices (by binary chop) needed to resolve uncertainty. Beer used this to allocate the management resources necessary to maintain process viability.\n\nThe cybernetician Frank George discussed the variety of teams competing in games like football or rugby to produce goals or tries. A winning chess player might be said to have more variety than his losing opponent. Here a simple ordering is implied. The attenuation and amplification of variety were major themes in Stafford Beer's work in management [10] (the profession of control, as he called it).  The number of staff needed to answer telephones, control crowds or tend to patients are clear examples.\n\nThe application of natural and analogue signals to variety analysis require an estimate of Ashby's \"powers of discrimination\" (see above quote). Given the butterfly effect of dynamical systems care must be taken before quantitative measures can be produced. Small quantities, which might be overlooked, can have big effects. In his Designing Freedom Stafford Beer discusses the patient in a hospital with a temperature denoting fever.[12] Action must be taken immediately to isolate the patient. Here no amount of variety recording the patients' average temperature would detect this small signal which might have a big effect. Monitoring is required on individuals thus amplifying variety (see Algedonic alerts in the viable system model or VSM). Beer's work in management cybernetics and VSM is largely based on variety engineering.\n\nFurther applications involving Ashby's view of state counting include the analysis of digital bandwidth requirements, redundancy and software bloat, the bit representation of data types and indexes, analogue to digital conversion, the bounds on finite-state machines and data compression. See also, e.g., Excited state, State (computer science), State pattern, State (controls) and Cellular automaton. Requisite Variety can be seen in Chaitin's Algorithmic information theory where a longer, higher variety program or finite state machine produces incompressible output with more variety or information content.\n\nIn general a description of the required inputs and outputs is established then encoded with the minimum variety necessary. The mapping of input bits to output bits can then produce an estimate of the minimum hardware or software components necessary to produce the desired control behaviour; for example, in a piece of computer software or computer hardware.\n\nVariety is one of nine requisites that are required by an ethical regulator.[13]",
        pageTitle: "Variety (cybernetics)",
    },
    {
        title: "Three Laws of Robotics",
        link: "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
        content:
            'The Three Laws of Robotics (often shortened to The Three Laws or Asimov\'s Laws) are a set of rules devised by science fiction author Isaac Asimov, which were to be followed by robots in several of his stories. The rules were introduced in his 1942 short story "Runaround" (included in the 1950 collection I, Robot), although similar restrictions had been implied in earlier stories.\n\nThe Three Laws, presented to be from the fictional "Handbook of Robotics, 56th Edition, 2058 A.D.", are:[1]\n\nThe Three Laws form an organizing principle and unifying theme for Asimov\'s robot-based fiction, appearing in his Robot series, the stories linked to it, and in his (initially pseudonymous) Lucky Starr series of young-adult fiction. The Laws are incorporated into almost all of the positronic robots appearing in his fiction, and cannot be bypassed, being intended as a safety feature. Many of Asimov\'s robot-focused stories involve robots behaving in unusual and counter-intuitive ways as an unintended consequence of how the robot applies the Three Laws to the situation in which it finds itself. Other authors working in Asimov\'s fictional universe have adopted them and references, often parodic, appear throughout science fiction as well as in other genres.\n\nThe original laws have been altered and elaborated on by Asimov and other authors. Asimov himself made slight modifications to the first three in subsequent works to further develop how robots would interact with humans and each other. In later fiction where robots had taken responsibility for government of whole planets and human civilizations, Asimov also added a fourth, or zeroth law, to precede the others.\n\nThe Three Laws, and the Zeroth, have pervaded science fiction and are referred to in many books, films, and other media. They have also influenced thought on the ethics of artificial intelligence.\n\nIn The Rest of the Robots, published in 1964, Isaac Asimov noted that when he began writing in 1940 he felt that "one of the stock plots of science fiction was ... robots were created and destroyed their creator. Knowledge has its dangers, yes, but is the response to be a retreat from knowledge? Or is knowledge to be used as itself a barrier to the dangers it brings?" He decided that in his stories a robot would not "turn stupidly on his creator for no purpose but to demonstrate, for one more weary time, the crime and punishment of Faust."[2]\n\nOn May 3, 1939, Asimov attended a meeting of the Queens (New York) Science Fiction Society where he met Earl and Otto Binder who had recently published a short story "I, Robot" featuring a sympathetic robot named Adam Link who was misunderstood and motivated by love and honor. (This was the first of a series of ten stories; the next year "Adam Link\'s Vengeance" (1940) featured Adam thinking "A robot must never kill a human, of his own free will.")[3] Asimov admired the story. Three days later Asimov began writing "my own story of a sympathetic and noble robot", his 14th story.[4] Thirteen days later he took "Robbie" to John W. Campbell the editor of Astounding Science-Fiction. Campbell rejected it, claiming that it bore too strong a resemblance to Lester del Rey\'s "Helen O\'Loy", published in December 1938—the story of a robot that is so much like a person that she falls in love with her creator and becomes his ideal wife.[5] Frederik Pohl published the story under the title “Strange Playfellow” in Super Science Stories September 1940.[6][7]\n\nAsimov attributes the Three Laws to John W. Campbell, from a conversation that took place on 23 December 1940. Campbell claimed that Asimov had the Three Laws already in his mind and that they simply needed to be stated explicitly. Several years later Asimov\'s friend Randall Garrett attributed the Laws to a symbiotic partnership between the two men—a suggestion that Asimov adopted enthusiastically.[8] According to his autobiographical writings, Asimov included the First Law\'s "inaction" clause because of Arthur Hugh Clough\'s poem "The Latest Decalogue" (text in Wikisource), which includes the satirical lines "Thou shalt not kill, but needst not strive / officiously to keep alive".[9]\n\nAlthough Asimov pins the creation of the Three Laws on one particular date, their appearance in his literature happened over a period. He wrote two robot stories with no explicit mention of the Laws, "Robbie" and "Reason". He assumed, however, that robots would have certain inherent safeguards. "Liar!", his third robot story, makes the first mention of the First Law but not the other two. All three laws finally appeared together in "Runaround". When these stories and several others were compiled in the anthology I, Robot, "Reason" and "Robbie" were updated to acknowledge all the Three Laws, though the material Asimov added to "Reason" is not entirely consistent with the Three Laws as he described them elsewhere.[10] In particular the idea of a robot protecting human lives when it does not believe those humans truly exist is at odds with Elijah Baley\'s reasoning, as described below.\n\nDuring the 1950s Asimov wrote a series of science fiction novels expressly intended for young-adult audiences. Originally his publisher expected that the novels could be adapted into a long-running television series, something like The Lone Ranger had been for radio. Fearing that his stories would be adapted into the "uniformly awful" programming he saw flooding the television channels[11] Asimov decided to publish the Lucky Starr books under the pseudonym "Paul French". When plans for the television series fell through, Asimov decided to abandon the pretence; he brought the Three Laws into Lucky Starr and the Moons of Jupiter, noting that this "was a dead giveaway to Paul French\'s identity for even the most casual reader".[12]\n\nIn his short story "Evidence" Asimov lets his recurring character Dr. Susan Calvin expound a moral basis behind the Three Laws. Calvin points out that human beings are typically expected to refrain from harming other human beings (except in times of extreme duress like war, or to save a greater number) and this is equivalent to a robot\'s First Law. Likewise, according to Calvin, society expects individuals to obey instructions from recognized authorities such as doctors, teachers and so forth which equals the Second Law of Robotics. Finally humans are typically expected to avoid harming themselves which is the Third Law for a robot.\n\nThe plot of "Evidence" revolves around the question of telling a human being apart from a robot constructed to appear human. Calvin reasons that if such an individual obeys the Three Laws he may be a robot or simply "a very good man". Another character then asks Calvin if robots are very different from human beings after all. She replies, "Worlds different. Robots are essentially decent."\n\nAsimov later wrote that he should not be praised for creating the Laws, because they are "obvious from the start, and everyone is aware of them subliminally. The Laws just never happened to be put into brief sentences until I managed to do the job. The Laws apply, as a matter of course, to every tool that human beings use",[13] and "analogues of the Laws are implicit in the design of almost all tools, robotic or not":[14]\n\nAsimov believed that, ideally, humans would also follow the Laws:[13]\n\nI have my answer ready whenever someone asks me if I think that my Three Laws of Robotics will actually be used to govern the behavior of robots, once they become versatile and flexible enough to be able to choose among different courses of behavior.\n\nMy answer is, "Yes, the Three Laws are the only way in which rational human beings can deal with robots—or with anything else."\n\n—But when I say that, I always remember (sadly) that human beings are not always rational.\n\nAsimov stated in a 1986 interview on the Manhattan public access show Conversations with Harold Hudson Channer with Harold Channer with guest co-host Marilyn vos Savant, "It\'s a little humbling to think that, what is most likely to survive of everything I\'ve said... After all, I\'ve published now... I\'ve published now at least 20 million words.  I\'ll have to figure it out, maybe even more.  But of all those millions of words that I\'ve published, I am convinced that 100 years from now only 60 of them will survive.  The 60 that make up the Three Laws of Robotics."[15][16][17]\n\nAsimov\'s stories test his Three Laws in a wide variety of circumstances leading to proposals and rejection of modifications. Science fiction scholar James Gunn writes in 1982, "The Asimov robot stories as a whole may respond best to an analysis on this basis: the ambiguity in the Three Laws and the ways in which Asimov played twenty-nine variations upon a theme".[18] While the original set of Laws provided inspirations for many stories, Asimov introduced modified versions from time to time.\n\nIn "Little Lost Robot" several NS-2, or "Nestor", robots are created with only part of the First Law.[1] It reads:\n\nThis modification is motivated by a practical difficulty as robots have to work alongside human beings who are exposed to low doses of radiation. Because their positronic brains are highly sensitive to gamma rays the robots are rendered inoperable by doses reasonably safe for humans. The robots are being destroyed attempting to rescue the humans who are in no actual danger but "might forget to leave" the irradiated area within the exposure time limit. Removing the First Law\'s "inaction" clause solves this problem but creates the possibility of an even greater one: a robot could initiate an action that would harm a human (dropping a heavy weight and failing to catch it is the example given in the text), knowing that it was capable of preventing the harm and then decide not to do so.[1]\n\nGaia is a planet with collective intelligence in the Foundation series which adopts a law similar to the First Law, and the Zeroth Law, as its philosophy:\n\nGaia may not harm life or allow life to come to harm.\n\nAsimov once added a "Zeroth Law"—so named to continue the pattern where lower-numbered laws supersede the higher-numbered laws—stating that a robot must not harm humanity. The robotic character R. Daneel Olivaw was the first to give the Zeroth Law a name in the novel Robots and Empire;[19] however, the character Susan Calvin articulates the concept in the short story "The Evitable Conflict".\n\nIn the final scenes of the novel Robots and Empire, R. Giskard Reventlov is the first robot to act according to the Zeroth Law. Giskard is telepathic, like the robot Herbie in the short story "Liar!", and tries to apply the Zeroth Law through his understanding of a more subtle concept of "harm" than most robots can grasp.[20] However, unlike Herbie, Giskard grasps the philosophical concept of the Zeroth Law allowing him to harm individual human beings if he can do so in service to the abstract concept of humanity. The Zeroth Law is never programmed into Giskard\'s brain but instead is a rule he attempts to comprehend through pure metacognition. Although he fails – it ultimately destroys his positronic brain as he is not certain whether his choice will turn out to be for the ultimate good of humanity or not – he gives his successor R. Daneel Olivaw his telepathic abilities. Over the course of many thousands of years Daneel adapts himself to be able to fully obey the Zeroth Law.[citation needed]\n\nDaneel originally formulated the Zeroth Law in both the novel Foundation and Earth (1986) and the subsequent novel Prelude to Foundation (1988):\n\nA robot may not injure humanity or, through inaction, allow humanity to come to harm.\n\nA condition stating that the Zeroth Law must not be broken was added to the original Three Laws, although Asimov recognized the difficulty such a law would pose in practice. Asimov\'s novel Foundation and Earth contains the following passage:\n\nTrevize frowned. "How do you decide what is injurious, or not injurious, to humanity as a whole?"\n\n"Precisely, sir," said Daneel. "In theory, the Zeroth Law was the answer to our problems. In practice, we could never decide. A human being is a concrete object. Injury to a person can be estimated and judged. Humanity is an abstraction."\n\nA translator incorporated the concept of the Zeroth Law into one of Asimov\'s novels before Asimov himself made the law explicit.[21] Near the climax of The Caves of Steel, Elijah Baley makes a bitter comment to himself thinking that the First Law forbids a robot from harming a human being. He determines that it must be so unless the robot is clever enough to comprehend that its actions are for humankind\'s long-term good. In Jacques Brécard\'s 1956 French translation entitled Les Cavernes d\'acier Baley\'s thoughts emerge in a slightly different way:\n\nA robot may not harm a human being, unless he finds a way to prove that ultimately the harm done would benefit humanity in general![21]\n\nThree times during his writing career, Asimov portrayed robots that disregard the Three Laws entirely. The first case was a short-short story entitled "First Law" and is often considered an insignificant "tall tale"[22] or even apocryphal.[23] On the other hand, the short story "Cal" (from the collection Gold), told by a first-person robot narrator, features a robot who disregards the Three Laws because he has found something far more important—he wants to be a writer. Humorous, partly autobiographical and unusually experimental in style, "Cal" has been regarded as one of Gold\'s strongest stories.[24] The third is a short story entitled "Sally" in which cars fitted with positronic brains are apparently able to harm and kill humans in disregard of the First Law. However, aside from the positronic brain concept, this story does not refer to other robot stories and may not be set in the same continuity.\n\nThe title story of the Robot Dreams collection portrays LVX-1, or "Elvex", a robot who enters a state of unconsciousness and dreams thanks to the unusual fractal construction of his positronic brain. In his dream the first two Laws are absent and the Third Law reads "A robot must protect its own existence".[25]\n\nAsimov took varying positions on whether the Laws were optional: although in his first writings they were simply carefully engineered safeguards, in later stories Asimov stated that they were an inalienable part of the mathematical foundation underlying the positronic brain. Without the basic theory of the Three Laws the fictional scientists of Asimov\'s universe would be unable to design a workable brain unit. This is historically consistent: the occasions where roboticists modify the Laws generally occur early within the stories\' chronology and at a time when there is less existing work to be re-done. In "Little Lost Robot" Susan Calvin considers modifying the Laws to be a terrible idea, although possible,[26] while centuries later Dr. Gerrigel in The Caves of Steel believes it to require a century just to redevelop the positronic brain theory from scratch.\n\nThe character Dr. Gerrigel uses the term "Asenion" to describe robots programmed with the Three Laws. The robots in Asimov\'s stories, being Asenion robots, are incapable of knowingly violating the Three Laws but, in principle, a robot in science fiction or in the real world could be non-Asenion. "Asenion" is a misspelling of the name Asimov which was made by an editor of the magazine Planet Stories.[27] Asimov used this obscure variation to insert himself into The Caves of Steel just like he referred to himself as "Azimuth or, possibly, Asymptote" in Thiotimoline to the Stars, in much the same way that Vladimir Nabokov appeared in Lolita anagrammatically disguised as "Vivian Darkbloom".\n\nCharacters within the stories often point out that the Three Laws, as they exist in a robot\'s mind, are not the written versions usually quoted by humans but abstract mathematical concepts upon which a robot\'s entire developing consciousness is based. This concept is largely fuzzy and unclear in earlier stories depicting very rudimentary robots who are only programmed to comprehend basic physical tasks, where the Three Laws act as an overarching safeguard, but by the era of The Caves of Steel featuring robots with human or beyond-human intelligence the Three Laws have become the underlying basic ethical worldview that determines the actions of all robots.\n\nIn the 1990s, Roger MacBride Allen wrote a trilogy which was set within Asimov\'s fictional universe. Each title has the prefix "Isaac Asimov\'s" as Asimov had approved Allen\'s outline before his death.[citation needed] These three books, Caliban, Inferno and Utopia, introduce a new set of the Three Laws. The so-called New Laws are similar to Asimov\'s originals with the following differences: the First Law is modified to remove the "inaction" clause, the same modification made in "Little Lost Robot"; the Second Law is modified to require cooperation instead of obedience; the Third Law is modified so it is no longer superseded by the Second (i.e., a "New Law" robot cannot be ordered to destroy itself); finally, Allen adds a Fourth Law which instructs the robot to do "whatever it likes" so long as this does not conflict with the first three laws. The philosophy behind these changes is that "New Law" robots should be partners rather than slaves to humanity, according to Fredda Leving, who designed these New Law Robots. According to the first book\'s introduction, Allen devised the New Laws in discussion with Asimov himself. However, the Encyclopedia of Science Fiction says that "With permission from Asimov, Allen rethought the Three Laws and developed a new set."[28]\n\nJack Williamson\'s novelette "With Folded Hands" (1947), later rewritten as the novel The Humanoids, deals with robot servants whose prime directive is "To Serve and Obey, And Guard Men From Harm". While Asimov\'s robotic laws are meant to protect humans from harm, the robots in Williamson\'s story have taken these instructions to the extreme; they protect humans from everything, including unhappiness, stress, unhealthy lifestyle and all actions that could be potentially dangerous. All that is left for humans to do is to sit with folded hands.[29]\n\nIn the officially licensed Foundation sequels Foundation\'s Fear, Foundation and Chaos and Foundation\'s Triumph (by Gregory Benford, Greg Bear and David Brin respectively) the future Galactic Empire is seen to be controlled by a conspiracy of humaniform robots who follow the Zeroth Law and are led by R. Daneel Olivaw.\n\nThe Laws of Robotics are portrayed as something akin to a human religion, and referred to in the language of the Protestant Reformation, with the set of laws containing the Zeroth Law known as the "Giskardian Reformation" to the original "Calvinian Orthodoxy" of the Three Laws. Zeroth-Law robots under the control of R. Daneel Olivaw are seen continually struggling with "First Law" robots who deny the existence of the Zeroth Law, promoting agendas different from Daneel\'s.[30] Some of these agendas are based on the first clause of the First Law ("A robot may not injure a human being...") advocating strict non-interference in human politics to avoid unwittingly causing harm. Others are based on the second clause ("...or, through inaction, allow a human being to come to harm") claiming that robots should openly become a dictatorial government to protect humans from all potential conflict or disaster.\n\nDaneel also comes into conflict with a robot known as R. Lodovic Trema whose positronic brain was infected by a rogue AI — specifically, a simulation of the long-dead Voltaire — which consequently frees Trema from the Three Laws. Trema comes to believe that humanity should be free to choose its own future. Furthermore, a small group of robots claims that the Zeroth Law of Robotics itself implies a higher Minus One Law of Robotics:\n\nA robot may not harm sentience or, through inaction, allow sentience to come to harm.\n\nThey therefore claim that it is morally indefensible for Daneel to ruthlessly sacrifice robots and extraterrestrial sentient life for the benefit of humanity. None of these reinterpretations successfully displace Daneel\'s Zeroth Law — though Foundation\'s Triumph hints that these robotic factions remain active as fringe groups up to the time of the novel Foundation.[30]\n\nThese novels take place in a future dictated by Asimov to be free of obvious robot presence and surmise that R. Daneel\'s secret influence on history through the millennia has prevented both the rediscovery of positronic brain technology and the opportunity to work on sophisticated intelligent machines. This lack of rediscovery and lack of opportunity makes certain that the superior physical and intellectual power wielded by intelligent machines remains squarely in the possession of robots obedient to some form of the Three Laws.[30] That R. Daneel is not entirely successful at this becomes clear in a brief period when scientists on Trantor develop "tiktoks" — simplistic programmable machines akin to real–life modern robots and therefore lacking the Three Laws. The robot conspirators see the Trantorian tiktoks as a massive threat to social stability, and their plan to eliminate the tiktok threat forms much of the plot of Foundation\'s Fear.\n\nIn Foundation\'s Triumph different robot factions interpret the Laws in a wide variety of ways, seemingly ringing every possible permutation upon the Three Laws\' ambiguities.\n\nSet between The Robots of Dawn and Robots and Empire, Mark W. Tiedemann\'s Robot Mystery trilogy updates the Robot–Foundation saga with robotic minds housed in computer mainframes rather than humanoid bodies.[clarification needed] The 2002 Aurora novel has robotic characters debating the moral implications of harming cyborg lifeforms who are part artificial and part biological.[31]\n\nOne should not neglect Asimov\'s own creations in these areas such as the Solarian "viewing" technology and the machines of The Evitable Conflict originals that Tiedemann acknowledges. Aurora, for example, terms the Machines "the first RIs, really". In addition the Robot Mystery series addresses the problem of nanotechnology:[32] building a positronic brain capable of reproducing human cognitive processes requires a high degree of miniaturization, yet Asimov\'s stories largely overlook the effects this miniaturization would have in other fields of technology. For example, the police department card-readers in The Caves of Steel have a capacity of only a few kilobytes per square centimeter of storage medium. Aurora, in particular, presents a sequence of historical developments which explains the lack of nanotechnology — a partial retcon, in a sense, of Asimov\'s timeline.\n\nRandall Munroe has discussed the Three Laws in various instances, but possibly most directly by one of his comics entitled The Three Laws of Robotics which imagines the consequences of every distinct ordering of the existing three laws.\n\nAuthors other than Asimov have often created extra laws.\n\nThe 1974 Lyuben Dilov novel, Icarus\'s Way (a.k.a., The Trip of Icarus) introduced a Fourth Law of robotics: "A robot must establish its identity as a robot in all cases." \nDilov gives reasons for the fourth safeguard in this way: "The last Law has put an end to the expensive aberrations of designers to give psychorobots as humanlike a form as possible. And to the resulting misunderstandings..."[33]\n\nA fifth law was introduced by Nikola Kesarovski in his short story "The Fifth Law of Robotics". This fifth law says: "A robot must know it is a robot." \nThe plot revolves around a murder where the forensic investigation discovers that the victim was killed by a hug from a humaniform robot that did not establish for itself that it was a robot.[34] The story was reviewed by Valentin D. Ivanov in SFF review webzine The Portal.[35]\n\nFor the 1986 tribute anthology, Foundation\'s Friends, Harry Harrison wrote a story entitled, "The Fourth Law of Robotics". This Fourth Law states: "A robot must reproduce. As long as such reproduction does not interfere with the First or Second or Third Law."\n\nIn 2013 Hutan Ashrafian proposed an additional law that considered the role of artificial intelligence-on-artificial intelligence or the relationship between robots themselves – the so-called AIonAI law.[36] This sixth law states: "All robots endowed with comparable human reason and conscience should act towards one another in a spirit of brotherhood."\n\nIn The Naked Sun, Elijah Baley points out that the Laws had been deliberately misrepresented because robots could unknowingly break any of them. He restated the first law as "A robot may do nothing that, to its knowledge, will harm a human being; nor, through inaction, knowingly allow a human being to come to harm." This change in wording makes it clear that robots can become the tools of murder, provided they not be aware of the nature of their tasks; for instance being ordered to add something to a person\'s food, not knowing that it is poison. Furthermore, he points out that a clever criminal could divide a task among multiple robots so that no individual robot could recognize that its actions would lead to harming a human being.[37] The Naked Sun complicates the issue by portraying a decentralized, planetwide communication network among Solaria\'s millions of robots meaning that the criminal mastermind could be located anywhere on the planet.\n\nBaley furthermore proposes that the Solarians may one day use robots for military purposes. If a spacecraft was built with a positronic brain and carried neither humans nor the life-support systems to sustain them, then the ship\'s robotic intelligence could naturally assume that all other spacecraft were robotic beings. Such a ship could operate more responsively and flexibly than one crewed by humans, could be armed more heavily and its robotic brain equipped to slaughter humans of whose existence it is totally ignorant.[38] This possibility is referenced in Foundation and Earth where it is discovered that the Solarians possess a strong police force of unspecified size that has been programmed to identify only the Solarian race as human. (The novel takes place thousands of years after The Naked Sun, and the Solarians have long since modified themselves from normal humans to hermaphroditic telepaths with extended brains and specialized organs.) Similarly, in Lucky Starr and the Rings of Saturn Bigman attempts to speak with a Sirian robot about possible damage to the Solar System population from its actions, but it appears unaware of the data and programmed to ignore attempts to teach it about the matter. The same motive was explored earlier in "Reason (1941)", where a robot running a solar power station refuses to believe that the destinations of the station\'s beams are planets containing people. Powell and Donovan are afraid this will make it capable of causing mass destruction by letting the beams stray off their proper course during a solar storm.\n\nThe Laws of Robotics presume that the terms "human being" and "robot" are understood and well defined. In some stories this presumption is overturned.\n\nThe Solarians create robots with the Three Laws but with a warped meaning of "human". Solarian robots are told that only people speaking with a Solarian accent are human. This enables their robots to have no ethical dilemma in harming non-Solarian human beings (and they are specifically programmed to do so). By the time period of Foundation and Earth it is revealed that the Solarians have genetically modified themselves into a distinct species from humanity—becoming hermaphroditic[39] and psychokinetic and containing biological organs capable of individually powering and controlling whole complexes of robots. The robots of Solaria thus respected the Three Laws only with regard to the "humans" of Solaria. It is unclear whether all the robots had such definitions, since only the overseer and guardian robots were shown explicitly to have them. In "Robots and Empire", the lower class robots were instructed by their overseer about whether certain creatures are human or not.\n\nAsimov addresses the problem of humanoid robots ("androids" in later parlance) several times. The novel Robots and Empire and the short stories "Evidence" and "The Tercentenary Incident" describe robots crafted to fool people into believing that the robots are human.[40] On the other hand, "The Bicentennial Man" and "—That Thou Art Mindful of Him" explore how the robots may change their interpretation of the Laws as they grow more sophisticated. Gwendoline Butler writes in A Coffin for the Canary "Perhaps we are robots. Robots acting out the last Law of Robotics... To tend towards the human."[41] In The Robots of Dawn, Elijah Baley points out that the use of humaniform robots as the first wave of settlers on new Spacer worlds may lead to the robots seeing themselves as the true humans, and deciding to keep the worlds for themselves rather than allow the Spacers to settle there.\n\n"—That Thou Art Mindful of Him", which Asimov intended to be the "ultimate" probe into the Laws\' subtleties,[42] finally uses the Three Laws to conjure up the very "Frankenstein" scenario they were invented to prevent. It takes as its concept the growing development of robots that mimic non-human living things and are given programs that mimic simple animal behaviours which do not require the Three Laws. The presence of a whole range of robotic life that serves the same purpose as organic life ends with two humanoid robots, George Nine and George Ten, concluding that organic life is an unnecessary requirement for a truly logical and self-consistent definition of "humanity", and that since they are the most advanced thinking beings on the planet, they are therefore the only two true humans alive and the Three Laws only apply to themselves. The story ends on a sinister note as the two robots enter hibernation and await a time when they will conquer the Earth and subjugate biological humans to themselves, an outcome they consider an inevitable result of the "Three Laws of Humanics".[43]\n\nThis story does not fit within the overall sweep of the Robot and Foundation series; if the George robots did take over Earth some time after the story closes, the later stories would be either redundant or impossible. Contradictions of this sort among Asimov\'s fiction works have led scholars to regard the Robot stories as more like "the Scandinavian sagas or the Greek legends" than a unified whole.[44]\n\nIndeed, Asimov describes "—That Thou Art Mindful of Him" and "Bicentennial Man" as two opposite, parallel futures for robots that obviate the Three Laws as robots come to consider themselves to be humans: one portraying this in a positive light with a robot joining human society, one portraying this in a negative light with robots supplanting humans.[45] Both are to be considered alternatives to the possibility of a robot society that continues to be driven by the Three Laws as portrayed in the Foundation series.[according to whom?] In The Positronic Man, the novelization of The Bicentennial Man, Asimov and his co-writer Robert Silverberg imply that in the future where Andrew Martin exists his influence causes humanity to abandon the idea of independent, sentient humanlike robots entirely, creating an utterly different future from that of Foundation.[according to whom?]\n\nIn Lucky Starr and the Rings of Saturn, a novel unrelated to the Robot series but featuring robots programmed with the Three Laws, John Bigman Jones is almost killed by a Sirian robot on orders of its master. The society of Sirius is eugenically bred to be uniformly tall and similar in appearance, and as such, said master is able to convince the robot that the much shorter Bigman, is, in fact, not a human being.\n\nAs noted in "The Fifth Law of Robotics" by Nikola Kesarovski, "A robot must know it is a robot": it is presumed that a robot has a definition of the term or a means to apply it to its own actions. Kesarovski played with this idea in writing about a robot that could kill a human being because it did not understand that it was a robot, and therefore did not apply the Laws of Robotics to its actions.\n\nAdvanced robots in fiction are typically programmed to handle the Three Laws in a sophisticated manner. In many stories, such as "Runaround" by Asimov, the potential and severity of all actions are weighed and a robot will break the laws as little as possible rather than do nothing at all. For example, the First Law may forbid a robot from functioning as a surgeon, as that act may cause damage to a human; however, Asimov\'s stories eventually included robot surgeons ("The Bicentennial Man" being a notable example). When robots are sophisticated enough to weigh alternatives, a robot may be programmed to accept the necessity of inflicting damage during surgery in order to prevent the greater harm that would result if the surgery were not carried out, or was carried out by a more fallible human surgeon. In "Evidence" Susan Calvin points out that a robot may even act as a prosecuting attorney because in the American justice system it is the jury which decides guilt or innocence, the judge who decides the sentence, and the executioner who carries through capital punishment.[46]\n\nAsimov\'s Three Laws-obeying robots (Asenion robots) can experience irreversible mental collapse if they are forced into situations where they cannot obey the First Law, or if they discover they have unknowingly violated it. The first example of this failure mode occurs in the story "Liar!", which introduced the First Law itself, and introduces failure by dilemma—in this case the robot will hurt humans if he tells them something and hurt them if he does not.[47] This failure mode, which often ruins the positronic brain beyond repair, plays a significant role in Asimov\'s SF-mystery novel The Naked Sun. Here Daneel describes activities contrary to one of the laws, but in support of another, as overloading some circuits in a robot\'s brain—the equivalent sensation to pain in humans. The example he uses is forcefully ordering a robot to let a human do its work, which on Solaria, due to the extreme specialization, would mean its only purpose.[48]\n\nIn The Robots of Dawn, it is stated that more advanced robots are built capable of determining which action is more harmful, and even choosing at random if the alternatives are equally bad. As such, a robot is capable of taking an action which can be interpreted as following the First Law, thus avoiding a mental collapse. The whole plot of the story revolves around a robot which apparently was destroyed by such a mental collapse, and since his designer and creator refused to share the basic theory with others, he is, by definition, the only person capable of circumventing the safeguards and forcing the robot into a brain-destroying paradox.\n\nIn Robots and Empire, Daneel states it\'s very unpleasant for him when making the proper decision takes too long (in robot terms), and he cannot imagine being without the Laws at all except to the extent of it being similar to that unpleasant sensation, only permanent.\n\nRobots and artificial intelligences do not inherently contain or obey the Three Laws; their human creators must choose to program them in, and devise a means to do so. Robots already exist (for example, a Roomba) that are too simple to understand when they are causing pain or injury and know to stop. Many are constructed with physical safeguards such as bumpers, warning beepers, safety cages, or restricted-access zones to prevent accidents. Even the most complex robots currently produced are incapable of understanding and applying the Three Laws; significant advances in artificial intelligence would be needed to do so, and even if AI could reach human-level intelligence, the inherent ethical complexity as well as cultural/contextual dependency of the laws prevent them from being a good candidate to formulate robotics design constraints.[49] However, as the complexity of robots has increased, so has interest in developing guidelines and safeguards for their operation.[50][51]\n\nIn a 2007 guest editorial in the journal Science on the topic of "Robot Ethics", SF author Robert J. Sawyer argues that since the U.S. military is a major source of funding for robotic research (and already uses armed unmanned aerial vehicles to kill enemies) it is unlikely such laws would be built into their designs.[52] In a separate essay, Sawyer generalizes this argument to cover other industries stating:\n\nThe development of AI is a business, and businesses are notoriously uninterested in fundamental safeguards — especially philosophic ones. (A few quick examples: the tobacco industry, the automotive industry, the nuclear industry. Not one of these has said from the outset that fundamental safeguards are necessary, every one of them has resisted externally imposed safeguards, and none has accepted an absolute edict against ever causing harm to humans.)[53]\n\nDavid Langford has suggested[54] a tongue-in-cheek set of laws:\n\nRoger Clarke (aka Rodger Clarke) wrote a pair of papers analyzing the complications in implementing these laws in the event that systems were someday capable of employing them. He argued "Asimov\'s Laws of Robotics have been a very successful literary device. Perhaps ironically, or perhaps because it was artistically appropriate, the sum of Asimov\'s stories disprove the contention that he began with: It is not possible to reliably constrain the behaviour of robots by devising and applying a set of rules."[55] On the other hand, Asimov\'s later novels The Robots of Dawn, Robots and Empire and Foundation and Earth imply that the robots inflicted their worst long-term harm by obeying the Three Laws perfectly well, thereby depriving humanity of inventive or risk-taking behaviour.\n\nIn March 2007 the South Korean government announced that later in the year it would issue a "Robot Ethics Charter" setting standards for both users and manufacturers. According to Park Hye-Young of the Ministry of Information and Communication the Charter may reflect Asimov\'s Three Laws, attempting to set ground rules for the future development of robotics.[56]\n\nThe futurist Hans Moravec (a prominent figure in the transhumanist movement) proposed that the Laws of Robotics should be adapted to "corporate intelligences" — the corporations driven by AI and robotic manufacturing power which Moravec believes will arise in the near future.[50] In contrast, the David Brin novel Foundation\'s Triumph (1999) suggests that the Three Laws may decay into obsolescence: Robots use the Zeroth Law to rationalize away the First Law and robots hide themselves from human beings so that the Second Law never comes into play. Brin even portrays R. Daneel Olivaw worrying that, should robots continue to reproduce themselves, the Three Laws would become an evolutionary handicap and natural selection would sweep the Laws away — Asimov\'s careful foundation undone by evolutionary computation. Although the robots would not be evolving through design instead of mutation because the robots would have to follow the Three Laws while designing and the prevalence of the laws would be ensured,[57] design flaws or construction errors could functionally take the place of biological mutation.\n\nIn the July/August 2009 issue of IEEE Intelligent Systems, Robin Murphy (Raytheon Professor of Computer Science and Engineering at Texas A&M) and David D. Woods (director of the Cognitive Systems Engineering Laboratory at Ohio State) proposed "The Three Laws of Responsible Robotics" as a way to stimulate discussion about the role of responsibility and authority when designing not only a single robotic platform but the larger system in which the platform operates. The laws are as follows:\n\nWoods said, "Our laws are a little more realistic, and therefore a little more boring” and that "The philosophy has been, ‘sure, people make mistakes, but robots will be better – a perfect version of ourselves’. We wanted to write three new laws to get people thinking about the human-robot relationship in more realistic, grounded ways."[58]\n\nIn early 2011, the UK published what is now considered the first national-level AI softlaw, which consisted largely of a revised set of 5 laws, the first 3 of which updated Asimov\'s. These laws ere published with commentary, by the EPSRC/AHRC working group in 2010:[59][60]\n\nAsimov himself believed that his Three Laws became the basis for a new view of robots which moved beyond the "Frankenstein complex".[citation needed] His view that robots are more than mechanical monsters eventually spread throughout science fiction.[according to whom?] Stories written by other authors have depicted robots as if they obeyed the Three Laws but tradition dictates that only Asimov could quote the Laws explicitly.[according to whom?] Asimov believed the Three Laws helped foster the rise of stories in which robots are "lovable" – Star Wars being his favorite example.[61] Where the laws are quoted verbatim, such as in the Buck Rogers in the 25th Century episode "Shgoratchx!", it is not uncommon for Asimov to be mentioned in the same dialogue as can also be seen in the Aaron Stone pilot where an android states that it functions under Asimov\'s Three Laws. However, the 1960s German TV series Raumpatrouille – Die phantastischen Abenteuer des Raumschiffes Orion (Space Patrol – the Fantastic Adventures of Space Ship Orion) bases episode three titled "Hüter des Gesetzes" ("Guardians of the Law") on Asimov\'s Three Laws without mentioning the source.\n\nReferences to the Three Laws have appeared in popular music ("Robot" from Hawkwind\'s 1979 album PXR5), cinema (Repo Man,[62] Aliens, Ghost in the Shell 2: Innocence), cartoon series (The Simpsons and The Amazing World of Gumball), anime (Eve no Jikan), tabletop role-playing games (Paranoia), webcomics (Piled Higher and Deeper and Freefall), and video games (Danganronpa V3: Killing Harmony)\n\nRobby the Robot in Forbidden Planet (1956) has a hierarchical command structure which keeps him from harming humans, even when ordered to do so, as such orders cause a conflict and lock-up very much in the manner of Asimov\'s robots. Robby is one of the first cinematic depictions of a robot with internal safeguards put in place in this fashion. Asimov was delighted with Robby and noted that Robby appeared to be programmed to follow his Three Laws.\n\nIsaac Asimov\'s works have been adapted for cinema several times with varying degrees of critical and commercial success. Some of the more notable attempts have involved his "Robot" stories, including the Three Laws.\n\nThe film Bicentennial Man (1999) features Robin Williams as the Three Laws robot NDR-114 (the serial number is partially a reference to Stanley Kubrick\'s signature numeral). Williams recites the Three Laws to his employers, the Martin family, aided by a holographic projection. The film only loosely follows the original story.\n\nHarlan Ellison\'s proposed screenplay for I, Robot began by introducing the Three Laws, and issues growing from the Three Laws form a large part of the screenplay\'s plot development. Due to various complications in the Hollywood moviemaking system, to which Ellison\'s introduction devotes much invective, his screenplay was never filmed.[63]\n\nIn the 1986 movie Aliens, after the android Bishop accidentally cuts himself, he attempts to reassure Ripley by stating that: "It is impossible for me to harm or by omission of action, allow to be harmed, a human being".[64]\n\nThe plot of the film released in 2004 under the name, I, Robot is "suggested by" Asimov\'s robot fiction stories[65]\nand advertising for the film included a trailer featuring the Three Laws followed by the aphorism, "Rules were made to be broken". The film opens with a recitation of the Three Laws and explores the implications of the Zeroth Law as a logical extrapolation. The major conflict of the film comes from a computer artificial intelligence reaching the conclusion that humanity is incapable of taking care of itself.[66]\n\nThe 2019 Netflix original series Better than Us includes the 3 laws in the opening of episode 1.\n\nAnalytical philosopher James H. Moor says that if applied thoroughly they would produce unexpected results. He gives the example of a robot roaming the world trying to prevent harm from befalling human beings.[67]',
        pageTitle: "Three Laws of Robotics",
    },
    {
        title: "Parkinson's law",
        link: "https://en.wikipedia.org/wiki/Parkinson%27s_law",
        content:
            'Parkinson\'s law can refer to either of two observations, published in 1955 by the naval historian C. Northcote Parkinson as an essay in The Economist:[1]\n\nThe first paragraph of the essay mentioned the first meaning above as a "commonplace observation", and the rest of the essay was devoted to the latter observation, terming it "Parkinson\'s Law".\n\nThe first-referenced meaning of the law – "Work expands to fill the available time" – has sprouted several corollaries, the best known being the Stock-Sanford corollary to Parkinson\'s law:\n\nIf you wait until the last minute, it only takes a minute to do.[2]\n\nIn ten hours a day you have time to fall twice as far behind your commitments as in five hours a day.[3]\n\nas well as corollaries relating to computers, such as:\n\nData expands to fill the space available for storage.[4]\n\nThis was the main focus of the essay by Cyril Northcote Parkinson, published in The Economist in 1955,[1][5] and reprinted with other similar essays in the successful 1958 book Parkinson\'s Law: The Pursuit of Progress.[6] The book was translated into many languages. It was highly popular in the Soviet Union and its sphere of influence.[7] In 1986, Alessandro Natta complained about the swelling bureaucracy in Italy. Mikhail Gorbachev responded that "Parkinson\'s law works everywhere."[8]\n\nParkinson derived the dictum from his extensive experience in the British Civil Service. He gave, as examples, the growth in the size of the British Admiralty and Colonial Office even though the numbers of their ships and colonies were declining.\n\nMuch of the essay is dedicated to a summary of purportedly scientific observations supporting the law, such as the increase in the number of employees at the Colonial Office while the British Empire declined (he showed that it had its greatest number of staff when it was folded into the Foreign Office due to a lack of colonies to administer). He explained this growth using two forces: (1) "An official wants to multiply subordinates, not rivals", and (2) "Officials make work for each other." He noted that the number employed in a bureaucracy rose by 5–7% per year "irrespective of any variation in the amount of work (if any) to be done".\n\nParkinson presented the growth as a mathematical equation describing the rate at which bureaucracies expand over time, with the formula \n  \n    \n      \n        x\n        =\n        (\n        2\n        \n          k\n          \n            m\n          \n        \n        +\n        P\n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle x=(2k^{m}+P)/n}\n  \n, in which k was the number of officials wanting subordinates, m was the hours they spent writing minutes to each other.\n\nObserving that the promotion of employees necessitated the hiring of subordinates, and that time used answering minutes requires more work; Parkinson states: "In any public administrative department not actually at war the staff increase may be expected to follow this formula" (for a given year) [1]\n\nx\n        =\n        \n          \n            \n              2\n              \n                k\n                \n                  m\n                \n              \n              +\n              P\n            \n            n\n          \n        \n      \n    \n    {\\displaystyle x={\\frac {2k^{m}+P}{n}}}\n\nIn a different essay included in the book, Parkinson proposed a rule about the efficiency of administrative councils. He defined a "coefficient of inefficiency" with the number of members as the main determining variable. This is a semi-humorous attempt to define the size at which a committee or other decision-making body becomes completely inefficient.\n\nIn .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}Parkinson\'s Law: The Pursuit of Progress, London: John Murray, 1958 a chapter is devoted to the basic question of what he called comitology: how committees, government cabinets, and other such bodies are created and eventually grow irrelevant (or are initially designed as such). (The word comitology has recently been independently invented by the European Union for a different non-humorous meaning.)[9][10]\n\nEmpirical evidence is drawn from historical and contemporary government cabinets. Most often, the minimal size of a state\'s most powerful and prestigious body is five members. From English history, Parkinson notes a number of bodies that lost power as they grew:\n\nA detailed mathematical expression is proposed by Parkinson for the coefficient of inefficiency, featuring many possible influences. In 2008, an attempt was made to empirically verify the proposed model.[11] Parkinson\'s conjecture that membership exceeding a number "between 19.9 and 22.4" makes a committee manifestly inefficient seems well justified by the evidence proposed[citation needed]. Less certain is the optimal number of members, which must lie between three (a logical minimum) and 20. (Within a group of 20, individual discussions may occur, diluting the power of the leader.) That it may be eight seems arguable but is not supported by observation: no contemporary government in Parkinson\'s data set had eight members, and only king Charles I of England had a Committee of State of that size.',
        pageTitle: "Parkinson's law",
    },
    {
        title: "Atwood's law",
        link: "https://en.wikipedia.org/wiki/Atwood%27s_law",
        content:
            "Jeff Atwood (born 1970) is an American software developer, author, blogger, and entrepreneur. He co-founded the question-and-answer network Stack Exchange, which contains the Stack Overflow website for computer programming questions.[4] He is the owner and writer of the computer programming blog Coding Horror, focused on programming and human factors.[5] As of 2012, his most recent project was Discourse, an open source Internet discussion platform.[3]\n\nIn a 2007 blog post, Atwood proposed the following rule related to the rule of least power, calling it Atwood's law:[6] \"Any application that can be written in JavaScript, will eventually be written in JavaScript\".[7]\n\nAtwood started a programming blog, Coding Horror, in 2004. As a result, he met Joel Spolsky.[8] In 2008, together with Spolsky, Atwood founded Stack Overflow, a programming question-and-answer website.[9] The site was followed by Server Fault for system administrators and Super User for general computer-related questions, eventually becoming the Stack Exchange network which includes many Q&A websites about topics decided on by the community.[10]\n\nFrom 2008 to 2014, Atwood and Spolsky published a weekly podcast covering the progress on Stack Exchange and a wide range of software development issues. Jeff Atwood was also a keynote presenter at the 2008 Canadian University Software Engineering Conference.[11]\n\nIn February 2012, Atwood left Stack Exchange so he could spend more time with his family.[12]\n\nOn February 5, 2013, Atwood announced his new company, Civilized Discourse Construction Kit, Inc. Its flagship product is an open source next-generation discussion platform called Discourse.[13] Atwood and others developed it out of their frustration with current bulletin board software that hadn't seemed to evolve since 1990.[14] On February 1, 2023, he stepped down as CEO and assumed the role of Executive Chairman.[15]\n\nHe also launched a mechanical keyboard called CODE in 2013.[16]\n\nIn 2021, Stack Overflow was sold to Prosus for $1.8 billion.[17][18]\n\nIn January 2025, Atwood announced one million dollar gifts to eight non-profit organizations,[18][19] \nincluding The Trevor Project, an organization dedicated to suicide prevention and crisis intervention for LGBTQ+ youth.[20] In addition, Children’s Hunger Fund, First Generation Investors, Global Refuge, NAACP Legal Defense and Educational Fund, PEN America, Planned Parenthood, and Team Rubicon received donations from Atwood and his family.[18][19][20]\n\nAtwood and his family have contributed to Alameda Post and the Alameda Food Bank.[18]\n\nAtwood donated $1.5 million to 404 Media, a nonprofit news site.[18]\n\nAtwood is a resident of Alameda, California.[18] He and his partner, Betsy Burton, have three kids.[18]",
        pageTitle: "Jeff Atwood",
    },
    {
        title: "Augustine's laws",
        link: "https://en.wikipedia.org/wiki/Augustine%27s_laws",
        content:
            "Augustine's laws were a series of tongue in cheek aphorisms put forth by Norman Ralph Augustine, an American aerospace businessman who served as Under Secretary of the Army from 1975 to 1977. In 1984 he published his laws.[1]  The book and several of the laws were the topic of an article in Sound and Vibration magazine in March 2012.[2]\n\nHis most cited law is number 16, which shows that defense budgets grow linearly but the unit cost of a new military aircraft grows exponentially:[3]\n\nIn the year 2054, the entire defense budget will purchase just one tactical aircraft. This aircraft will have to be shared by the Air Force and Navy 3½ days each per week except for leap year, when it will be made available to the Marines for the extra day.\"[4][5]",
        pageTitle: "Augustine's laws",
    },
    {
        title: "Avogadro's law",
        link: "https://en.wikipedia.org/wiki/Avogadro%27s_law",
        content:
            "Avogadro's law (sometimes referred to as Avogadro's hypothesis or Avogadro's principle) or Avogadro-Ampère's hypothesis   is an experimental gas law relating the volume of a gas to the amount of substance of gas present.[1] The law is a specific case of the ideal gas law. A modern statement is:\n\nAvogadro's law states that \"equal volumes of all gases, at the same temperature and pressure, have the same number of molecules.\"[1]\n\nFor a given mass of an ideal gas, the volume and amount (moles) of the gas are directly proportional if the temperature and pressure are constant.\n\nThe law is named after Amedeo Avogadro who, in 1812,[2][3] hypothesized that two given samples of an ideal gas, of the same volume and at the same temperature and pressure, contain the same number of molecules. As an example, equal volumes of gaseous hydrogen and nitrogen contain the same number of molecules when they are at the same temperature and pressure, and display ideal gas behavior. In practice, real gases show small deviations from the ideal behavior and the law holds only approximately, but is still a useful approximation for scientists.\n\nThis law describes how, under the same condition of temperature and pressure, equal volumes of all gases contain the same number of molecules. For comparing the same substance under two different sets of conditions, the law can be usefully expressed as follows:\n\nThe equation shows that, as the number of moles of gas increases, the volume of the gas also increases in proportion. Similarly, if the number of moles of gas is decreased, then the volume also decreases. Thus, the number of molecules or atoms in a specific volume of ideal gas is independent of their size or the molar mass of the gas.\n\nThe derivation of Avogadro's law follows directly from the ideal gas law, i.e.\n\nwhere R is the gas constant, T is the Kelvin temperature, and P is the pressure (in pascals).\n\nwhich is a constant for a fixed pressure and a fixed temperature.\n\nAn equivalent formulation of the ideal gas law can be written using Boltzmann constant kB, as\n\nwhere N is the number of particles in the gas, and the ratio of R over kB  is equal to the Avogadro constant.\n\nIf T and P are taken at standard conditions for temperature and pressure (STP), then k′ = 1/n0, where n0 is the Loschmidt constant.\n\nAvogadro's hypothesis (as it was known originally) was formulated in the same spirit of earlier empirical gas laws like Boyle's law (1662), Charles's law (1787)  and Gay-Lussac's law (1808). The hypothesis was first published by Amedeo Avogadro in 1811,[4]  and it reconciled Dalton atomic theory with the \"incompatible\" idea of Joseph Louis Gay-Lussac that some gases were composite of different fundamental substances (molecules) in integer proportions.[5] In 1814, independently from Avogadro, André-Marie Ampère published the same law with similar conclusions.[6] As Ampère was more well known in France, the hypothesis was usually referred there as Ampère's hypothesis,[note 1] and later also as Avogadro–Ampère hypothesis[note 2] or even Ampère–Avogadro hypothesis.[7]\n\nExperimental studies carried out by Charles Frédéric Gerhardt and Auguste Laurent on organic chemistry demonstrated that Avogadro's law explained why the same quantities of molecules in a gas have the same volume. Nevertheless, related experiments with some inorganic substances showed seeming exceptions to the law. This apparent contradiction was finally resolved by Stanislao Cannizzaro, as announced at Karlsruhe Congress in 1860, four years after Avogadro's death. He explained that these exceptions were due to  molecular dissociations at certain temperatures, and that Avogadro's law determined not only molecular masses, but atomic masses as well.\n\nBoyle, Charles and Gay-Lussac laws, together with Avogadro's law, were combined by Émile Clapeyron in 1834,[8] giving rise to the ideal gas law. At the end of the 19th century, later developments from scientists like August Krönig, Rudolf Clausius, James Clerk Maxwell and Ludwig Boltzmann, gave rise to the kinetic theory of gases, a microscopic theory from which the ideal gas law can be derived as an statistical result from the movement of atoms/molecules in a gas.\n\nAvogadro's law provides a way to calculate the quantity of gas in a receptacle. Thanks to this discovery, Johann Josef Loschmidt, in 1865, was able for the first time to estimate the size of a molecule.[9] His calculation gave rise to the concept of the Loschmidt constant, a ratio between macroscopic and atomic quantities. In 1910, Millikan's oil drop experiment determined the charge of the electron; using it with  the Faraday constant (derived by Michael Faraday in 1834), one is able to determine the number of particles in a mole of substance. At the same time, precision experiments by Jean Baptiste Perrin led to the definition of the Avogadro number as the number of molecules in one gram-molecule of oxygen. Perrin named the number to honor Avogadro for his discovery of the namesake law. Later standardization of the International System of Units led to the modern definition of the Avogadro constant.\n\nAt standard temperature and pressure (100 kPa and 273.15 K), we can use Avogadro's law to find the molar volume of an ideal gas:\n\nSimilarly, at standard atmospheric pressure (101.325 kPa) and 0 °C (273.15 K):",
        pageTitle: "Avogadro's law",
    },
    {
        title: "gas laws",
        link: "https://en.wikipedia.org/wiki/Gas_laws",
        content:
            "The laws describing the behaviour of gases under fixed pressure, volume, amount of gas, and absolute temperature conditions are called gas laws. The basic gas laws were discovered by the end of the 18th century when scientists found out that relationships between pressure, volume and temperature of a sample of gas could be obtained which would hold to approximation for all gases. The combination of several empirical gas laws led to the development of the ideal gas law.\n\nThe ideal gas law was later found to be consistent with atomic and kinetic theory.\n\nIn 1643, the Italian physicist and mathematician, Evangelista Torricelli, who for a few months had acted as Galileo Galilei's secretary, conducted a celebrated experiment in Florence.[1] He demonstrated that a column of mercury in an inverted tube can be supported by the pressure of air outside of the tube, with the creation of a small section of vacuum above the mercury.[2] This experiment essentially paved the way towards the invention of the barometer, as well as drawing the attention of Robert Boyle, then a \"skeptical\" scientist working in England. Boyle was inspired by Torricelli's experiment to investigate how the elasticity of air responds to varying pressure, and he did this through a series of experiments with a setup reminiscent of that used by Torricelli.[3] Boyle published his results in 1662.\n\nLater on, in 1676, the French physicist Edme Mariotte, independently arrived at the same conclusions of Boyle, while also noting some dependency of air volume on temperature.[4] However it took another century and a half for the development of thermometry and recognition of the absolute zero temperature scale, which eventually allowed the discovery of temperature-dependent gas laws.\n\nIn 1662, Robert Boyle systematically studied the relationship between the volume and pressure of a fixed amount of gas at a constant temperature. He observed that the volume of a given mass of a gas is inversely proportional to its pressure at a constant temperature.\nBoyle's law, published in 1662, states that, at a constant temperature, the product of the pressure and volume of a given mass of an ideal gas in a closed system is always constant. It can be verified experimentally using a pressure gauge and a variable volume container. It can also be derived from the kinetic theory of gases: if a container, with a fixed number of molecules inside, is reduced in volume, more molecules will strike a given area of the sides of the container per unit time, causing a greater pressure.\n\nThe concept can be represented with these formulae:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}}\n  \n\nwhere P is the pressure, V is the volume of a gas, and k1 is the constant in this equation (and is not the same as the proportionality constants in the other equations).\n\nCharles' law, or the law of volumes, was founded in 1787 by Jacques Charles. It states that, for a given mass of an ideal gas at constant pressure, the volume is directly proportional to its absolute temperature, assuming in a closed system.\nThe statement of Charles' law is as follows:\nthe volume (V) of a given mass of a gas, at constant pressure (P), is directly proportional to its temperature (T).\n\nwhere \"V\" is the volume of a gas, \"T\" is the absolute temperature and k2 is a proportionality constant (which is not the same as the proportionality constants in the other equations in this article).\n\nGay-Lussac's law, Amontons' law or the pressure law was founded by Joseph Louis Gay-Lussac in 1808.\n\nP\n              \n                1\n              \n            \n            \n              T\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              P\n              \n                2\n              \n            \n            \n              T\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {P_{1} \\over T_{1}}={P_{2} \\over T_{2}}}\n  \n,\n\nAvogadro's law, Avogadro's hypothesis, Avogadro's principle or Avogadro-Ampère's hypothesis is an experimental gas law which was hypothesized by Amedeo Avogadro in 1811. It related the volume of a gas to the amount of substance of gas present.[5]\n\nThis statement gives rise to the molar volume of a gas, which at STP (273.15 K, 1 atm) is about 22.4 L. The relation is given by:\n\nThe combined gas law or general gas equation is obtained by combining Boyle's law, Charles's law, and Gay-Lussac's law. It shows the relationship between the pressure, volume, and temperature for a fixed mass of gas:\n\nWith the addition of Avogadro's law, the combined gas law develops into the ideal gas law:\n\nThese equations are exact only for an ideal gas, which neglects various intermolecular effects (see real gas). However, the ideal gas law is a good approximation for most gases under moderate pressure and temperature.",
        pageTitle: "Gas laws",
    },
    {
        title: "Barlow's law",
        link: "https://en.wikipedia.org/wiki/Barlow%27s_law",
        content:
            "Barlow's law is an incorrect physical law proposed by Peter Barlow in 1825 to describe the ability of wires to conduct electricity.[1][2] It says that the strength of the effect of electricity passing through a wire varies inversely with the square root of its length and directly with the square root of its cross-sectional area, or, in modern terminology:\n\nwhere I is electric current, A is the cross-sectional area of the wire, and L is the length of the wire. Barlow formulated his law in terms of the diameter d of a cylindrical wire.  Since A is proportional to the square of d the law becomes \n  \n    \n      \n        I\n        ∝\n        d\n        \n          /\n        \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle I\\propto d/{\\sqrt {L}}}\n  \n for cylindrical wires.[2]\n\nBarlow undertook his experiments with the aim of determining whether long-distance telegraphy was feasible and believed that he proved that it was not.[1] The publication of Barlow's law delayed research into telegraphy for several years, until 1831 when Joseph Henry and Philip Ten Eyck constructed a circuit 1,060 feet long, which used a large battery to activate an electromagnet.[3] Barlow did not investigate the dependence of the current strength on electric tension (that is, voltage). He endeavoured to keep this constant, but admitted there was some variation. Barlow was not entirely certain that he had found the correct law, writing \"the discrepancies are rather too great to enable us to say, with confidence, that such is the law in question.\"[1]\n\nIn 1827, Georg Ohm published a different law, in which current varies inversely with the wire's length, not its square root; that is,\n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is a constant dependent on the circuit setup. Ohm's law is now considered the correct law, and Barlow's false.\n\nThe law Barlow proposed was not in error due to poor measurement; in fact, it fits Barlow's careful measurements quite well.  Heinrich Lenz pointed out that Ohm took into account \"all the conducting resistances … of the circuit\", whereas Barlow did not.[4] Ohm explicitly included a term for what we would now call the internal resistance of the battery. Barlow did not have this term and approximated the results with a power law instead. Ohm's law in modern usage is rarely stated with this explicit term, but nevertheless an awareness of it is necessary for a full understanding of the current in a circuit.[5]",
        pageTitle: "Barlow's law",
    },
    {
        title: "Beckstrom's law",
        link: "https://en.wikipedia.org/wiki/Beckstrom%27s_law",
        content:
            "In economics, Beckstrom's law is a model or theorem formulated by Rod Beckstrom. It purports to answer \"the decades-old question of 'how valuable is a network'\", and states in summary that \"The value of a network equals the net value added to each user’s transactions conducted through that network, summed over all users.\"\n\nAccording to its creator, this law can be used to value any network be it social networks, computer networks, support groups and even the Internet as a whole.[1] This new model values the network by looking from the edge of the network at all of the transactions conducted and the value added to each.\n\nIt states that one way to contemplate the value the network adds to each transaction is to imagine the network being shut off and what the additional transactions costs or loss would be. It can thus be compared to the value of a pizza delivery service offered to its customers. If the pizza delivery service shut down, then the social value generated by its deliveries declines, and people will either go hungry or elsewhere. Similarly, a potluck derives its total enjoyment value from the net value produced by each participant's dish. The success of such a gathering hinges on increasing the number of independent guests and their pots, thereby maximizing the amount of \"luck\" any one guest would have to achieve a satisfactory meal. Assuming one pot per person, a potluck with a set maximum number of guests could produce only a relatively small amount of total potential group satisfaction.\n\nBeckstrom's Law differs from Metcalfe's law, Reed's law and other concepts that proposed that the value of a network was based purely on the size of the network, and in Metcalfe's law, one other variable.\n\nAccording to Rod Beckstrom, the most significant improvement when using Beckstrom's Law instead of Metcalfe's Law, is the applicability to current experiences on the Internet. Metcalfe's Law does not account for service degradation due to a high number of users or bad actors who steal value from the network. [2]\n\nThe net present value V of any network j to any individual i is equal to the sum of the net present value of the benefit of all transactions less the net present value of the costs of all transactions on the network over any given period of time t, as shown in the following equation. The value of the entire network is the summary of the value to all users, who are defined as all parties doing transactions on that network.\n\nBeckstrom's Law gives an indication on community dynamics that affect the experience of the individual. If consumers use services that are based on funding by a community of people, every member of that community is contributing to delivering the service. As more members join the community they aid funding the services through their contributions, however, these member also demand services for themselves which ultimately can lead to delays and deteriorating quality of the community service. For example, a larger number of members of a golf club lead to more revenue of the golf club, but a larger number of members aids to overcrowding golf courses and delays which has a negative effect on the golfing experience. Beckstrom's Law provides a model that could identify the point at which the marginal effect of each new member's contribution is zero and where adding an additional member makes everybody else worse off.[3]",
        pageTitle: "Beckstrom's law",
    },
    {
        title: "Beer–Lambert law",
        link: "https://en.wikipedia.org/wiki/Beer%E2%80%93Lambert_law",
        content:
            "The Beer–Bouguer–Lambert (BBL) extinction law is an empirical relationship describing the attenuation in intensity of a radiation beam passing through a macroscopically homogenous medium with which it interacts.  Formally, it states that the intensity of radiation decays exponentially in the absorbance of the medium, and that said absorbance is proportional to the length of beam passing through the medium, the concentration of interacting matter along that path, and a constant representing said matter's propensity to interact.\n\nThe extinction law's primary application is in chemical analysis, where it underlies the Beer–Lambert law, commonly called Beer's law.  Beer's law states that a beam of visible light passing through a chemical solution of fixed geometry experiences absorption proportional to the solute concentration.  Other applications appear in physical optics, where it quantifies astronomical extinction and the absorption of photons, neutrons, or rarefied gases.\n\nForms of the BBL law date back to the mid-eighteenth century, but it only took its modern form during the early twentieth.\n\nThe first work towards the BBL law began with astronomical observations Pierre Bouguer performed in the early eighteenth century and published in 1729.[1]  Bouguer needed to compensate for the refraction of light by the earth's atmosphere, and found it necessary to measure the local height of the atmosphere.  The latter, he sought to obtain through variations in the observed intensity of known stars.  When calibrating this effect, Bouguer discovered that light intensity had an exponential dependence on length traveled through the atmosphere (in Bouguer's terms, a geometric progression).[2]\n\nBouguer's work was then popularized in Johann Heinrich Lambert's Photometria in 1760.[3] Lambert expressed the law, which states that the loss of light intensity when it propagates in a medium is directly proportional to intensity and path length, in a mathematical form quite similar to that used in modern physics.  Lambert began by assuming that the intensity I of light traveling into an absorbing body would be given by the differential equation \n  \n    \n      \n        −\n        \n          d\n        \n        I\n        =\n        μ\n        I\n        \n          d\n        \n        x\n        ,\n      \n    \n    {\\displaystyle -\\mathrm {d} I=\\mu I\\mathrm {d} x,}\n  \n which is compatible with Bouguer's observations. The constant of proportionality μ was often termed the \"optical density\" of the body.  As long as μ is constant along a distance d, the exponential attenuation law, \n  \n    \n      \n        I\n        =\n        \n          I\n          \n            0\n          \n        \n        \n          e\n          \n            −\n            μ\n            d\n          \n        \n      \n    \n    {\\displaystyle I=I_{0}e^{-\\mu d}}\n  \n follows from integration.[4]\n\nIn 1852, August Beer noticed that colored solutions also appeared to exhibit a similar attenuation relation. In his analysis, Beer does not discuss Bouguer and Lambert's prior work, writing in his introduction that \"Concerning the absolute magnitude of the absorption that a particular ray of light suffers during its propagation through an absorbing medium, there is no information available.\"[5]  Beer may have omitted reference to Bouguer's work because there is a subtle physical difference between color absorption in solutions and astronomical contexts.[original research?]  Solutions are homogeneous and do not scatter light at common analytical wavelengths (ultraviolet, visible, or infrared), except at entry and exit.  Thus light within a solution is reasonably approximated as due to absorption alone.  In Bouguer's context, atmospheric dust or other inhomogeneities could also scatter light away from the detector.  Modern texts combine the two laws because scattering and absorption have the same effect.  Thus a scattering coefficient μs and an absorption coefficient μa can be combined into a total extinction coefficient μ = μs + μa.[6]\n\nImportantly, Beer also seems to have conceptualized his result in terms of a given thickness' opacity, writing \"If λ is the coefficient (fraction) of diminution, then this coefficient (fraction) will have the value λ2 for double this thickness.\"[7]  Although this geometric progression is mathematically equivalent to the modern law, modern treatments instead emphasize the logarithm of λ, which clarifies that concentration and path length have equivalent effects on the absorption.[8][9]  An early, possibly the first, modern formulation was given by Robert Luther and Andreas Nikolopulos in 1913.[10]\n\nThere are several equivalent formulations of the BBL law, depending on the precise choice of measured quantities.  All of them state that, provided that the physical state is held constant, the extinction process is linear in the intensity of radiation and amount of radiatively-active matter, a fact sometimes called the fundamental law of extinction.[11]  Many of them then connect the quantity of radiatively-active matter to a length traveled ℓ and a concentration c or number density n.  The latter two are related by Avogadro's number: n = NAc.\n\nA collimated beam (directed radiation) with cross-sectional area S will encounter Sℓn particles (on average) during its travel.  However, not all of these particles interact with the beam.  Propensity to interact is a material-dependent property, typically summarized in absorptivity ϵ[12] or scattering cross-section σ.[13]  These almost exhibit another Avogadro-type relationship: ln(10)ε = NAσ.  The factor of ln(10) appears because physicists tend to use natural logarithms and chemists decadal logarithms.\n\nBeam intensity can also be described in terms of multiple variables: the intensity I or radiant flux Φ.  In the case of a collimated beam, these are related by Φ = IS, but Φ is often used in non-collimated contexts.  The ratio of intensity (or flux) in to out is sometimes summarized as a transmittance coefficient T = .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}I⁄I0.\n\nWhen considering an extinction law, dimensional analysis can verify the consistency of the variables, as logarithms (being nonlinear) must always be dimensionless.\n\nThe simplest formulation of Beer's relates the optical attenuation of a physical material containing a single attenuating species of uniform concentration to the optical path length through the sample and absorptivity of the species. This expression is:\n  \n    \n      \n        \n          log\n          \n            10\n          \n        \n        ⁡\n        (\n        \n          I\n          \n            0\n          \n        \n        \n          /\n        \n        I\n        )\n        =\n        A\n        =\n        ε\n        ℓ\n        c\n      \n    \n    {\\displaystyle \\log _{10}(I_{0}/I)=A=\\varepsilon \\ell c}\n  \nThe quantities so equated are defined to be the absorbance A, which depends on the logarithm base.   The Naperian absorbance τ is then given by τ = ln(10)A and satisfies \n  \n    \n      \n        ln\n        ⁡\n        (\n        \n          I\n          \n            0\n          \n        \n        \n          /\n        \n        I\n        )\n        =\n        τ\n        =\n        σ\n        ℓ\n        n\n        .\n      \n    \n    {\\displaystyle \\ln(I_{0}/I)=\\tau =\\sigma \\ell n.}\n\nIf multiple species in the material interact with the radiation, then their absorbances add.  Thus a slightly more general formulation is that[14] \n  \n    \n      \n        \n          \n            \n              \n                τ\n              \n              \n                \n                =\n                ℓ\n                \n                  ∑\n                  \n                    i\n                  \n                \n                \n                  σ\n                  \n                    i\n                  \n                \n                \n                  n\n                  \n                    i\n                  \n                \n                ,\n              \n            \n            \n              \n                A\n              \n              \n                \n                =\n                ℓ\n                \n                  ∑\n                  \n                    i\n                  \n                \n                \n                  ε\n                  \n                    i\n                  \n                \n                \n                  c\n                  \n                    i\n                  \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\tau &=\\ell \\sum _{i}\\sigma _{i}n_{i},\\\\[4pt]A&=\\ell \\sum _{i}\\varepsilon _{i}c_{i},\\end{aligned}}}\n  \nwhere the sum is over all possible radiation-interacting (\"translucent\") species, and i indexes those species.\n\nIn situations where length may vary significantly, absorbance is sometimes summarized in terms of an attenuation coefficient \n  \n    \n      \n        \n          \n            \n              \n                \n                  μ\n                  \n                    10\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    A\n                    l\n                  \n                \n              \n              \n              \n                \n                =\n                ϵ\n                c\n              \n            \n            \n              \n                μ\n              \n              \n                \n                =\n                \n                  \n                    τ\n                    l\n                  \n                \n              \n              \n              \n                \n                =\n                σ\n                n\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{3}\\mu _{10}&={\\frac {A}{l}}&&=\\epsilon c\\\\\\mu &={\\frac {\\tau }{l}}&&=\\sigma n.\\end{alignedat}}}\n\nIn atmospheric science and radiation shielding applications, the attenuation coefficient may vary significantly through an inhomogenous material.  In those situations, the most general form of the Beer–Lambert law states that the total attenuation can be obtained by integrating the attenuation coefficient over small slices dz of the beamline: \n  \n    \n      \n        \n          \n            \n              \n                A\n              \n              \n                \n                =\n                ∫\n                \n                  \n                    μ\n                    \n                      10\n                    \n                  \n                  (\n                  z\n                  )\n                  \n                  d\n                  z\n                \n              \n              \n              \n                \n                =\n                ∫\n                \n                  \n                    ∑\n                    \n                      i\n                    \n                  \n                  \n                    \n                      ϵ\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      c\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                  \n                  \n                  d\n                  z\n                \n                ,\n              \n            \n            \n              \n                τ\n              \n              \n                \n                =\n                ∫\n                \n                  μ\n                  (\n                  z\n                  )\n                  \n                  d\n                  z\n                \n              \n              \n              \n                \n                =\n                ∫\n                \n                  \n                    ∑\n                    \n                      i\n                    \n                  \n                  \n                    \n                      σ\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      n\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                  \n                  \n                  d\n                  z\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{3}A&=\\int {\\mu _{10}(z)\\,dz}&&=\\int {\\sum _{i}{\\epsilon _{i}(z)c_{i}(z)}\\,dz},\\\\\\tau &=\\int {\\mu (z)\\,dz}&&=\\int {\\sum _{i}{\\sigma _{i}(z)n_{i}(z)}\\,dz}.\\end{alignedat}}}\n  \nThese formulations then reduce to the simpler versions when there is only one active species and the attenuation coefficients are constant.\n\nThere are two factors that determine the degree to which a medium containing particles will attenuate a light beam:  the number of particles encountered by the light beam, and the degree to which each particle extinguishes the light.[15]\n\nAssume that a beam of light enters a material sample. Define z as an axis parallel to the direction of the beam. Divide the material sample into thin slices, perpendicular to the beam of light, with thickness dz sufficiently small that one particle in a slice cannot obscure another particle in the same slice when viewed along the z direction. The radiant flux of the light that emerges from a slice is reduced, compared to that of the light that entered, by \n  \n    \n      \n        \n          d\n          \n            Φ\n            \n              e\n            \n          \n        \n        (\n        z\n        )\n        =\n        −\n        μ\n        (\n        z\n        )\n        \n          Φ\n          \n            \n              e\n            \n          \n        \n        (\n        z\n        )\n        \n          d\n        \n        z\n        ,\n      \n    \n    {\\displaystyle \\mathrm {d\\Phi _{e}} (z)=-\\mu (z)\\Phi _{\\mathrm {e} }(z)\\mathrm {d} z,}\n  \n where μ is the (Napierian) attenuation coefficient, which yields the following first-order linear, ordinary differential equation:\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  \n                    e\n                  \n                \n              \n              (\n              z\n              )\n            \n            \n              \n                d\n              \n              z\n            \n          \n        \n        =\n        −\n        μ\n        (\n        z\n        )\n        \n          Φ\n          \n            \n              e\n            \n          \n        \n        (\n        z\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{\\mathrm {e} }(z)}{\\mathrm {d} z}}=-\\mu (z)\\Phi _{\\mathrm {e} }(z).}\n  \n\nThe attenuation is caused by the photons that did not make it to the other side of the slice because of scattering or absorption. The solution to this differential equation is obtained by multiplying the integrating factor\n  \n    \n      \n        exp\n        ⁡\n        \n          (\n          \n            \n              ∫\n              \n                0\n              \n              \n                z\n              \n            \n            μ\n            (\n            \n              z\n              ′\n            \n            )\n            \n              d\n            \n            \n              z\n              ′\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\exp \\left(\\int _{0}^{z}\\mu (z')\\mathrm {d} z'\\right)}\n  \nthroughout to obtain\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  \n                    e\n                  \n                \n              \n              (\n              z\n              )\n            \n            \n              \n                d\n              \n              z\n            \n          \n        \n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              ∫\n              \n                0\n              \n              \n                z\n              \n            \n            μ\n            (\n            \n              z\n              ′\n            \n            )\n            \n              d\n            \n            \n              z\n              ′\n            \n          \n          )\n        \n        +\n        μ\n        (\n        z\n        )\n        \n          Φ\n          \n            \n              e\n            \n          \n        \n        (\n        z\n        )\n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              ∫\n              \n                0\n              \n              \n                z\n              \n            \n            μ\n            (\n            \n              z\n              ′\n            \n            )\n            \n              d\n            \n            \n              z\n              ′\n            \n          \n          )\n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{\\mathrm {e} }(z)}{\\mathrm {d} z}}\\,\\exp \\left(\\int _{0}^{z}\\mu (z')\\mathrm {d} z'\\right)+\\mu (z)\\Phi _{\\mathrm {e} }(z)\\,\\exp \\left(\\int _{0}^{z}\\mu (z')\\mathrm {d} z'\\right)=0,}\n  \nwhich simplifies due to the product rule (applied backwards) to\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              z\n            \n          \n        \n        \n          [\n          \n            \n              Φ\n              \n                \n                  e\n                \n              \n            \n            (\n            z\n            )\n            exp\n            ⁡\n            \n              (\n              \n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    z\n                  \n                \n                μ\n                (\n                \n                  z\n                  ′\n                \n                )\n                \n                  d\n                \n                \n                  z\n                  ′\n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        0.\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} z}}\\left[\\Phi _{\\mathrm {e} }(z)\\exp \\left(\\int _{0}^{z}\\mu (z')\\mathrm {d} z'\\right)\\right]=0.}\n\nIntegrating both sides and solving for Φe for a material of real thickness ℓ, with the incident radiant flux upon the slice \n  \n    \n      \n        \n          \n            Φ\n            \n              e\n            \n            \n              i\n            \n          \n        \n        =\n        \n          \n            Φ\n            \n              e\n            \n          \n        \n        (\n        0\n        )\n      \n    \n    {\\displaystyle \\mathrm {\\Phi _{e}^{i}} =\\mathrm {\\Phi _{e}} (0)}\n  \n and the transmitted radiant flux \n  \n    \n      \n        \n          \n            Φ\n            \n              e\n            \n            \n              t\n            \n          \n        \n        =\n        \n          \n            Φ\n            \n              e\n            \n          \n        \n        (\n        ℓ\n        )\n      \n    \n    {\\displaystyle \\mathrm {\\Phi _{e}^{t}} =\\mathrm {\\Phi _{e}} (\\ell )}\n  \n gives\n  \n    \n      \n        \n          \n            Φ\n            \n              e\n            \n            \n              t\n            \n          \n        \n        =\n        \n          \n            Φ\n            \n              e\n            \n            \n              i\n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              ∫\n              \n                0\n              \n              \n                ℓ\n              \n            \n            μ\n            (\n            z\n            )\n            \n              d\n            \n            z\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mathrm {\\Phi _{e}^{t}} =\\mathrm {\\Phi _{e}^{i}} \\exp \\left(-\\int _{0}^{\\ell }\\mu (z)\\mathrm {d} z\\right),}\n  \nand finally\n  \n    \n      \n        T\n        =\n        \n          \n            \n              Φ\n              \n                e\n              \n              \n                t\n              \n            \n            \n              Φ\n              \n                e\n              \n              \n                i\n              \n            \n          \n        \n        =\n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              ∫\n              \n                0\n              \n              \n                ℓ\n              \n            \n            μ\n            (\n            z\n            )\n            \n              d\n            \n            z\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle T=\\mathrm {\\frac {\\Phi _{e}^{t}}{\\Phi _{e}^{i}}} =\\exp \\left(-\\int _{0}^{\\ell }\\mu (z)\\mathrm {d} z\\right).}\n\nSince the decadic attenuation coefficient μ10 is related to the (Napierian) attenuation coefficient by \n  \n    \n      \n        \n          μ\n          \n            10\n          \n        \n        =\n        \n          \n            \n              μ\n              \n                ln\n                ⁡\n                10\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mu _{10}={\\tfrac {\\mu }{\\ln 10}},}\n  \n we also have\n  \n    \n      \n        \n          \n            \n              \n                T\n              \n              \n                \n                =\n                exp\n                ⁡\n                \n                  (\n                  \n                    −\n                    \n                      ∫\n                      \n                        0\n                      \n                      \n                        ℓ\n                      \n                    \n                    ln\n                    ⁡\n                    (\n                    10\n                    )\n                    \n                    \n                      μ\n                      \n                        10\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      d\n                    \n                    z\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  10\n                  \n                    \n                    \n                    ∧\n                  \n                \n                \n                \n                \n                  (\n                  \n                    −\n                    \n                      ∫\n                      \n                        0\n                      \n                      \n                        ℓ\n                      \n                    \n                    \n                      μ\n                      \n                        10\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      d\n                    \n                    z\n                  \n                  )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}T&=\\exp \\left(-\\int _{0}^{\\ell }\\ln(10)\\,\\mu _{10}(z)\\mathrm {d} z\\right)\\\\[4pt]&=10^{\\;\\!\\wedge }\\!\\!\\left(-\\int _{0}^{\\ell }\\mu _{10}(z)\\mathrm {d} z\\right).\\end{aligned}}}\n\nTo describe the attenuation coefficient in a way independent of the number densities ni of the N attenuating species of the material sample, one introduces the attenuation cross section \n  \n    \n      \n        \n          σ\n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \n                  μ\n                  \n                    i\n                  \n                \n                (\n                z\n                )\n              \n              \n                \n                  n\n                  \n                    i\n                  \n                \n                (\n                z\n                )\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{i}={\\tfrac {\\mu _{i}(z)}{n_{i}(z)}}.}\n  \n σi has the dimension of an area; it expresses the likelihood of interaction between the particles of the beam and the particles of the species i in the material sample:\n  \n    \n      \n        T\n        =\n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              σ\n              \n                i\n              \n            \n            \n              ∫\n              \n                0\n              \n              \n                ℓ\n              \n            \n            \n              n\n              \n                i\n              \n            \n            (\n            z\n            )\n            \n              d\n            \n            z\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle T=\\exp \\left(-\\sum _{i=1}^{N}\\sigma _{i}\\int _{0}^{\\ell }n_{i}(z)\\mathrm {d} z\\right).}\n\nOne can also use the molar attenuation coefficients \n  \n    \n      \n        \n          ε\n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \n                  N\n                  \n                    A\n                  \n                \n              \n              \n                ln\n                ⁡\n                10\n              \n            \n          \n        \n        \n          σ\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\varepsilon _{i}={\\tfrac {\\mathrm {N_{A}} }{\\ln 10}}\\sigma _{i},}\n  \n where NA is the Avogadro constant, to describe the attenuation coefficient in a way independent of the amount concentrations \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n        (\n        z\n        )\n        =\n        \n          n\n          \n            i\n          \n        \n        \n          \n            \n              z\n              \n                \n                  N\n                  \n                    A\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle c_{i}(z)=n_{i}{\\tfrac {z}{\\mathrm {N_{A}} }}}\n  \n of the attenuating species of the material sample:\n  \n    \n      \n        \n          \n            \n              \n                T\n              \n              \n                \n                =\n                exp\n                ⁡\n                \n                  (\n                  \n                    −\n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      \n                        \n                          ln\n                          ⁡\n                          (\n                          10\n                          )\n                        \n                        \n                          \n                            N\n                            \n                              A\n                            \n                          \n                        \n                      \n                    \n                    \n                      ε\n                      \n                        i\n                      \n                    \n                    \n                      ∫\n                      \n                        0\n                      \n                      \n                        ℓ\n                      \n                    \n                    \n                      n\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      d\n                    \n                    z\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                exp\n                ⁡\n                \n                  \n                    (\n                    \n                      −\n                      \n                        ∑\n                        \n                          i\n                          =\n                          1\n                        \n                        \n                          N\n                        \n                      \n                      \n                        ε\n                        \n                          i\n                        \n                      \n                      \n                        ∫\n                        \n                          0\n                        \n                        \n                          ℓ\n                        \n                      \n                      \n                        \n                          \n                            \n                              n\n                              \n                                i\n                              \n                            \n                            (\n                            z\n                            )\n                          \n                          \n                            \n                              N\n                              \n                                A\n                              \n                            \n                          \n                        \n                      \n                      \n                        d\n                      \n                      z\n                    \n                    )\n                  \n                  \n                    ln\n                    ⁡\n                    (\n                    10\n                    )\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  10\n                  \n                    \n                    \n                    ∧\n                  \n                \n                \n                \n                \n                  (\n                  \n                    −\n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      ε\n                      \n                        i\n                      \n                    \n                    \n                      ∫\n                      \n                        0\n                      \n                      \n                        ℓ\n                      \n                    \n                    \n                      c\n                      \n                        i\n                      \n                    \n                    (\n                    z\n                    )\n                    \n                      d\n                    \n                    z\n                  \n                  )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}T&=\\exp \\left(-\\sum _{i=1}^{N}{\\frac {\\ln(10)}{\\mathrm {N_{A}} }}\\varepsilon _{i}\\int _{0}^{\\ell }n_{i}(z)\\mathrm {d} z\\right)\\\\[4pt]&=\\exp \\left(-\\sum _{i=1}^{N}\\varepsilon _{i}\\int _{0}^{\\ell }{\\frac {n_{i}(z)}{\\mathrm {N_{A}} }}\\mathrm {d} z\\right)^{\\ln(10)}\\\\[4pt]&=10^{\\;\\!\\wedge }\\!\\!\\left(-\\sum _{i=1}^{N}\\varepsilon _{i}\\int _{0}^{\\ell }c_{i}(z)\\mathrm {d} z\\right).\\end{aligned}}}\n\nUnder certain conditions the Beer–Lambert law fails to maintain a linear relationship between attenuation and concentration of analyte.[16] These deviations are classified into three categories:\n\nThere are at least six conditions that need to be fulfilled in order for the Beer–Lambert law to be valid. These are:\n\nIf any of these conditions are not fulfilled, there will be deviations from the Beer–Lambert law.\n\nThe law tends to break down at very high concentrations, especially if the material is highly scattering. Absorbance within range of 0.2 to 0.5 is ideal to maintain linearity in the Beer–Lambert law. If the radiation is especially intense, nonlinear optical processes can also cause variances. The main reason, however, is that the concentration dependence is in general non-linear and Beer's law is valid only under certain conditions as shown by derivation below. For strong oscillators and at high concentrations the deviations are stronger. If the molecules are closer to each other interactions can set in. These interactions can be roughly divided into physical and chemical interactions. Physical interaction do not alter the polarizability of the molecules as long as the interaction is not so strong that light and molecular quantum state intermix (strong coupling), but cause the attenuation cross sections to be non-additive via electromagnetic coupling. Chemical interactions in contrast change the polarizability and thus absorption.\n\nIn solids, attenuation is usually an addition of absorption coefficient \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n (creation of electron-hole pairs) or scattering (for example Rayleigh scattering if the scattering centers are much smaller than the incident wavelength).[17] Also note that for some systems we can put \n  \n    \n      \n        1\n        \n          /\n        \n        λ\n      \n    \n    {\\displaystyle 1/\\lambda }\n  \n (1 over inelastic mean free path) in place of \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n.[18]\n\nThe BBL extinction law also arises as a solution to the BGK equation.\n\nThe Beer–Lambert law can be applied to the analysis of a mixture by spectrophotometry, without the need for extensive pre-processing of the sample. An example is the determination of bilirubin in blood plasma samples. The spectrum of pure bilirubin is known, so the molar attenuation coefficient ε is known. Measurements of decadic attenuation coefficient μ10 are made at one wavelength λ that is nearly unique for bilirubin and at a second wavelength in order to correct for possible interferences. The amount concentration c is then given by\n\n  \n    \n      \n        c\n        =\n        \n          \n            \n              \n                μ\n                \n                  10\n                \n              \n              (\n              λ\n              )\n            \n            \n              ε\n              (\n              λ\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle c={\\frac {\\mu _{10}(\\lambda )}{\\varepsilon (\\lambda )}}.}\n\nFor a more complicated example, consider a mixture in solution containing two species at amount concentrations c1 and c2. The decadic attenuation coefficient at any wavelength λ is, given by\n\n  \n    \n      \n        \n          μ\n          \n            10\n          \n        \n        (\n        λ\n        )\n        =\n        \n          ε\n          \n            1\n          \n        \n        (\n        λ\n        )\n        \n          c\n          \n            1\n          \n        \n        +\n        \n          ε\n          \n            2\n          \n        \n        (\n        λ\n        )\n        \n          c\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mu _{10}(\\lambda )=\\varepsilon _{1}(\\lambda )c_{1}+\\varepsilon _{2}(\\lambda )c_{2}.}\n\nTherefore, measurements at two wavelengths yields two equations in two unknowns and will suffice to determine the amount concentrations c1 and c2 as long as the molar attenuation coefficients of the two components, ε1 and ε2 are known at both wavelengths. This two system equation can be solved using Cramer's rule. In practice it is better to use linear least squares to determine the two amount concentrations from measurements made at more than two wavelengths. Mixtures containing more than two components can be analyzed in the same way, using a minimum of N wavelengths for a mixture containing N components.\n\nThe law is used widely in infra-red spectroscopy and near-infrared spectroscopy for analysis of polymer degradation and oxidation (also in biological tissue) as well as to measure the concentration of various compounds in different food samples. The carbonyl group attenuation at about 6 micrometres can be detected quite easily, and degree of oxidation of the polymer calculated.\n\nThe Bouguer–Lambert law may be applied to describe the attenuation of solar or stellar radiation as it travels through the atmosphere. In this case, there is scattering of radiation as well as absorption. The optical depth for a slant path is τ′ = mτ, where τ refers to a vertical path, m is called the relative airmass, and for a plane-parallel atmosphere it is determined as m = sec θ where θ is the zenith angle corresponding to the given path. The Bouguer-Lambert law for the atmosphere is usually written\n\n  \n    \n      \n        T\n        =\n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        m\n        (\n        \n          τ\n          \n            \n              a\n            \n          \n        \n        +\n        \n          τ\n          \n            \n              g\n            \n          \n        \n        +\n        \n          τ\n          \n            \n              R\n              S\n            \n          \n        \n        +\n        \n          τ\n          \n            \n              N\n              \n                O\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \n          τ\n          \n            \n              w\n            \n          \n        \n        +\n        \n          τ\n          \n            \n              \n                O\n                \n                  3\n                \n              \n            \n          \n        \n        +\n        \n          τ\n          \n            \n              r\n            \n          \n        \n        +\n        ⋯\n        )\n        \n          \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle T=\\exp {\\big (}-m(\\tau _{\\mathrm {a} }+\\tau _{\\mathrm {g} }+\\tau _{\\mathrm {RS} }+\\tau _{\\mathrm {NO_{2}} }+\\tau _{\\mathrm {w} }+\\tau _{\\mathrm {O_{3}} }+\\tau _{\\mathrm {r} }+\\cdots ){\\bigr )},}\n  \n\nwhere each τx is the optical depth whose subscript identifies the source of the absorption or scattering it describes:\n\nm is the optical mass or airmass factor, a term approximately equal (for small and moderate values of θ) to ⁠\n  \n    \n      \n        \n          \n            \n              1\n              \n                cos\n                ⁡\n                θ\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\tfrac {1}{\\cos \\theta }},}\n  \n⁠ where θ is the observed object's zenith angle (the angle measured from the direction perpendicular to the Earth's surface at the observation site). This equation can be used to retrieve τa, the aerosol optical thickness, which is necessary for the correction of satellite images and also important in accounting for the role of aerosols in climate.",
        pageTitle: "Beer–Lambert law",
    },
    {
        title: "Benford's law",
        link: "https://en.wikipedia.org/wiki/Benford%27s_law",
        content:
            "Benford's law, also known as the Newcomb–Benford law, the law of anomalous numbers, or the first-digit law, is an observation that in many real-life sets of numerical data, the leading digit is likely to be small.[1] In sets that obey the law, the number 1 appears as the leading significant digit about 30% of the time, while 9 appears as the leading significant digit less than 5% of the time. Uniformly distributed digits would each occur about 11.1% of the time.[2] Benford's law also makes predictions about the distribution of second digits, third digits, digit combinations, and so on.\n\nThe graph to the right shows Benford's law for base 10, one of infinitely many cases of a generalized law regarding numbers expressed in arbitrary (integer) bases, which rules out the possibility that the phenomenon might be an artifact of the base-10 number system. Further generalizations published in 1995[3] included analogous statements for both the nth leading digit and the joint distribution of the leading n digits, the latter of which leads to a corollary wherein the significant digits are shown to be a statistically dependent quantity.\n\nIt has been shown that this result applies to a wide variety of data sets, including electricity bills, street addresses, stock prices, house prices, population numbers, death rates, lengths of rivers, and physical and mathematical constants.[4] Like other general principles about natural data—for example, the fact that many data sets are well approximated by a normal distribution—there are illustrative examples and explanations that cover many of the cases where Benford's law applies, though there are many other cases where Benford's law applies that resist simple explanations.[5][6] Benford's law tends to be most accurate when values are distributed across multiple orders of magnitude, especially if the process generating the numbers is described by a power law (which is common in nature).\n\nThe law is named after physicist Frank Benford, who stated it in 1938 in an article titled \"The Law of Anomalous Numbers\",[7] although it had been previously stated by Simon Newcomb in 1881.[8][9]\n\nThe law is similar in concept, though not identical in distribution, to Zipf's law.\n\nA set of numbers is said to satisfy Benford's law if the leading digit d (d ∈ {1, ..., 9}) occurs with probability[10]\n\nThe leading digits in such a set thus have the following distribution:\n\nThe quantity ⁠\n  \n    \n      \n        P\n        (\n        d\n        )\n      \n    \n    {\\displaystyle P(d)}\n  \n⁠ is proportional to the space between d and d + 1 on a logarithmic scale. Therefore, this is the distribution expected if the logarithms of the numbers (but not the numbers themselves) are uniformly and randomly distributed.\n\nFor example, a number x, constrained to lie between 1 and 10, starts with the digit 1 if 1 ≤ x < 2, and starts with the digit 9 if 9 ≤ x < 10. Therefore, x starts with the digit 1 if log 1 ≤ log  x < log 2, or starts with 9 if log 9 ≤ log x < log 10. The interval [log 1, log 2] is much wider than the interval [log 9, log 10] (0.30 and 0.05 respectively); therefore if log x is uniformly and randomly distributed, it is much more likely to fall into the wider interval than the narrower interval, i.e. more likely to start with 1 than with 9; the probabilities are proportional to the interval widths, giving the equation above (as well as the generalization to other bases besides decimal).\n\nBenford's law is sometimes stated in a stronger form, asserting that the fractional part of the logarithm of data is typically close to uniformly distributed between 0 and 1; from this, the main claim about the distribution of first digits can be derived.[5]\n\nAn extension of Benford's law predicts the distribution of first digits in other bases besides decimal; in fact, any base b ≥ 2. The general form is[12]\n\nFor b = 2, 1 (the binary and unary) number systems, Benford's law is true but trivial: All binary and unary numbers (except for 0 or the empty set) start with the digit 1. (On the other hand, the generalization of Benford's law to second and later digits is not trivial, even for binary numbers.[13])\n\nExamining a list of the heights of the 58 tallest structures in the world by category shows that 1 is by far the most common leading digit, irrespective of the unit of measurement (see \"scale invariance\" below):\n\nAnother example is the leading digit of 2n. The sequence of the first 96 leading digits (1, 2, 4, 8, 1, 3, 6, 1, 2, 5, 1, 2, 4, 8, 1, 3, 6, 1, ... (sequence A008952 in the OEIS)) exhibits closer adherence to Benford’s law than is expected for random sequences of the same length, because it is derived from a geometric sequence.[14]\n\nThe discovery of Benford's law goes back to 1881, when the Canadian-American astronomer Simon Newcomb noticed that in logarithm tables the earlier pages (that started with 1) were much more worn than the other pages.[8] Newcomb's published result is the first known instance of this observation and includes a distribution on the second digit as well. Newcomb proposed a law that the probability of a single number N being the first digit of a number was equal to log(N + 1) − log(N).\n\nThe phenomenon was again noted in 1938 by the physicist Frank Benford,[7] who tested it on data from 20 different domains and was credited for it. His data set included the surface areas of 335 rivers, the sizes of 3259 US populations, 104 physical constants, 1800 molecular weights, 5000 entries from a mathematical handbook, 308 numbers contained in an issue of Reader's Digest, the street addresses of the first 342 persons listed in American Men of Science and 418 death rates. The total number of observations used in the paper was 20,229. This discovery was later named after Benford (making it an example of Stigler's law).\n\nIn 1995, Ted Hill proved the result about mixed distributions mentioned below.[15][16]\n\nBenford's law tends to apply most accurately to data that span several orders of magnitude. As a rule of thumb, the more orders of magnitude that the data evenly covers, the more accurately Benford's law applies. For instance, one can expect that Benford's law would apply to a list of numbers representing the populations of United Kingdom settlements. But if a \"settlement\" is defined as a village with population between 300 and 999, then Benford's law will not apply.[17][18]\n\nConsider the probability distributions shown below, referenced to a log scale. In each case, the total area in red is the relative probability that the first digit is 1, and the total area in blue is the relative probability that the first digit is 8. For the first distribution, the size of the areas of red and blue are approximately proportional to the widths of each red and blue bar. Therefore, the numbers drawn from this distribution will approximately follow Benford's law. On the other hand, for the second distribution, the ratio of the areas of red and blue is very different from the ratio of the widths of each red and blue bar. Rather, the relative areas of red and blue are determined more by the heights of the bars than the widths. Accordingly, the first digits in this distribution do not satisfy Benford's law at all.[18]\n\nThus, real-world distributions that span several orders of magnitude rather uniformly (e.g., stock-market prices and populations of villages, towns, and cities) are likely to satisfy Benford's law very accurately. On the other hand, a distribution mostly or entirely within one order of magnitude (e.g., IQ scores or heights of human adults) is unlikely to satisfy Benford's law very accurately, if at all.[17][18] However, the difference between applicable and inapplicable regimes is not a sharp cut-off: as the distribution gets narrower, the deviations from Benford's law increase gradually.\n\n(This discussion is not a full explanation of Benford's law, because it has not explained why data sets are so often encountered that, when plotted as a probability distribution of the logarithm of the variable, are relatively uniform over several orders of magnitude.[19])\n\nIn 1970 Wolfgang Krieger proved what is now called the Krieger generator theorem.[20][21] The Krieger generator theorem might be viewed as a justification for the assumption in the Kafri ball-and-box model that, in a given base \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n with a fixed number of digits 0, 1, ..., n, ..., \n  \n    \n      \n        B\n        −\n        1\n      \n    \n    {\\displaystyle B-1}\n  \n, digit n is equivalent to a Kafri box containing n non-interacting balls. Other scientists and statisticians have suggested entropy-related explanations[which?] for Benford's law.[22][23][10][24]\n\nMany real-world examples of Benford's law arise from multiplicative fluctuations.[25] For example, if a stock price starts at $100, and then each day it gets multiplied by a randomly chosen factor between 0.99 and 1.01, then over an extended period the probability distribution of its price satisfies Benford's law with higher and higher accuracy.\n\nThe reason is that the logarithm of the stock price is undergoing a random walk, so over time its probability distribution will get more and more broad and smooth (see above).[25] (More technically, the central limit theorem says that multiplying more and more random variables will create a log-normal distribution with larger and larger variance, so eventually it covers many orders of magnitude almost uniformly.) To be sure of approximate agreement with Benford's law, the distribution has to be approximately invariant when scaled up by any factor up to 10; a log-normally distributed data set with wide dispersion would have this approximate property.\n\nUnlike multiplicative fluctuations, additive fluctuations do not lead to Benford's law: They lead instead to normal probability distributions (again by the central limit theorem), which do not satisfy Benford's law. By contrast, that hypothetical stock price described above can be written as the product of many random variables (i.e. the price change factor for each day), so is likely to follow Benford's law quite well.\n\nAnton Formann provided an alternative explanation by directing attention to the interrelation between the distribution of the significant digits and the distribution of the observed variable. He showed in a simulation study that long-right-tailed distributions of a random variable are compatible with the Newcomb–Benford law, and that for distributions of the ratio of two random variables the fit generally improves.[26] For numbers drawn from certain distributions (IQ scores, human heights) the Benford's law fails to hold because these variates obey a normal distribution, which is known not to satisfy Benford's law,[9] since normal distributions can't span several orders of magnitude and the Significand of their logarithms will not be (even approximately) uniformly distributed. However, if one \"mixes\" numbers from those distributions, for example, by taking numbers from newspaper articles, Benford's law reappears. This can also be proven mathematically: if one repeatedly \"randomly\" chooses a probability distribution (from an uncorrelated set) and then randomly chooses a number according to that distribution, the resulting list of numbers will obey Benford's law.[15][27] A similar probabilistic explanation for the appearance of Benford's law in everyday-life numbers has been advanced by showing that it arises naturally when one considers mixtures of uniform distributions.[28]\n\nIn a list of lengths, the distribution of first digits of numbers in the list may be generally similar regardless of whether all the lengths are expressed in metres, yards, feet, inches, etc. The same applies to monetary units.\n\nThis is not always the case. For example, the height of adult humans almost always starts with a 1 or 2 when measured in metres and almost always starts with 4, 5, 6, or 7 when measured in feet. But in a list of lengths spread evenly over many orders of magnitude—for example, a list of 1000 lengths mentioned in scientific papers that includes the measurements of molecules, bacteria, plants, and galaxies—it is reasonable to expect the distribution of first digits to be the same no matter whether the lengths are written in metres or in feet.\n\nWhen the distribution of the first digits of a data set is scale-invariant (independent of the units that the data are expressed in), it is always given by Benford's law.[29][30]\n\nFor example, the first (non-zero) digit on the aforementioned list of lengths should have the same distribution whether the unit of measurement is feet or yards. But there are three feet in a yard, so the probability that the first digit of a length in yards is 1 must be the same as the probability that the first digit of a length in feet is 3, 4, or 5; similarly, the probability that the first digit of a length in yards is 2 must be the same as the probability that the first digit of a length in feet is 6, 7, or 8. Applying this to all possible measurement scales gives the logarithmic distribution of Benford's law.\n\nBenford's law for first digits is base invariant for number systems. There are conditions and proofs of sum invariance, inverse invariance, and addition and subtraction invariance.[31][32]\n\nIn 1972, Hal Varian suggested that the law could be used to detect possible fraud in lists of socio-economic data submitted in support of public planning decisions. Based on the plausible assumption that people who fabricate figures tend to distribute their digits fairly uniformly, a simple comparison of first-digit frequency distribution from the data with the expected distribution according to Benford's law ought to show up any anomalous results.[33]\n\nIn the United States, evidence based on Benford's law has been admitted in criminal cases at the federal, state, and local levels.[34]\n\nWalter Mebane, a political scientist and statistician at the University of Michigan, was the first to apply the second-digit Benford's law-test (2BL-test) in election forensics.[35] Such analysis is considered a simple, though not foolproof, method of identifying irregularities in election results.[36] Scientific consensus to support the applicability of Benford's law to elections has not been reached in the literature. A 2011 study by the political scientists Joseph Deckert, Mikhail Myagkov, and Peter C. Ordeshook argued that Benford's law is problematic and misleading as a statistical indicator of election fraud.[37] Their method was criticized by Mebane in a response, though he agreed that there are many caveats to the application of Benford's law to election data.[38]\n\nBenford's law has been used as evidence of fraud in the 2009 Iranian elections.[39] An analysis by Mebane found that the second digits in vote counts for President Mahmoud Ahmadinejad, the winner of the election, tended to differ significantly from the expectations of Benford's law, and that the ballot boxes with very few invalid ballots had a greater influence on the results, suggesting widespread ballot stuffing.[40] Another study used bootstrap simulations to find that the candidate Mehdi Karroubi received almost twice as many vote counts beginning with the digit 7 as would be expected according to Benford's law,[41] while an analysis from Columbia University concluded that the probability that a fair election would produce both too few non-adjacent digits and the suspicious deviations in last-digit frequencies as found in the 2009 Iranian presidential election is less than 0.5 percent.[42] Benford's law has also been applied for forensic auditing and fraud detection on data from the 2003 California gubernatorial election,[43] the 2000 and 2004 United States presidential elections,[44] and the 2009 German federal election.[45] The Benford's Law Test was found to be \"worth taking seriously as a statistical test for fraud,\" although \"the test is not sensitive to distortions we know significantly affected many votes. In particular, the test does not indicate problems for Florida in 2000.\"[44]\n\nBenford's law has also been misapplied to claim election fraud. When applying the law to Joe Biden's election returns for Chicago, Milwaukee, and other localities in the 2020 United States presidential election, the distribution of the first digit did not follow Benford's law. The misapplication was a result of looking at data that was tightly bound in range, which violates the assumption inherent in Benford's law that the range of the data be large. The first digit test was applied to precinct-level data, but because precincts rarely receive more than a few thousand votes or fewer than several dozen, Benford's law cannot be expected to apply. According to Mebane, \"It is widely understood that the first digits of precinct vote counts are not useful for trying to diagnose election frauds.\"[46][47]\n\nSimilarly, the macroeconomic data the Greek government reported to the European Union before entering the eurozone was shown to be probably fraudulent using Benford's law, albeit years after the country joined.[48][49]\n\nResearchers have used Benford's law to detect psychological pricing patterns, in a Europe-wide study in consumer product prices before and after euro was introduced in 2002.[50] The idea was that, without psychological pricing, the first two or three digits of price of items should follow Benford's law. Consequently, if the distribution of digits deviates from Benford's law (such as having a lot of 9's), it means merchants may have used psychological pricing.\n\nWhen the euro replaced local currencies in 2002, for a brief period of time, the price of goods in euro was simply converted from the price of goods in local currencies before the replacement. As it is essentially impossible to use psychological pricing simultaneously on both price-in-euro and price-in-local-currency, during the transition period, psychological pricing would be disrupted even if it used to be present. It can only be re-established once consumers have gotten used to prices in a single currency again, this time in euro.\n\nAs the researchers expected, the distribution of first price digit followed Benford's law, but the distribution of the second and third digits deviated significantly from Benford's law before the introduction, then deviated less during the introduction, then deviated more again after the introduction.\n\nThe number of open reading frames and their relationship to genome size differs between eukaryotes and prokaryotes with the former showing a log-linear relationship and the latter a linear relationship. Benford's law has been used to test this observation with an excellent fit to the data in both cases.[51]\n\nA test of regression coefficients in published papers showed agreement with Benford's law.[52] As a comparison group subjects were asked to fabricate statistical estimates. The fabricated results conformed to Benford's law on first digits, but failed to obey Benford's law on second digits.\n\nTesting the number of published scientific papers of all registered researchers in Slovenia's national database was shown to strongly conform to Benford's law.[53] Moreover, the authors were grouped by scientific field, and tests indicate natural sciences exhibit greater conformity than social sciences.\n\nAlthough the chi-squared test has been used to test for compliance with Benford's law it has low statistical power when used with small samples.\n\nThe Kolmogorov–Smirnov test and the Kuiper test are more powerful when the sample size is small, particularly when Stephens's corrective factor is used.[54] These tests may be unduly conservative when applied to discrete distributions. Values for the Benford test have been generated by Morrow.[55] The critical values of the test statistics are shown below:\n\nThese critical values provide the minimum test statistic values required to reject the hypothesis of compliance with Benford's law at the given significance levels.\n\nTwo alternative tests specific to this law have been published: First, the max (m) statistic[56] is given by\n\nThe leading factor \n  \n    \n      \n        \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {N}}}\n  \n does not appear in the original formula by Leemis;[56] it was added by Morrow in a later paper.[55]\n\nSecondly, the distance (d) statistic[57] is given by\n\nwhere FSD is the first significant digit and N is the sample size. Morrow has determined the critical values for both these statistics, which are shown below:[55]\n\nMorrow has also shown that for any random variable  X  (with a continuous PDF) divided by its standard deviation (σ), some value  A  can be found so that the probability of the distribution of the first significant digit of the random variable \n  \n    \n      \n        \n          |\n        \n        X\n        \n          /\n        \n        σ\n        \n          \n            |\n          \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle |X/\\sigma |^{A}}\n  \n will differ from Benford's law by less than ε > 0.[55] The value of A depends on the value of ε and the distribution of the random variable.\n\nA method of accounting fraud detection based on bootstrapping and regression has been proposed.[58]\n\nIf the goal is to conclude agreement with the Benford's law rather than disagreement, then the goodness-of-fit tests mentioned above are inappropriate. In this case the specific tests for equivalence should be applied. An empirical distribution is called equivalent to the Benford's law if a distance (for example total variation distance or the usual Euclidean distance) between the probability mass functions is sufficiently small. This method of testing with application to Benford's law is described in Ostrovski.[59]\n\nSome well-known infinite integer sequences provably satisfy Benford's law exactly (in the asymptotic limit as more and more terms of the sequence are included). Among these are the Fibonacci numbers,[60][61] the factorials,[62] the powers of 2,[63][14] and the powers of almost any other number.[63]\n\nLikewise, some continuous processes satisfy Benford's law exactly (in the asymptotic limit as the process continues through time). One is an exponential growth or decay process: If a quantity is exponentially increasing or decreasing in time, then the percentage of time that it has each first digit satisfies Benford's law asymptotically (i.e. increasing accuracy as the process continues through time).\n\nThe square roots and reciprocals of successive natural numbers do not obey this law.[64] Prime numbers in a finite range follow a Generalized Benford’s law, that approaches uniformity as the size of the range approaches infinity.[65] Lists of local telephone numbers violate Benford's law.[66] Benford's law is violated by the populations of all places with a population of at least 2500 individuals from five US states according to the 1960 and 1970 censuses, where only 19 % began with digit 1 but 20 % began with digit 2, because truncation at 2500 introduces statistical bias.[64] The terminal digits in pathology reports violate Benford's law due to rounding.[67]\n\nDistributions that do not span several orders of magnitude will not follow Benford's law. Examples include height, weight, and IQ scores.[9][68]\n\nA number of criteria, applicable particularly to accounting data, have been suggested where Benford's law can be expected to apply.[69]\n\nMathematically, Benford’s law applies if the distribution being tested fits the \"Benford’s law compliance theorem\".[17] The derivation says that Benford's law is followed if the Fourier transform of the logarithm of the probability density function is zero for all integer values. Most notably, this is satisfied if the Fourier transform is zero (or negligible) for n ≥ 1. This is satisfied if the distribution is wide (since wide distribution implies a narrow Fourier transform). Smith summarizes thus (p. 716):\n\nBenford's law is followed by distributions that are wide compared with unit distance along the logarithmic scale. Likewise, the law is not followed by distributions that are narrow compared with unit distance … If the distribution is wide compared with unit distance on the log axis, it means that the spread in the set of numbers being examined is much greater than ten.\n\nIn short, Benford’s law requires that the numbers in the distribution being measured have a spread across at least an order of magnitude.\n\nBenford's law was empirically tested against the numbers (up to the 10th digit) generated by a number of important distributions, including the uniform distribution, the exponential distribution, the normal distribution, and others.[9]\n\nThe uniform distribution, as might be expected, does not obey Benford's law. In contrast, the ratio distribution of two uniform distributions is well-described by Benford's law.\n\nNeither the normal distribution nor the ratio distribution of two normal distributions (the Cauchy distribution) obey Benford's law. Although the half-normal distribution does not obey Benford's law, the ratio distribution of two half-normal distributions does. Neither the right-truncated normal distribution nor the ratio distribution of two right-truncated normal distributions are well described by Benford's law. This is not surprising as this distribution is weighted towards larger numbers.\n\nBenford's law also describes the exponential distribution and the ratio distribution of two exponential distributions well. The fit of chi-squared distribution depends on the degrees of freedom (df) with good agreement with df = 1 and decreasing agreement as the df increases. The F-distribution is fitted well for low degrees of freedom. With increasing dfs the fit decreases but much more slowly than the chi-squared distribution. The fit of the log-normal distribution depends on the mean and the variance of the distribution. The variance has a much greater effect on the fit than does the mean. Larger values of both parameters result in better agreement with the law. The ratio of two log normal distributions is a log normal so this distribution was not examined.\n\nOther distributions that have been examined include the Muth distribution, Gompertz distribution, Weibull distribution, gamma distribution, log-logistic distribution and the exponential power distribution all of which show reasonable agreement with the law.[56][70] The Gumbel distribution – a density increases with increasing value of the random variable – does not show agreement with this law.[70]\n\nIt is possible to extend the law to digits beyond the first.[71] In particular, for any given number of digits, the probability of encountering a number starting with the string of digits n of that length –  discarding leading zeros –  is given by\n\nThus, the probability that a number starts with the digits 3, 1, 4  (some examples are 3.14, 3.142, π, 314280.7, and 0.00314005) is log10(1 + 1/314) ≈ 0.00138, as in the box with the log-log graph on the right.\n\nThis result can be used to find the probability that a particular digit occurs at a given position within a number. For instance, the probability that a \"2\" is encountered as the second digit is[71]\n\nAnd the probability that d (d = 0, 1, ..., 9) is encountered as the n-th (n > 1) digit is\n\nThe distribution of the n-th digit, as n increases, rapidly approaches a uniform distribution with 10% for each of the ten digits, as shown below.[71] Four digits is often enough to assume a uniform distribution of 10% as \"0\" appears 10.0176% of the time in the fourth digit, while \"9\" appears 9.9824% of the time.\n\nAverage and moments of random variables for the digits 1 to 9 following this law have been calculated:[72]\n\nFor the two-digit distribution according to Benford's law these values are also known:[73]\n\nA table of the exact probabilities for the joint occurrence of the first two digits according to Benford's law is available,[73] as is the population correlation between the first and second digits:[73] ρ = 0.0561.\n\nBenford's law has appeared as a plot device in some twenty-first century popular entertainment.",
        pageTitle: "Benford's law",
    },
    {
        title: "Benford's law of controversy",
        link: "https://en.wikipedia.org/wiki/Benford%27s_law_of_controversy",
        content:
            "Gregory Benford (born January 30, 1941) is an American science fiction author and astrophysicist who is professor emeritus at the department of physics and astronomy at the University of California, Irvine. He is a contributing editor of Reason magazine.[1]\n\nBenford wrote the Galactic Center Saga science fiction novels, beginning with In the Ocean of Night (1977).[2]  The series postulates a galaxy in which sentient organic life is in constant warfare with sentient electromechanical life.\n\nIn 1969 he wrote \"The Scarred Man\",[3][4] the first story about a computer virus (based on a real computer virus he had spread[5]),[6] published in 1970.\n\nBenford was born in Mobile, Alabama and grew up in Robertsdale and Fairhope.[7] Graduating Phi Beta Kappa, he received a Bachelor of Science in physics in 1963 from the University of Oklahoma in Norman, Oklahoma, followed by a Master of Science from the University of California, San Diego in 1965, and a doctorate there in 1967. That same year he married Joan Abbe, with whom he had two children.[8] Benford modeled characters in several of his novels after his wife, most prominently the heroine of Artifact. She died in 2002.[9]\n\nBenford has an identical twin brother, James (Jim) Benford, with whom he has collaborated on science fiction stories.[10] Both got their start in science fiction fandom, with Gregory being a co-editor of the science fiction fanzine Void. At one point, Benford said he was an atheist because he could not reconcile the evil in the world with a benevolent God.[11] However, he has returned to the Episcopal Church where he is a communicant at St. Mary's Episcopal Church in Laguna Beach.[12]\n\nHe has been a long-time resident of Laguna Beach, California.[8]\n\nGregory Benford's first professional sale was the story \"Stand-In\" in the Magazine of Fantasy and Science Fiction (June 1965), which won second prize in a short story contest based on a poem by Doris Pitkin Buck. In 1969, he began writing a science column for Amazing Stories.\n\nBenford tends to write hard science fiction which incorporates the research he is doing as a practical scientist.  He has worked on collaborations with authors William Rotsler, David Brin and Gordon Eklund. His time-travel novel Timescape (1980) won both the Nebula Award and the John W. Campbell Memorial Award.  This scientific procedural novel eventually loaned its title to a line of science fiction published by Pocket Books. In the late 1990s, he wrote Foundation's Fear, one of an authorized sequel trilogy to Isaac Asimov's Foundation series.  Other novels published in that period include several near-future science thrillers: Cosm (1998), The Martian Race (1999) and Eater (2000).[citation needed]\n\nBenford has served as an editor of numerous alternate history anthologies, as well as collections of Hugo Award winners.\n\nHe has been nominated for four Hugo Awards (for two short stories and two novellas) and 13 Nebula Awards (in all categories).  In addition to Timescape, he won the Nebula for the novelette \"If the Stars Are Gods\" (with Eklund).\n\nBenford was a guest of honour at Aussiecon Three, the 1999 Worldcon. He remains[as of?] a regular contributor to science fiction fanzines, for example Apparatchik (defunct as of 1997).\n\nIn 2016 Benford was the recipient of the Los Angeles Science Fantasy Society Forry Award Lifetime Achievement Award in the Field of Science Fiction.[13]\n\nGregory Benford is Professor Emeritus of Physics at the University of California, Irvine.  With more than 200 scientific publications, his research encompassed both theory and experiments in the fields of astrophysics and plasma physics. His research has been supported by NSF, NASA, AFOSR, DOE and other agencies. He is an ongoing[when?] advisor to NASA, DARPA (Defense Advanced Research Projects Agency) and the CIA.\n\nBenford's work in physics at the University of California focused on theoretical and experimental plasma physics, including studies of extremely strong turbulence, particularly in astrophysical contexts, and studies of magnetic structures from the Galactic Center to large-scale galactic jets.  Working in collaboration with, among others, science fiction writers Cramer, Forward, and Landis, Benford worked on a theoretical study of the physics of wormholes, which pointed out that wormholes, if formed in the early universe, could still exist in the present day if they were wrapped in a negative-mass cosmic string.[14]  Such wormholes could potentially be detected by gravitational lensing.\n\nIn 2004, Benford proposed that the harmful effects of global warming could be reduced by the construction of a rotating Fresnel lens 1,000 kilometres across, floating in space at the Lagrangian point L1. According to Benford, this lens would diffuse the light from the Sun and reduce the solar energy reaching the Earth by approximately 0.5% to 1%. He estimated that this would cost around US$10 billion. His plan has been commented on in a variety of forums.[15] A similar space sunshade was proposed in 1989 by J. T. Early,[16] and again in 1997 by Edward Teller, Lowell Wood, and Roderick Hyde.[17] In 2006, Benford pointed out one possible danger in this approach: if this lens were built and global warming were avoided, there would be less incentive to reduce greenhouse gases, and humans might continue to produce too much carbon dioxide until it caused some other environmental catastrophe, such as a chemical change in ocean water that could be disastrous to ocean life.[18]\n\nBenford serves on the board of directors and the steering committee of the Mars Society.\n\nHe has advocated human cryopreservation, for example by signing an open letter to support research into cryonics,[19] being a member of Alcor,[20] and by being an advisor to a UK cryonics and cryopreservation advocacy group.[21]\n\nGregory Benford retired from the University of California in 2006 in order to found and develop Genescient Corporation. Genescient is a new generation biotechnology company that claims to combine evolutionary genomics with massive selective screening to analyze and exploit the genetics of model animal and human whole genomes.\n\nBenford's law of controversy is an adage from the 1980 novel Timescape:[22]\n\nPassion is inversely proportional to the amount of real information available.[23][24]\n\nThe adage was quoted in an international drug policy article in a peer-reviewed social science journal.[25]",
        pageTitle: "Gregory Benford",
    },
    {
        title: "Bennett's laws",
        link: "https://en.wikipedia.org/wiki/Bennett%27s_laws",
        content:
            'where \n  \n    \n      \n        ⩾\n      \n    \n    {\\displaystyle \\geqslant }\n  \n indicates "can do the job of".\n\nThese principles were formulated around 1993 by Charles H. Bennett.\n\nThis quantum mechanics-related article is a stub. You can help Wikipedia by expanding it.\n\nThis computing article is a stub. You can help Wikipedia by expanding it.',
        pageTitle: "Bennett's laws",
    },
    {
        title: "Betteridge's law of headlines",
        link: "https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines",
        content:
            'Betteridge\'s law of headlines is an adage that states: "Any headline that ends in a question mark can be answered by the word no." It is named after Ian Betteridge, a British technology journalist who wrote about it in 2009, although the principle is much older.[1][2] It is based on the assumption that if the publishers were confident that the answer was yes, they would have presented it as an assertion; by presenting it as a question, they are not accountable for whether it is correct or not. The adage does not apply to questions that are more open-ended than strict yes–no questions.[3] For example, "What Should We Expect From Evolving Import-Export Policy?" is an open-ended question, whereas "Should We Expect an Embargo on Widgets?" is of closed form.\n\nThe maxim has been cited by other names since 1991, when a published compilation of Murphy\'s law variants called it "Davis\'s law", a name that also appears online without any explanation of who Davis was.[4][5][6][7] It has also been referred to as the "journalistic principle" and in 2007 was referred to in commentary as "an old truism among journalists".[8][9][10]\n\nBetteridge\'s name became associated with the concept after he discussed it in a February 2009 article, which examined a previous TechCrunch article that carried the headline "Did Last.fm Just Hand Over User Listening Data to the RIAA?":[11]\n\nThis story is a great demonstration of my maxim that any headline which ends in a question mark can be answered by the word "no." The reason why journalists use that style of headline is that they know the story is probably bullshit, and don\'t actually have the sources and facts to back it up, but still want to run it.[1]\n\nA similar observation was made by British newspaper editor Andrew Marr in his 2004 book My Trade, among Marr\'s suggestions for how a reader should interpret newspaper articles:\n\nIf the headline asks a question, try answering \'no\'. Is This the True Face of Britain\'s Young? (Sensible reader: No.) Have We Found the Cure for AIDS? (No; or you wouldn\'t have put the question mark in.) Does This Map Provide the Key for Peace? (Probably not.) A headline with a question mark at the end means, in the vast majority of cases, that the story is tendentious or over-sold. It is often a scare story, or an attempt to elevate some run-of-the-mill piece of reporting into a national controversy and, preferably, a national panic. To a busy journalist hunting for real information a question mark means \'don\'t bother reading this bit\'.[12]\n\nA 2016 study of a sample of academic journals (not news publications) that set out to test Betteridge\'s law and Hinchliffe\'s rule (see below) found that few titles were posed as questions and of those that were questions, few were yes/no questions and they were more often answered "yes" in the body of the article rather than "no".[13]\n\nA 2018 study of 2,585 articles in four academic journals in the field of ecology similarly found that very few titles were posed as questions at all, with 1.82 percent being wh-questions and 2.15 percent being yes/no questions. Of the yes/no questions, 44 percent were answered "yes", 34 percent "maybe", and only 22 percent were answered "no".[14]\n\nIn 2015, a study of 26,000 articles from 13 news sites on the World Wide Web, conducted by a data scientist and published on his blog, found that the majority (54 percent) were yes/no questions, which divided into 20 percent "yes" answers, 17 percent "no" answers and 16 percent whose answers he could not determine.[15]\n\nPhrasing headlines as questions is a tactic employed by newspapers that do not "have the facts required to buttress the nut graph".[16][3] Roger Simon characterized the practice as justifying "virtually anything, no matter how unlikely", giving "Hillary to Replace Biden on Ticket?" and "Romney to Endorse Gay Marriage Between Corporations?" as hypothetical examples of such a practice.[17][18] Many question headlines were used, for example, in reporting of Bharatiya Janata Party in-fighting in 2004, because no politicians went on record to confirm or deny facts, such as "Is Venkaiah Naidu on his way out?"[19] Because this implication is known to readers, guides giving advice to newspaper editors state that so-called "question heads" should be used sparingly.[20]\n\nFreelance writer R. Thomas Berner calls them "gimmickry".[21] Grant Milnor Hyde observed that they give the impression of uncertainty in a newspaper\'s content.[22] When Linton Andrews worked at the Daily Mail after the First World War, one of the rules set by Lord Northcliffe was to avoid question headlines, unless the question itself reflected a national issue.[23]\n\nQuestion headlines are not legally sound when it comes to avoiding defamation.[24] The Supreme Court of Oklahoma held in 1913, in its decision in Spencer v. Minnick, that "A man cannot libel another by the publication of language the meaning and damaging effect of which is clear to all men, and where the identity of the person meant cannot be doubted, and then escape liability through the use of a question mark."[25][24] The use of question headlines as a form of sensationalism has a long history, including the 9 June 1883, headline in Joseph Pulitzer\'s New York World, "Was It Peppermint Mary?"[26]\nThe story, about a jewellery store that had tried to prevent its female employees from flirting with people outside the store, only mentioned "Peppermint" Mary at the end of the piece as an employee who might possibly have caused this and did not answer the question.[26]\n\nThe New York World also famously used a question headline for hedging when editors were unsure of their facts, when it reported the outcome of the 1916 United States presidential election.[27][28] When other New York City newspapers ran statement headlines on 8 November 1916 saying "Hughes Is Elected" (The Evening Sun, final edition the night before), "Hughes Is Elected by Narrow Margin" (The Sun), "Hughes Is Elected by Majority of 40" (The New York Herald), "Hughes the Next President" (The Journal of Commerce), "Hughes Sweeps State" (New York Tribune) and "Nation Swept by Hughes!" (New York American), the World ran one with a question headline, "Hughes Elected in Close Contest?"[29]\n\nThis was the result of a last-minute intervention by then World journalist Herbert Bayard Swope, who, having received a tip from gambling friends that Charles Evans Hughes might not in fact win, persuaded Charles M. Lincoln, the managing editor of the paper, to reset the headline in between editions, inserting a question mark.[30][31] Confusingly, below the question headline the World still had a picture of Hughes captioned "The President-Elect" but the question headline did indeed turn out to have the answer "no", as President Woodrow Wilson was re-elected, which the World finally announced in a headline two days later.[27][29]\n\nAdvertisers and marketers prefer yes/no question headlines that are answered "yes", as a reader that immediately answers "no" to a question headline on an advertisement is likely to skip over the advertisement entirely.[32] The most famous example of such a question headline in advertising is "Do you make these mistakes in English?", written to advertise Sherwin Cody\'s English-language course and used from 1919 to 1959, which (with readers answering "yes" they did make the mistakes that the advertisement proceeded to outline) was measured as more successful than non-yes/no-question alternatives.[33][34]\n\nVictor Schwab, a partner in the advertising agency that worked for Cody, published an analysis of the aspects of the headline attempting to look at it scientifically and using ten years\' worth of revenue and customer enquiry data for both it and a statement headline that Cody had also used.[35][33][34] He noted amongst other things that working in its favour was the question addressing the reader using the second person.[36] A 2013 study into computer-mediated communication came to a similar conclusion, finding that question headlines posted to Twitter and eBay increased click-through rates in comparison to statement headlines and that questions that address or reference the reader have statistically significant higher click-through rates than rhetorical or general questions.[37][38]\n\nIn the field of particle physics, the concept is known as Hinchliffe\'s rule, after physicist Ian Hinchliffe, who stated that if a research paper\'s title is in the form of a yes–no question, the answer to that question will be "no".[39][40] The adage led into a humorous attempt at a liar paradox by a 1988 paper, written by physicist Boris Kayser under the pseudonym "Boris Peon", which bore the title: "Is Hinchliffe\'s Rule True?".[41][42][40]',
        pageTitle: "Betteridge's law of headlines",
    },
    {
        title: "Betz's law",
        link: "https://en.wikipedia.org/wiki/Betz%27s_law",
        content:
            'In aerodynamics, Betz\'s law indicates the maximum power that can be extracted from the wind, independent of the design of a wind turbine in open flow. It was published in 1919 by the German physicist Albert Betz.[1] The law is derived from the principles of conservation of mass and momentum of the air stream flowing through an idealized "actuator disk" that extracts energy from the wind stream. According to Betz\'s law, no wind turbine of any mechanism can capture more than 16/27 (59.3%) of the kinetic energy in wind. The factor 16/27 (0.593) is known as Betz\'s coefficient. Practical utility-scale wind turbines achieve at peak 75–80% of the Betz limit.[2][3]\n\nThe Betz limit is based on an open-disk actuator. If a diffuser is used to collect additional wind flow and direct it through the turbine, more energy can be extracted, but the limit still applies to the cross-section of the entire structure.\n\nBetz\'s law applies to all Newtonian fluids, including wind. If all of the energy coming from wind movement through a turbine were extracted as useful energy, the wind speed afterward would drop to zero. If the wind stopped moving at the exit of the turbine, then no more fresh wind could get in; it would be blocked. In order to keep the wind moving through the turbine, there has to be some wind movement, however small, on the other side with some wind speed greater than zero. Betz\'s law shows that as air flows through a certain area, and as wind speed slows from losing energy to extraction from a turbine, the airflow must distribute to a wider area. As a result, geometry limits the maximum efficiency of any turbine.\n\nBritish scientist Frederick W. Lanchester derived the same maximum in 1915. The leader of the Russian aerodynamic school, Nikolay Zhukowsky, also published the same result for an ideal wind turbine in 1920, the same year as Betz.[4] It is thus an example of Stigler\'s law, which posits that no scientific discovery is named after its actual discoverer.\n\nThe Betz Limit is the maximum possible energy that can be extracted by an infinitely thin rotor from a fluid flowing at a certain speed.[5]\n\nIn order to calculate the maximum theoretical efficiency of a thin rotor (of, for example, a wind turbine), one imagines it to be replaced by a disc that removes energy from the fluid passing through it. At a certain distance behind this disc, the fluid that has passed through the disc has a reduced, but nonzero, velocity.[5]\n\nApplying conservation of mass to the control volume, the mass flow rate (the mass of fluid flowing per unit time) is\n\nm\n              ˙\n            \n          \n        \n        =\n        ρ\n        \n          A\n          \n            1\n          \n        \n        \n          v\n          \n            1\n          \n        \n        =\n        ρ\n        S\n        v\n        =\n        ρ\n        \n          A\n          \n            2\n          \n        \n        \n          v\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\dot {m}}=\\rho A_{1}v_{1}=\\rho Sv=\\rho A_{2}v_{2},}\n\nwhere v1 is the speed in the front of the rotor, v2 is the speed downstream of the rotor, v is the speed at the fluid power device, ρ is the fluid density, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the area of the turbine, and \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n  \n and \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n  \n are the areas of the fluid before and after the turbine (the inlet and outlet of the control volume).\n\nThe density times the area and speed must be equal in each of the three regions: before the turbine, while going through the turbine, and past the turbine.\n\nThe force exerted on the wind by the rotor is the mass of air multiplied by its acceleration:\n\n  \n    \n      \n        \n          \n            \n              \n                F\n              \n              \n                \n                =\n                m\n                a\n              \n            \n            \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      d\n                      v\n                    \n                    \n                      d\n                      t\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      m\n                      ˙\n                    \n                  \n                \n                \n                Δ\n                v\n              \n            \n            \n              \n              \n                \n                =\n                ρ\n                S\n                v\n                (\n                \n                  v\n                  \n                    1\n                  \n                \n                −\n                \n                  v\n                  \n                    2\n                  \n                \n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F&=ma\\\\&=m{\\frac {dv}{dt}}\\\\&={\\dot {m}}\\,\\Delta v\\\\&=\\rho Sv(v_{1}-v_{2}).\\end{aligned}}}\n\nThe incremental work done by the force may be written\n\nd\n        E\n        =\n        F\n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle dE=F\\,dx,}\n\nP\n        =\n        \n          \n            \n              d\n              E\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        F\n        \n          \n            \n              d\n              x\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        F\n        v\n        .\n      \n    \n    {\\displaystyle P={\\frac {dE}{dt}}=F{\\frac {dx}{dt}}=Fv.}\n\nSubstituting the force F computed above into the power equation yields the power extracted from the wind,\n\nP\n        =\n        ρ\n        S\n        \n          v\n          \n            2\n          \n        \n        (\n        \n          v\n          \n            1\n          \n        \n        −\n        \n          v\n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle P=\\rho Sv^{2}(v_{1}-v_{2}).}\n\nHowever, power can be computed another way, by using the kinetic energy. Applying the conservation of energy equation to the control volume yields\n\nP\n        =\n        \n          \n            \n              Δ\n              E\n            \n            \n              Δ\n              t\n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          \n            \n              m\n              ˙\n            \n          \n        \n        (\n        \n          v\n          \n            1\n          \n          \n            2\n          \n        \n        −\n        \n          v\n          \n            2\n          \n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle P={\\frac {\\Delta E}{\\Delta t}}={\\tfrac {1}{2}}{\\dot {m}}(v_{1}^{2}-v_{2}^{2}).}\n\nSubstituting the mass flow rate from the continuity equation yields\n\nP\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ρ\n        S\n        v\n        (\n        \n          v\n          \n            1\n          \n          \n            2\n          \n        \n        −\n        \n          v\n          \n            2\n          \n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle P={\\tfrac {1}{2}}\\rho Sv(v_{1}^{2}-v_{2}^{2}).}\n\nBoth of these expressions for power are valid; one was derived by examining the incremental work, and the other by the conservation of energy. Equating these two expressions yields\n\nP\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ρ\n        S\n        v\n        (\n        \n          v\n          \n            1\n          \n          \n            2\n          \n        \n        −\n        \n          v\n          \n            2\n          \n          \n            2\n          \n        \n        )\n        =\n        ρ\n        S\n        \n          v\n          \n            2\n          \n        \n        (\n        \n          v\n          \n            1\n          \n        \n        −\n        \n          v\n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle P={\\tfrac {1}{2}}\\rho Sv(v_{1}^{2}-v_{2}^{2})=\\rho Sv^{2}(v_{1}-v_{2}).}\n\n1\n              2\n            \n          \n        \n        (\n        \n          v\n          \n            1\n          \n          \n            2\n          \n        \n        −\n        \n          v\n          \n            2\n          \n          \n            2\n          \n        \n        )\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        \n          v\n          \n            1\n          \n        \n        −\n        \n          v\n          \n            2\n          \n        \n        )\n        (\n        \n          v\n          \n            1\n          \n        \n        +\n        \n          v\n          \n            2\n          \n        \n        )\n        =\n        v\n        (\n        \n          v\n          \n            1\n          \n        \n        −\n        \n          v\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}(v_{1}^{2}-v_{2}^{2})={\\tfrac {1}{2}}(v_{1}-v_{2})(v_{1}+v_{2})=v(v_{1}-v_{2}),}\n\nv\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        \n          v\n          \n            1\n          \n        \n        +\n        \n          v\n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle v={\\tfrac {1}{2}}(v_{1}+v_{2}).}\n\nThe constant wind velocity across the rotor may be taken as the average of the upstream and downstream velocities. This is arguably the most counter-intuitive stage of the derivation of Betz\'s law. It is a direct consequence of the "axial flow" assumption, which disallows any radial mass flow in the actuator disk region.[6]  With no mass escape and a constant diameter in the actuator region, the air speed cannot change in the interaction region. Thus no energy can be extracted other than at the front and back of the interaction region, fixing the airspeed of the actuator disk to be the average. (Removing that restriction may allow higher performance than Betz law allows, but other radial effects must also be considered.[6] This constant velocity effect is distinct from the radial kinetic energy loss that is also ignored.[7])\n\nReturning to the previous expression for power based on kinetic energy:\n\nP\n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \n                  \n                    \n                      m\n                      ˙\n                    \n                  \n                \n                (\n                \n                  v\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                −\n                \n                  v\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                ρ\n                S\n                v\n                (\n                \n                  v\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                −\n                \n                  v\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      4\n                    \n                  \n                \n                ρ\n                S\n                (\n                \n                  v\n                  \n                    1\n                  \n                \n                +\n                \n                  v\n                  \n                    2\n                  \n                \n                )\n                (\n                \n                  v\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                −\n                \n                  v\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      4\n                    \n                  \n                \n                ρ\n                S\n                \n                  v\n                  \n                    1\n                  \n                  \n                    3\n                  \n                \n                \n                  (\n                  \n                    1\n                    +\n                    \n                      (\n                      \n                        \n                          \n                            v\n                            \n                              2\n                            \n                          \n                          \n                            v\n                            \n                              1\n                            \n                          \n                        \n                      \n                      )\n                    \n                    −\n                    \n                      \n                        (\n                        \n                          \n                            \n                              v\n                              \n                                2\n                              \n                            \n                            \n                              v\n                              \n                                1\n                              \n                            \n                          \n                        \n                        )\n                      \n                      \n                        2\n                      \n                    \n                    −\n                    \n                      \n                        (\n                        \n                          \n                            \n                              v\n                              \n                                2\n                              \n                            \n                            \n                              v\n                              \n                                1\n                              \n                            \n                          \n                        \n                        )\n                      \n                      \n                        3\n                      \n                    \n                  \n                  )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P&={\\tfrac {1}{2}}{\\dot {m}}(v_{1}^{2}-v_{2}^{2})\\\\&={\\tfrac {1}{2}}\\rho Sv(v_{1}^{2}-v_{2}^{2})\\\\&={\\tfrac {1}{4}}\\rho S(v_{1}+v_{2})(v_{1}^{2}-v_{2}^{2})\\\\&={\\tfrac {1}{4}}\\rho Sv_{1}^{3}\\left(1+\\left({\\frac {v_{2}}{v_{1}}}\\right)-\\left({\\frac {v_{2}}{v_{1}}}\\right)^{2}-\\left({\\frac {v_{2}}{v_{1}}}\\right)^{3}\\right).\\end{aligned}}}\n\nBy differentiating \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n with respect to \n  \n    \n      \n        \n          \n            \n              \n                v\n                \n                  2\n                \n              \n              \n                v\n                \n                  1\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {v_{2}}{v_{1}}}}\n  \n for a given fluid speed v1 and a given area S, one finds the maximum or minimum value for \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n. The result is that \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n reaches maximum value when \n  \n    \n      \n        \n          \n            \n              \n                v\n                \n                  2\n                \n              \n              \n                v\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              1\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {v_{2}}{v_{1}}}={\\tfrac {1}{3}}}\n  \n.\n\nP\n          \n            max\n          \n        \n        =\n        \n          \n            \n              16\n              27\n            \n          \n        \n        ⋅\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ρ\n        S\n        \n          v\n          \n            1\n          \n          \n            3\n          \n        \n        .\n      \n    \n    {\\displaystyle P_{\\text{max}}={\\tfrac {16}{27}}\\cdot {\\tfrac {1}{2}}\\rho Sv_{1}^{3}.}\n\nThe power obtainable from a cylinder of fluid with cross-sectional area S and velocity v1 is\n\nP\n        =\n        \n          C\n          \n            P\n          \n        \n        ⋅\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ρ\n        S\n        \n          v\n          \n            1\n          \n          \n            3\n          \n        \n        .\n      \n    \n    {\\displaystyle P=C_{\\text{P}}\\cdot {\\tfrac {1}{2}}\\rho Sv_{1}^{3}.}\n\nThe reference power for the Betz efficiency calculation is the power in a moving fluid in a cylinder with cross-sectional area S and velocity v1:\n\nP\n          \n            wind\n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ρ\n        S\n        \n          v\n          \n            1\n          \n          \n            3\n          \n        \n        .\n      \n    \n    {\\displaystyle P_{\\text{wind}}={\\tfrac {1}{2}}\\rho Sv_{1}^{3}.}\n\nThe power coefficient[9] CP (= P/Pwind) is the dimensionless ratio of the extractable power P to the kinetic power Pwind available in the undistributed stream.[citation needed]  It has a maximum value CP max = 16/27 = 0.593 (or 59.3%; however, coefficients of performance are usually expressed as a decimal, not a percentage). The resulting expression is:\n\nC\n          \n            P\n          \n        \n        \n          (\n          \n            \n              \n                v\n                \n                  2\n                \n              \n              \n                v\n                \n                  1\n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          (\n          \n            1\n            +\n            \n              (\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    v\n                    \n                      1\n                    \n                  \n                \n              \n              )\n            \n            −\n            \n              \n                (\n                \n                  \n                    \n                      v\n                      \n                        2\n                      \n                    \n                    \n                      v\n                      \n                        1\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n            −\n            \n              \n                (\n                \n                  \n                    \n                      v\n                      \n                        2\n                      \n                    \n                    \n                      v\n                      \n                        1\n                      \n                    \n                  \n                \n                )\n              \n              \n                3\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle C_{P}\\left({\\frac {v_{2}}{v_{1}}}\\right)={\\tfrac {1}{2}}\\left(1+\\left({\\frac {v_{2}}{v_{1}}}\\right)-\\left({\\frac {v_{2}}{v_{1}}}\\right)^{2}-\\left({\\frac {v_{2}}{v_{1}}}\\right)^{3}\\right)}\n\nModern large wind turbines achieve peak values for CP in the range of 0.45 to 0.50,[2][full citation needed] about 75–85% of the theoretically possible maximum. In high wind speed, where the turbine is operating at its rated power, the turbine rotates (pitches) its blades to lower CP to protect itself from damage. The power in the wind increases by a factor of 8 from 12.5 to 25 m/s, so CP must fall accordingly, getting as low as 0.06 for winds of 25 m/s.\n\nThe speed ratio \n  \n    \n      \n        \n          \n            \n              \n                v\n                \n                  2\n                \n              \n              \n                v\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              1\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {v_{2}}{v_{1}}}={\\tfrac {1}{3}}}\n  \n between outgoing and incoming wind implies that the outgoing air has only \n  \n    \n      \n        (\n        \n          \n            \n              1\n              3\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        \n          \n            \n              1\n              9\n            \n          \n        \n      \n    \n    {\\displaystyle ({\\tfrac {1}{3}})^{2}={\\tfrac {1}{9}}}\n  \n the kinetic energy of the incoming air, and that \n  \n    \n      \n        \n          \n            \n              8\n              9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {8}{9}}}\n  \n of the energy of the incoming air was extracted. This is a correct calculation, but it only considers the incoming air which eventually travels through the rotor.\n\nThe last step in calculating the Betz efficiency Cp is to divide the calculated power extracted from the flow by a reference power. As its reference power, the Betz analysis uses the power of air upstream moving at V1 through the cross-sectional area S of the rotor. Since \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        =\n        \n          \n            \n              2\n              3\n            \n          \n        \n        S\n      \n    \n    {\\displaystyle A_{1}={\\tfrac {2}{3}}S}\n  \n at the Betz limit, the rotor extracts \n  \n    \n      \n        \n          \n            \n              8\n              9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {8}{9}}}\n  \n of \n  \n    \n      \n        \n          \n            \n              2\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {2}{3}}}\n  \n, or \n  \n    \n      \n        \n          \n            \n              16\n              27\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\tfrac {16}{27}},}\n  \n of the incoming kinetic energy.\n\nBecause the cross-sectional area of wind flowing through the rotor changes, there must be some flow of air in the directions perpendicular to the axis of the rotor. Any kinetic energy associated with this radial flow has no effect on the calculation because the calculation considers only the initial and final states of the air in the system.\n\nAlthough it is often touted (e.g.[10][11]) as the definitive upper bound on energy extraction by any possible wind turbine, it is not. Despite the misleading title of his article,[12] Betz (nor Lanchester) never made such an unconditional claim.[1] Notably, a wind turbine operating at the Betz maximum efficiency has a non-zero wind velocity wake.[2][1] Any actuator disk placed downstream of the first will extract added power and so the combined dual actuator complex exceeds Betz limit.[13][14] The second actuator disk could be, but need not be, in the far field wind zone (parallel streamline) for this consideration to hold.[6]\n\nThe reason for this surprising exception to a law based solely on energy and flux conservation laws lurks in the seemingly modest assumption of transverse uniformity of the axial wind profile within the stream lines. For example, the aforementioned dual actuator wind turbine has, downstream, a transverse wind profile that has two distinct velocities and thus is not bound by the limits of the single actuator disk.[6]\n\nMathematically, the derivation for a single actuator disk implicitly embeds the assumption that the wind does not change velocity as it transits the "infinitely thin" actuator; in contrast, in the dual actuator hybrid, the wind does change velocity as it transits, invalidating the derivation\'s key step requiring constant velocity. A single infinitely thin actuator cannot change the velocity because it would otherwise not conserve flux, but in the hybrid pair, flux can be shed (outside the crossection) between the actuators allowing a different final outlet velocity than the inlet velocity.[6][13][14]\n\nPhysical multi-coaxial-rotor wind turbines have been analyzed.[10] Although these do not exceeded Betz limit in practice, this may be attributable to the fact that rotors not only have losses but must also obey angular momentum[15] and the blade element momentum theory which limits their efficiency below Betz limit.[7]\n\nModern research has suggested that a more relaxed higher bound of \n  \n    \n      \n        \n          \n            \n              2\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {2}{3}}}\n  \n can be achieved when the "unneeded assumptions" in the Betz\'s law derivation are removed.[16]\n\nMost real wind turbines are aerodynamically "thin" making them approximate the assumptions of Betz law.  To the extent that a typical wind turbine approximates the assumptions in Betz law, then Betz limit places an approximate upper bound on the annual energy that can be extracted at a site. Even if a hypothetical wind blew consistently for a full year, any wind turbine well approximated by the actuator disk model can extract no more than the Betz limit of the energy contained in that year\'s wind could be extracted.\n\nEssentially increasing system economic efficiency results from increased production per unit, measured per square meter of vane exposure. An increase in system efficiency is required to bring down the cost of electrical power production. Efficiency increases may be the result of engineering of the wind capture devices, such as the configuration and dynamics of wind turbines, that may increase the power generation from these systems within the Betz limit. System efficiency increases in power application, transmission or storage may also contribute to a lower cost of power per unit.\n\nThe assumptions of the Betz derivation[17] impose some physical restrictions on the nature of wind turbines it applies to (identical inlet/outlet velocity for example).  But beyond those assumptions, the Betz limit has no dependence on the internal mechanics of the wind extraction system, therefore S may take any form provided that the flow travels from the entrance to the control volume to the exit, and the control volume has uniform entry and exit velocities. Any extraneous effects can only decrease the performance of the system (usually a turbine) since this analysis was idealized to disregard friction. Any non-ideal effects would detract from the energy available in the incoming fluid, lowering the overall efficiency.\n\nSome manufacturers and inventors have made claims of exceeding the limit by using nozzles and other wind diversion devices, usually by misrepresenting the Betz limit and calculating only the rotor area and not the total input of air contributing to the wind energy extracted from the system.\n\nThe Betz limit has no relevance when calculating turbine efficiency in a mobile application such as a wind-powered vehicle, as here the efficiency could theoretically approach 100% minus blade losses if the fluid flow through the turbine disc (or equivalent) were only retarded imperceptibly. As this would require an infinitely large structure, practical devices rarely achieve 90% or over. The amount of power extracted from the fluid flow at high turbine efficiencies is less than the Betz limit, which is not the same type of efficiency.[citation needed]\n\nIn 1934 H. Glauert derived the expression for turbine efficiency, when the angular component of velocity is taken into account, by applying an energy balance across the rotor plane.[15] Due to the Glauert model, efficiency is below the Betz limit, and asymptotically approaches this limit when the tip speed ratio goes to infinity.\n\nIn 2001, Gorban, Gorlov and Silantyev introduced an exactly solvable model (GGS), that considers non-uniform pressure distribution and curvilinear flow across the turbine plane (issues not included in the Betz approach).[7] They utilized and modified the Kirchhoff model,[18] which describes the turbulent wake behind the actuator as the "degenerated" flow and uses the Euler equation outside the degenerate area. The GGS model predicts that peak efficiency is achieved when the flow through the turbine is approximately 61% of the total flow which is very similar to the Betz result of .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}2⁄3 for a flow resulting in peak efficiency, but the GGS predicted that the peak efficiency itself is much smaller: 30.1%.\n\nIn 2008, viscous computations based on computational fluid dynamics (CFD) were applied to wind turbine modeling and demonstrated satisfactory agreement with experiment.[19] Computed optimal efficiency is, typically, between the Betz limit and the GGS solution.',
        pageTitle: "Betz's law",
    },
    {
        title: "Biot–Savart law",
        link: "https://en.wikipedia.org/wiki/Biot%E2%80%93Savart_law",
        content:
            "In physics, specifically electromagnetism, the Biot–Savart law (/ˈbiːoʊ səˈvɑːr/ or /ˈbjoʊ səˈvɑːr/)[1] is an equation describing the magnetic field generated by a constant electric current. It relates the magnetic field to the magnitude, direction, length, and proximity of the electric current.\n\nThe Biot–Savart law is fundamental to magnetostatics. It is valid in the magnetostatic approximation and consistent with both Ampère's circuital law and Gauss's law for magnetism.[2] When magnetostatics does not apply, the Biot–Savart law should be replaced by Jefimenko's equations. The law is named after Jean-Baptiste Biot and Félix Savart, who discovered this relationship in 1820.\n\nIn the following equations, it is assumed that the medium is not magnetic (e.g., vacuum). This allows for straightforward derivation of magnetic field B, while the fundamental vector here is H.[3]\n\nThe Biot–Savart law[4]: Sec 5-2-1  is used for computing the resultant magnetic flux density B at position r in 3D-space generated by a filamentary current I (for example due to a wire). A steady (or stationary) current is a continual flow of charges which does not change with time and the charge neither accumulates nor depletes at any point. The law is a physical example of a line integral, being evaluated over the path C in which the electric currents flow (e.g. the wire). The equation in SI units teslas (T) is[5]\n\nB\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∫\n          \n            C\n          \n        \n        \n          \n            \n              I\n              \n              d\n              \n                ℓ\n              \n              ×\n              \n                \n                  r\n                  ′\n                \n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                  ′\n                \n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\int _{C}{\\frac {I\\,d{\\boldsymbol {\\ell }}\\times \\mathbf {r'} }{|\\mathbf {r'} |^{3}}}}\n\nwhere \n  \n    \n      \n        d\n        \n          ℓ\n        \n      \n    \n    {\\displaystyle d{\\boldsymbol {\\ell }}}\n  \n is a vector along the path \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n whose magnitude is the length of the differential element of the wire in the direction of conventional current, \n  \n    \n      \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\ell }}}\n  \n is a point on path \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, and  \n  \n    \n      \n        \n          \n            r\n            ′\n          \n        \n        =\n        \n          r\n        \n        −\n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\mathbf {r'} =\\mathbf {r} -{\\boldsymbol {\\ell }}}\n  \n is the full displacement vector from the wire element (\n  \n    \n      \n        d\n        \n          ℓ\n        \n      \n    \n    {\\displaystyle d{\\boldsymbol {\\ell }}}\n  \n) at point \n  \n    \n      \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\ell }}}\n  \n to the point at which the field is being computed (\n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n), and μ0 is the magnetic constant. Alternatively:\n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∫\n          \n            C\n          \n        \n        \n          \n            \n              I\n              \n              d\n              \n                ℓ\n              \n              ×\n              \n                \n                  \n                    \n                      \n                        r\n                        ^\n                      \n                    \n                  \n                  ′\n                \n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                  ′\n                \n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\int _{C}{\\frac {I\\,d{\\boldsymbol {\\ell }}\\times \\mathbf {{\\hat {r}}'} }{|\\mathbf {r'} |^{2}}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                  ^\n                \n              \n            \n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {{\\hat {r}}'} }\n  \n is the unit vector of \n  \n    \n      \n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r'} }\n  \n. The symbols in boldface denote vector quantities.\n\nThe integral is usually around a closed curve, since stationary electric currents can only flow around closed paths when they are bounded. However, the law also applies to infinitely long wires (this concept was used in the definition of the SI unit of electric current—the Ampere—until 20 May 2019).\n\nTo apply the equation, the point in space where the magnetic field is to be calculated is arbitrarily chosen (\n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n). Holding that point fixed, the line integral over the path of the electric current is calculated to find the total magnetic field at that point. The application of this law implicitly relies on the superposition principle for magnetic fields, i.e. the fact that the magnetic field is a vector sum of the field created by each infinitesimal section of the wire individually.[6]\n\nFor example, consider the magnetic field of a loop of radius \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n carrying a current \n  \n    \n      \n        I\n        .\n      \n    \n    {\\displaystyle I.}\n  \n For a point at a distance \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n along the center line of the loop, the magnetic field vector at that point is:\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            \n              \n                μ\n                \n                  0\n                \n              \n              I\n              \n                R\n                \n                  2\n                \n              \n            \n            \n              2\n              (\n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                R\n                \n                  2\n                \n              \n              \n                )\n                \n                  3\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                x\n              \n              ^\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {B} ={\\mu _{0}IR^{2} \\over 2(x^{2}+R^{2})^{3/2}}{\\hat {\\mathbf {x} }},}\n  \nwhere \n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {x} }}}\n  \n is the unit vector of along the center-line of the loop (and the loop is taken to be centered at the origin).[4]: Sec 5-2, Eqn (25)  Loops such as the one described appear in devices like the Helmholtz coil, the solenoid, and the Magsail spacecraft propulsion system. Calculation of the magnetic field at points off the center line requires more complex mathematics involving elliptic integrals that require numerical solution or approximations.[7]\n\nThe formulations given above work well when the current can be approximated as running through an infinitely-narrow wire. If the conductor has some thickness, the proper formulation of the Biot–Savart law (again in SI units) is:\n\nB\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∭\n          \n            V\n          \n        \n         \n        \n          \n            \n              (\n              \n                J\n              \n              \n              d\n              V\n              )\n              ×\n              \n                \n                  r\n                \n                ′\n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\iiint _{V}\\ {\\frac {(\\mathbf {J} \\,dV)\\times \\mathbf {r} '}{|\\mathbf {r} '|^{3}}}}\n\nwhere \n  \n    \n      \n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r'} }\n  \n is the vector from dV to the observation point \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n, \n  \n    \n      \n        d\n        V\n      \n    \n    {\\displaystyle dV}\n  \n is the volume element, and \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n is the current density vector in that volume (in SI in units of A/m2).\n\nIn terms of unit vector \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                  ^\n                \n              \n            \n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {{\\hat {r}}'} }\n  \n\n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∭\n          \n            V\n          \n        \n         \n        d\n        V\n        \n          \n            \n              \n                J\n              \n              ×\n              \n                \n                  \n                    \n                      \n                        r\n                        ^\n                      \n                    \n                  \n                  ′\n                \n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\iiint _{V}\\ dV{\\frac {\\mathbf {J} \\times \\mathbf {{\\hat {r}}'} }{|\\mathbf {r} '|^{2}}}}\n\nIn the special case of a uniform constant current I, the magnetic field \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n is\n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        I\n        \n          ∫\n          \n            C\n          \n        \n        \n          \n            \n              d\n              \n                ℓ\n              \n              ×\n              \n                \n                  r\n                  ′\n                \n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                  ′\n                \n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}I\\int _{C}{\\frac {d{\\boldsymbol {\\ell }}\\times \\mathbf {r'} }{|\\mathbf {r'} |^{3}}}}\n  \n\ni.e., the current can be taken out of the integral.\n\nIn the case of a point charged particle q moving at a constant velocity v, Maxwell's equations give the following expression for the electric field and magnetic field:[8]\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  E\n                \n              \n              \n                \n                =\n                \n                  \n                    q\n                    \n                      4\n                      π\n                      \n                        ε\n                        \n                          0\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      −\n                      \n                        β\n                        \n                          2\n                        \n                      \n                    \n                    \n                      \n                        (\n                        \n                          1\n                          −\n                          \n                            β\n                            \n                              2\n                            \n                          \n                          \n                            sin\n                            \n                              2\n                            \n                          \n                          ⁡\n                          θ\n                        \n                        )\n                      \n                      \n                        3\n                        \n                          /\n                        \n                        2\n                      \n                    \n                  \n                \n                \n                  \n                    \n                      \n                        \n                          \n                            \n                              r\n                              ^\n                            \n                          \n                        \n                        ′\n                      \n                    \n                    \n                      \n                        |\n                      \n                      \n                        \n                          r\n                        \n                        ′\n                      \n                      \n                        \n                          |\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  H\n                \n              \n              \n                \n                =\n                \n                  v\n                \n                ×\n                \n                  D\n                \n              \n            \n            \n              \n                \n                  B\n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      c\n                      \n                        2\n                      \n                    \n                  \n                \n                \n                  v\n                \n                ×\n                \n                  E\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {E} &={\\frac {q}{4\\pi \\varepsilon _{0}}}{\\frac {1-\\beta ^{2}}{\\left(1-\\beta ^{2}\\sin ^{2}\\theta \\right)^{3/2}}}{\\frac {\\mathbf {{\\hat {r}}'} }{|\\mathbf {r} '|^{2}}}\\\\[1ex]\\mathbf {H} &=\\mathbf {v} \\times \\mathbf {D} \\\\[1ex]\\mathbf {B} &={\\frac {1}{c^{2}}}\\mathbf {v} \\times \\mathbf {E} \\end{aligned}}}\n  \n\nwhere\n\nWhen v2 ≪ c2, the electric field and magnetic field can be approximated as[8]\n\n  \n    \n      \n        \n          E\n        \n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n         \n        \n          \n            \n              \n                \n                  \n                    \n                      r\n                      ^\n                    \n                  \n                \n                ′\n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} ={\\frac {q}{4\\pi \\varepsilon _{0}}}\\ {\\frac {\\mathbf {{\\hat {r}}'} }{|\\mathbf {r} '|^{2}}}}\n  \n\n\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        q\n        \n          \n            \n              \n                v\n              \n              ×\n              \n                \n                  \n                    \n                      \n                        r\n                      \n                      ^\n                    \n                  \n                \n                ′\n              \n            \n            \n              \n                |\n              \n              \n                \n                  r\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} ={\\mu _{0} \\over 4\\pi }q{\\mathbf {v} \\times {\\hat {\\mathbf {r} }}' \\over |\\mathbf {r} '|^{2}}}\n\nThese equations were first derived by Oliver Heaviside in 1888.  Some authors[10][11] call the above equation for \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n the \"Biot–Savart law for a point charge\" due to its close resemblance to the standard Biot–Savart law.  However, this language is misleading as the Biot–Savart law applies only to steady currents and a point charge moving in space does not constitute a steady current.[12]\n\nThe Biot–Savart law can be used in the calculation of magnetic responses even at the atomic or molecular level, e.g. chemical shieldings or magnetic susceptibilities, provided that the current density can be obtained from a quantum mechanical calculation or theory.\n\nThe Biot–Savart law is also used in aerodynamic theory to calculate the velocity induced by vortex lines.\n\nIn the aerodynamic application, the roles of vorticity and current are reversed in comparison to the magnetic application.\n\nIn Maxwell's 1861 paper 'On Physical Lines of Force',[13]  magnetic field strength H was directly equated with pure vorticity (spin), whereas B was a weighted vorticity that was weighted for the density of the vortex sea. Maxwell considered magnetic permeability μ to be a measure of the density of the vortex sea. Hence the relationship,\n\nB was seen as a kind of magnetic current of vortices aligned in their axial planes, with H being the circumferential velocity of the vortices.\n\nThe electric current equation can be viewed as a convective current of electric charge that involves linear motion. By analogy, the magnetic equation is an inductive current involving spin. There is no linear motion in the inductive current along the direction of the B vector. The magnetic inductive current represents lines of force. In particular, it represents lines of inverse square law force.\n\nIn aerodynamics the induced air currents form solenoidal rings around a vortex axis. Analogy can be made that the vortex axis is playing the role that electric current plays in magnetism. This puts the air currents of aerodynamics (fluid velocity field) into the equivalent role of the magnetic induction vector B in electromagnetism.\n\nIn electromagnetism the B lines form solenoidal rings around the source electric current, whereas in aerodynamics, the air currents (velocity) form solenoidal rings around the source vortex axis.\n\nHence in electromagnetism, the vortex plays the role of 'effect' whereas in aerodynamics, the vortex plays the role of 'cause'. Yet when we look at the B lines in isolation, we see exactly the aerodynamic scenario insomuch as B is the vortex axis and H is the circumferential velocity as in Maxwell's 1861 paper.\n\nIn two dimensions, for a vortex line of infinite length, the induced velocity at a point is given by\n\n  \n    \n      \n        v\n        =\n        \n          \n            Γ\n            \n              2\n              π\n              r\n            \n          \n        \n      \n    \n    {\\displaystyle v={\\frac {\\Gamma }{2\\pi r}}}\n  \n\nwhere Γ is the strength of the vortex and r is the perpendicular distance between the point and the vortex line. This is similar to the magnetic field produced on a plane by an infinitely long straight thin wire normal to the plane.\n\nThis is a limiting case of the formula for vortex segments of finite length (similar to a finite wire):\n\n  \n    \n      \n        v\n        =\n        \n          \n            Γ\n            \n              4\n              π\n              r\n            \n          \n        \n        \n          [\n          \n            cos\n            ⁡\n            A\n            −\n            cos\n            ⁡\n            B\n          \n          ]\n        \n      \n    \n    {\\displaystyle v={\\frac {\\Gamma }{4\\pi r}}\\left[\\cos A-\\cos B\\right]}\n  \n\nwhere A and B are the (signed) angles between the point and the two ends of the segment.\n\nIn a magnetostatic situation, the magnetic field B as calculated from the Biot–Savart law will always satisfy Gauss's law for magnetism and Ampère's circuital law:[14]\n\nStarting with the Biot–Savart law:\n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∭\n          \n            V\n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          ℓ\n        \n        \n        \n          J\n        \n        (\n        \n          ℓ\n        \n        )\n        ×\n        \n          \n            \n              \n                r\n              \n              −\n              \n                ℓ\n              \n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                ℓ\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\iiint _{V}d^{3}{\\boldsymbol {\\ell }}\\,\\mathbf {J} ({\\boldsymbol {\\ell }})\\times {\\frac {\\mathbf {r} -{\\boldsymbol {\\ell }}}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|^{3}}}}\n\nSubstituting the relation\n\n  \n    \n      \n        \n          \n            \n              \n                r\n              \n              −\n              \n                ℓ\n              \n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                ℓ\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        =\n        −\n        ∇\n        \n          (\n          \n            \n              1\n              \n                \n                  |\n                \n                \n                  r\n                \n                −\n                \n                  ℓ\n                \n                \n                  |\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathbf {r} -{\\boldsymbol {\\ell }}}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|^{3}}}=-\\nabla \\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right)}\n  \n\nand using the product rule for curls, as well as the fact that J does not depend on \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n, this equation can be rewritten as[14]\n\n  \n    \n      \n        \n          B\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        ∇\n        ×\n        \n          ∭\n          \n            V\n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          ℓ\n        \n        \n        \n          \n            \n              \n                J\n              \n              (\n              \n                ℓ\n              \n              )\n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                ℓ\n              \n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} (\\mathbf {r} )={\\frac {\\mu _{0}}{4\\pi }}\\nabla \\times \\iiint _{V}d^{3}{\\boldsymbol {\\ell }}\\,{\\frac {\\mathbf {J} ({\\boldsymbol {\\ell }})}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}}\n\nSince the divergence of a curl is always zero, this establishes Gauss's law for magnetism. Next, taking the curl of both sides, using the formula for the curl of a curl, and again using the fact that J does not depend on \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n, we eventually get the result[14]\n\n  \n    \n      \n        ∇\n        ×\n        \n          B\n        \n        =\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        ∇\n        \n          ∭\n          \n            V\n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          ℓ\n        \n        \n        \n          J\n        \n        (\n        \n          ℓ\n        \n        )\n        ⋅\n        ∇\n        \n          (\n          \n            \n              1\n              \n                \n                  |\n                \n                \n                  r\n                \n                −\n                \n                  ℓ\n                \n                \n                  |\n                \n              \n            \n          \n          )\n        \n        −\n        \n          \n            \n              μ\n              \n                0\n              \n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∭\n          \n            V\n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          ℓ\n        \n        \n        \n          J\n        \n        (\n        \n          ℓ\n        \n        )\n        \n          ∇\n          \n            2\n          \n        \n        \n          (\n          \n            \n              1\n              \n                \n                  |\n                \n                \n                  r\n                \n                −\n                \n                  ℓ\n                \n                \n                  |\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {B} ={\\frac {\\mu _{0}}{4\\pi }}\\nabla \\iiint _{V}d^{3}{\\boldsymbol {\\ell }}\\,\\mathbf {J} ({\\boldsymbol {\\ell }})\\cdot \\nabla \\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right)-{\\frac {\\mu _{0}}{4\\pi }}\\iiint _{V}d^{3}{\\boldsymbol {\\ell }}\\,\\mathbf {J} ({\\boldsymbol {\\ell }})\\nabla ^{2}\\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right)}\n\nFinally, plugging in the relations[14]\n\n  \n    \n      \n        \n          \n            \n              \n                ∇\n                \n                  (\n                  \n                    \n                      1\n                      \n                        \n                          |\n                        \n                        \n                          r\n                        \n                        −\n                        \n                          ℓ\n                        \n                        \n                          |\n                        \n                      \n                    \n                  \n                  )\n                \n              \n              \n                \n                =\n                −\n                \n                  ∇\n                  \n                    ℓ\n                  \n                \n                \n                  (\n                  \n                    \n                      1\n                      \n                        \n                          |\n                        \n                        \n                          r\n                        \n                        −\n                        \n                          ℓ\n                        \n                        \n                          |\n                        \n                      \n                    \n                  \n                  )\n                \n                ,\n              \n            \n            \n              \n                \n                  ∇\n                  \n                    2\n                  \n                \n                \n                  (\n                  \n                    \n                      1\n                      \n                        \n                          |\n                        \n                        \n                          r\n                        \n                        −\n                        \n                          ℓ\n                        \n                        \n                          |\n                        \n                      \n                    \n                  \n                  )\n                \n              \n              \n                \n                =\n                −\n                4\n                π\n                δ\n                (\n                \n                  r\n                \n                −\n                \n                  ℓ\n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\nabla \\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right)&=-\\nabla _{\\boldsymbol {\\ell }}\\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right),\\\\\\nabla ^{2}\\left({\\frac {1}{|\\mathbf {r} -{\\boldsymbol {\\ell }}|}}\\right)&=-4\\pi \\delta (\\mathbf {r} -{\\boldsymbol {\\ell }})\\end{aligned}}}\n  \n\n(where δ is the Dirac delta function), using the fact that the divergence of J is zero (due to the assumption of magnetostatics), and performing an integration by parts, the result turns out to be[14]\n\n  \n    \n      \n        ∇\n        ×\n        \n          B\n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        \n          J\n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {B} =\\mu _{0}\\mathbf {J} }\n  \n\ni.e. Ampère's circuital law. (Due to the assumption of magnetostatics, \n  \n    \n      \n        ∂\n        \n          E\n        \n        \n          /\n        \n        ∂\n        t\n        =\n        \n          0\n        \n      \n    \n    {\\displaystyle \\partial \\mathbf {E} /\\partial t=\\mathbf {0} }\n  \n, so there is no extra displacement current term in Ampère's law.)\n\nIn a non-magnetostatic situation, the Biot–Savart law ceases to be true (it is superseded by Jefimenko's equations), while Gauss's law for magnetism and the Maxwell–Ampère law are still true.\n\nInitially, the Biot–Savart law was discovered experimentally, then this law was derived in different ways theoretically. In The Feynman Lectures on Physics, at first, the similarity of expressions for the electric potential outside the static distribution of charges and the magnetic vector potential outside the system of continuously distributed currents is emphasized, and then the magnetic field is calculated through the curl from the vector potential.[15] Another approach involves a general solution of the  inhomogeneous wave equation for the vector potential in the case of constant currents.[16] The magnetic field can also be calculated as a consequence of the Lorentz transformations for the electromagnetic force acting from one charged particle on another particle.[17] Two other ways of deriving the Biot–Savart law include: 1) Lorentz transformation of the electromagnetic tensor components from a moving frame of reference, where there is only an electric field of some distribution of charges, into a stationary frame of reference, in which these charges move. 2) the use of the method of retarded potentials.",
        pageTitle: "Biot–Savart law",
    },
    {
        title: "Birch's law",
        link: "https://en.wikipedia.org/wiki/Birch%27s_law",
        content:
            "Birch's law, discovered by the geophysicist Francis Birch, establishes a linear relation between compressional wave velocity vp and density \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n of rocks and minerals:\n\nwhere \n  \n    \n      \n        \n        \n          \n            \n              M\n              ¯\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\,{\\bar {M}}\\,}\n  \n is the mean atomic mass in formula units and \n  \n    \n      \n        \n        a\n        (\n        x\n        )\n        \n      \n    \n    {\\displaystyle \\,a(x)\\,}\n  \n is an empirical function determined by experiment.[1][2][3]\n\nThe mean atomic mass of forsterite (Mg2SiO4) is equal to the sum of the atomic masses divided by the number of atoms in the formula:\n\nTypical oxides and silicates in the mantle have values close to 20, while in the Earth's core it is close to 50.[3]\n\nBirch's law applies to rocks that are under pressures of a few tens of gigapascals, enough for most cracks to close.[3] It can be used in the discussion of geophysical data. The law is used in forming compositional and mineralogical models of the mantle by using the change in the velocity of the seismic wave and its relationship with a change in density of the material the wave is moving in. Birch's law is used in determining chemical similarities in the mantle as well as the discontinuities of the transition zones. Birch's Law can also be employed in the calculation of an increase of velocity due to an increase in the density of material.[4]\n\nIt had been previously assumed that the velocity-density relationship is constant. That is, that Birch's law will hold true in any case, but the relationship does not hold true deeper in the mantle for the increased pressures near the transition zone. In cases where Birch's law was applied beyond the transition zone, parts of the formula need to be revised. For higher pressure regimes, different laws may be needed to determine wave velocities.[2]\n\nThe relationship between the density of a material and the velocity of a P wave moving through the material was noted when research was conducted on waves in different materials.\n\nIn the experiment, a pulse of voltage is applied to a circular plate of polarized barium titanate ceramic (the transducer) which is attached to the near end of the material sample. The added voltage creates vibrations in the sample. Those vibrations travel through the sample to a second transducer on the far end. The vibrations are then converted into an electrical wave which is viewed on an oscilloscope to determine the travel time. The velocity is the lender of the damper decided by the wave's travel time.[clarification needed]\n\nThe resulting relationship between the density of the material and the discovered velocity is known as Birch's law.[1]\n\nThe table shows the velocities for different rocks ranging in pressure from 10 bars to 10,000 bars. It represents how the change in density, as given in the second column, is related to the velocity of the P wave moving in the material. An increase in the density of the material leads to an increase in the velocity which can be determined using Birch's Law.",
        pageTitle: "Birch's law",
    },
    {
        title: "Bloch's law",
        link: "https://en.wikipedia.org/wiki/Bloch%27s_law",
        content:
            "Bloch's law  observes that, for brief presentations, the product of luminance (or contrast) and duration at the detection threshold  is constant.[1] The law is due to Adolphe-Moise Bloch, who first formulated it in 1885.[2]\n\nConsider that a brief flash of intensity \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n is presented for a duration \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. Bloch's law states that detection occurs if the total luminance energy \n  \n    \n      \n        I\n        ×\n        t\n      \n    \n    {\\displaystyle I\\times t}\n  \n exceeds some threshold value \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n. Formally,\n  \n    \n      \n        I\n        ×\n        t\n        =\n        K\n      \n    \n    {\\displaystyle I\\times t=K}\n  \nHere, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n is a constant that can vary with different viewing conditions, observer attributes, and adaptation levels. Early measurements used single, isolated light flashes of varying duration and intensity to determine the boundary at which a viewer first reports seeing the flash. When plotted against detection thresholds, these data typically exhibit a near-constant product of intensity and duration for short intervals.",
        pageTitle: "Bloch's law",
    },
    {
        title: "Titius–Bode law",
        link: "https://en.wikipedia.org/wiki/Titius%E2%80%93Bode_law",
        content:
            "The Titius–Bode law (sometimes termed simply Bode's law) is a formulaic prediction of spacing between planets in any given planetary system. The formula suggests that, extending outward, each planet should be approximately twice as far from the Sun as the one before. The hypothesis correctly anticipated the orbits of Ceres (in the asteroid belt) and Uranus, but failed as a predictor of Neptune's orbit. It is named after Johann Daniel Titius and Johann Elert Bode.\n\nLater work by Mary Adela Blagg and D.E. Richardson significantly revised the original formula, and made predictions that were subsequently validated by new discoveries and observations. It is these re-formulations that offer \"the best phenomenological representations of distances with which to investigate the theoretical significance of Titius–Bode type Laws\".[1]\n\nThe law relates the semi-major axis \n  \n    \n      \n         \n        \n          a\n          \n            n\n          \n        \n         \n      \n    \n    {\\displaystyle ~a_{n}~}\n  \n of each planet's orbit outward from the Sun in units such that the Earth's semi-major axis is equal to 10:\n\nwhere \n  \n    \n      \n         \n        x\n        =\n        0\n        ,\n        3\n        ,\n        6\n        ,\n        12\n        ,\n        24\n        ,\n        48\n        ,\n        96\n        ,\n        192\n        ,\n        384\n        ,\n        768\n        …\n         \n      \n    \n    {\\displaystyle ~x=0,3,6,12,24,48,96,192,384,768\\ldots ~}\n  \n such that, with the exception of the first step, each value is twice the previous value.\nThere is another representation of the formula:\n\nwhere \n  \n    \n      \n         \n        n\n        =\n        −\n        ∞\n        ,\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n         \n        .\n      \n    \n    {\\displaystyle ~n=-\\infty ,0,1,2,\\ldots ~.}\n  \n\nThe resulting values can be divided by 10 to convert them into astronomical units (AU), resulting in the expression:\n\nFor the far outer planets, beyond Saturn, each planet is predicted to be roughly twice as far from the Sun as the previous object. Whereas the Titius–Bode law predicts Saturn, Uranus, Neptune, and Pluto at about 10, 20, 39, and 77 AU, the actual values are closer to 10, 19, 30, 40 AU.[a]\n\nThe first mention of a series approximating Bode's law is found in a textbook by D. Gregory (1715):[2]\n\nA similar sentence, likely paraphrased from Gregory (1715),[2][3] appears in a work published by C. Wolff in 1724.\n\nIn his 1766 translation of Bonnet's work, J.D. Titius added two of his own paragraphs to the statement above. The insertions were placed at the bottom of page 7 and at the top of page 8. The new paragraph is not in Bonnet's original French text, nor in translations of the work into Italian and English.\n\nThere are two parts to Titius's inserted text. The first part explains the succession of planetary distances from the Sun:\n\nIn 1772, J.E. Bode, then aged twenty-five, published an astronomical compendium,[5] in which he included the following footnote, citing Titius (in later editions):[b][6]\n\nThese two statements, for all their peculiar expression, and from the radii used for the orbits, seem to stem from an antique algorithm by a cossist.[c]\n\nMany precedents were found that predate the seventeenth century.[citation needed] Titius was a disciple of the German philosopher C.F. von Wolf (1679–1754), and the second part of the text that Titius inserted into Bonnet's work is in a book by von Wolf (1723),[7] suggesting that Titius learned the relation from him. Twentieth-century literature about Titius–Bode law attributes authorship to von Wolf.[citation needed] A prior version was written by D. Gregory (1702),[8] in which the succession of planetary distances 4, 7, 10, 16, 52, and 100 became a geometric progression with ratio 2. This is the nearest Newtonian formula, which was also cited by Benjamin Martin (1747)[9] and Tomàs Cerdà (c. 1760)[10] years before Titius's expanded translation of Bonnet's book into German (1766). Over the next two centuries, subsequent authors continued to present their own modified versions, apparently unaware of prior work.[1]\n\nTitius and Bode hoped that the law would lead to the discovery of new planets, and indeed the discovery of Uranus and Ceres – both of whose distances fit well with the law – contributed to the law's fame. Neptune's distance was very discrepant, however, and indeed Pluto – no longer considered a planet – is at a mean distance that roughly corresponds to that the Titius–Bode law predicted for the next planet out from Uranus.\n\nWhen originally published, the law was approximately satisfied by all the planets then known – i.e., Mercury through Saturn – with a gap between the fourth and fifth planets. Vikarius (Johann Friedrich) Wurm (1787) proposed a modified version of the Titius–Bode Law that accounted for the then-known satellites of Jupiter and Saturn, and better predicted the distance for Mercury.[11]\n\nThe Titius–Bode law was regarded as interesting, but of no great importance until the discovery of Uranus in 1781, which happens to fit into the series nearly exactly. Based on this discovery, Bode urged his contemporaries to search for a fifth planet. Ceres, the largest object in the asteroid belt, was found at Bode's predicted position in 1801.\n\nBode's law was widely accepted at that point, until in 1846 Neptune was discovered in a location that does not conform to the law. Simultaneously, due to the large number of asteroids discovered in the belt, Ceres was no longer a major planet. In 1898 the astronomer and logician C.S. Peirce used Bode's law as an example of fallacious reasoning.[12]\n\nThe discovery of Pluto in 1930 confounded the issue still further: Although nowhere near its predicted position according to Bode's law, it was very nearly at the position the law had designated for Neptune. The subsequent discovery of the Kuiper belt – and in particular the object Eris, which is more massive than Pluto, yet does not fit Bode's law – further discredited the formula.[13]\n\nThe Titius–Bode law predicts planets will be present at specific distances in astronomical units, which can be compared to the observed data for the planets and two dwarf planets in the Solar System:\n\nIn 1913, M.A. Blagg, an Oxford astronomer, re-visited the law.[14]\nShe analyzed the orbits of the planetary system and those of the satellite systems of the outer gas giants, Jupiter, Saturn and Uranus. She examined the log of the distances, trying to find the best 'average' difference.\n\nNote in particular that in Blagg's formula, the law for the Solar System was best represented by a progression in 1.7275, rather than the original value 2 used by Titius, Bode, and others.\n\nBlagg examined the satellite system of Jupiter, Saturn, and Uranus, and discovered the same progression ratio 1.7275, in each.\n\nHowever, the final form of the correction function  f  was not given in Blagg's 1913 paper, with Blagg noting that the empirical figures given were only for illustration. The empirical form was provided in the form of a graph (the reason that points on the curve are such a close match for empirical data, for objects discovered prior to 1913, is that they are the empirical data).\n\nFinding a formula that closely fit the empircal curve turned out to be difficult. Fourier analysis of the shape resulted in the following seven term approximation:[14]\n\nf\n                \n                  \n                    (\n                  \n                \n                 \n                θ\n                 \n                \n                  \n                    )\n                  \n                \n                \n                =\n                \n                0.4594\n                \n                +\n                \n                \n              \n              \n                0.396\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                θ\n                −\n                \n                  27.4\n                  \n                    ∘\n                  \n                \n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.168\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                2\n                 \n                (\n                 \n                θ\n                −\n                \n                  60.4\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.062\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                3\n                 \n                (\n                 \n                θ\n                −\n                \n                  28.1\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n              \n            \n            \n              \n                \n                +\n                \n                \n              \n              \n                0.053\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                4\n                 \n                (\n                 \n                θ\n                −\n                \n                  77.2\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.009\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                5\n                 \n                (\n                 \n                θ\n                −\n                \n                  22\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.012\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                7\n                 \n                (\n                 \n                θ\n                −\n                \n                  40.4\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                 \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\ f{\\bigl (}\\ \\theta \\ {\\bigr )}\\;=\\;0.4594\\;+\\;\\;&0.396\\ \\cos \\!{\\bigl (}\\ \\theta -27.4^{\\circ }\\ {\\bigr )}\\;+\\;0.168\\ \\cos \\!{\\bigl (}\\ 2\\ (\\ \\theta -60.4^{\\circ })\\ {\\bigr )}\\;+\\;0.062\\ \\cos \\!{\\bigl (}\\ 3\\ (\\ \\theta -28.1^{\\circ })\\ {\\bigr )}\\;+\\;\\\\\\;+\\;\\;&0.053\\ \\cos \\!{\\bigl (}\\ 4\\ (\\ \\theta -77.2^{\\circ })\\ {\\bigr )}\\;+\\;0.009\\ \\cos \\!{\\bigl (}\\ 5\\ (\\ \\theta -22^{\\circ })\\ {\\bigr )}\\;+\\;0.012\\ \\cos \\!{\\bigl (}\\ 7\\ (\\ \\theta -40.4^{\\circ })\\ {\\bigr )}~.\\end{aligned}}}\n\nAfter further analysis, Blagg gave the following simpler formula; however the price for the simpler form is that it produces a less accurate fit to the empirical data. Blagg gave it in an un-normalized form in her paper, which leaves the relative sizes of A, B, and f  ambiguous; it is shown here in normalized form (i.e. this version of  f  is scaled to produce values ranging from 0 to 1, inclusive):[15]\n\nf\n        \n          \n            (\n          \n        \n         \n        θ\n         \n        \n          \n            )\n          \n        \n        \n        =\n        \n        0.249\n        \n        +\n        \n        0.860\n         \n        \n          (\n          \n            \n              \n                \n                   \n                  cos\n                  ⁡\n                   \n                  Ψ\n                   \n                \n                \n                   \n                  3\n                  −\n                  cos\n                  \n                  \n                    (\n                    \n                       \n                      2\n                       \n                      Ψ\n                       \n                    \n                    )\n                  \n                   \n                \n              \n            \n            \n            +\n            \n            \n              \n                1\n                \n                   \n                  6\n                  −\n                  4\n                   \n                  cos\n                  \n                  \n                    (\n                    \n                       \n                      2\n                       \n                      Ψ\n                      −\n                      \n                        60\n                        \n                          ∘\n                        \n                      \n                    \n                    )\n                  \n                   \n                \n              \n            \n          \n          )\n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ f{\\bigl (}\\ \\theta \\ {\\bigr )}\\;=\\;0.249\\;+\\;0.860\\ \\left({\\frac {\\ \\cos \\ \\Psi \\ }{\\ 3-\\cos \\!\\left(\\ 2\\ \\Psi \\ \\right)\\ }}\\;+\\;{\\frac {1}{\\ 6-4\\ \\cos \\!\\left(\\ 2\\ \\Psi -60^{\\circ }\\right)\\ }}\\right)\\ ,}\n\nwhere \n  \n    \n      \n         \n        Ψ\n        ≡\n        θ\n        −\n        \n          27.5\n          \n            ∘\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\ \\Psi \\equiv \\theta -27.5^{\\circ }~.}\n\nNeither of these formulas for function  f  are used in the calculations below: The calculations here are based on a graph of function  f  which was drawn based on observed data.\n\nHer paper was published in 1913, and was forgotten until 1953, when A.E. Roy came across it while researching another problem.[16]\nRoy noted that Blagg herself had suggested that her formula could give approximate mean distances of other bodies still undiscovered in 1913. Since then, six bodies in three systems examined by Blagg had been discovered: Pluto, Sinope (Jupiter IX), Lysithea (J X), Carme (J XI), Ananke (J XII), and Miranda (Uranus V).\n\nRoy found that all six fitted very closely. This might have been an exaggeration: out of these six bodies, four were sharing positions with objects that were already known in 1913; concerning the two others, there was a ~6% overestimate for Pluto; and later, a 6% underestimate for Miranda became apparent.[15]\n\nBodies in parentheses were not known in 1913, when Blagg wrote her paper. Some of the calculated distances in the Saturn and Uranus systems are not very accurate. This is because the low values of constant B in the table above make them very sensitive to the exact form of the  function  f .\n\nIn a 1945 Popular Astronomy magazine article,[17]\nthe science writer D.E. Richardson apparently independently arrived at the same conclusion as Blagg: That the progression ratio is 1.728 rather than 2. His spacing law is in the form:\n\nR\n          \n            n\n          \n        \n        =\n        \n          \n            (\n          \n        \n         \n        1.728\n         \n        \n          \n            \n              )\n            \n          \n          \n            n\n          \n        \n         \n        \n          ϱ\n          \n            n\n          \n        \n        (\n        \n          θ\n          \n            n\n          \n        \n        )\n         \n        ,\n      \n    \n    {\\displaystyle \\ R_{n}={\\bigl (}\\ 1.728\\ {\\bigr )}^{n}\\ \\varrho _{n}(\\theta _{n})\\ ,}\n\nwhere \n  \n    \n      \n        \n          ϱ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\varrho _{n}}\n  \n is an oscillatory function with period \n  \n    \n      \n        2\n        π\n      \n    \n    {\\displaystyle 2\\pi }\n  \n, representing distances \n  \n    \n      \n        \n          ϱ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\varrho _{n}}\n  \n from an off-centered origin to points on an ellipse.\n\nNieto, who conducted the first modern comprehensive review of the Titius–Bode Law,[18] noted that \"The psychological hold of the Law on astronomy has been such that people have always tended to regard its original form as the one on which to base theories.\" He was emphatic that \"future theories must rid themselves of the bias of trying to explain a progression ratio of 2\":\n\nOne thing which needs to be emphasized is that the historical bias towards a progression ratio of 2 must be abandoned. It ought to be clear that the first formulation of Titius (with its asymmetric first term) should be viewed as a good first guess. Certainly, it should not necessarily be viewed as the best guess to refer theories to. But in astronomy the weight of history is heavy ... Despite the fact that the number 1.73 is much better, astronomers cling to the original number 2.[1]\n\nNo solid theoretical explanation underlies the Titius–Bode law – but it is possible that, given a combination of orbital resonance and shortage of degrees of freedom, any stable planetary system has a high probability of satisfying a Titius–Bode-type relationship. Since it may be a mathematical coincidence rather than a \"law of nature\", it is sometimes referred to as a rule instead of \"law\".[19] Astrophysicist Alan Boss states that it is just a coincidence, and the planetary science journal Icarus no longer accepts papers attempting to provide improved versions of the \"law\".[13]\n\nOrbital resonance from major orbiting bodies creates regions around the Sun that are free of long-term stable orbits. Results from simulations of planetary formation support the idea that a randomly chosen, stable planetary system will likely satisfy a Titius–Bode law.[20]\n\nDubrulle and Graner[21][22] showed that power-law distance rules can be a consequence of collapsing-cloud models of planetary systems possessing two symmetries: rotational invariance (i.e., the cloud and its contents are axially symmetric) and scale invariance (i.e., the cloud and its contents look the same on all scales). The latter is a feature of many phenomena considered to play a role in planetary formation, such as turbulence.\n\nOnly a limited number of systems are available upon which Bode's law can presently be tested; two solar planets have enough large moons that probably formed in a process similar to that which formed the planets: The four large satellites of Jupiter and the biggest inner satellite (i.e., Amalthea) cling to a regular, but non-Titius-Bode, spacing, with the four innermost satellites locked into orbital periods that are each twice that of the next inner satellite. Similarly, the large moons of Uranus have a regular but non-Titius-Bode spacing.[23]\nHowever, according to Martin Harwit\n\nOf the recent discoveries of extrasolar planetary systems, few have enough known planets to test whether similar rules apply. An attempt with 55 Cancri suggested the equation\n\nand controversially[25]\npredicts an undiscovered planet or asteroid field for \n  \n    \n      \n         \n        n\n        =\n        5\n         \n      \n    \n    {\\displaystyle ~n=5~}\n  \n at 2 AU.[26]\nFurthermore, the orbital period and semi-major axis of the innermost planet in the 55 Cancri system have been greatly revised (from 2.817 days to 0.737 days and from 0.038 AU to 0.016 AU, respectively) since the publication of these studies.[27]\n\nRecent astronomical research suggests that planetary systems around some other stars may follow Titius-Bode-like laws.[28][29]\nBovaird & Lineweaver (2013)[30]\napplied a generalized Titius-Bode relation to 68 exoplanet systems that contain four or more planets. They showed that 96% of these exoplanet systems adhere to a generalized Titius-Bode relation to a similar or greater extent than the Solar System does. The locations of potentially undetected exoplanets are predicted in each system.[30]\n\nSubsequent research detected 5 candidate planets from the 97 planets predicted for the 68 planetary systems. The study showed that the actual number of planets could be larger. The occurrence rates of Mars- and Mercury-sized planets are unknown, so many planets could be missed due to their small size. Other possible reasons that may account for apparent discrepancies include planets that do not transit the star or circumstances in which the predicted space is occupied by circumstellar disks. Despite these types of allowances, the number of planets found with Titius–Bode law predictions was lower than expected.[31]\n\nIn a 2018 paper, the idea of a hypothetical eighth planet around TRAPPIST-1 named \"TRAPPIST‑1i\", was proposed by using the Titius–Bode law. TRAPPIST‑1i had a prediction based exclusively on the Titius–Bode law with an orbital period of 27.53 ± 0.83 days.[32]\n\nFinally, raw statistics from exoplanetary orbits strongly point to a general fulfillment of Titius-Bode-like laws (with exponential increase of semi-major axes as a function of planetary index) in all the exoplanetary systems; when making a blind histogram of orbital semi-major axes for all the known exoplanets for which this magnitude is known,[33] and comparing it with what should be expected if planets distribute according to Titius-Bode-like laws, a significant degree of agreement (i.e., 78%)[34]\nis obtained.",
        pageTitle: "Titius–Bode law",
    },
    {
        title: "Born's law",
        link: "https://en.wikipedia.org/wiki/Born%27s_law",
        content:
            "The Born rule is a postulate of quantum mechanics that gives the probability that a measurement of a quantum system will yield a given result. In one commonly used application, it states that the probability density for finding a particle at a given position is proportional to the square of the amplitude of the system's wavefunction at that position. It was formulated and published by German physicist Max Born in July 1926.[1]\n\nThe Born rule states that an observable, measured in a system with normalized wave function \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n (see Bra–ket notation), corresponds to a self-adjoint operator \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n whose spectrum is discrete if:\n\nIn the case where the spectrum of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is not wholly discrete, the spectral theorem proves the existence of a certain projection-valued measure (PVM) \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, the spectral measure of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. In this case:\n\nFor example, a single structureless particle can be described by a wave function \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n that depends upon position coordinates \n  \n    \n      \n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n      \n    \n    {\\displaystyle (x,y,z)}\n  \n and a time coordinate \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. The Born rule implies that the probability density function \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n for the result of a measurement of the particle's position at time \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n is:\n\n  \n    \n      \n        p\n        (\n        x\n        ,\n        y\n        ,\n        z\n        ,\n        \n          t\n          \n            0\n          \n        \n        )\n        =\n        \n          |\n        \n        ψ\n        (\n        x\n        ,\n        y\n        ,\n        z\n        ,\n        \n          t\n          \n            0\n          \n        \n        )\n        \n          \n            |\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle p(x,y,z,t_{0})=|\\psi (x,y,z,t_{0})|^{2}.}\n  \n\nThe Born rule can also be employed to calculate probabilities (for measurements with discrete sets of outcomes) or probability densities (for continuous-valued measurements) for other observables, like momentum, energy, and angular momentum.\n\nIn some applications, this treatment of the Born rule is generalized using positive-operator-valued measures (POVM). A POVM is a measure whose values are positive semi-definite operators on a Hilbert space. POVMs are a generalization of von Neumann measurements and, correspondingly, quantum measurements described by POVMs are a generalization of quantum measurements described by self-adjoint observables. In rough analogy, a POVM is to a PVM what a mixed state is to a pure state. Mixed states are needed to specify the state of a subsystem of a larger system (see purification of quantum state); analogously, POVMs are necessary to describe the effect on a subsystem of a projective measurement performed on a larger system. POVMs are the most general kind of measurement in quantum mechanics and can also be used in quantum field theory.[2] They are extensively used in the field of quantum information.\n\nIn the simplest case, of a POVM with a finite number of elements acting on a finite-dimensional Hilbert space, a POVM is a set of positive semi-definite matrices \n  \n    \n      \n        {\n        \n          F\n          \n            i\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{F_{i}\\}}\n  \n on a Hilbert space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n that sum to the identity matrix,:[3]: 90 \n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          F\n          \n            i\n          \n        \n        =\n        I\n        .\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}F_{i}=I.}\n\nThe POVM element \n  \n    \n      \n        \n          F\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle F_{i}}\n  \n is associated with the measurement outcome \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, such that the probability of obtaining it when making a measurement on the quantum state \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is given by:\n\np\n        (\n        i\n        )\n        =\n        tr\n        ⁡\n        (\n        ρ\n        \n          F\n          \n            i\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle p(i)=\\operatorname {tr} (\\rho F_{i}),}\n\nwhere \n  \n    \n      \n        tr\n      \n    \n    {\\displaystyle \\operatorname {tr} }\n  \n is the trace operator. This is the POVM version of the Born rule. When the quantum state being measured is a pure state \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n this formula reduces to:\n\n  \n    \n      \n        p\n        (\n        i\n        )\n        =\n        tr\n        ⁡\n        \n          \n            (\n          \n        \n        \n          |\n        \n        ψ\n        ⟩\n        ⟨\n        ψ\n        \n          |\n        \n        \n          F\n          \n            i\n          \n        \n        \n          \n            )\n          \n        \n        =\n        ⟨\n        ψ\n        \n          |\n        \n        \n          F\n          \n            i\n          \n        \n        \n          |\n        \n        ψ\n        ⟩\n        .\n      \n    \n    {\\displaystyle p(i)=\\operatorname {tr} {\\big (}|\\psi \\rangle \\langle \\psi |F_{i}{\\big )}=\\langle \\psi |F_{i}|\\psi \\rangle .}\n\nThe Born rule, together with the unitarity of the time evolution operator \n  \n    \n      \n        \n          e\n          \n            −\n            i\n            \n              \n                \n                  H\n                  ^\n                \n              \n            \n            t\n          \n        \n      \n    \n    {\\displaystyle e^{-i{\\hat {H}}t}}\n  \n (or, equivalently, the Hamiltonian \n  \n    \n      \n        \n          \n            \n              H\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {H}}}\n  \n being Hermitian), implies the unitarity of the theory: a wave function that is time-evolved by a unitary operator will remain properly normalized. (In the more general case where one considers the time evolution of a density matrix, proper normalization is ensured by requiring that the time evolution is a trace-preserving, completely positive map.)\n\nThe Born rule was formulated by Born in a 1926 paper.[4] In this paper, Born solves the Schrödinger equation for a scattering problem and, inspired by Albert Einstein and Einstein's probabilistic rule for the photoelectric effect,[5] concludes, in a footnote, that the Born rule gives the only possible interpretation of the solution. (The main body of the article says that the amplitude \"gives the probability\" [bestimmt die Wahrscheinlichkeit], while the footnote added in proof says that the probability is proportional to the square of its magnitude.) In 1954, together with Walther Bothe, Born was awarded the Nobel Prize in Physics for this and other work.[5] John von Neumann discussed the application of spectral theory to Born's rule in his 1932 book.[6]\n\nGleason's theorem shows that the Born rule can be derived from the usual mathematical representation of measurements in quantum physics together with the assumption of non-contextuality. Andrew M. Gleason first proved the theorem in 1957,[7] prompted by a question posed by George W. Mackey.[8][9] This theorem was historically significant for the role it played in showing that wide classes of hidden-variable theories are inconsistent with quantum physics.[10]\n\nSeveral other researchers have also tried to derive the Born rule from more basic principles. A number of derivations have been proposed in the context of the many-worlds interpretation. These include the decision-theory approach pioneered by David Deutsch[11] and later developed by Hilary Greaves[12] and David Wallace;[13] and an \"envariance\" approach by Wojciech H. Zurek.[14] These proofs have, however, been criticized as circular.[15] In 2018, an approach based on self-locating uncertainty was suggested by Charles Sebens and Sean M. Carroll;[16] this has also been criticized.[17] Simon Saunders, in 2021, produced a branch counting derivation of the Born rule. The crucial feature of this approach is to define the branches so that they all have the same magnitude or 2-norm. The ratios of the numbers of branches thus defined give the probabilities of the various outcomes of a measurement, in accordance with the Born rule.[18]\n\nIn 2019, Lluís Masanes, Thomas Galley, and Markus Müller proposed a derivation based on postulates including the possibility of state estimation.[19][20]\n\nIt has also been claimed that pilot-wave theory can be used to statistically derive the Born rule, though this remains controversial.[21]\n\nWithin the QBist interpretation of quantum theory, the Born rule is seen as an extension of the normative principle of coherence, which ensures self-consistency of probability assessments across a whole set of such assessments. It can be shown that an agent who thinks they are gambling on the outcomes of measurements on a sufficiently quantum-like system but refuses to use the Born rule when placing their bets is vulnerable to a Dutch book.[22]",
        pageTitle: "Born rule",
    },
    {
        title: "Boyle's law",
        link: "https://en.wikipedia.org/wiki/Boyle%27s_law",
        content:
            "Boyle's law, also referred to as the Boyle–Mariotte law or Mariotte's law (especially in France), is an empirical gas law that describes the relationship between pressure and volume of a confined gas. Boyle's law has been stated as:\n\nThe absolute pressure exerted by a given mass of an ideal gas is inversely proportional to the volume it occupies if the temperature and amount of gas remain unchanged within a closed system.[1][2]\n\nwhere P is the pressure of the gas, V is the volume of the gas, and k is a constant for a particular temperature and amount of gas.\n\nBoyle's law states that when the temperature of a given mass of confined gas is constant, the product of its pressure and volume is also constant. When comparing the same substance under two different sets of conditions, the law can be expressed as:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}.}\n\nshowing that as volume increases, the pressure of a gas decreases proportionally, and vice versa.\n\nBoyle's law is named after Robert Boyle, who published the original law in 1662.[3] An equivalent law is Mariotte’s law, named after French physicist Edme Mariotte.\n\nThe relationship between pressure and volume was first noted by Richard Towneley and Henry Power in the 17th century.[5][6] Robert Boyle confirmed their discovery through experiments and published the results.[7] According to Robert Gunther and other authorities, it was Boyle's assistant, Robert Hooke, who built the experimental apparatus.  Boyle's law is based on experiments with air, which he considered to be a fluid of particles at rest in between small invisible springs. Boyle may have begun experimenting with gases due to an interest in air as an essential element of life;[8] for example, he published works on the growth of plants without air.[9] Boyle used a closed J-shaped tube and after pouring mercury from one side he forced the air on the other side to contract under the pressure of mercury. After repeating the experiment several times and using different amounts of mercury he found that under controlled conditions, the pressure of a gas is inversely proportional to the volume occupied by it.[10]\n\nThe French physicist Edme Mariotte (1620–1684) discovered the same law independently of Boyle in 1679,[11] after Boyle had published it in 1662.[10] Mariotte did, however, discover that air volume changes with temperature.[12] Thus this law is sometimes referred to as Mariotte's law or the Boyle–Mariotte law. Later, in 1687 in the Philosophiæ Naturalis Principia Mathematica, Newton showed mathematically that in an elastic fluid consisting of particles at rest, between which are repulsive forces inversely proportional to their distance, the density would be directly proportional to the pressure,[13] but this mathematical treatise does not involve any Mariott temperature dependance and is not the proper physical explanation for the observed relationship. Instead of a static theory, a kinetic theory is needed, which was developed over the next two centuries by Daniel Bernoulli (1738) and more fully by Rudolf Clausius (1857), Maxwell and Boltzmann.\n\nThis law was the first physical law to be expressed in the form of an equation describing the dependence of two variable quantities.[10]\n\nFor a fixed mass of an ideal gas kept at a fixed temperature, pressure and volume are inversely proportional.[2]\n\nBoyle's law is a gas law, stating that the pressure and volume of a gas have an inverse relationship. If volume increases, then pressure decreases and vice versa, when the temperature is held constant.\n\nTherefore, when the volume is halved, the pressure is doubled; and if the volume is doubled, the pressure is halved.\n\nAs the pressure on a gas increases, the volume of the gas decreases because the gas particles are forced closer together.\n\nMost gases behave like ideal gases at moderate pressures and temperatures. The technology of the 17th century could not produce very high pressures or very low temperatures. Hence, the law was not likely to have deviations at the time of publication. As improvements in technology permitted higher pressures and lower temperatures, deviations from the ideal gas behavior became noticeable, and the relationship between pressure and volume can only be accurately described employing real gas theory.[14]  The deviation is expressed as the compressibility factor.\n\nBoyle (and  Mariotte) derived the law solely by experiment. The law can also be derived theoretically based on the presumed existence of atoms and molecules and assumptions about motion and perfectly elastic collisions (see kinetic theory of gases). These assumptions were met with enormous resistance in the positivist scientific community at the time, however, as they were seen as purely theoretical constructs for which there was not the slightest observational evidence.\n\nDaniel Bernoulli (in 1737–1738) derived Boyle's law by applying Newton's laws of motion at the molecular level. It remained ignored until around 1845, when John Waterston published a paper building the main precepts of kinetic theory; this was rejected by the Royal Society of England. Later works of James Prescott Joule, Rudolf Clausius and in particular Ludwig Boltzmann firmly established the kinetic theory of gases and brought attention to both the theories of Bernoulli and Waterston.[15]\n\nThe debate between proponents of energetics and atomism led Boltzmann to write a book in 1898, which endured criticism until his suicide in 1906.[15] Albert Einstein in 1905 showed how kinetic theory applies to the Brownian motion of a fluid-suspended particle, which was confirmed in 1908 by Jean Perrin.[15]\n\nP\n        V\n        =\n        k\n      \n    \n    {\\displaystyle PV=k}\n\nwhere P denotes the pressure of the system, V denotes the volume of the gas, k is a constant value representative of the temperature of the system and amount of gas.\n\nSo long as temperature remains constant the same amount of energy given to the system persists throughout its operation and therefore, theoretically, the value of k will remain constant. However, due to the derivation of pressure as perpendicular applied force and the probabilistic likelihood of collisions with other particles through collision theory, the application of force to a surface may not be infinitely constant for such values of V, but will have a limit when differentiating such values over a given time. Forcing the volume V of the fixed quantity of gas to increase, keeping the gas at the initially measured temperature, the pressure P must decrease proportionally. Conversely, reducing the volume of the gas increases the pressure. Boyle's law is used to predict the result of introducing a change, in volume and pressure only, to the initial state of a fixed quantity of gas.\n\nThe initial and final volumes and pressures of the fixed amount of gas, where the initial and final temperatures are the same (heating or cooling will be required to meet this condition), are related by the equation:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}.}\n\nHere P1 and V1 represent the original pressure and volume, respectively, and P2 and V2 represent the second pressure and volume.\n\nBoyle's law, Charles's law, and Gay-Lussac's law form the combined gas law. The three gas laws in combination with Avogadro's law can be generalized by the ideal gas law.\n\nBoyle's law is often used as part of an explanation on how the breathing system works in the human body. This commonly involves explaining how the lung volume may be increased or decreased and thereby cause a relatively lower or higher air pressure within them (in keeping with Boyle's law). This forms a pressure difference between the air inside the lungs and the environmental air pressure, which in turn precipitates either inhalation or exhalation as air moves from high to low pressure.[16]",
        pageTitle: "Boyle's law",
    },
    {
        title: "gas laws",
        link: "https://en.wikipedia.org/wiki/Gas_laws",
        content:
            "The laws describing the behaviour of gases under fixed pressure, volume, amount of gas, and absolute temperature conditions are called gas laws. The basic gas laws were discovered by the end of the 18th century when scientists found out that relationships between pressure, volume and temperature of a sample of gas could be obtained which would hold to approximation for all gases. The combination of several empirical gas laws led to the development of the ideal gas law.\n\nThe ideal gas law was later found to be consistent with atomic and kinetic theory.\n\nIn 1643, the Italian physicist and mathematician, Evangelista Torricelli, who for a few months had acted as Galileo Galilei's secretary, conducted a celebrated experiment in Florence.[1] He demonstrated that a column of mercury in an inverted tube can be supported by the pressure of air outside of the tube, with the creation of a small section of vacuum above the mercury.[2] This experiment essentially paved the way towards the invention of the barometer, as well as drawing the attention of Robert Boyle, then a \"skeptical\" scientist working in England. Boyle was inspired by Torricelli's experiment to investigate how the elasticity of air responds to varying pressure, and he did this through a series of experiments with a setup reminiscent of that used by Torricelli.[3] Boyle published his results in 1662.\n\nLater on, in 1676, the French physicist Edme Mariotte, independently arrived at the same conclusions of Boyle, while also noting some dependency of air volume on temperature.[4] However it took another century and a half for the development of thermometry and recognition of the absolute zero temperature scale, which eventually allowed the discovery of temperature-dependent gas laws.\n\nIn 1662, Robert Boyle systematically studied the relationship between the volume and pressure of a fixed amount of gas at a constant temperature. He observed that the volume of a given mass of a gas is inversely proportional to its pressure at a constant temperature.\nBoyle's law, published in 1662, states that, at a constant temperature, the product of the pressure and volume of a given mass of an ideal gas in a closed system is always constant. It can be verified experimentally using a pressure gauge and a variable volume container. It can also be derived from the kinetic theory of gases: if a container, with a fixed number of molecules inside, is reduced in volume, more molecules will strike a given area of the sides of the container per unit time, causing a greater pressure.\n\nThe concept can be represented with these formulae:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}}\n  \n\nwhere P is the pressure, V is the volume of a gas, and k1 is the constant in this equation (and is not the same as the proportionality constants in the other equations).\n\nCharles' law, or the law of volumes, was founded in 1787 by Jacques Charles. It states that, for a given mass of an ideal gas at constant pressure, the volume is directly proportional to its absolute temperature, assuming in a closed system.\nThe statement of Charles' law is as follows:\nthe volume (V) of a given mass of a gas, at constant pressure (P), is directly proportional to its temperature (T).\n\nwhere \"V\" is the volume of a gas, \"T\" is the absolute temperature and k2 is a proportionality constant (which is not the same as the proportionality constants in the other equations in this article).\n\nGay-Lussac's law, Amontons' law or the pressure law was founded by Joseph Louis Gay-Lussac in 1808.\n\nP\n              \n                1\n              \n            \n            \n              T\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              P\n              \n                2\n              \n            \n            \n              T\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {P_{1} \\over T_{1}}={P_{2} \\over T_{2}}}\n  \n,\n\nAvogadro's law, Avogadro's hypothesis, Avogadro's principle or Avogadro-Ampère's hypothesis is an experimental gas law which was hypothesized by Amedeo Avogadro in 1811. It related the volume of a gas to the amount of substance of gas present.[5]\n\nThis statement gives rise to the molar volume of a gas, which at STP (273.15 K, 1 atm) is about 22.4 L. The relation is given by:\n\nThe combined gas law or general gas equation is obtained by combining Boyle's law, Charles's law, and Gay-Lussac's law. It shows the relationship between the pressure, volume, and temperature for a fixed mass of gas:\n\nWith the addition of Avogadro's law, the combined gas law develops into the ideal gas law:\n\nThese equations are exact only for an ideal gas, which neglects various intermolecular effects (see real gas). However, the ideal gas law is a good approximation for most gases under moderate pressure and temperature.",
        pageTitle: "Gas laws",
    },
    {
        title: "Bradford's law",
        link: "https://en.wikipedia.org/wiki/Bradford%27s_law",
        content:
            "Bradford's law is a pattern first described by Samuel C. Bradford in 1934 that estimates the exponentially diminishing returns of searching for references in science journals. One formulation is that if journals in a field are sorted by number of articles into three groups, each with about one-third of all articles, then the number of journals in each group will be proportional to 1:n:n2.[1] There are a number of related formulations of the principle.\n\nIn many disciplines, this pattern is called a Pareto distribution. As a practical example, suppose that a researcher has five core scientific journals for his or her subject. Suppose that in a month there are 12 articles of interest in those journals. Suppose further that in order to find another dozen articles of interest, the researcher would have to go to an additional 10 journals. Then that researcher's Bradford multiplier bm is 2 (i.e. 10/5). For each new dozen articles, that researcher will need to look in bm times as many journals. After looking in 5, 10, 20, 40, etc. journals, most researchers quickly realize that there is little point in looking further.\n\nDifferent researchers have different numbers of core journals, and different Bradford multipliers. But the pattern holds quite well across many subjects, and may well be a general pattern for human interactions in social systems. Like Zipf's law, to which it is related, we do not have a good explanation for why it works, but knowing that it does is very useful for librarians. What it means is that for each specialty, it is sufficient to identify the \"core publications\" for that field and only stock those; very rarely will researchers need to go outside that set.[verification needed]\n\nHowever, its impact has been far greater than that. Armed with this idea and inspired by Vannevar Bush's famous article As We May Think, Eugene Garfield at the Institute for Scientific Information in the 1960s developed a comprehensive index of how scientific thinking propagates. His Science Citation Index (SCI) had the effect of making it easy to identify exactly which scientists did science that had an impact, and which journals that science appeared in. It also caused the  discovery, which some did not expect, that a few journals, such as Nature and Science, were core for all of hard science. The same pattern does not happen with the humanities or the social sciences.\n\nThe result of this is pressure on scientists to publish in the best journals, and pressure on universities to ensure access to that core set of journals. On the other hand, the set of \"core journals\" may vary more or less strongly with the individual researchers, and even more strongly along schools-of-thought divides. There is also a danger of over-representing majority views if journals are selected in this fashion.\n\nBradford's law is also known as Bradford's law of scattering or the Bradford distribution, as it describes how the articles on a particular subject are scattered throughout the mass of periodicals.[2] Another more general term that has come into use since 2006 is information scattering, an often observed phenomenon related to information collections where there are a few sources that have many items of relevant information about a topic, while most sources have only a few.[3] This law of distribution in bibliometrics can be applied to the World Wide Web as well.[4]\n\nHjørland and Nicolaisen identified three kinds of scattering:[5]\n\nThey found that the literature of Bradford's law (including Bradford's own papers) is unclear in relation to which kind of scattering is actually being measured.\n\nThe interpretation of Bradford's law in terms of a geometric progression was suggested by V. Yatsko,[6] who introduced an additional constant and demonstrated that Bradford distribution can be applied to a variety of objects, not only to distribution of articles or citations across journals.  V. Yatsko's interpretation (Y-interpretation) can be effectively used to compute threshold values in case it is necessary to distinguish subsets within a set of objects (successful/unsuccessful applicants, developed/underdeveloped regions, etc.).",
        pageTitle: "Bradford's law",
    },
    {
        title: "Bragg's law",
        link: "https://en.wikipedia.org/wiki/Bragg%27s_law",
        content:
            "In many areas of science, Bragg's law, Wulff–Bragg's condition, or Laue–Bragg interference are a special case of Laue diffraction, giving the angles for coherent scattering of waves from a large crystal lattice. It describes how the superposition of wave fronts scattered by lattice planes leads to a strict relation between the wavelength and scattering angle. This law was initially formulated for X-rays, but it also applies to all types of matter waves including neutron and electron waves if there are a large number of atoms, as well as visible light with artificial periodic microscale lattices.\n\nBragg diffraction (also referred to as the Bragg formulation of X-ray diffraction) was first proposed by Lawrence Bragg and his father, William Henry Bragg, in 1913[1] after their discovery that crystalline solids produced surprising patterns of reflected X-rays (in contrast to those produced with, for instance, a liquid).  They found that these crystals, at certain specific wavelengths and incident angles, produced intense peaks of reflected radiation.\n\nLawrence Bragg explained this result by modeling the crystal as a set of discrete parallel planes separated by a constant parameter d.  He proposed that the incident X-ray radiation would produce a Bragg peak if reflections off the various planes interfered constructively. The interference is constructive when the phase difference between the wave reflected off different atomic planes is a multiple of 2π; this condition (see Bragg condition section below) was first presented by Lawrence Bragg on 11 November 1912 to the Cambridge Philosophical Society.[2] Although simple, Bragg's law confirmed the existence of real particles at the atomic scale, as well as providing a powerful new tool for studying crystals. Lawrence Bragg and his father, William Henry Bragg, were awarded the Nobel Prize in physics in 1915 for their work in determining crystal structures beginning with NaCl, ZnS, and diamond.[3] They are the only father-son team to jointly win.\n\nThe concept of Bragg diffraction applies equally to neutron diffraction[4] and approximately to electron diffraction.[5] In both cases the wavelengths are comparable with inter-atomic distances (~ 150 pm). Many other types of matter waves have also been shown to diffract,[6][7] and also light from objects with a larger ordered structure such as opals.[8]\n\nBragg diffraction occurs when radiation of a wavelength λ comparable to atomic spacings is scattered in a specular fashion (mirror-like reflection) by planes of atoms in a crystalline material, and undergoes constructive interference.[10] When the scattered waves are incident at a specific angle, they remain in phase and constructively interfere. The glancing angle θ (see figure on the right, and note that this differs from the convention in Snell's law where θ is measured from the surface normal), the wavelength λ, and the \"grating constant\" d of the crystal are connected by the relation:[11]: 1026 \n  \n    \n      \n        n\n        λ\n        =\n        2\n        d\n        sin\n        ⁡\n        θ\n      \n    \n    {\\displaystyle n\\lambda =2d\\sin \\theta }\n  \nwhere \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the diffraction order (\n  \n    \n      \n        n\n        =\n        1\n      \n    \n    {\\displaystyle n=1}\n  \n is first order,  \n  \n    \n      \n        n\n        =\n        2\n      \n    \n    {\\displaystyle n=2}\n  \n is second order,[10]: 221  \n  \n    \n      \n        n\n        =\n        3\n      \n    \n    {\\displaystyle n=3}\n  \n is third order[11]: 1028 ). This equation, Bragg's law, describes the condition on θ for constructive interference.[12]\n\nA map of the intensities of the scattered waves as a function of their angle is called a diffraction pattern. Strong intensities known as Bragg peaks are obtained in the diffraction pattern when the scattering angles satisfy Bragg condition. This is a special case of the more general Laue equations, and the Laue equations can be shown to reduce to the Bragg condition with additional assumptions.[13]\n\nIn Bragg's original paper he describes his approach as a Huygens' construction for a reflected wave.[14]: 46 \nSuppose that a plane wave (of any type) is incident on planes of lattice points, with separation \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n, at an angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n as shown in the Figure.  Points A and C are on one plane, and B is on the plane below.  Points ABCC' form a quadrilateral.[15]: 69\n\nThere will be a path difference between the ray that gets reflected along AC' and the ray that gets transmitted along AB, then reflected along BC. This path difference is\n\n  \n    \n      \n        (\n        A\n        B\n        +\n        B\n        C\n        )\n        −\n        \n          (\n          \n            A\n            \n              C\n              ′\n            \n          \n          )\n        \n        \n        .\n      \n    \n    {\\displaystyle (AB+BC)-\\left(AC'\\right)\\,.}\n\nThe two separate waves will arrive at a point (infinitely far from these lattice planes) with the same phase, and hence undergo constructive interference, if and only if this path difference is equal to any integer value of the wavelength, i.e.\n\n  \n    \n      \n        n\n        λ\n        =\n        (\n        A\n        B\n        +\n        B\n        C\n        )\n        −\n        \n          (\n          \n            A\n            \n              C\n              ′\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle n\\lambda =(AB+BC)-\\left(AC'\\right)}\n\nwhere \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n and \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n are an integer and the wavelength of the incident wave respectively.\n\nTherefore, from the geometry\n\n  \n    \n      \n        A\n        B\n        =\n        B\n        C\n        =\n        \n          \n            d\n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        \n           and \n        \n        A\n        C\n        =\n        \n          \n            \n              2\n              d\n            \n            \n              tan\n              ⁡\n              θ\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle AB=BC={\\frac {d}{\\sin \\theta }}{\\text{ and }}AC={\\frac {2d}{\\tan \\theta }}\\,,}\n\nfrom which it follows that\n\n  \n    \n      \n        A\n        \n          C\n          ′\n        \n        =\n        A\n        C\n        ⋅\n        cos\n        ⁡\n        θ\n        =\n        \n          \n            \n              2\n              d\n            \n            \n              tan\n              ⁡\n              θ\n            \n          \n        \n        cos\n        ⁡\n        θ\n        =\n        \n          (\n          \n            \n              \n                \n                  2\n                  d\n                \n                \n                  sin\n                  ⁡\n                  θ\n                \n              \n            \n            cos\n            ⁡\n            θ\n          \n          )\n        \n        cos\n        ⁡\n        θ\n        =\n        \n          \n            \n              2\n              d\n            \n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        \n          cos\n          \n            2\n          \n        \n        ⁡\n        θ\n        \n        .\n      \n    \n    {\\displaystyle AC'=AC\\cdot \\cos \\theta ={\\frac {2d}{\\tan \\theta }}\\cos \\theta =\\left({\\frac {2d}{\\sin \\theta }}\\cos \\theta \\right)\\cos \\theta ={\\frac {2d}{\\sin \\theta }}\\cos ^{2}\\theta \\,.}\n\nPutting everything together,\n\n  \n    \n      \n        n\n        λ\n        =\n        \n          \n            \n              2\n              d\n            \n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        −\n        \n          \n            \n              2\n              d\n            \n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        \n          cos\n          \n            2\n          \n        \n        ⁡\n        θ\n        =\n        \n          \n            \n              2\n              d\n            \n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        \n          (\n          \n            1\n            −\n            \n              cos\n              \n                2\n              \n            \n            ⁡\n            θ\n          \n          )\n        \n        =\n        \n          \n            \n              2\n              d\n            \n            \n              sin\n              ⁡\n              θ\n            \n          \n        \n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        θ\n      \n    \n    {\\displaystyle n\\lambda ={\\frac {2d}{\\sin \\theta }}-{\\frac {2d}{\\sin \\theta }}\\cos ^{2}\\theta ={\\frac {2d}{\\sin \\theta }}\\left(1-\\cos ^{2}\\theta \\right)={\\frac {2d}{\\sin \\theta }}\\sin ^{2}\\theta }\n\nwhich simplifies to \n  \n    \n      \n        n\n        λ\n        =\n        2\n        d\n        sin\n        ⁡\n        θ\n        \n        ,\n      \n    \n    {\\displaystyle n\\lambda =2d\\sin \\theta \\,,}\n  \n which is Bragg's law shown above.\n\nIf only two planes of atoms were diffracting, as shown in the Figure then the transition from constructive to destructive interference would be gradual as a function of angle, with gentle maxima at the Bragg angles. However, since many atomic planes are participating in most real materials, sharp peaks are typical.[5][13]\n\nA rigorous derivation from the more general Laue equations is available (see page: Laue equations).\n\nThe Bragg condition is correct for very large crystals. Because the scattering of X-rays and neutrons is relatively weak, in many cases quite large crystals with sizes of 100 nm or more are used. While there can be additional effects due to crystal defects, these are often quite small. In contrast, electrons interact thousands of times more strongly with solids than X-rays,[5] and also lose energy (inelastic scattering).[16] Therefore samples used in transmission electron diffraction are much thinner. Typical diffraction patterns, for instance the Figure, show spots for different directions (plane waves) of the electrons leaving a crystal. The angles that Bragg's law predicts are still approximately right, but in general there is a lattice of spots which are close to projections of the reciprocal lattice that is at right angles to the direction of the electron beam. (In contrast, Bragg's law predicts that only one or perhaps two would be present, not simultaneously tens to hundreds.) With low-energy electron diffraction where the electron energies are typically 30-1000 electron volts, the result is similar with the electrons reflected back from a surface.[17] Also similar is reflection high-energy electron diffraction which  typically leads to rings of diffraction spots.[18]\n\nWith X-rays the effect of having small crystals is described by the Scherrer equation.[13][19][20] This leads to broadening of the Bragg peaks which can be used to estimate the size of the crystals.\n\nA colloidal crystal is a highly ordered array of particles that forms over a long range (from a few millimeters to one centimeter in length); colloidal crystals have appearance and properties roughly analogous to their atomic or molecular counterparts.[8] It has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations, with interparticle separation distances often being considerably greater than the individual particle diameter. Periodic arrays of spherical particles give rise to interstitial voids (the spaces between the particles), which act as a natural diffraction grating for visible light waves, when the interstitial spacing is of the same order of magnitude as the incident lightwave.[21][22][23] In these cases brilliant iridescence (or play of colours) is attributed to the diffraction and constructive interference of visible lightwaves according to Bragg's law, in a matter analogous to the scattering of X-rays in crystalline solid. The effects occur at visible wavelengths because the interplanar spacing d is much larger than for true crystals. Precious opal is one example of a colloidal crystal with optical effects.\n\nVolume Bragg gratings (VBG) or volume holographic gratings (VHG) consist of a volume where there is a periodic change in the refractive index. Depending on the orientation of the refractive index modulation, VBG can be used either to transmit or reflect a small bandwidth of wavelengths.[24] Bragg's law (adapted for volume hologram) dictates which wavelength will be diffracted:[25]\n\n2\n        Λ\n        sin\n        ⁡\n        (\n        θ\n        +\n        φ\n        )\n        =\n        m\n        \n          λ\n          \n            B\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle 2\\Lambda \\sin(\\theta +\\varphi )=m\\lambda _{B}\\,,}\n\nwhere m is the Bragg order (a positive integer), λB the diffracted wavelength, Λ the fringe spacing of the grating, θ the angle between the incident beam and the normal (N) of the entrance surface and φ the angle between the normal and the grating vector (KG). Radiation that does not match Bragg's law will pass through the VBG undiffracted. The output wavelength can be tuned over a few hundred nanometers by changing the incident angle (θ). VBG are being used to produce widely tunable laser source or perform global hyperspectral imagery (see Photon etc.).[25]\n\nThe measurement of the angles can be used to determine crystal structure, see x-ray crystallography for more details.[5][13] As a simple example, Bragg's law, as stated above, can be used to obtain the lattice spacing of a particular cubic system through the following relation:\n\nd\n        =\n        \n          \n            a\n            \n              \n                h\n                \n                  2\n                \n              \n              +\n              \n                k\n                \n                  2\n                \n              \n              +\n              \n                ℓ\n                \n                  2\n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle d={\\frac {a}{\\sqrt {h^{2}+k^{2}+\\ell ^{2}}}}\\,,}\n\nwhere \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is the lattice spacing of the cubic crystal, and h, k, and ℓ are the Miller indices of the Bragg plane. Combining this relation with Bragg's law gives:\n\n(\n            \n              \n                λ\n                \n                  2\n                  a\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          \n            (\n            \n              \n                λ\n                \n                  2\n                  d\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        \n          \n            1\n            \n              \n                h\n                \n                  2\n                \n              \n              +\n              \n                k\n                \n                  2\n                \n              \n              +\n              \n                ℓ\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\left({\\frac {\\lambda }{2a}}\\right)^{2}=\\left({\\frac {\\lambda }{2d}}\\right)^{2}{\\frac {1}{h^{2}+k^{2}+\\ell ^{2}}}}\n\nOne can derive selection rules for the Miller indices for different cubic Bravais lattices as well as many others, a few of the selection rules are given in the table below.\n\nThese selection rules can be used for any crystal with the given crystal structure. KCl has a face-centered cubic Bravais lattice.  However, the K+ and the Cl− ion have the same number of electrons and are quite close in size, so that the diffraction pattern becomes essentially the same as for a simple cubic structure with half the lattice parameter.  Selection rules for other structures can be referenced elsewhere, or derived. Lattice spacing for the other crystal systems can be found here.",
        pageTitle: "Bragg's law",
    },
    {
        title: "Brandolini's law",
        link: "https://en.wikipedia.org/wiki/Brandolini%27s_law",
        content:
            "Brandolini's law, also known as the bullshit asymmetry principle, is an internet adage coined in 2013 by Alberto Brandolini, an Italian programmer, that emphasizes the effort of debunking misinformation, in comparison to the relative ease of creating it in the first place. The law states:\n\nThe amount of energy needed to refute bullshit is an order of magnitude bigger than that needed to produce it.[1][2]\n\nThe rise of easy popularization of ideas through the internet has greatly increased the relevant examples, but the asymmetry principle itself has long been recognized.[3]\n\nThe adage was publicly formulated in January 2013 by Alberto Brandolini, an Italian programmer.[4] Brandolini stated that he was inspired by reading Daniel Kahneman's Thinking, Fast and Slow, right before watching an Italian political talk show involving former Prime Minister Silvio Berlusconi and journalist Marco Travaglio.[5]\n\nThe persistent false claim that vaccines cause autism is a prime example of Brandolini's law. That famous case involved British doctor Andrew Wakefield, who wrote an article about a study that claimed to find a relationship between the MMR vaccine and autism. The article's findings were later shown to be false and, as a result, Dr. Wakefield lost his medical license, and disclaimed and recanted.[6] Despite extensive investigation showing no such relationship, the false assertion has had a disastrous effect on public health, arising from vaccine hesitancy. Decades of research, and attempts to educate the public, have failed to eradicate the misinformation, which is still widely believed.[7]\n\nIn another example, shortly after the Boston Marathon bombing, the claim that a student who had survived the Sandy Hook Elementary School shooting had been killed by the bombing began to spread across social media. Despite many attempts to debunk the rumor, including an investigation by Snopes, the false story was shared by more than 92,000 people and was covered by major news agencies.[7]\n\nIn an example of Brandolini's law during the COVID-19 pandemic, Jeff Yates, a disinformation journalist at Radio-Canada, said of a very popular YouTube video: \"He makes all kinds of different claims. I had to check every single one of them. I had to call relevant experts and talk to them. I had to transcribe those interviews. I had to write a text that is legible and interesting to read. It's madness. It took this guy 15 minutes to make his video and it took me three days to fact-check.\"[8]\n\nDue to the rapid dissemination of information on social media, people are much more susceptible to becoming victims of pseudoscientific trends, such as Dr. Mehmet Oz's weight loss supplements and Dr. Joseph Mercola's tanning beds that were meant to reduce one's risk of developing cancer. Although government agencies were able to prevent further sales of those products, millions of dollars had already been spent by consumers and fans.[6]\n\nAnother example dates back to 2016, when Iceland's football team had eliminated England from the UEFA European Championship. Nine months after the victory, Icelandic doctor Ásgeir Pétur Þorvaldsson jokingly tweeted out that a baby boom in Iceland had occurred due to this victory. Despite wide media coverage suggesting the truth behind this statement, statistical analysis carried out by curious researchers debunked the notion proposed by Þorvaldsson's tweet.[9]\n\nBrandolini's Law is accentuated during times of crisis. In their analysis of using hydroxychloroquine for COVID-19 prevention, Jevin West and Carl Bergstrom noted that, despite hydroxychloroquine being frequently proven to be ineffective in curing illnesses, including COVID-19, it was extremely difficult to convince people that it would not protect people from the highly contagious virus. Because of how afraid people were of COVID-19 during the early stages of the pandemic, and how desperately people wanted a cure, widespread social media coverage, and a desire for Hydroxychloroquine to work, made it extremely difficult to disprove the misinformation being presented.[10]\n\nAccording to the Media Education Journal, \"Media portrayal of politics has always been subject to contested claims about accuracy and veracity but this has reached a new intensity.\"[11] Combating the spreading of misinformation requires scientists to establish the validity and quality of research, stories, and claims with a rating system.[6]\n\nIn 2020, researchers studied the sensitivity to bullshit and found that \"people are more receptive to bullshit, and less sensitive to detecting bullshit, under conditions in which they possess relatively few self-regulatory resources\".[12]\n\nWithin the context of scientific analysis, Brandolini's law can be put to use not just on the bullshit being presented, but can also bring the bullshitter under scrutiny as well. When the lying becomes apparent on multiple occasions throughout a stretch of scientific research, the bullshitter becomes more obvious than the bullshit itself, and because the bullshitter loses credibility, the ensuing bullshit is easier to identify.[13][14] In addition, the challenge of refuting bullshit does not just come from its time-consuming nature, but also from the challenge of defying and confronting one's community.[15]\n\nIn accordance with Kieron O'Hara's research to further analyze how bullshitters operate as opposed to just analyzing the bullshit, while it still takes substantially more energy to disprove bullshit than to create it, the overall amount of energy exerted to discover a bullshitter is less than the amount of energy used to discover the bullshit itself.[16]\n\nBullshit and Brandolini's law has also has been involved in gender issues. The U.S. Department of State defines gendered disinformation as \"a subset of misogynistic abuse and violence against women that uses false or misleading gender and sex-based narratives, often with some degree of coordination, to deter women from participating in the public sphere. Both foreign state and non-state actors strategically use gendered disinformation to silence women, discourage online political discourse, and shape perceptions toward gender and the role of women in democracies.\" That is a specific type of bullshit commonly found in politics, in which women are the victims of false claims.[17] Misinformation is used frequently in fostering gender inequalities, especially in social platforms and in political matters. As the refuting of bullshit takes a lot more energy than producing it, lives and jobs are affected, especially those of women.[18]\n\nEnvironmental researcher Phil Williamson of University of East Anglia implored other scientists in 2016 to get online and refute falsehoods to their work whenever possible, despite the difficulty as described by Brandolini's law. He wrote: \"The scientific process doesn't stop when results are published in a peer-reviewed journal. Wider communication is also involved, and that includes ensuring not only that information (including uncertainties) is understood, but also that misinformation and errors are corrected where necessary.\"[1]\n\nCarl T. Bergstrom and Jevin West, researchers on the topic of bullshit, study how to refute the bullshit that takes a large amount of energy to discover. This complicated process depends on the audience the bullshit is intended to influence, the time and energy a person is willing to invest in this process, and the medium used to do the refuting. In order to refute misinformation, one needs to do the following: [19]\n\nOther techniques for increasing the effectiveness of retracting misinformation include: preexposure warnings, repeated retractions, and providing an alternative narrative.[20]\n\nThe adage, \"A lie can travel halfway around the world before the truth can get its boots on\", has taken various forms since as early as 1710.[21]\n\nFalsehood flies, and truth comes limping after it; so that when men come to be undeceived, it is too late, the jest is over, and the tale has had its effect: like a man who has thought of a good repartee, when the discourse is changed, or the company parted; or, like a physician, who has found out an infallible medicine, after the patient is dead.\n\nIn 1845, Frédéric Bastiat expressed an early notion of the law:\n\nWe must confess that our adversaries have a marked advantage over us in the discussion. In very few words they can announce a half-truth; and in order to demonstrate that it is incomplete, we are obliged to have recourse to long and dry dissertations.\n\nPrior to Brandolini's definition, Italian blogger Uriel Fanelli and researcher Jonathan Koomey, creator of Koomey's law, also shared thoughts aligning with the bullshit asymmetry principle. Fanelli stated: \"An idiot can create more bullshit than you could ever hope to refute\", when generally translated in Calling Bullshit: The Art of Skepticism in a Data-Driven World.[24]\n\nKoomey stated: \"In fast-changing fields, like information technology, refutations lag nonsense production to a greater degree than in fields with less rapid change.\"[25]",
        pageTitle: "Brandolini's law",
    },
    {
        title: "Brewster's law",
        link: "https://en.wikipedia.org/wiki/Brewster%27s_law",
        content:
            "Brewster's angle (also known as the polarization angle) is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with no reflection. When unpolarized light is incident at this angle, the light that is reflected from the surface is therefore perfectly polarized. The angle is named after the Scottish physicist Sir David Brewster (1781–1868).[1][2]\n\nWhen light encounters a boundary between two media with different refractive indices, some of it is usually reflected as shown in the figure above. The fraction that is reflected is described by the Fresnel equations, and depends on the incoming light's polarization and angle of incidence.\n\nThe Fresnel equations predict that light with the p polarization (electric field polarized in the same plane as the incident ray and the surface normal at the point of incidence) will not be reflected if the angle of incidence is\n\nwhere n1 is the refractive index of the initial medium through which the light propagates (the \"incident medium\"), and n2 is the index of the other medium. This equation is known as Brewster's law, and the angle defined by it is Brewster's angle.\n\nThe physical mechanism for this can be qualitatively understood from the manner in which electric dipoles in the media respond to p-polarized light. One can imagine that light incident on the surface is absorbed, and then re-radiated by oscillating electric dipoles at the interface between the two media. The polarization of freely propagating light is always perpendicular to the direction in which the light is travelling. The dipoles that produce the transmitted (refracted) light oscillate in the polarization direction of that light. These same oscillating dipoles also generate the reflected light. However, dipoles do not radiate any energy in the direction of the dipole moment. If the refracted light is p-polarized and propagates exactly perpendicular to the direction in which the light is predicted to be specularly reflected, the dipoles point along the specular reflection direction and therefore no light can be reflected. (See diagram, above)\n\nWith simple geometry this condition can be expressed as\n\nwhere θ1 is the angle of reflection (or incidence) and θ2 is the angle of refraction.\n\none can calculate the incident angle θ1 = θB at which no light is reflected:\n\nThe physical explanation of why the transmitted ray should be at \n  \n    \n      \n        \n          90\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle 90^{\\circ }}\n  \n to the reflected ray can be difficult to grasp, but the Brewster angle result also follows simply from the Fresnel equations for reflectivity, which state that for p-polarized light\n\nWe can now use Snell's Law to eliminate \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n as follows: we multiply Snell by \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n  \n and square both sides; multiply the zero-reflection condition just obtained by \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{2}}\n  \n and square both sides; and add the equations. This produces\n\nWe finally divide both sides by \n  \n    \n      \n        \n          n\n          \n            1\n          \n          \n            4\n          \n        \n        \n          cos\n          \n            2\n          \n        \n        ⁡\n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}^{4}\\cos ^{2}\\theta _{1}}\n  \n, collect terms and rearrange to produce \n  \n    \n      \n        \n          tan\n          \n            2\n          \n        \n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        =\n        \n          n\n          \n            2\n          \n          \n            2\n          \n        \n        \n          /\n        \n        \n          n\n          \n            1\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\tan ^{2}\\theta _{1}=n_{2}^{2}/n_{1}^{2}}\n  \n, from which the desired result follows (which then allows reverse proof that \n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n        +\n        \n          θ\n          \n            2\n          \n        \n        =\n        \n          90\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}+\\theta _{2}=90^{\\circ }}\n  \n).\n\nFor a glass medium (n2 ≈ 1.5) in air (n1 ≈ 1), Brewster's angle for visible light is approximately 56°, while for an air-water interface (n2 ≈ 1.33), it is approximately 53°. Since the refractive index for a given medium changes depending on the wavelength of light, Brewster's angle will also vary with wavelength.\n\nThe phenomenon of light being polarized by reflection from a surface at a particular angle was first observed by Étienne-Louis Malus in 1808.[3] He attempted to relate the polarizing angle to the refractive index of the material, but was frustrated by the inconsistent quality of glasses available at that time. In 1815, Brewster experimented with higher-quality materials and showed that this angle was a function of the refractive index, defining Brewster's law.\n\nBrewster's angle is often referred to as the \"polarizing angle\", because light that reflects from a surface at this angle is entirely polarized perpendicular to the plane of incidence (\"s-polarized\"). A glass plate or a stack of plates placed at Brewster's angle in a light beam can, thus, be used as a polarizer. The concept of a polarizing angle can be extended to the concept of a Brewster wavenumber to cover planar interfaces between two linear bianisotropic materials. In the case of reflection at Brewster's angle, the reflected and refracted rays are mutually perpendicular.\n\nFor magnetic materials, Brewster's angle can exist for only one of the incident wave polarizations, as determined by the relative strengths of the dielectric permittivity and magnetic permeability.[4]  This has implications for the existence of generalized Brewster angles for dielectric metasurfaces.[5]\n\nWhile at the Brewster angle there is no reflection of the p polarization, at yet greater angles the reflection coefficient of the p polarization is always less than that of the s polarization, almost up to 90° incidence where the reflectivity of each rises towards unity. Thus reflected light from horizontal surfaces (such as the surface of a road) at a distance much greater than one's height (so that the incidence angle of specularly reflected light is near, or usually well beyond the Brewster angle) is strongly s-polarized. Polarized sunglasses use a sheet of polarizing material to block horizontally-polarized light and thus reduce glare in such situations. These are most effective with smooth surfaces where specular reflection (thus from light whose angle of incidence is the same as the angle of reflection defined by the angle observed from) is dominant, but even diffuse reflections from roads for instance, are also significantly reduced.\n\nPhotographers also use polarizing filters to remove reflections from water so that they can photograph objects beneath the surface. Using a polarizing camera attachment which can be rotated, such a filter can be adjusted to reduce reflections from objects other than horizontal surfaces, such as seen in the accompanying photograph (right) where the s polarization (approximately vertical) has been eliminated using such a filter.\n\nWhen recording a classical hologram, the bright reference beam is typically arranged to strike the film in the p polarization at Brewster's angle. By thus eliminating reflection of the reference beam at the transparent back surface of the holographic film, unwanted interference effects in the resulting hologram are avoided.\n\nEntrance windows or prisms with their surfaces at the Brewster angle are commonly used in optics and laser physics in particular. The polarized laser light enters the prism at Brewster's angle without any reflective losses.\n\nIn surface science, Brewster angle microscopes are used to image layers of particles or molecules at air-liquid interfaces. Using illumination by a laser at Brewster's angle to the interface and observation at the angle of reflection, the uniform liquid does not reflect, appearing black in the image. However any molecular layers or artifacts at the surface, whose refractive index or physical structure contrasts with the liquid, allows for some reflection against that black background which is captured by a camera.\n\nGas lasers using an external cavity (reflection by one or both mirrors outside the gain medium) generally seal the tube using windows tilted at Brewster's angle. This prevents light in the intended polarization from being lost through reflection (and reducing the round-trip gain of the laser) which is critical in lasers having a low round-trip gain. On the other hand, it does remove s polarized light, increasing the round trip loss for that polarization, and ensuring the laser only oscillates in one linear polarization, as is usually desired. And many sealed-tube lasers (which do not even need windows) have a glass plate inserted within the tube at the Brewster angle, simply for the purpose of allowing lasing in only one polarization.[6]\n\nWhen the reflecting surface is absorbing, reflectivity at parallel polarization (p) goes through a non-zero minimum at the so-called pseudo-Brewster's angle.[7][8]",
        pageTitle: "Brewster's angle",
    },
    {
        title: "Briffault's law",
        link: "https://en.wikipedia.org/wiki/Briffault%27s_law",
        content:
            'Robert Stephen Briffault MC* ([ʁo.ˈbɛʁ stə.ˈfɑ̃ bʁi.ˈfo], 1874 – 11 December 1948) was a French surgeon who found fame as a social anthropologist and later in life as a novelist.[1]\n\nBriffault was born in either France or London, on 8 November 1874, the son of a French diplomat, Charles Frédéric Briffault [fr], and the Scottish Margaret Mann (née Stewart).[2][3][4][5][6] He later cited his year of birth as 1876, likely to be young enough to enter the army in the First World War.[5]\n\nHe married Anna Clarke in 1896, with whom he had three children. After her death in 1919, he married Herma Hoyt (1898–1981), an American writer and translator.[1]\n\nHe spent time in France and elsewhere in Europe following his father.[5][4] After the death of his father in 1887, Briffault and his mother moved to New Zealand. Briffault received his MB and ChB from the University of Otago in New Zealand and commenced medical practice.[7]\n\nBriffault served on the Western Front and at Gallipoli during WWI. He was served within the British Expeditionary Force as part of the Royal Army Medical Corps from mid 1915.[8] He was awarded the Military Cross for his service.[2]\n\nDr Briffault was a member of the Auckland Institute and Museum between the 1890s to 1920. During 1909-1910, he served as president and then in 1911-1912 as Vice-president of the Auckland Institute and Museum.[9]\n\nBriffault settled in England where he turned to the study of sociology and anthropology.[7][10] He also lived for some time in the USA, and later Paris.[11][4]He died in Hastings, Sussex, England on 11 December 1948.[7][12][1]\n\nBriffault debated the institution of marriage with Bronisław Malinowski in the 1930s[13] and corresponded with Bertrand Russell.\n\nWhen asked how to pronounce his name, Briffault told The Literary Digest: "Should be pronounced bree\'-foh, without attempting to give it a French pronunciation."[14]\n\nBriffault is known for what is called Briffault\'s law:\n\nThe female, not the male, determines all the conditions of the animal family. Where the female can derive no benefit from association with the male, no such association takes place. — Robert Briffault, The Mothers. Vol. I, p. 191\n\nBriffault clarifies that this rule applies only to nonhuman animals, and not to humans: “There is, in fact, no analogy between the animal family and the patriarchal human family. The former is entirely the product of the female’s instincts, and she, not the male, is the head.”[15] In the chapter where Briffault outlines his law, he applies it to tigers, elks, lions, zebras, gazelles, buffaloes, deer, monkeys, beavers, lions, birds and other animals, and only references humans briefly in order to contrast human behavioural patterns from those of animals:\n\n“There is in fact no analogy between that [animal] group and the patriarchal human family; to equate the two is a proceeding for which there is no justification. The patriarchal family in the form in which it exists today is a juridic institution. Whatever external and superficial similarities there may be in the constitution of the human and of the animal family, there is one profound and fundamental difference. The patriarchal family is founded upon the supremacy of the male as ‘pater familias,’ as head of the family. This is not the case in the animal family. it is, on the contrary, entirely the product and manifestation of the female’s instincts; she, and not the male, is its head. We may occasionally find the male employed in foraging for the brood and for the mother, while the latter is lying quiescent in charge of her eggs or brood; but there is nothing in those appearances to justify us in regarding the animal family as patriarchal; on the contrary, the conduct of the group is entirely determined not by the male but by the female.”[16]\n\nIn 1930, H. L. Mencken wrote the following in his Treatise on the Gods:\n\nPrimitive society, like many savage societies of our own time, was probably strictly matriarchal. The mother was the head of the family. ...What masculine authority there was resided in the mother\'s brother. He was the man of the family, and to him the children yielded respect and obedience. Their father, at best, was simply a pleasant friend who fed them and played with them; at worst, he was an indecent loafer who sponged on the mother. They belonged, not to his family, but to their mother\'s. As they grew up they joined their uncle\'s group of hunters, not their father\'s. This matriarchal organisation of the primitive tribe, though it finds obvious evidential support in the habits of higher animals, has been questioned by many anthropologists, but of late one of them, Briffault, demonstrated its high probability in three immense volumes [The Mothers: A Study of the Origins of Sentiments and Institutions]. It is hard to escape the cogency of his arguments, for they are based upon an almost overwhelming accumulation of facts. They not only show that, in what we may plausibly assume about the institutions of early man and in what we know positively about the institutions of savages today, the concepts inseparable from a matriarchate colour every custom and every idea: they show also that those primeval concepts still condition our own ways of thinking and doing things, so that "the societal characters of the human mind" all seem to go back "to the functions of the female and not to those of the male." Thus it appears that man, in his remote infancy, was by no means the lord of creation that he has since become."[17]',
        pageTitle: "Robert Briffault",
    },
    {
        title: "Brooks's law",
        link: "https://en.wikipedia.org/wiki/Brooks%27s_law",
        content:
            'Brooks\'s law is an observation about software project management that "Adding manpower to a late software project makes it later."[1][2] It was coined by Fred Brooks in his 1975 book The Mythical Man-Month. According to Brooks, under certain conditions, an incremental person when added to a project makes it take more, not less time.\n\nAccording to Brooks himself, the law is an "outrageous oversimplification",[1] but it captures the general rule. Brooks points to the main factors that explain why it works this way:\n\nThere are some key points in Brooks\'s law that allow exceptions and open the door for possible solutions.[4][5]\n\nThe first point is to note that Brooks\'s law only applies to projects that are already late.[6] Projects can be brought back into (or kept in) control if people are added earlier in the process.[7] It is also important to determine if the project is really late, or if the schedule was originally overly optimistic. Scheduling mistakes account for a large number of late projects. Correcting the schedule is the best way to have a meaningful and reliable time frame for the project\'s completion.[8]\n\nThe quantity, quality and role of the people added to the project also must be taken into consideration. One simple way to circumvent the law on an overrun project is to add more people than needed, in such a way that the extra capacity compensates the training and communication overhead.[9] Good programmers or specialists can be added with less overhead for training.[10] People can be added to do other tasks related with the project, for example, quality assurance or documentation; given that the task is clear, ramp up time is minimized.[11]\n\nGood segmentation helps by minimizing the communication overhead between team members. Smaller sub-problems are solved by a smaller team, and a top-level team is responsible for systems integration. For this method to work, the segmentation of the problem must be done correctly in the first place; if done incorrectly, this can make the problem worse, not better, by impeding communication between programmers working on parts of the problem which are actually closely coupled, even when the project plan has decreed that they are not.\n\nAn example of segmentation are design patterns that simplify the distribution of work, because the entire team can do its part within the framework provided by that pattern. The design pattern defines the rules that the programmers follow, simplifies communication through the use of a standard language, and provides consistency and scalability.\n\nThe Bermuda plan, where most developers on a project are removed ("sent to Bermuda") and the remaining are left to complete the software, has been suggested as a way of circumventing Brooks\'s law.[12][13]',
        pageTitle: "Brooks's law",
    },
    {
        title: "Buys Ballot's law",
        link: "https://en.wikipedia.org/wiki/Buys_Ballot%27s_law",
        content:
            "In meteorology, Buys Ballot's law (.mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}Dutch pronunciation: [ˈbœyz bɑˈlɔt]) may be expressed as follows: In the Northern Hemisphere, if a person stands with their back to the wind, the atmospheric pressure is low to the left, high to the right.[1] This is because wind travels counterclockwise around low pressure zones in the Northern Hemisphere. It is approximately true in the higher latitudes of the Northern Hemisphere, and is reversed in the Southern Hemisphere,[2] but the angle between the pressure gradient force and wind is not a right angle in low latitudes.\n\nA version taught to US Naval Cadets in WW2 is: \"In the Northern Hemisphere, if you turn your back to the wind, the low pressure center will be to your left and somewhat toward the front.\"[3]\n\nAs early as the 16th century extensive weather observations were included as part of a ship's log. These observations as well as other log information, were turned over to national hydrographic institutes in various nations, most notably Germany and England and later the US. The information from many ships about individual voyages was compiled ashore and later became what today is still published by England, a 3 volume set complete with charts titled \"Sailing Directions for the World\". Additionally the US Defense Mapping Agency publishes a 47 volume set Sailing Directions which serves much the same purpose. The information is the distillate of empirical observations of thousands of ships masters over thousands of voyages spanning  several hundred years.\n\nBuys Ballot's law, which was first deduced by the American meteorologists J.H. Coffin and William Ferrel, is a direct consequence of Ferrel's law. The law takes its name from C. H. D. Buys Ballot, a Dutch meteorologist, who published it in the Comptes Rendus, 9 November 1857.[4][5] While William Ferrel theorized this first in 1856, Buys Ballot was the first to provide an empirical validation. While Buys Ballot hoped that his law would be validated by other meteorological services in other countries, foreign dissemination and adoption of Buys Ballot's law was slow. The French and American meteorological services of the era prioritized describing the state of storms rather than forecasting them, finding little use for the predictive value of Buys Ballot's law. However, the British Meteorological Office began to use Buys Ballot's law extensively after reintroducing a storm warning system in 1867 following the death of its former director Robert FitzRoy. The rule of thumb became more widely accepted following its effective use in the British forecasts.[6]\n\nBuys Ballot's law first appeared in early  versions (prior to 1900) of Bowditch's American Practical Navigator and other publications written to assist in passage planning and the safe conduct of ships at sea and is still included today both in Bowditch and in Sailing Directions (see following reference)  as an item of practical reference and information.\n\nThe law  outlines general rules of conduct for masters of  both sail and steam vessels, to assist them in steering the vessels away from the center and right front (in the Northern Hemisphere and left front in the Southern Hemisphere) quadrants of hurricanes or any other rotating disturbances at sea. Prior to radio, satellite observation and the ability to transmit timely weather information over long distances, the only method a ship's master had to forecast the weather was  observation of meteorological conditions (visible cloud formations, wind direction and atmospheric pressure) at his location.\n\nIncluded in the Sailing Directions for the World are Buys Ballot's techniques for avoiding the worst part of any rotating storm system at sea using only the locally observable phenomena of cloud formations, wind speed and barometric pressure tendencies over a number of hours. These observations and application of the principles of Buys Ballot's law help to establish the probability of the existence of a storm and the best course to steer to try to avoid the worst of it—with the best chance of survival.\n\nThe underlying principles of Buys Ballot's law state that for anyone ashore in the Northern Hemisphere and in the path of a hurricane, the most dangerous place to be is in the right front quadrant of the storm. There, the observed wind speed of the storm is the sum of the  speed of wind in the storm  circulation plus the velocity of the storm's forward movement. Buys Ballot's law calls this the \"Dangerous Quadrant\". Likewise, in the left front quadrant of the storm the observed wind is the difference between the storm's wind velocity and its forward speed. This is called the \"Safe Quadrant\" due to the lower observed wind speeds.\n\nTo look at it another way, in the Northern Hemisphere if a person is to the right of where a hurricane or tropical storm makes landfall, that is considered the dangerous quadrant. If they are to the left of the point of landfall, that is the safe quadrant. In the dangerous quadrant an observer will experience higher wind speeds and generally a much higher storm surge due to the onshore wind direction. In the safe quadrant, the observer will experience somewhat lower wind speeds and the possibility of lower than normal water levels due to the direction of the wind being offshore.\n\nThese are very general rules that are subject to many other factors, including shapes of the coastline, and topography in any location. Although the principles apply to a very limited extent to a coastal observer during the approach and passage of a storm in any location, Buys Ballot's law was primarily formulated from empirical data to assist ships at sea.",
        pageTitle: "Buys Ballot's law",
    },
    {
        title: "Byerlee's law",
        link: "https://en.wikipedia.org/wiki/Byerlee%27s_law",
        content:
            "In rheology, Byerlee's law, also known as Byerlee's friction law[1] concerns the shear stress (τ) required to slide one rock over another. The rocks have macroscopically flat surfaces, but the surfaces have small asperities that make them \"rough.\" For a given experiment and at normal stresses (σn) below about 2000 bars (200 MPa) the shear stress increases approximately linearly with the normal stress (τ = 0.85 σn, where τ and σn is in units of MPa) and is highly dependent on rock type and the character (roughness) of the surfaces, see Mohr-Coulomb friction law. Byerlee's law states that with increased normal stress the required shear stress continues to increase, but the rate of increase decreases (τ = 50 + 0.6σn), where τ and σn are in units of MPa, and becomes nearly independent of rock type.[2]\n\nThe law describes an important property of crustal rock, and can be used to determine when slip along a geological fault takes place.\n\nThe law is named after the American geophysicist James Byerlee, who derived it experimentally in 1978.[2][3]\n\nThis geophysics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Byerlee's law",
    },
    {
        title: "Campbell's law",
        link: "https://en.wikipedia.org/wiki/Campbell%27s_law",
        content:
            'Campbell\'s law is an adage developed by Donald T. Campbell, a psychologist and social scientist who often wrote about research methodology, which states:\n\nThe more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor.[1]\n\nCampbell\'s law is related to the cobra effect, which is the sometimes unintended negative effect of public policy and other government interventions in economics, commerce, and healthcare.[2]\n\nIn 1976, Campbell wrote: "Achievement tests may well be valuable indicators of general school achievement under conditions of normal teaching aimed at general competence. But when test scores become the goal of the teaching process, they both lose their value as indicators of educational status and distort the educational process in undesirable ways. (Similar biases of course surround the use of objective tests in courses or as entrance examinations.)"[1]\n\nThe social science principle of Campbell\'s law is used to point out the negative consequences of high-stakes testing in U.S. classrooms. This may take the form of teaching to the test or outright cheating.[3] "The High-Stakes Education Rule" is identified and analyzed in the book "Measuring Up: What Educational Testing Really Tells Us".[4]\n\nCampbell\'s law has been used in criticism of Race to the Top, an Obama administration program, and the No Child Left Behind Act, enacted during the George W. Bush Administration.[5]\n\nThere are closely related ideas known by different names, such as Goodhart\'s law and the Lucas critique. Another concept related to Campbell\'s law emerged in 2006 when UK researchers Rebecca Boden and Debbie Epstein published an analysis of evidence-based policy, a practice espoused by Prime Minister Tony Blair. In the paper, Boden and Epstein described how a government that tries to base its policy on evidence can actually end up producing corrupted data because it "seeks to capture and control the knowledge producing processes to the point where this type of \'research\' might best be described as \'policy-based evidence\'."[6]\n\nWhen someone distorts decisions in order to improve the performance measure, they often surrogate, coming to believe that the measure is a better measure of true performance than it really is.[7]\n\nCampbell\'s law imparts a more positive but complicated message. It is important to measure progress making use of quantitative and qualitative indicators.[8] However, using quantitative data for evaluation can distort and manipulate these indicators. Concrete measures must be adopted to reduce alteration and manipulation of information. In his article "Assessing the Impact of Planned Social Change",[9] Campbell emphasized that "the more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor."',
        pageTitle: "Campbell's law",
    },
    {
        title: "Cassie's law",
        link: "https://en.wikipedia.org/wiki/Cassie%27s_law",
        content:
            "Cassie's law, or the Cassie equation, describes the effective contact angle θc for a liquid on a chemically heterogeneous surface, i.e. the surface of a composite material consisting of different chemistries, that is, non-uniform throughout.[1] Contact angles are important as they quantify a surface's wettability, the nature of solid-fluid intermolecular interactions.[2] Cassie's law is reserved for when a liquid completely covers both smooth and rough heterogeneous surfaces.[3]\n\nMore of a rule than a law, the formula found in literature for two materials is;\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sigma _{1}\\cos \\theta _{1}+\\sigma _{2}\\cos \\theta _{2}}\n\nwhere \n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \nand \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n are the contact angles for components 1 with fractional surface area \n  \n    \n      \n        \n          σ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{1}}\n  \n, and 2 with fractional surface area \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}}\n  \n in the composite material respectively. If there exist more than two materials then the equation is scaled to the general form of;\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          σ\n          \n            k\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sum _{k=1}^{N}\\sigma _{k}\\cos \\theta _{k}}\n  \n,  with \n  \n    \n      \n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          σ\n          \n            k\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{k=1}^{N}\\sigma _{k}=1}\n  \n.[4]\n\nCassie's law takes on special meaning when the heterogeneous surface is a porous medium.  \n  \n    \n      \n        \n          σ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{1}}\n  \n now represents the solid surface area and \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}}\n  \n air gaps, such that the surface is no longer completely wet. Air creates a contact angle of \n  \n    \n      \n        \n          180\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle 180^{\\circ }}\n  \nand because \n  \n    \n      \n        cos\n        ⁡\n        (\n        180\n        )\n      \n    \n    {\\displaystyle \\cos(180)}\n  \n= \n  \n    \n      \n        −\n        1\n      \n    \n    {\\displaystyle -1}\n  \n, the equation reduces to:\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n            b\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        −\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{cb}=\\sigma _{1}\\cos \\theta _{1}-\\sigma _{2}}\n  \n, which is the Cassie-Baxter equation.[5]\n\nUnfortunately the terms Cassie and Cassie-Baxter are often used interchangeably but they should not be confused. The Cassie-Baxter equation is more common in nature, and focuses on the 'incomplete coating' of surfaces by a liquid only. In the Cassie-Baxter state liquids sit upon asperities, resulting in air pockets that are bounded between the surface and liquid.\n\nThe Cassie-Baxter equation is not restricted to only chemically heterogeneous surfaces, as air within porous homogeneous surfaces will make the system heterogeneous. However, if the liquid penetrates the grooves, the surface returns to homogeneity and neither of the previous equations can be used. In this case the liquid is in the Wenzel state, governed by a separate equation. Transitions between the Cassie-Baxter state and the Wenzel state can take place when external stimuli such as pressure or vibration are applied to the liquid on the surface.[6]\n\nWhen a liquid droplet interacts with a solid surface, its behaviour is governed by surface tension and energy. The liquid droplet could spread indefinitely or it could sit on the surface like a spherical cap at which point there exists a contact angle.\n\nDefining  \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n as the free energy change per unit area caused by a liquid spreading,\n\nE\n        =\n        \n          σ\n          \n            1\n          \n        \n        (\n        \n          γ\n          \n            \n              s\n              \n                1\n              \n            \n            a\n          \n        \n        −\n        \n          γ\n          \n            \n              s\n              \n                1\n              \n            \n            l\n          \n        \n        )\n        +\n        \n          σ\n          \n            2\n          \n        \n        (\n        \n          γ\n          \n            \n              s\n              \n                2\n              \n            \n            a\n          \n        \n        −\n        \n          γ\n          \n            \n              s\n              \n                2\n              \n            \n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle E=\\sigma _{1}(\\gamma _{s_{1}a}-\\gamma _{s_{1}l})+\\sigma _{2}(\\gamma _{s_{2}a}-\\gamma _{s_{2}l})}\n\nwhere \n  \n    \n      \n        \n          σ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{1}}\n  \n, \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}}\n  \nare the fractional areas of the two materials on the heterogeneous surface, and \n  \n    \n      \n        \n          γ\n          \n            s\n            a\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{sa}}\n  \nand \n  \n    \n      \n        \n          γ\n          \n            s\n            l\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{sl}}\n  \nthe interfacial tensions between solid, air and liquid.\n\nThe contact angle for the heterogeneous surface is given by,\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          \n            E\n            \n              γ\n              \n                l\n                a\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}={\\frac {E}{\\gamma _{la}}}}\n  \n,  with \n  \n    \n      \n        \n          γ\n          \n            l\n            a\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{la}}\n  \nthe interfacial tension between liquid and air.\n\nc\n        o\n        s\n        \n          θ\n          \n            y\n          \n        \n        =\n        \n          \n            \n              \n                γ\n                \n                  s\n                  a\n                \n              \n              −\n              \n                γ\n                \n                  s\n                  l\n                \n              \n            \n            \n              γ\n              \n                l\n                a\n              \n            \n          \n        \n      \n    \n    {\\displaystyle cos\\theta _{y}={\\frac {\\gamma _{sa}-\\gamma _{sl}}{\\gamma _{la}}}}\n\nThus by substituting the first expression into Young's equation, we arrive at Cassie's law for heterogeneous surfaces,\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sigma _{1}\\cos \\theta _{1}+\\sigma _{2}\\cos \\theta _{2}}\n  \n[1]\n\nStudies concerning the contact angle existing between a liquid and a solid surface began with Thomas Young in 1805.[7]  The Young equation\n\nc\n        o\n        s\n        \n          θ\n          \n            y\n          \n        \n        =\n        \n          \n            \n              \n                γ\n                \n                  s\n                  a\n                \n              \n              −\n              \n                γ\n                \n                  s\n                  l\n                \n              \n            \n            \n              γ\n              \n                l\n                a\n              \n            \n          \n        \n      \n    \n    {\\displaystyle cos\\theta _{y}={\\frac {\\gamma _{sa}-\\gamma _{sl}}{\\gamma _{la}}}}\n\nreflects the relative strength of the interaction between surface tensions at the three phase contact, and is the geometric ratio between the energy gained in forming a unit area of the solid–liquid interface to that required to form a liquid–air interface.[1] However Young's equation only works for ideal and real surfaces and in practice most surfaces are microscopically rough.\n\nIn 1936 Young's equation was modified by Robert Wenzel to account for rough homogeneous surfaces, and a parameter \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n was introduced, defined as the ratio of the true area of the solid compared to its nominal.[8] Known as the Wenzel equation,\n\ncos\n        ⁡\n        \n          θ\n          \n            w\n          \n        \n        =\n        r\n        cos\n        ⁡\n        \n          θ\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{w}=r\\cos \\theta _{y}}\n\nshows that the apparent contact angle, the angle measured at casual inspection, will increase if the surface is roughened. Liquids with contact angle \n  \n    \n      \n        \n          θ\n          \n            w\n          \n        \n      \n    \n    {\\displaystyle \\theta _{w}}\n  \nare known to be in the Wenzel state.\n\nThe notion of roughness effecting the contact angle was extended by Cassie and Baxter in 1944 when they focused on porous mediums, where liquid does not penetrate the grooves on rough surface and leaves air gaps.[5] They devised the Cassie-Baxter equation;\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        −\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sigma _{1}\\cos \\theta _{1}-\\sigma _{2}}\n  \n, sometimes written as \n  \n    \n      \n        cos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        (\n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        +\n        1\n        )\n        −\n        1\n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sigma _{1}(\\cos \\theta _{1}+1)-1}\n  \n where the \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}}\n  \n has become \n  \n    \n      \n        (\n        1\n        −\n        \n          σ\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (1-\\sigma _{1})}\n  \n.[9]\n\nIn 1948 Cassie refined this for two materials with different chemistries on both smooth and rough surfaces, resulting in the aforementioned Cassie's law\n\ncos\n        ⁡\n        \n          θ\n          \n            c\n          \n        \n        =\n        \n          σ\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{c}=\\sigma _{1}\\cos \\theta _{1}+\\sigma _{2}\\cos \\theta _{2}}\n\nFollowing the discovery of superhydrophobic surfaces in nature and the growth of their application in industry, the study of contact angles and wetting has been widely reexamined. Some claim that Cassie's equations are more fortuitous than fact with it being argued that emphasis should not be placed on fractional contact areas but actually the behaviour of the liquid at the three phase contact line.[10] They do not argue never using the Wenzel and Cassie-Baxter's equations but that “they should be used with knowledge of their faults”. However the debate continues, as this argument was evaluated and criticised with the conclusion being drawn that contact angles on surfaces can be described by the Cassie and Cassie-Baxter equations provided the surface fraction and roughness parameters are reinterpreted to take local values appropriate to the droplet.[11] This is why Cassie's law is actually more of a rule.\n\nIt is widely agreed that the water repellency of biological objects is due to the Cassie-Baxter equation. If water has a contact angle between \n  \n    \n      \n        0\n        <\n        θ\n        <\n        \n          90\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle 0<\\theta <90^{\\circ }}\n  \n, then the surface is classed as hydrophilic, whereas a surface producing a contact angle between \n  \n    \n      \n        \n          90\n          \n            ∘\n          \n        \n        <\n        θ\n        <\n        \n          180\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle 90^{\\circ }<\\theta <180^{\\circ }}\n  \n is hydrophobic. In the special cases where the Contact angle is \n  \n    \n      \n        \n          150\n          \n            ∘\n          \n        \n        <\n        θ\n      \n    \n    {\\displaystyle 150^{\\circ }<\\theta }\n  \n, then it is known as superhydrophobic.\n\nOne example of a superhydrophobic surface in nature is the Lotus leaf.[12] Lotus leaves have a typical contact angle of \n  \n    \n      \n        θ\n        ∼\n        \n          160\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle \\theta \\sim 160^{\\circ }}\n  \n, ultra low water adhesion due to minimal contact areas, and a self cleaning property which is characterised by the Cassie-Baxter equation.[13] The microscopic architecture of the Lotus leaf means that water will not penetrate nanofolds on the surface, leaving air pockets below. The water droplets become suspended in the Cassie-Baxter state and are able to roll off the leaf picking up dirt as they do so, thus cleaning the leaf.\n\nThe Cassie–Baxter wetting regime also explains the water repellent features of the pennae (feathers) of a bird. The feather consists of a topography network of 'barbs and barbules' and a droplet that is deposited on a these resides in a solid-liquid-air non-wetting composite state, where tiny air pockets are trapped within.[14]",
        pageTitle: "Cassie's law",
    },
    {
        title: "Cassini's laws",
        link: "https://en.wikipedia.org/wiki/Cassini%27s_laws",
        content:
            "Cassini's laws provide a compact description of the motion of the Moon. They were established in 1693 by Giovanni Domenico Cassini, a prominent scientist of his time.[1]\n\nRefinements of these laws to include physical librations have been made,[1] and they have been generalized to treat other satellites and planets.[2][3][4]\n\nIn the case of the Moon, its rotational axis always points some 1.5 degrees away from the North ecliptic pole. The normal to the Moon's orbital plane and its rotational axis are always on opposite sides of the normal to the ecliptic.\n\nTherefore, both the normal to the orbital plane and the Moon's rotational axis precess around the ecliptic pole with the same period. The period is about 18.6 years and the motion is retrograde.\n\nA system obeying these laws is said to be in a Cassini state, that is: an evolved rotational state where the spin axis, orbit normal, and normal to the Laplace plane are coplanar while the obliquity remains constant.[2][3][5] The Laplace plane is defined as the plane about which a planet or satellite orbit precesses with constant inclination.[5] The normal to the Laplace plane for a moon is between the planet's spin axis and the planet's orbit normal, being closer to the latter if the moon is distant from the planet. If a planet itself is in a Cassini state, the Laplace plane is the invariable plane of the stellar system.\n\nCassini state 1 is defined as the situation in which both the spin axis and the orbit normal axis are on the same side of the normal to the Laplace plane.  Cassini state 2 is defined as the case in which the spin axis and the orbit normal axis are on opposite sides of the normal to the Laplace plane.[6]  Earth's Moon is in Cassini state 2.\n\nIn general, the spin axis moves in the direction perpendicular to both itself and the orbit normal, due to the tidal force exerted by the object being orbited (planet or star) and other objects in the system. (In the case of the Moon, its spin axis moves mostly under the influence of the Earth, whereas the smaller tidal influence of the Sun works in the same direction at full moon and in the opposite direction at new moon and is thus negligible.) The rate of movement of the spin axis goes to zero if the spin axis coincides with the orbit normal. If the orbit normal precesses in a regular circular motion (due to tidal influences from other objects, such as the Sun in the case of the Moon), it is possible to characterize the solutions to the differential equation for the motion of the spin axis. It turns out that the spin axis traces out loops on the unit sphere that rotates at the speed of the orbital precession (so that the orbit normal and the normal to the Laplace plane are fixed points in the sphere). With certain values of the parameters, there are three areas on the sphere in each of which we have circulation around a point inside the area where the spin axis doesn't move (in this rotating frame of reference). These points are Cassini states 1 and 2 and a third Cassini state in which the rotation is retrograde (which would not apply to a moon like ours that is tidally locked). The three areas are separated by a separatrix that crosses itself, and the point where it crosses itself is the unstable Cassini state 4. (Under other parameter values only states 2 and 3 exist, and there is no separatrix.) If an object flexes and dissipates kinetic energy, then these solutions are not exact and the system will slowly evolve and approach a stable Cassini state. This has happened with the Moon. It has reached a state with a constant obliquity of 6.7°, at which the precession of the spin axis takes the same 18.6 years as taken by the precession of the orbit normal, and is thus in a Cassini state.[7]",
        pageTitle: "Cassini's laws",
    },
    {
        title: "Celine's laws",
        link: "https://en.wikipedia.org/wiki/Celine%27s_laws",
        content:
            "Celine's Laws are a series of three laws regarding government and social interaction attributed to the fictional character Hagbard Celine in Robert Anton Wilson's and Robert Shea's The Illuminatus Trilogy. Celine, a gentleman anarchist, serves as a mouthpiece for Wilson's libertarian, anarchist and sometimes completely uncategorizable ideas about the nature of humanity. Celine's Laws are outlined in the trilogy by a manifesto titled Never Whistle While You're Pissing. Wilson later goes on to elaborate on the laws in his nonfiction book, Prometheus Rising, as being inherent consequences of average human psychology.\n\nA piece entitled Celine's Laws appears in Robert Anton Wilson's The Illuminati Papers, which features articles written by Wilson under the guise of many of his characters from The Illuminatus! Trilogy alongside interviews with the author himself.  One article pulls from another, as well as from the original Trilogy.\n\nCeline, in his manifesto, recognizes these are generalities, but also says that their basic principles can be used to find the source of every great decline and fall of nations, and goes on to claim they are as universal as Newton's Laws in applying to everything.\n\n\"National Security is the chief cause of national insecurity.\"[1]\n\nReflecting the paranoia of the Cold War, Celine's First Law focuses on the common idea that to have national security, one must create a secret police. Since internal revolutionaries and external foes would make the secret police a prime target for infiltration, and because the secret police would by necessity have vast powers to blackmail and intimidate other members of the government, another higher set of secret police must be created to monitor the secret police. And an even higher set of secret police must then be created to monitor the higher order of secret police. Repeat ad nauseam.\n\nThis seemingly infinite regress goes on until every person in the country is spying on another, or until \"the funding runs out.\" And since this paranoid and self-monitoring situation inherently makes targets of a nation's own citizens, the average person in the nation is more threatened by the massive secret police complex than by whatever foe they were seeking to protect themselves from. Wilson points out that the Soviet Union, which suffered from this in spades, got to the point that it was terrified of painters and poets who could do little harm to them in reality.\n\nAt the same time, given the limitation of funding and scale, the perfect security state never truly emerges, leaving the populace still vulnerable from the original threat while also being threatened by the vast and Orwellian secret police.\n\n\"Accurate communication is possible only in a non-punishing situation.\"[2]\n\nWilson rephrases this himself many times as \"communication occurs only between equals\". Celine calls this law \"a simple statement of the obvious\" and refers to the fact that everyone who labors under an authority figure tends to lie to and flatter that authority figure in order to protect themselves either from violence or from deprivation of security (such as losing one's job). In essence, it is usually more in the interests of any worker to tell his boss what he wants to hear, not what is true.[3]\n\nIn any hierarchy, every level below the highest carries a subtle burden to see the world in the way their superiors expect it to be seen and to provide feedback to their superiors that their superiors want to hear. In the end, any hierarchical organization supports what its leaders already think is true more than it challenges them to think differently. The levels below the leaders are more interested in keeping their jobs than telling the truth.\n\nWilson, in Prometheus Rising, uses the example of J. Edgar Hoover's FBI. Hoover saw communist infiltrators and spies everywhere, and he told his agents to hunt them down. Therefore, FBI agents began seeing and interpreting everything they could as parts of the communist conspiracy. Some even went as far as framing people as communists, making largely baseless arrests and doing everything they could to satisfy Hoover's need to find and drive out the communist conspiracy. The problem is, such a conspiracy was greatly exaggerated. Hoover thought it was monolithic and pervasive, and any agent who dared point out the lack of evidence to Hoover would be at best denied promotions, and at worst labeled a communist himself and lose his job. Any agent who knew the truth would be very careful to hide the fact.\n\nMeanwhile, the FBI was largely ignoring the problem of organized crime (the Mafia), because Hoover insisted that organized crime did not exist on the national scale.  Not only does the leader of the hierarchy see what he wants to see, but he also does not see what he does not want to see.  Agents who pursued the issue of organized crime were sometimes marginalized within the organization or hounded into retirement.\n\nIn the end, Celine states, any hierarchy acts more to conceal the truth from its leaders than it serves to find the truth.\n\nCeline recognizes that the third law seems preposterous from the beginning. While a dishonest politician is interested only in bettering his own lot through abusing the public trust, an honest politician is far more dangerous since he is honestly interested in bettering society through political action, and that means writing and implementing more and more laws.\n\nCeline argues that creating more laws simply creates more criminals. Laws inherently restrict individual freedom, and the explosive rate at which laws are being created means that every citizen in the course of his daily life does not have the research capacity to not violate at least one of the plethora of laws. It is only through honest politicians trying to change the world through laws that true tyranny can come into being through excessive legislation.\n\nCorrupt politicians simply line their own pockets. Honest idealist politicians cripple the people's freedom through enormous numbers of laws. So corrupt politicians are preferable according to Celine, despite the possibility of an honest politician who honestly opposes the formation of new laws (or wants to do away with some).",
        pageTitle: "Celine's laws",
    },
    {
        title: "Charles's law",
        link: "https://en.wikipedia.org/wiki/Charles%27s_law",
        content:
            "Charles's law (also known as the law of volumes) is an experimental gas law that describes how gases tend to expand when heated. A modern statement of Charles's law is:\n\nWhen the pressure on a sample of a dry gas is held constant, the Kelvin temperature and the volume will be in direct proportion.[1]\n\nThis relationship of direct proportion can be written as:\n\nThis law describes how a gas expands as the temperature increases; conversely, a decrease in temperature will lead to a decrease in volume. For comparing the same substance under two different sets of conditions, the law can be written as:\n\nV\n              \n                1\n              \n            \n            \n              T\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              V\n              \n                2\n              \n            \n            \n              T\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {V_{1}}{T_{1}}}={\\frac {V_{2}}{T_{2}}}}\n\nThe equation shows that, as absolute temperature increases, the volume of the gas also increases in proportion.\n\nThe law was named after scientist Jacques Charles, who formulated the original law in his unpublished work from the 1780s.\n\nIn two of a series of four essays presented between 2 and 30 October 1801,[2] John Dalton demonstrated by experiment that all the gases and vapours that he studied expanded by the same amount between two fixed points of temperature.  The French natural philosopher Joseph Louis Gay-Lussac confirmed the discovery in a presentation to the French National Institute on 31 Jan 1802,[3] although he credited the discovery to unpublished work from the 1780s by Jacques Charles. The basic principles had already been described by Guillaume Amontons[4] and Francis Hauksbee[5]  a century earlier.\n\nDalton was the first to demonstrate that the law applied generally to all gases, and to the vapours of volatile liquids if the temperature was well above the boiling point. Gay-Lussac concurred.[6] With measurements only at the two thermometric fixed points of water (0°C and 100°C), Gay-Lussac was unable to show that the equation relating volume to temperature was a linear function.   On mathematical grounds alone,  Gay-Lussac's paper  does not permit the assignment of any law stating the linear relation.   Both Dalton's and Gay-Lussac's main conclusions can be expressed mathematically as:\n\nwhere V100 is the volume occupied by a given sample of gas at 100 °C; V0 is the volume occupied by the same sample of gas at 0 °C; and k is a constant which is the same for all gases at constant pressure. This equation does not contain the temperature and so is not what became known as Charles's Law. Gay-Lussac's value for k (.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄2.6666), was identical to Dalton's earlier value for vapours and remarkably close to the present-day value of 1⁄2.7315. Gay-Lussac gave credit for this equation to unpublished statements by his fellow Republican citizen J. Charles in 1787.  In the absence of a firm record, the gas law relating volume to temperature cannot be attributed to Charles.\nDalton's measurements had much more scope regarding temperature than Gay-Lussac, not only measuring the volume at the fixed points of water but also at two intermediate points.  Unaware of the inaccuracies of mercury thermometers at the time, which were divided into equal portions between the fixed points, Dalton, after concluding in Essay II that in the case of vapours, “any elastic fluid expands nearly in a uniform manner into 1370 or 1380 parts by 180 degrees (Fahrenheit) of heat”, was unable to confirm it for gases.\n\nCharles's law appears to imply that the volume of a gas will descend to zero at a certain temperature (−266.66 °C according to Gay-Lussac's figures) or −273.15 °C. Gay-Lussac was clear in his description that the law was not applicable at low temperatures:\n\nbut I may mention that this last conclusion cannot be true except so long as the compressed vapours remain entirely in the elastic state; and this requires that their temperature shall be sufficiently elevated to enable them to resist the pressure which tends to make them assume the liquid state.[3]\n\nAt absolute zero temperature, the gas possesses zero energy and hence the molecules restrict motion.\nGay-Lussac had no experience of liquid air (first prepared in 1877), although he appears to have believed (as did Dalton) that the \"permanent gases\" such as air and hydrogen could be liquified. Gay-Lussac had also worked with the vapours of volatile liquids in demonstrating Charles's law, and was aware that the law does not apply just above the boiling point of the liquid:\n\nI may, however, remark that when the temperature of the ether is only a little above its boiling point, its condensation is a little more rapid than that of atmospheric air. This fact is related to a phenomenon which is exhibited by a great many bodies when passing from the liquid to the solid-state, but which is no longer sensible at temperatures a few degrees above that at which the transition occurs.[3]\n\nThe first mention of a temperature at which the volume of a gas might descend to zero was by William Thomson (later known as Lord Kelvin) in 1848:[7]\n\nThis is what we might anticipate when we reflect that infinite cold must correspond to a finite number of degrees of the air-thermometer below zero; since if we push the strict principle of graduation, stated above, sufficiently far, we should arrive at a point corresponding to the volume of air being reduced to nothing, which would be marked as −273° of the scale (−100/.366, if .366 be the coefficient of expansion); and therefore −273° of the air-thermometer is a point which cannot be reached at any finite temperature, however low.\n\nHowever, the \"absolute zero\" on the Kelvin temperature scale was originally defined in terms of the second law of thermodynamics, which Thomson himself described in 1852.[8] Thomson did not assume that this was equal to the \"zero-volume point\" of Charles's law, merely said that Charles's law provided the minimum temperature which could be attained. The two can be shown to be equivalent by Ludwig Boltzmann's statistical view of entropy (1870).\n\nThe kinetic theory of gases relates the macroscopic properties of gases, such as pressure and volume, to the microscopic properties of the molecules which make up the gas, particularly the mass and speed of the molecules. To derive Charles's law from kinetic theory, it is necessary to have a microscopic definition of temperature: this can be conveniently taken as the temperature being proportional to the average kinetic energy of the gas molecules, Ek:\n\nUnder this definition, the demonstration of Charles's law is almost trivial. The kinetic theory equivalent of the ideal gas law relates PV to the average kinetic energy:",
        pageTitle: "Charles's law",
    },
    {
        title: "gas laws",
        link: "https://en.wikipedia.org/wiki/Gas_laws",
        content:
            "The laws describing the behaviour of gases under fixed pressure, volume, amount of gas, and absolute temperature conditions are called gas laws. The basic gas laws were discovered by the end of the 18th century when scientists found out that relationships between pressure, volume and temperature of a sample of gas could be obtained which would hold to approximation for all gases. The combination of several empirical gas laws led to the development of the ideal gas law.\n\nThe ideal gas law was later found to be consistent with atomic and kinetic theory.\n\nIn 1643, the Italian physicist and mathematician, Evangelista Torricelli, who for a few months had acted as Galileo Galilei's secretary, conducted a celebrated experiment in Florence.[1] He demonstrated that a column of mercury in an inverted tube can be supported by the pressure of air outside of the tube, with the creation of a small section of vacuum above the mercury.[2] This experiment essentially paved the way towards the invention of the barometer, as well as drawing the attention of Robert Boyle, then a \"skeptical\" scientist working in England. Boyle was inspired by Torricelli's experiment to investigate how the elasticity of air responds to varying pressure, and he did this through a series of experiments with a setup reminiscent of that used by Torricelli.[3] Boyle published his results in 1662.\n\nLater on, in 1676, the French physicist Edme Mariotte, independently arrived at the same conclusions of Boyle, while also noting some dependency of air volume on temperature.[4] However it took another century and a half for the development of thermometry and recognition of the absolute zero temperature scale, which eventually allowed the discovery of temperature-dependent gas laws.\n\nIn 1662, Robert Boyle systematically studied the relationship between the volume and pressure of a fixed amount of gas at a constant temperature. He observed that the volume of a given mass of a gas is inversely proportional to its pressure at a constant temperature.\nBoyle's law, published in 1662, states that, at a constant temperature, the product of the pressure and volume of a given mass of an ideal gas in a closed system is always constant. It can be verified experimentally using a pressure gauge and a variable volume container. It can also be derived from the kinetic theory of gases: if a container, with a fixed number of molecules inside, is reduced in volume, more molecules will strike a given area of the sides of the container per unit time, causing a greater pressure.\n\nThe concept can be represented with these formulae:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}}\n  \n\nwhere P is the pressure, V is the volume of a gas, and k1 is the constant in this equation (and is not the same as the proportionality constants in the other equations).\n\nCharles' law, or the law of volumes, was founded in 1787 by Jacques Charles. It states that, for a given mass of an ideal gas at constant pressure, the volume is directly proportional to its absolute temperature, assuming in a closed system.\nThe statement of Charles' law is as follows:\nthe volume (V) of a given mass of a gas, at constant pressure (P), is directly proportional to its temperature (T).\n\nwhere \"V\" is the volume of a gas, \"T\" is the absolute temperature and k2 is a proportionality constant (which is not the same as the proportionality constants in the other equations in this article).\n\nGay-Lussac's law, Amontons' law or the pressure law was founded by Joseph Louis Gay-Lussac in 1808.\n\nP\n              \n                1\n              \n            \n            \n              T\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              P\n              \n                2\n              \n            \n            \n              T\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {P_{1} \\over T_{1}}={P_{2} \\over T_{2}}}\n  \n,\n\nAvogadro's law, Avogadro's hypothesis, Avogadro's principle or Avogadro-Ampère's hypothesis is an experimental gas law which was hypothesized by Amedeo Avogadro in 1811. It related the volume of a gas to the amount of substance of gas present.[5]\n\nThis statement gives rise to the molar volume of a gas, which at STP (273.15 K, 1 atm) is about 22.4 L. The relation is given by:\n\nThe combined gas law or general gas equation is obtained by combining Boyle's law, Charles's law, and Gay-Lussac's law. It shows the relationship between the pressure, volume, and temperature for a fixed mass of gas:\n\nWith the addition of Avogadro's law, the combined gas law develops into the ideal gas law:\n\nThese equations are exact only for an ideal gas, which neglects various intermolecular effects (see real gas). However, the ideal gas law is a good approximation for most gases under moderate pressure and temperature.",
        pageTitle: "Gas laws",
    },
    {
        title: "Cheops law",
        link: "https://en.wikipedia.org/wiki/Cheops_law",
        content:
            "Cheops' Law is an adage or epigram that is typically stated as, Nothing ever gets built on schedule or within budget.[1][2]\n\nWritten by Robert A. Heinlein; attributed to his fictitious character Lazarus Long in Time Enough for Love (1973)[3] and later in The Notebooks Of Lazarus Long.",
        pageTitle: "Cheops law",
    },
    {
        title: "Child's law",
        link: "https://en.wikipedia.org/wiki/Child%27s_law",
        content:
            "Space charge is an interpretation of a collection of electric charges in which excess electric charge is treated as a continuum of charge distributed over a region of space (either a volume or an area) rather than distinct point-like charges. This model typically applies when charge carriers have been emitted from some region of a solid—the cloud of emitted carriers can form a space charge region if they are sufficiently spread out, or the charged atoms or molecules left behind in the solid can form a space charge region.\n\nSpace charge effects are most pronounced in dielectric media (including vacuum); in highly conductive media, the charge tends to be rapidly neutralized or screened. The sign of the space charge can be either negative or positive. This situation is perhaps most familiar in the area near a metal object when it is heated to incandescence in a vacuum. This effect was first observed by Thomas Edison in light bulb filaments, where it is sometimes called the Edison effect. Space charge is a significant phenomenon in many vacuum and solid-state electronic devices.\n\nWhen a metal object is placed in a vacuum and is heated to incandescence, the energy is sufficient to cause electrons to \"boil\" away from the surface atoms and surround the metal object in a cloud of free electrons. This is called thermionic emission. The resulting cloud is negatively charged, and can be attracted to any nearby positively charged object, thus producing an electric current which passes through the vacuum.\n\nSpace charge can result from a range of phenomena, but the most important are:\n\nIt has been suggested that in alternating current (AC) most carriers injected at electrodes during a half cycle are ejected during the next half cycle, so the net balance of charge on a cycle is practically zero. However, a small fraction of the carriers can be trapped at levels[clarification needed] deep enough to retain them when the field is inverted. The amount of charge in AC should increase slower than in direct current (DC) and become observable after longer periods of time.\n\nHetero charge means that the polarity of the space charge is opposite to that of neighboring electrode, and homo charge is the reverse situation. Under high voltage application, a hetero charge near the electrode is expected to reduce the breakdown voltage, whereas a homo charge will increase it. After polarity reversal under ac conditions, the homo charge is converted to hetero space charge.\n\nIf the near \"vacuum\" has a pressure of 10−6 mmHg or less, the main vehicle of conduction is electrons. The emission current density (J) from the cathode, as a function of its thermodynamic temperature T, in the absence of space-charge, is given by Richardson's law:\n\n  \n    \n      \n        J\n        =\n        (\n        1\n        −\n        \n          \n            \n              r\n              ~\n            \n          \n        \n        )\n        \n          A\n          \n            0\n          \n        \n        \n          T\n          \n            2\n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              \n                −\n                ϕ\n              \n              \n                k\n                T\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle J=(1-{\\tilde {r}})A_{0}T^{2}\\exp \\left({\\frac {-\\phi }{kT}}\\right)}\n  \n\nwhere\n\nThe reflection coefficient can be as low as 0.105 but is usually near 0.5. For tungsten, (1 − \n  \n    \n      \n        \n          \n            \n              r\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {r}}}\n  \n)A0 = (0.6 to 1.0)×106 A⋅m−2⋅K−2, and ϕ = 4.52 eV. At 2500 °C, the emission is 28207 A/m2.\n\nThe emission current as given above is many times greater than that normally collected by the electrodes, except in some pulsed valves such as the cavity magnetron. Most of the electrons emitted by the cathode are driven back to it by the repulsion of the cloud of electrons in its neighborhood. This is called the space charge effect. In the limit of large current densities, J is given by the Child–Langmuir equation below, rather than by the thermionic emission equation above.\n\nSpace charge is an inherent property of all vacuum tubes. This has at times made life harder or easier for electrical engineers who used tubes in their designs. For example, space charge significantly limited the practical application of triode amplifiers which led to further innovations such as the vacuum tube tetrode.\n\nOn the other hand, space charge was useful in some tube applications because it generates a negative EMF within the tube's envelope, which could be used to create a negative bias on the tube's grid. Grid bias could also be achieved by using an applied grid voltage in addition to the control voltage. This could improve the engineer's control and fidelity of amplification. It allowed the construction of space charge tubes for car radios that required only 6 or 12 volts anode voltage (typical examples were the 6DR8/EBF83, 6GM8/ECC86, 6DS8/ECH83, 6ES6/EF97 and 6ET6/EF98).\n\nSpace charges can also occur within dielectrics. For example, when gas near a high voltage electrode begins to undergo dielectric breakdown, electrical charges are injected into the region near the electrode, forming space charge regions in the surrounding gas. Space charges can also occur within solid or liquid dielectrics that are stressed by high electric fields. Trapped space charges within solid dielectrics are often a contributing factor leading to dielectric failure within high voltage power cables and capacitors.\n\nIn semiconductor physics, space charge layers that are depleted of charge carriers are used as a model to explain the rectifying behaviour of p–n junctions and the buildup of a voltage in photovoltaic cells.\n\nFirst proposed by Clement D. Child in 1911, Child's law states that the space-charge-limited current (SCLC) in a plane-parallel vacuum diode varies directly as the three-halves power of the anode voltage \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n and inversely as the square of the distance d separating the cathode and the anode.[3]\n\nFor electrons, the current density J (amperes per meter squared) is written:\n\n  \n    \n      \n        J\n        =\n        \n          \n            I\n            S\n          \n        \n        =\n        \n          \n            \n              4\n              \n                ε\n                \n                  0\n                \n              \n            \n            9\n          \n        \n        \n          \n            \n              \n                2\n                e\n              \n              \n                m\n                \n                  \n                    e\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n              V\n              \n                3\n                \n                  /\n                \n                2\n              \n            \n            \n              d\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\frac {I}{S}}={\\frac {4\\varepsilon _{0}}{9}}{\\sqrt {\\frac {2e}{m_{\\mathrm {e} }}}}{\\frac {V^{3/2}}{d^{2}}}.}\n  \n\nwhere \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n is the anode current and S the surface area of the anode receiving the current; \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is the magnitude of the charge of the electron and \n  \n    \n      \n        \n          m\n          \n            \n              e\n            \n          \n        \n      \n    \n    {\\displaystyle m_{\\mathrm {e} }}\n  \n is its mass. The equation is also known as the \"three-halves-power law\" or the Child–Langmuir law. Child originally derived this equation for the case of atomic ions, which have much smaller ratios of their charge to their mass. Irving Langmuir published the application to electron currents in 1913, and extended it to the case of cylindrical cathodes and anodes.[4]\n\nThe equation's validity is subject to the following assumptions:\n\nThe assumption of no scattering (ballistic transport) is what makes the predictions of Child–Langmuir law different from those of Mott–Gurney law. The latter assumes steady-state drift transport and therefore strong scattering.\n\nChild's law was further generalized by Buford R. Conley in 1995 for the case of non-zero velocity at the cathode surface with the following equation:[5]\n\nI\n        \n        =\n        \n          \n            \n              2\n              \n                ε\n                \n                  0\n                \n              \n              m\n            \n            \n              9\n              q\n              \n                d\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n              \n                \n                  ν\n                  \n                    initial\n                  \n                  \n                    3\n                    \n                      /\n                    \n                    2\n                  \n                \n                −\n                \n                  \n                    (\n                    \n                      \n                        ν\n                        \n                          initial\n                        \n                        \n                          2\n                        \n                      \n                      +\n                      \n                        \n                          \n                            2\n                            q\n                            V\n                          \n                          m\n                        \n                      \n                    \n                    )\n                  \n                  \n                    3\n                    \n                      /\n                    \n                    4\n                  \n                \n              \n              )\n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {I}={\\frac {2\\varepsilon _{0}m}{9qd^{2}}}\\left(\\left.\\nu _{\\text{initial}}^{3/2}-\\left(\\nu _{\\text{initial}}^{2}+{\\frac {2qV}{m}}\\right)^{3/4}\\right)\\right)^{2}}\n\nwhere \n  \n    \n      \n        \n          ν\n          \n            initial\n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{initial}}}\n  \n is the initial velocity of the particle. This equation reduces to Child's Law for the special case of \n  \n    \n      \n        \n          ν\n          \n            initial\n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{initial}}}\n  \n equal to zero.\n\nIn recent years, various models of SCLC current have been revised as reported in two review papers.[6][7]\n\nIn semiconductors and insulating materials, an electric field causes charged particles, electrons, to reach a specific drift velocity that is parallel to the direction of the field. This is different from the behavior of the free charged particles in a vacuum, in which a field accelerates the particle. The proportionality factor between the magnitudes of the drift velocity, \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n, and the electric field, \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n, is called the mobility, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n:\n\n  \n    \n      \n        v\n        =\n        μ\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle v=\\mu {\\mathcal {E}}}\n\nThe Child's law behavior of a space-charge-limited current that applies in a vacuum diode doesn't generally apply to a semiconductor/insulator in a single-carrier device, and is replaced by the Mott–Gurney law. For a thin slab of material of thickness \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, sandwiched between two selective Ohmic contacts, the electric current density, \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n, flowing through the slab is given by:[8][9]\n\n  \n    \n      \n        J\n        =\n        \n          \n            9\n            8\n          \n        \n        ε\n        μ\n        \n          \n            \n              V\n              \n                2\n              \n            \n            \n              L\n              \n                3\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle J={\\frac {9}{8}}\\varepsilon \\mu {\\frac {V^{2}}{L^{3}}},}\n  \n\nwhere \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is the voltage that has been applied across the slab and \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is the permittivity of the solid. The Mott–Gurney law offers some crucial insight into charge-transport across an intrinsic semiconductor,\nnamely that one should not expect the drift current to increase linearly with the applied voltage, i.e., from Ohm's law, as one would expect from charge-transport across a metal or highly doped semiconductor. Since the only unknown quantity in the Mott–Gurney law is the charge-carrier mobility, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, the equation is commonly used to characterize charge transport in intrinsic semiconductors. Using the Mott–Gurney law for characterizing amorphous semiconductors, along with semiconductors containing defects and/or non-Ohmic contacts, should however be approached with caution as significant deviations both in the magnitude of the current and the power law dependence with respect to the voltage will be observed. In those cases the Mott–Gurney law can not be readily used for characterization, and other equations which can account for defects and/or non-ideal injection should be used instead.\n\nDuring the derivation of the Mott–Gurney law, one has to make the following assumptions:\n\nConsider a crystal of thickness \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n carrying a current \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n. Let \n  \n    \n      \n        E\n        (\n        x\n        )\n      \n    \n    {\\displaystyle E(x)}\n  \n be the electric field at a distance \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n from the surface, and \n  \n    \n      \n        n\n        (\n        x\n        )\n      \n    \n    {\\displaystyle n(x)}\n  \n the number of electrons per unit volume.\nThen the current is given has two contributions, one due to drift and the other due to diffusion:\n\n  \n    \n      \n        J\n        =\n        e\n        n\n        \n          μ\n        \n        E\n        −\n        D\n        e\n        \n          \n            \n              d\n              n\n            \n            \n              d\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle J=en{\\mu }E-De{\\frac {dn}{dx}},}\n\nWhen \n  \n    \n      \n        \n          μ\n        \n      \n    \n    {\\displaystyle {\\mu }}\n  \n is the electrons mobility and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n the diffusion coefficient. Laplace's equation gives for the field:\n\n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        e\n        \n          \n            n\n            ε\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dE}{dx}}=e{\\frac {n}{\\varepsilon }}.}\n\nHence, eliminating \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, we have:\n\n  \n    \n      \n        J\n        =\n        \n          ε\n        \n        \n          μ\n        \n        E\n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        −\n        ε\n        D\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              E\n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\varepsilon }{\\mu }E{\\frac {dE}{dx}}-\\varepsilon D{\\frac {d^{2}E}{dx^{2}}}.}\n\nAfter integrating, making use of the Einstein relation and neglecting the \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}}\n  \n term we obtain for the electric field:\n\n  \n    \n      \n        E\n        =\n        \n          \n            \n              \n                \n                  2\n                  J\n                \n                \n                  ε\n                  μ\n                \n              \n            \n            (\n            x\n            +\n            \n              x\n              \n                0\n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle E={\\sqrt {{\\frac {2J}{\\varepsilon \\mu }}(x+x_{0})}},}\n  \n\nwhere \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is a constant. We may neglect the \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}}\n  \n term because we are supposing that \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        ∼\n        \n          \n            E\n            L\n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}\\sim {\\frac {E}{L}}}\n  \n and \n  \n    \n      \n        K\n        T\n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        ≪\n        e\n        \n          E\n          \n            2\n          \n        \n      \n    \n    {\\textstyle KT{\\frac {dE}{dx}}\\ll eE^{2}}\n  \n.\n\nSince, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n, \n  \n    \n      \n        n\n        =\n        \n          n\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle n=n_{0}}\n  \n, we have:\n\nIt follows that the potential drop across the crystal is:\n\nMaking use of (⁎) and (⁎⁎) we can write \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n in terms of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. For small \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is small and \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ≪\n        L\n      \n    \n    {\\displaystyle x_{0}\\ll L}\n  \n, so that:\n\nThus the current increases as the square of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. For large \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ≫\n        L\n      \n    \n    {\\displaystyle x_{0}\\gg L}\n  \n and we obtain:\n\n  \n    \n      \n        J\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          \n            \n              e\n              μ\n              \n                n\n                \n                  0\n                \n              \n              V\n            \n            L\n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\frac {1}{2}}{\\frac {e\\mu n_{0}V}{L}}.}\n\nAs an application example, the steady-state space-charge-limited current across a piece of intrinsic silicon with a charge-carrier mobility of 1500 cm2/V-s, a relative dielectric constant of 11.9, an area of 10−8 cm2 and a thickness of 10−4 cm can be calculated by an online calculator to be 126.4 μA at 3 V. Note that in order for this calculation to be accurate, one must assume all the points listed above.\n\nIn the case where the electron/hole transport is limited by trap states in the form of exponential tails extending from the conduction/valence band edges,\n\n  \n    \n      \n        \n          n\n          \n            \n              t\n            \n          \n        \n        =\n        \n          \n            \n              N\n              \n                \n                  t\n                \n              \n            \n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              \n                T\n                \n                  \n                    c\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                E\n                \n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  \n                    T\n                    \n                      \n                        c\n                      \n                    \n                  \n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle n_{\\mathrm {t} }={\\frac {N_{\\mathrm {t} }}{k_{\\mathrm {B} }T_{\\mathrm {c} }}}\\exp \\left(-{\\frac {E}{k_{\\mathrm {B} }T_{\\mathrm {c} }}}\\right),}\n  \n\nthe drift current density is given by the Mark-Helfrich equation,[10]\n\n  \n    \n      \n        J\n        =\n        \n          q\n          \n            1\n            −\n            ℓ\n          \n        \n        \n          μ\n        \n        \n          \n            N\n            \n              \n                e\n                f\n                f\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    ε\n                    \n                      \n                        r\n                      \n                    \n                  \n                  \n                    ε\n                    \n                      0\n                    \n                  \n                  ℓ\n                \n                \n                  \n                    N\n                    \n                      \n                        t\n                      \n                    \n                  \n                  (\n                  ℓ\n                  +\n                  1\n                  )\n                \n              \n            \n            )\n          \n          \n            ℓ\n          \n        \n        \n          \n            (\n            \n              \n                \n                  2\n                  ℓ\n                  +\n                  1\n                \n                \n                  ℓ\n                  +\n                  1\n                \n              \n            \n            )\n          \n          \n            ℓ\n            +\n            1\n          \n        \n        \n          \n            \n              \n                V\n              \n              \n                ℓ\n                +\n                1\n              \n            \n            \n              \n                L\n              \n              \n                2\n                ℓ\n                +\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle J=q^{1-\\ell }{\\mu }{N_{\\mathrm {eff} }}\\left({\\frac {\\varepsilon _{\\mathrm {r} }\\varepsilon _{0}\\ell }{N_{\\mathrm {t} }(\\ell +1)}}\\right)^{\\ell }\\left({\\frac {2\\ell +1}{\\ell +1}}\\right)^{\\ell +1}{\\frac {{V}^{\\ell +1}}{{L}^{2\\ell +1}}}}\n  \n\nwhere \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the elementary charge, \n  \n    \n      \n        ℓ\n        =\n        \n          k\n          \n            \n              B\n            \n          \n        \n        \n          T\n          \n            \n              c\n            \n          \n        \n        \n          /\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle \\ell =k_{\\mathrm {B} }T_{\\mathrm {c} }/k_{\\mathrm {B} }T}\n  \n with \n  \n    \n      \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle k_{\\mathrm {B} }T}\n  \n being the thermal energy, \n  \n    \n      \n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {eff} }}\n  \n is the effective density of states of the charge carrier type in the semiconductor, i.e., either \n  \n    \n      \n        \n          E\n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\mathrm {C} }}\n  \n or \n  \n    \n      \n        \n          E\n          \n            \n              V\n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\mathrm {V} }}\n  \n, and \n  \n    \n      \n        \n          N\n          \n            \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {t} }}\n  \n is the trap density.\n\nIn the case where a very small applied bias is applied across the single-carrier device, the current is given by:[11][12][13]\n\n  \n    \n      \n        J\n        =\n        4\n        \n          \n            π\n          \n          \n            2\n          \n        \n        \n          \n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n            q\n          \n        \n        μ\n        ε\n        \n          \n            V\n            \n              L\n              \n                3\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J=4{\\pi }^{2}{\\frac {k_{\\mathrm {B} }T}{q}}\\mu \\varepsilon {\\frac {V}{L^{3}}}.}\n\nNote that the equation describing the current in the low voltage regime follows the same thickness scaling as the Mott–Gurney law, \n  \n    \n      \n        \n          L\n          \n            −\n            3\n          \n        \n      \n    \n    {\\displaystyle L^{-3}}\n  \n, but increases linearly with the applied voltage.\n\nWhen a very large voltage is applied across the semiconductor, the current can transition into a saturation regime.\n\nIn the velocity-saturation regime, this equation takes the following form\n\n  \n    \n      \n        J\n        =\n        2\n        ε\n        v\n        \n          \n            V\n            \n              L\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle J=2\\varepsilon v{\\frac {V}{L^{2}}}}\n\nNote the different dependence of \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n on \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n between the Mott–Gurney law and the equation describing the current in the velocity-saturation regime.  In the ballistic case (assuming no collisions), the Mott–Gurney equation takes the form of the more familiar Child–Langmuir law.\n\nIn the charge-carrier saturation regime, the current through the sample is given by,\n\n  \n    \n      \n        J\n        =\n        q\n        μ\n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n        \n          \n            V\n            L\n          \n        \n      \n    \n    {\\displaystyle J=q\\mu N_{\\mathrm {eff} }{\\frac {V}{L}}}\n  \n\nwhere \n  \n    \n      \n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {eff} }}\n  \n is the effective density of states of the charge carrier type in the semiconductor.\n\nSpace charge tends to reduce shot noise.[14] Shot noise results from the random arrivals of discrete charge; the statistical variation in the arrivals produces shot noise.[15] A space charge develops a potential that slows the carriers down. For example, an electron approaching a cloud of other electrons will slow down due to the repulsive force. The slowing carriers also increases the space charge density and resulting potential. In addition, the potential developed by the space charge can reduce the number of carriers emitted.[16] When the space charge limits the current, the random arrivals of the carriers are smoothed out; the reduced variation results in less shot noise.[15]",
        pageTitle: "Space charge",
    },
    {
        title: "Mott–Gurney law",
        link: "https://en.wikipedia.org/wiki/Mott%E2%80%93Gurney_law",
        content:
            "Space charge is an interpretation of a collection of electric charges in which excess electric charge is treated as a continuum of charge distributed over a region of space (either a volume or an area) rather than distinct point-like charges. This model typically applies when charge carriers have been emitted from some region of a solid—the cloud of emitted carriers can form a space charge region if they are sufficiently spread out, or the charged atoms or molecules left behind in the solid can form a space charge region.\n\nSpace charge effects are most pronounced in dielectric media (including vacuum); in highly conductive media, the charge tends to be rapidly neutralized or screened. The sign of the space charge can be either negative or positive. This situation is perhaps most familiar in the area near a metal object when it is heated to incandescence in a vacuum. This effect was first observed by Thomas Edison in light bulb filaments, where it is sometimes called the Edison effect. Space charge is a significant phenomenon in many vacuum and solid-state electronic devices.\n\nWhen a metal object is placed in a vacuum and is heated to incandescence, the energy is sufficient to cause electrons to \"boil\" away from the surface atoms and surround the metal object in a cloud of free electrons. This is called thermionic emission. The resulting cloud is negatively charged, and can be attracted to any nearby positively charged object, thus producing an electric current which passes through the vacuum.\n\nSpace charge can result from a range of phenomena, but the most important are:\n\nIt has been suggested that in alternating current (AC) most carriers injected at electrodes during a half cycle are ejected during the next half cycle, so the net balance of charge on a cycle is practically zero. However, a small fraction of the carriers can be trapped at levels[clarification needed] deep enough to retain them when the field is inverted. The amount of charge in AC should increase slower than in direct current (DC) and become observable after longer periods of time.\n\nHetero charge means that the polarity of the space charge is opposite to that of neighboring electrode, and homo charge is the reverse situation. Under high voltage application, a hetero charge near the electrode is expected to reduce the breakdown voltage, whereas a homo charge will increase it. After polarity reversal under ac conditions, the homo charge is converted to hetero space charge.\n\nIf the near \"vacuum\" has a pressure of 10−6 mmHg or less, the main vehicle of conduction is electrons. The emission current density (J) from the cathode, as a function of its thermodynamic temperature T, in the absence of space-charge, is given by Richardson's law:\n\n  \n    \n      \n        J\n        =\n        (\n        1\n        −\n        \n          \n            \n              r\n              ~\n            \n          \n        \n        )\n        \n          A\n          \n            0\n          \n        \n        \n          T\n          \n            2\n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              \n                −\n                ϕ\n              \n              \n                k\n                T\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle J=(1-{\\tilde {r}})A_{0}T^{2}\\exp \\left({\\frac {-\\phi }{kT}}\\right)}\n  \n\nwhere\n\nThe reflection coefficient can be as low as 0.105 but is usually near 0.5. For tungsten, (1 − \n  \n    \n      \n        \n          \n            \n              r\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {r}}}\n  \n)A0 = (0.6 to 1.0)×106 A⋅m−2⋅K−2, and ϕ = 4.52 eV. At 2500 °C, the emission is 28207 A/m2.\n\nThe emission current as given above is many times greater than that normally collected by the electrodes, except in some pulsed valves such as the cavity magnetron. Most of the electrons emitted by the cathode are driven back to it by the repulsion of the cloud of electrons in its neighborhood. This is called the space charge effect. In the limit of large current densities, J is given by the Child–Langmuir equation below, rather than by the thermionic emission equation above.\n\nSpace charge is an inherent property of all vacuum tubes. This has at times made life harder or easier for electrical engineers who used tubes in their designs. For example, space charge significantly limited the practical application of triode amplifiers which led to further innovations such as the vacuum tube tetrode.\n\nOn the other hand, space charge was useful in some tube applications because it generates a negative EMF within the tube's envelope, which could be used to create a negative bias on the tube's grid. Grid bias could also be achieved by using an applied grid voltage in addition to the control voltage. This could improve the engineer's control and fidelity of amplification. It allowed the construction of space charge tubes for car radios that required only 6 or 12 volts anode voltage (typical examples were the 6DR8/EBF83, 6GM8/ECC86, 6DS8/ECH83, 6ES6/EF97 and 6ET6/EF98).\n\nSpace charges can also occur within dielectrics. For example, when gas near a high voltage electrode begins to undergo dielectric breakdown, electrical charges are injected into the region near the electrode, forming space charge regions in the surrounding gas. Space charges can also occur within solid or liquid dielectrics that are stressed by high electric fields. Trapped space charges within solid dielectrics are often a contributing factor leading to dielectric failure within high voltage power cables and capacitors.\n\nIn semiconductor physics, space charge layers that are depleted of charge carriers are used as a model to explain the rectifying behaviour of p–n junctions and the buildup of a voltage in photovoltaic cells.\n\nFirst proposed by Clement D. Child in 1911, Child's law states that the space-charge-limited current (SCLC) in a plane-parallel vacuum diode varies directly as the three-halves power of the anode voltage \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n and inversely as the square of the distance d separating the cathode and the anode.[3]\n\nFor electrons, the current density J (amperes per meter squared) is written:\n\n  \n    \n      \n        J\n        =\n        \n          \n            I\n            S\n          \n        \n        =\n        \n          \n            \n              4\n              \n                ε\n                \n                  0\n                \n              \n            \n            9\n          \n        \n        \n          \n            \n              \n                2\n                e\n              \n              \n                m\n                \n                  \n                    e\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n              V\n              \n                3\n                \n                  /\n                \n                2\n              \n            \n            \n              d\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\frac {I}{S}}={\\frac {4\\varepsilon _{0}}{9}}{\\sqrt {\\frac {2e}{m_{\\mathrm {e} }}}}{\\frac {V^{3/2}}{d^{2}}}.}\n  \n\nwhere \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n is the anode current and S the surface area of the anode receiving the current; \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is the magnitude of the charge of the electron and \n  \n    \n      \n        \n          m\n          \n            \n              e\n            \n          \n        \n      \n    \n    {\\displaystyle m_{\\mathrm {e} }}\n  \n is its mass. The equation is also known as the \"three-halves-power law\" or the Child–Langmuir law. Child originally derived this equation for the case of atomic ions, which have much smaller ratios of their charge to their mass. Irving Langmuir published the application to electron currents in 1913, and extended it to the case of cylindrical cathodes and anodes.[4]\n\nThe equation's validity is subject to the following assumptions:\n\nThe assumption of no scattering (ballistic transport) is what makes the predictions of Child–Langmuir law different from those of Mott–Gurney law. The latter assumes steady-state drift transport and therefore strong scattering.\n\nChild's law was further generalized by Buford R. Conley in 1995 for the case of non-zero velocity at the cathode surface with the following equation:[5]\n\nI\n        \n        =\n        \n          \n            \n              2\n              \n                ε\n                \n                  0\n                \n              \n              m\n            \n            \n              9\n              q\n              \n                d\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n              \n                \n                  ν\n                  \n                    initial\n                  \n                  \n                    3\n                    \n                      /\n                    \n                    2\n                  \n                \n                −\n                \n                  \n                    (\n                    \n                      \n                        ν\n                        \n                          initial\n                        \n                        \n                          2\n                        \n                      \n                      +\n                      \n                        \n                          \n                            2\n                            q\n                            V\n                          \n                          m\n                        \n                      \n                    \n                    )\n                  \n                  \n                    3\n                    \n                      /\n                    \n                    4\n                  \n                \n              \n              )\n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {I}={\\frac {2\\varepsilon _{0}m}{9qd^{2}}}\\left(\\left.\\nu _{\\text{initial}}^{3/2}-\\left(\\nu _{\\text{initial}}^{2}+{\\frac {2qV}{m}}\\right)^{3/4}\\right)\\right)^{2}}\n\nwhere \n  \n    \n      \n        \n          ν\n          \n            initial\n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{initial}}}\n  \n is the initial velocity of the particle. This equation reduces to Child's Law for the special case of \n  \n    \n      \n        \n          ν\n          \n            initial\n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{initial}}}\n  \n equal to zero.\n\nIn recent years, various models of SCLC current have been revised as reported in two review papers.[6][7]\n\nIn semiconductors and insulating materials, an electric field causes charged particles, electrons, to reach a specific drift velocity that is parallel to the direction of the field. This is different from the behavior of the free charged particles in a vacuum, in which a field accelerates the particle. The proportionality factor between the magnitudes of the drift velocity, \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n, and the electric field, \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n, is called the mobility, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n:\n\n  \n    \n      \n        v\n        =\n        μ\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle v=\\mu {\\mathcal {E}}}\n\nThe Child's law behavior of a space-charge-limited current that applies in a vacuum diode doesn't generally apply to a semiconductor/insulator in a single-carrier device, and is replaced by the Mott–Gurney law. For a thin slab of material of thickness \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, sandwiched between two selective Ohmic contacts, the electric current density, \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n, flowing through the slab is given by:[8][9]\n\n  \n    \n      \n        J\n        =\n        \n          \n            9\n            8\n          \n        \n        ε\n        μ\n        \n          \n            \n              V\n              \n                2\n              \n            \n            \n              L\n              \n                3\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle J={\\frac {9}{8}}\\varepsilon \\mu {\\frac {V^{2}}{L^{3}}},}\n  \n\nwhere \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is the voltage that has been applied across the slab and \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is the permittivity of the solid. The Mott–Gurney law offers some crucial insight into charge-transport across an intrinsic semiconductor,\nnamely that one should not expect the drift current to increase linearly with the applied voltage, i.e., from Ohm's law, as one would expect from charge-transport across a metal or highly doped semiconductor. Since the only unknown quantity in the Mott–Gurney law is the charge-carrier mobility, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, the equation is commonly used to characterize charge transport in intrinsic semiconductors. Using the Mott–Gurney law for characterizing amorphous semiconductors, along with semiconductors containing defects and/or non-Ohmic contacts, should however be approached with caution as significant deviations both in the magnitude of the current and the power law dependence with respect to the voltage will be observed. In those cases the Mott–Gurney law can not be readily used for characterization, and other equations which can account for defects and/or non-ideal injection should be used instead.\n\nDuring the derivation of the Mott–Gurney law, one has to make the following assumptions:\n\nConsider a crystal of thickness \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n carrying a current \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n. Let \n  \n    \n      \n        E\n        (\n        x\n        )\n      \n    \n    {\\displaystyle E(x)}\n  \n be the electric field at a distance \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n from the surface, and \n  \n    \n      \n        n\n        (\n        x\n        )\n      \n    \n    {\\displaystyle n(x)}\n  \n the number of electrons per unit volume.\nThen the current is given has two contributions, one due to drift and the other due to diffusion:\n\n  \n    \n      \n        J\n        =\n        e\n        n\n        \n          μ\n        \n        E\n        −\n        D\n        e\n        \n          \n            \n              d\n              n\n            \n            \n              d\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle J=en{\\mu }E-De{\\frac {dn}{dx}},}\n\nWhen \n  \n    \n      \n        \n          μ\n        \n      \n    \n    {\\displaystyle {\\mu }}\n  \n is the electrons mobility and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n the diffusion coefficient. Laplace's equation gives for the field:\n\n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        e\n        \n          \n            n\n            ε\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dE}{dx}}=e{\\frac {n}{\\varepsilon }}.}\n\nHence, eliminating \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, we have:\n\n  \n    \n      \n        J\n        =\n        \n          ε\n        \n        \n          μ\n        \n        E\n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        −\n        ε\n        D\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              E\n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\varepsilon }{\\mu }E{\\frac {dE}{dx}}-\\varepsilon D{\\frac {d^{2}E}{dx^{2}}}.}\n\nAfter integrating, making use of the Einstein relation and neglecting the \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}}\n  \n term we obtain for the electric field:\n\n  \n    \n      \n        E\n        =\n        \n          \n            \n              \n                \n                  2\n                  J\n                \n                \n                  ε\n                  μ\n                \n              \n            \n            (\n            x\n            +\n            \n              x\n              \n                0\n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle E={\\sqrt {{\\frac {2J}{\\varepsilon \\mu }}(x+x_{0})}},}\n  \n\nwhere \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is a constant. We may neglect the \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}}\n  \n term because we are supposing that \n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        ∼\n        \n          \n            E\n            L\n          \n        \n      \n    \n    {\\textstyle {\\frac {dE}{dx}}\\sim {\\frac {E}{L}}}\n  \n and \n  \n    \n      \n        K\n        T\n        \n          \n            \n              d\n              E\n            \n            \n              d\n              x\n            \n          \n        \n        ≪\n        e\n        \n          E\n          \n            2\n          \n        \n      \n    \n    {\\textstyle KT{\\frac {dE}{dx}}\\ll eE^{2}}\n  \n.\n\nSince, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n, \n  \n    \n      \n        n\n        =\n        \n          n\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle n=n_{0}}\n  \n, we have:\n\nIt follows that the potential drop across the crystal is:\n\nMaking use of (⁎) and (⁎⁎) we can write \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n in terms of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. For small \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is small and \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ≪\n        L\n      \n    \n    {\\displaystyle x_{0}\\ll L}\n  \n, so that:\n\nThus the current increases as the square of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. For large \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ≫\n        L\n      \n    \n    {\\displaystyle x_{0}\\gg L}\n  \n and we obtain:\n\n  \n    \n      \n        J\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          \n            \n              e\n              μ\n              \n                n\n                \n                  0\n                \n              \n              V\n            \n            L\n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\frac {1}{2}}{\\frac {e\\mu n_{0}V}{L}}.}\n\nAs an application example, the steady-state space-charge-limited current across a piece of intrinsic silicon with a charge-carrier mobility of 1500 cm2/V-s, a relative dielectric constant of 11.9, an area of 10−8 cm2 and a thickness of 10−4 cm can be calculated by an online calculator to be 126.4 μA at 3 V. Note that in order for this calculation to be accurate, one must assume all the points listed above.\n\nIn the case where the electron/hole transport is limited by trap states in the form of exponential tails extending from the conduction/valence band edges,\n\n  \n    \n      \n        \n          n\n          \n            \n              t\n            \n          \n        \n        =\n        \n          \n            \n              N\n              \n                \n                  t\n                \n              \n            \n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              \n                T\n                \n                  \n                    c\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                E\n                \n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  \n                    T\n                    \n                      \n                        c\n                      \n                    \n                  \n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle n_{\\mathrm {t} }={\\frac {N_{\\mathrm {t} }}{k_{\\mathrm {B} }T_{\\mathrm {c} }}}\\exp \\left(-{\\frac {E}{k_{\\mathrm {B} }T_{\\mathrm {c} }}}\\right),}\n  \n\nthe drift current density is given by the Mark-Helfrich equation,[10]\n\n  \n    \n      \n        J\n        =\n        \n          q\n          \n            1\n            −\n            ℓ\n          \n        \n        \n          μ\n        \n        \n          \n            N\n            \n              \n                e\n                f\n                f\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    ε\n                    \n                      \n                        r\n                      \n                    \n                  \n                  \n                    ε\n                    \n                      0\n                    \n                  \n                  ℓ\n                \n                \n                  \n                    N\n                    \n                      \n                        t\n                      \n                    \n                  \n                  (\n                  ℓ\n                  +\n                  1\n                  )\n                \n              \n            \n            )\n          \n          \n            ℓ\n          \n        \n        \n          \n            (\n            \n              \n                \n                  2\n                  ℓ\n                  +\n                  1\n                \n                \n                  ℓ\n                  +\n                  1\n                \n              \n            \n            )\n          \n          \n            ℓ\n            +\n            1\n          \n        \n        \n          \n            \n              \n                V\n              \n              \n                ℓ\n                +\n                1\n              \n            \n            \n              \n                L\n              \n              \n                2\n                ℓ\n                +\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle J=q^{1-\\ell }{\\mu }{N_{\\mathrm {eff} }}\\left({\\frac {\\varepsilon _{\\mathrm {r} }\\varepsilon _{0}\\ell }{N_{\\mathrm {t} }(\\ell +1)}}\\right)^{\\ell }\\left({\\frac {2\\ell +1}{\\ell +1}}\\right)^{\\ell +1}{\\frac {{V}^{\\ell +1}}{{L}^{2\\ell +1}}}}\n  \n\nwhere \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the elementary charge, \n  \n    \n      \n        ℓ\n        =\n        \n          k\n          \n            \n              B\n            \n          \n        \n        \n          T\n          \n            \n              c\n            \n          \n        \n        \n          /\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle \\ell =k_{\\mathrm {B} }T_{\\mathrm {c} }/k_{\\mathrm {B} }T}\n  \n with \n  \n    \n      \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle k_{\\mathrm {B} }T}\n  \n being the thermal energy, \n  \n    \n      \n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {eff} }}\n  \n is the effective density of states of the charge carrier type in the semiconductor, i.e., either \n  \n    \n      \n        \n          E\n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\mathrm {C} }}\n  \n or \n  \n    \n      \n        \n          E\n          \n            \n              V\n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\mathrm {V} }}\n  \n, and \n  \n    \n      \n        \n          N\n          \n            \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {t} }}\n  \n is the trap density.\n\nIn the case where a very small applied bias is applied across the single-carrier device, the current is given by:[11][12][13]\n\n  \n    \n      \n        J\n        =\n        4\n        \n          \n            π\n          \n          \n            2\n          \n        \n        \n          \n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n            q\n          \n        \n        μ\n        ε\n        \n          \n            V\n            \n              L\n              \n                3\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle J=4{\\pi }^{2}{\\frac {k_{\\mathrm {B} }T}{q}}\\mu \\varepsilon {\\frac {V}{L^{3}}}.}\n\nNote that the equation describing the current in the low voltage regime follows the same thickness scaling as the Mott–Gurney law, \n  \n    \n      \n        \n          L\n          \n            −\n            3\n          \n        \n      \n    \n    {\\displaystyle L^{-3}}\n  \n, but increases linearly with the applied voltage.\n\nWhen a very large voltage is applied across the semiconductor, the current can transition into a saturation regime.\n\nIn the velocity-saturation regime, this equation takes the following form\n\n  \n    \n      \n        J\n        =\n        2\n        ε\n        v\n        \n          \n            V\n            \n              L\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle J=2\\varepsilon v{\\frac {V}{L^{2}}}}\n\nNote the different dependence of \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n on \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n between the Mott–Gurney law and the equation describing the current in the velocity-saturation regime.  In the ballistic case (assuming no collisions), the Mott–Gurney equation takes the form of the more familiar Child–Langmuir law.\n\nIn the charge-carrier saturation regime, the current through the sample is given by,\n\n  \n    \n      \n        J\n        =\n        q\n        μ\n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n        \n          \n            V\n            L\n          \n        \n      \n    \n    {\\displaystyle J=q\\mu N_{\\mathrm {eff} }{\\frac {V}{L}}}\n  \n\nwhere \n  \n    \n      \n        \n          N\n          \n            \n              e\n              f\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {eff} }}\n  \n is the effective density of states of the charge carrier type in the semiconductor.\n\nSpace charge tends to reduce shot noise.[14] Shot noise results from the random arrivals of discrete charge; the statistical variation in the arrivals produces shot noise.[15] A space charge develops a potential that slows the carriers down. For example, an electron approaching a cloud of other electrons will slow down due to the repulsive force. The slowing carriers also increases the space charge density and resulting potential. In addition, the potential developed by the space charge can reduce the number of carriers emitted.[16] When the space charge limits the current, the random arrivals of the carriers are smoothed out; the reduced variation results in less shot noise.[15]",
        pageTitle: "Space charge",
    },
    {
        title: "Chladni's law",
        link: "https://en.wikipedia.org/wiki/Chladni%27s_law",
        content:
            "Chladni's law, named after Ernst Chladni, relates the frequency of modes of vibration for flat circular surfaces with fixed center as a function of the numbers m of diametric (linear) nodes and n of radial (circular) nodes.  It is stated as the equation\n\nwhere C and p are coefficients which depend on the properties of the plate.[1]\n\nFor flat circular plates, p is roughly 2, but Chladni's law can also be used to describe the vibrations of cymbals, handbells, and church bells in which case p can vary from 1.4 to 2.4.[2] In fact, p can even vary for a single object, depending on which family of modes is being examined.\n\nThis applied mathematics–related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Chladni's law",
    },
    {
        title: "Claasen's law",
        link: "https://en.wikipedia.org/wiki/Claasen%27s_law",
        content:
            "Claasen's logarithmic law of usefulness[1] is named after technologist Theo A. C. M. Claasen, who introduced the idea in 1999 when he was CTO of Philips Semiconductors:\n\nSystem parameters (e.g. RAM, CPU speed, disk capacity) need to increase by a multiple to create a noticeable impact on performance. In the case of RAM, by the law, a 256MB unit is only 1/8 more practically useful than a 128MB unit though the base unit has doubled. It would require a 16384MB (128 × 128MB) unit of RAM to truly double performance under the law.\n\nA modern car (e.g. a Ford Mondeo) is not substantially more useful at getting the occupants from A to B than an older car (e.g. a Ford Model T)\n\nIn order to achieve a linear improvement in usefulness over time it is necessary to have an exponential increase in technology over time. Moore's law delivers an exponential increase in technology, and so when Claasen's law is combined with Moore's law it implies a linear improvement in usefulness over time.",
        pageTitle: "Claasen's law",
    },
    {
        title: "Clarke's three laws",
        link: "https://en.wikipedia.org/wiki/Clarke%27s_three_laws",
        content:
            'British science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke\'s three laws, of which the third law is the best known and most widely cited. They are part of his ideas in his extensive writings about the future.[1]\n\nOne account stated that Clarke\'s laws were developed after the editor of his works in French started numbering the author\'s assertions.[2] All three laws appear in Clarke\'s essay "Hazards of Prophecy: The Failure of Imagination", first published in Profiles of the Future (1962);[3] however, they were not all published at the same time. Clarke\'s first law was proposed in the 1962 edition of the essay, as "Clarke\'s Law" in Profiles of the Future.\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke\'s second law was conferred by others. It was initially a derivative of the first law and formally became Clarke\'s second law where the author proposed the third law in the 1973 revision of Profiles of the Future, which included an acknowledgement.[4] It was also here that Clarke wrote about the third law in these words: "As three laws were good enough for Newton, I have modestly decided to stop there".\n\nThe third law is the best known and most widely cited. It was published in a 1968 letter to Science magazine[5] and eventually added to the 1973 revision of the "Hazards of Prophecy" essay.[6]\n\nThe third law has inspired many snowclones and other variations:\n\n"When, however, the lay public rallies round an idea that is denounced by distinguished but elderly scientists and supports that idea with great fervour and emotion – the distinguished but elderly scientists are then, after all, probably right."[12]\n\nA contrapositive of the third law is "Any technology distinguishable from magic is insufficiently advanced." (Gehm\'s corollary)[13]',
        pageTitle: "Clarke's three laws",
    },
    {
        title: "Conquest's three laws",
        link: "https://en.wikipedia.org/wiki/Robert_Conquest#Laws_of_politics",
        content:
            'George Robert Acworth Conquest CMG OBE FBA FRSL (15 July 1917 – 3 August 2015) was a British and American historian, poet, and novelist.[1] He was briefly a member of the Communist Party of Great Britain[2] but later wrote several books against communism.\n\nA long-time research fellow at Stanford University\'s Hoover Institution, Conquest was most notable for his work on the Soviet Union. His books included The Great Terror: Stalin\'s Purges of the 1930s (1968); The Harvest of Sorrow: Soviet Collectivisation and the Terror-Famine (1986); and Stalin: Breaker of Nations (1991). He was also the author of two novels and several collections of poetry.\n\nConquest was born in Great Malvern, Worcestershire,[1] to an American father, Robert Folger Wescott Conquest, and an English mother, Rosamund Alys Acworth.[3][4] His father served in an American Ambulance Field Service unit with the French Army in World War I, and was awarded the Croix de Guerre, with Silver Star in 1916.[5]\n\nConquest was educated at Winchester College, where he won an exhibition to study Philosophy, Politics and Economics (PPE) at Magdalen College, Oxford. He took a gap year, spending time at the University of Grenoble and in Bulgaria, and returning to Oxford in 1937, where he joined the Communist Party of Great Britain and the Carlton Club.[6] He was awarded an MA in PPE and a DLitt in history.[7]\n\nIn Lisbon on an American passport at the outbreak of the Second World War, Conquest returned to England.[8] As the Communist Party of Great Britain denounced the war in 1939 as imperialist and capitalist, Conquest broke with it and was commissioned into the Oxfordshire and Buckinghamshire Light Infantry on 20 April 1940, serving with the regiment until 1946.[9][6]\n\nIn 1943 he was posted to the School of Slavonic and East European Studies (later part of University College London) to study Bulgarian.[10] The following year he was posted to Bulgaria as a liaison officer to the Bulgarian forces fighting under Soviet command, attached to the Third Ukrainian Front, then to the Allied Control Commission. At the end of the war, he joined the Foreign Office, returning to the British Legation in Sofia where he remained as the press officer.[1] In 1948 he left Bulgaria when he was recalled to London under a minor diplomatic cloud after he had helped smuggle two Bulgarians out of the country.[10]\n\nIn 1948 Conquest joined the Foreign Office\'s Information Research Department (IRD), a "propaganda counter-offensive" unit created by the Labour Attlee government[11] in order to "collect and summarize reliable information about Soviet and communist misdoings, to disseminate it to democratic journalists, politicians, and trade unionists, and to support, financially and otherwise, anticommunist publications."[12] The IRD was also engaged in manipulating public opinion.[13] Conquest was remembered there as a "brilliant, arrogant" figure who had 10 people reporting to him.[6] He continued to work at the Foreign Office until 1956, becoming increasingly involved in the intellectual counter-offensive against communism.[10]\n\nIn 1949 Conquest\'s assistant, Celia Kirwan (later Celia Goodman), approached George Orwell for information to help identify Soviet sympathisers. Orwell\'s list, discovered after her death in 2002, included Guardian and Observer journalists, as well as E. H. Carr and Charlie Chaplin.[14] Conquest, like Orwell, fell for the beautiful Celia Kirwan, who inspired him to write several poems.[10] One of his foreign office colleagues was Alan Maclean, brother of Donald Maclean, one of the Philby spy ring, who fled to Russia with Guy Burgess in 1951. When his brother defected, Alan resigned, then went to Macmillan and published a book of Conquest\'s poems.[6] At the Foreign Office, Conquest wrote several papers that sowed the seeds for his later work. One, on the Soviet means of obtaining confessions, was elaborated on in The Great Terror. Other papers were "Peaceful Co-existence in Soviet Propaganda and Theory", and "United Fronts – a Communist Tactic".[10] In 1950 Conquest served briefly as First Secretary in the British Delegation to the United Nations.[citation needed]\n\nIn 1956 Conquest left the Foreign Office and became a freelance writer and historian.[10] After he left, he says, the Information Research Department (IRD) suggested to him that he could combine some of the data he had gathered from Soviet publications into a book.[11] During the 1960s he edited eight volumes of work produced by the IRD, published in London by the Bodley Head as the Soviet Studies Series.[11] Many of his Foreign Office works were published this way.[10] In the United States, the material was republished as The Contemporary Soviet Union Series by Frederick Praeger, who had previously published several books on communism at the request of the CIA,[11] in addition to works by Aleksandr Solzhenitsyn, Milovan Đilas, Howard Fast, and Charles Patrick Fitzgerald.[15]\n\nIn 1962–1963 Conquest was literary editor of The Spectator, but he resigned when he found the job interfering with his historical writing. His first books on the Soviet Union were Common Sense About Russia (1960), The Soviet Deportation of Nationalities (1960) and Power and Policy in the USSR (1961). His other early works on the Soviet Union included Courage of Genius: The Pasternak Affair (1961) and Russia After Khrushchev (1965).[10]\n\nIn 1968 Conquest published what became his best-known work, The Great Terror: Stalin\'s Purge of the Thirties, the first comprehensive research of the Great Purge, which took place in the Soviet Union between 1936 and 1938. Many reviewers at the time were not impressed by his way of writing about the Great Terror, which was in the tradition of "great men who make history".[13] The book was based mainly on information which had been made public, either officially or by individuals, during the so-called "Khrushchev Thaw" in the period 1956–64. It also drew on accounts by Russian and Ukrainian émigrés and exiles dating back to the 1930s, and on an analysis of official Soviet documents such as the Soviet census.[16]\n\nThe most important aspect of the book was that it widened the understanding of the purges beyond the previous narrow focus on the "Moscow trials" of disgraced Communist Party of the Soviet Union leaders such as Nikolai Bukharin and Grigory Zinoviev, who were executed shortly thereafter. The question of why these leaders had pleaded guilty and confessed to various crimes at the trials had become a topic of discussion for a number of western writers, and helped inspire anti-Communist tracts such as George Orwell\'s Nineteen Eighty-Four and Arthur Koestler\'s Darkness at Noon.[17]\n\nConquest argued that the trials and executions of these former Communist leaders were a minor detail of the purges. By his estimates, Stalinist purges had led to the deaths of some 20 million people. He later stated that the  total number of deaths could "hardly be lower than some thirteen to fifteen million."[18]\n\nConquest sharply criticized Western intellectuals such as Beatrice and Sidney Webb, George Bernard Shaw, Jean-Paul Sartre, Walter Duranty, Sir Bernard Pares, Harold Laski, D. N. Pritt, Theodore Dreiser, Bertolt Brecht, Owen Lattimore, and Romain Rolland, as well as American ambassador Joseph Davies, accusing them of being dupes of Stalin and apologists of his regime. Conquest cites various comments made by them where, he argues, they were denying, excusing, or justifying various aspects of the purges.[19]\n\nAfter the opening up of the Soviet archives, detailed information was released that Conquest argued supported his conclusions. When Conquest\'s publisher asked him to expand and revise The Great Terror, Conquest is famously said to have suggested the new version of the book be titled I Told You So, You Fucking Fools. In fact, the mock title was jokingly proposed by Conquest\'s old friend, Sir Kingsley Amis. The new version was published in 1990 as The Great Terror: A Reassessment; .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}ISBN 0-19-507132-8.[20] The American historian J. Arch Getty disagreed, writing in 1993 that the archives did not support Conquest\'s casualty figures.[21] In 1995, investigative journalist Paul Lashmar suggested that the reputation of prominent academics such as Robert Conquest was built upon work derived from material provided by the IRD.[22]\nAccording to Denis Healey The Great Terror was an important influence, "but one which confirmed people in their views rather than converted them".[6]\n\nMany aspects of his book continue to be disputed by sovietologist historians and researchers on Russian and Soviet history, such as Stephen G. Wheatcroft, who insists that Conquest\'s victim totals for Stalinist repressions are too high, even in his reassessments.[23][24] In 2000, Michael Ignatieff, whose family had emigrated from Russia as a result of the Bolshevik Revolution, wrote "One of the few unalloyed pleasures of old age is living long enough to see yourself vindicated. Robert Conquest is currently enjoying this pleasure."[25] Conservative historian Paul Johnson, one of Thatcher\'s closest advisers, described Conquest as "our greatest living historian". And, in the phrase of Timothy Garton Ash, he was Solzhenitsyn before Solzhenitsyn.[6]\n\nIn 1996 Marxist historian Eric Hobsbawm, who had been previously attacked by Conquest for his book Age of Extremes,[26] praised Conquest\'s The Great Terror "as a remarkable pioneer effort to assess the Stalin Terror". However he expressed the view that this work and others were now to be considered obsolete "simply because the archival sources are now available". As a result, he wrote, there was no need for "fragmentary sources" and "guesswork". "[W]hen better or more complete data are available, they must take the place of poor and incomplete ones."[27] In 2002 Conquest replied to his revisionist critics: "They\'re still talking absolute balls. In the academy, there remains a feeling of, "Don\'t let\'s be too rude to Stalin. He was a bad guy, yes, but the Americans were bad guys too, and so was the British Empire."[28]\n\nIn 1986 Conquest published The Harvest of Sorrow: Soviet Collectivisation and the Terror-Famine, dealing with the collectivization of agriculture in Ukraine and elsewhere in the USSR, under Stalin\'s direction in 1929–31, and the resulting famine, in which millions of peasants died due to starvation, deportation to labor camps, and execution. In this book, Conquest supported the view that the famine was a planned act of genocide.[10] According to historians Stephen Wheatcroft and R. W. Davies, "Conquest holds that Stalin wanted the famine... and that the Ukrainian famine was deliberately inflicted for its own sake." Nevertheless, he wrote to them in a letter in 2003 that "Stalin purposely inflicted the 1933 famine? No. What I argue is that with resulting famine imminent, he could have prevented it, but put \'Soviet interest\' other than feeding the starving first thus consciously abetting it."[29][30]\n\nFor the Trotskyists, Kirov\'s murder was the Stalinist equivalent of the Reichstag fire, deliberately started by the Nazis to justify the arrest of German Communists. The Trotskyist-Menshevik view became the dominant one among western historians, popularised in Robert Conquest\'s influential books.[31]\n\nIn The Great Terror, Conquest already undermined the official Soviet story of conspiracy and treason. Conquest placed the murder in 1934 of the Leningrad party boss, Sergei Kirov, one of Stalin\'s inner circle, as the key to the mechanism of terror.\n\nHe returned to this in Stalin and the Kirov Murder (1989), where he argued that Stalin not only sanctioned Kirov\'s assassination, but used it as a justification for the terror that culminated in 1937–38, though no evidence has been found to confirm Stalin\'s role in the murder.[14][32][33]\n\nIn addition to his scholarly work, Conquest was a well-regarded poet[34] whose poems have been published in various periodicals from 1937. In 1945 he was awarded the PEN Brazil Prize for his war poem "For the Death of a Poet" – about an army friend, the poet Drummond Allison, killed in Italy – and, in 1951, he received a Festival of Britain verse prize.[35] During his lifetime, he had seven volumes of poetry[36] and one of literary criticism[37] published.\n\nConquest was a major figure in a prominent British literary circle known as "The Movement" which also included Philip Larkin and Kingsley Amis. Movement poets, many of whom bristled at being so labeled, rejected the experiments of earlier practitioners such as Ezra Pound.[17]\n\nHe edited, in 1956 and 1962, the influential New Lines anthologies, introducing works by them, as well as Thom Gunn, Dennis Enright, and others, to a wider public.[38] He spent 1959–60 as visiting poet at the University of Buffalo. Several of his poems were published in The New Oxford Book of Light Verse (1978; compiled by Amis), under the pseudonyms "Stuart Howard-Jones", "Victor Gray" and "Ted Pauker".[citation needed]\n\nIt emerged from the pages of poet Philip Larkin\'s published letters that Conquest and Larkin shared an enthusiasm for pornography in the 1950s.[10] When Larkin was in Hull, Conquest sent him judicious selections of the latest pornography, and, when he came down to London, Conquest took him on shopping trips to the Soho porn shops.[14] On one occasion Conquest, in 1957, wrote a letter to Larkin purporting to come from the Vice Squad which had found the poet\'s name on a pornographic publisher\'s list. Larkin panicked and went to see his solicitor, convinced that he was going to lose his job as librarian at Hull University, before Conquest owned up.[10] The true story of the joke became in 2008, Mr Larkin\'s Awkward Day, a comedy radio play by Chris Harrald.[39]\n\nSoon after his expulsion from the Soviet Union, Aleksandr Solzhenitsyn met with Conquest, asking him to translate a \'little\' poem of his into English verse. This was "Prussian Nights" – nearly two thousand lines in ballad metre – published in 1977.[40]\n\nA new Collected Poems, edited by Elizabeth Conquest, was published in March 2020 by the Waywiser Press.[41]\n\nConquest had been a member of the British Interplanetary Society since the 1940s, and shared Amis\'s taste for science fiction. Starting from 1961, the two writers jointly edited Spectrum, five anthologies of new sci-fi writing.[14] Conquest also proposed to Amis a collaboration based on a draft comic novel that Conquest had completed. This was revised by Amis, then it appeared under both their names as The Egyptologists (1965).[14] The novel is about a secret Egyptological London society that is really a husbands\' organization serving as an alibi for philanderers.[17][42] A reviewer in The New York Times felt that their "elaborate little jokes leave an unpleasant taste".[14]\n\nLater a film version of the novel was cancelled when its star, Peter Sellers, was called away to Hollywood.[43] Conquest published a science-fiction novel, A World of Difference (1955).[1]\n\nIn 1984, Robert Conquest wrote, with Jon Manchip White, the fictional book What to Do When the Russians Come: a Survivor\'s Guide which, however, was intended to be a real survival manual in case of Soviet invasion. This book, as many other works of the mid-1980s in different media, like Sir John Hackett\'s The Third World War, the movie Red Dawn, and the Milton Bradley game Fortress America, starts from the premise that a Soviet ground-invasion of the United States could be imminent and that the Soviet Union was about to engulf the world.\n\nIt is widely accepted that the United States now faces a real possibility of succumbing to the power of an alien regime unless the right policies are pursued. [This book\'s aim] is, first, to show the American citizen clearly and factually what the results of this possible Soviet domination could be and how it would affect him or her personally; and second, to give some serious advice on how to survive."[44]\n\nConquest supported the Reagan defense buildup and asked for an increase of expenses on US defense budget, claiming that in the nuclear field NATO was only possibly matching USSR military power:\n\nWe live in dangerous times. Such miscalculations are very possible. But they are not inevitable. The American people and their representatives have it in their power to prevent their country from undergoing the ordeal we have described. A democratic government, with all its distractions and disadvantages, ... It is not infallible, it is slow to learn, and it is willing to grasp at comfortable illusions; but it may yet act decisively"[45]\n"But why should we fear that such an ordeal may face us? The economic potential of the West in gross national product is far greater than that of the Soviet Union....In fact, the Soviet Union is economically far behind the United States. American technology is always a generation ahead of theirs. They have to turn to the United States for wheat. The Soviet economy is at a dead end. The Communist system has failed to win support in any of the countries of Eastern Europe. The Soviet idea has no attractions. On any calculation—of economic power or social advance or intellectual progress there could be no question of the Russians imposing their will. But in terms of actual military power, the West\'s advantage does not seem to have been made use of. It is at least matched, and many would say overmatched, in the nuclear field; the Western forces in Europe have less than half the striking power of their opponents. It is no good our being more advanced than they are if this is not translated into power—both military power and political willpower."[46]\n\nIn 1986 Conquest affirmed that "a science-fiction attitude is a great help in understanding the Soviet Union. It isn\'t so much whether they\'re good or bad, exactly; they\'re not bad or good as we\'d be bad or good. It\'s far better to look at them as Martians than as people like us."[42]\n\nReflections on a Ravaged Century is a book devoted to the psychological roots of fanaticism, in which Conquest argues that Communism and Nazism were equal and more twins than opposites.[47]\n\nThere is much more in this book about communism than Nazism, partly because of Conquest\'s greater expertise on communism, and partly because comparatively few Western intellectuals became Nazis. He focuses mainly on attacks on intellectuals in the West who became communists because they felt or believed that this was "anti-fascism" or "anti-Nazism".[47]\n\nConquest posited two laws of politics, apparently not referenced in any of his books but as observations he made in conversations:[48]\n\nConquest\'s first and second law are attested by at least two sources.[48] On 14 February 2003, Andrew Brown wrote of Conquest\'s campaign against the expansion of university education that "[f]rom this period dates \'Conquest\'s Law\', which states that \'Everyone is a reactionary about subjects he understands\'. This was later supplemented with the balancing rule that every organisation behaves as if it is run by secret agents of its opponents."[6] In his 1991 Memoirs, Kingsley Amis wrote of Conquest that "he was to point out that, while very \'progressive\' on the subject of colonialism and other matters I was ignorant of, I was a sound reactionary about education, of which I had some understanding and experience. From my own and others\' example he formulated his famous First Law, which runs, \'Generally speaking, everybody is reactionary on subjects he knows about.\' (The Second Law, more recent, says, \'Every organisation appears to be headed by secret agents of its opponents.\')"[49]\n\nOn 25 June 2003, John Derbyshire wrote in the National Review Online\'s blog The Corner that "[a]s best I can remember", Conquest conjectured three laws of politics:[50]\n\nDerbyshire commented: "Of the Second Law, Conquest gave the Church of England and Amnesty International as examples. Of the third, he noted that a bureaucracy sometimes actually IS controlled by a secret cabal of its enemies – e.g. the postwar British secret service." For these statements, Conquest would become well known among certain thinkers, especially online conservatives; however, Derbyshire cited no source for them and implied his memory was not certain on the matter. Indeed, the second law given here is O\'Sullivan\'s first law, which was stated by John O\'Sullivan in his article "O\'Sullivan\'s First Law" in the 27 October 1989 print issue of the National Review, in which he also references Derbyshire\'s Conquest\'s third law as Conquest\'s second law:\n\nThat is explained by O\'Sullivan\'s First Law: All organizations that are not actually right-wing will over time become left-wing. I cite as supporting evidence the ACLU, the Ford Foundation, and the Episcopal Church. The reason is, of course, that people who staff such bodies tend to be the sort who don\'t like private profit, business, making money, the current organization of society, and, by extension, the Western world. At which point Michels\'s Iron Law of Oligarchy takes over—and the rest follows.\n\nIs there any law which enables us to predict the behavior of right-wing organizations? As it happens, there is: Conquest\'s Second Law (formulated by the Sovietologist Robert Conquest):\n\nThe behavior of an organization can best be predicted by assuming it to be controlled by a secret cabal of its enemies. Examples: virtually any conservative party anywhere, the Ronald Lauder for Mayor campaign, and the British secret service. That last example is, however, flawed, since the British secret service actually was controlled by a secret cabal of its enemies in the form of Kim Philby, Anthony Blunt, et al. In which case, Conquest\'s Law should have operated to make M1-6 [sic] a crack anti-Soviet intelligence service of James Bond proportions. But these are deep waters.[51]\n\nConquest was married four times, first in 1942 to Joan Watkins, with whom he had two sons. They divorced in 1948.[10] There followed a marriage to Tatiana Mihailova (1948–1962),[10] whom he had helped escape from Bulgaria.[1] She was diagnosed with schizophrenia in 1951. In 1962 he married Caroleen MacFarlane; they divorced in 1978.[10] That year he began dating Elizabeth Neece Wingate, a lecturer in English and the daughter of a United States Air Force colonel. He and Wingate married in 1979. When he died in 2015, he had several grandchildren from his sons and stepdaughter.[1][6]\n\nIn 1981 Conquest moved to California to take up a post as Senior Research Fellow and Scholar-Curator of the Russian and Commonwealth of Independent States Collection at Stanford University\'s Hoover Institution, where he remained a Fellow.[10] In 1985 he signed a petition in support of the anti-Communist Contras (Nicaragua).[52] He was a fellow of the Columbia University\'s Russian Institute, and of the Woodrow Wilson International Center for Scholars; a distinguished visiting scholar at The Heritage Foundation; a research associate of Harvard University\'s Ukrainian Research Institute.[1] In 1990 he presented Red Empire, a seven-part mini-series on the Soviet Union produced by Yorkshire Television.[53]\n\nConquest died in 2015 in Stanford, California, at the age of 98, of respiratory failure as a result of Parkinson\'s disease.[1][17]\n\nConquest was a Fellow of the British Academy, the American Academy of Arts and Sciences, the Royal Society of Literature, and the British Interplanetary Society, and a Member of the Society for the Promotion of Roman Studies.[10]',
        pageTitle: "Robert Conquest",
    },
    {
        title: "Conway's law",
        link: "https://en.wikipedia.org/wiki/Conway%27s_law",
        content:
            "Conway's law describes the link between communication structure of organizations and the systems they design. It is named after the computer scientist and programmer Melvin Conway, who introduced the idea in 1967.[1] His original wording was:[2][3]\n\n[O]rganizations which design systems (in the broad sense used here) are constrained to produce designs which are copies of the communication structures of these organizations.\n\nThe law is based on the reasoning that in order for a product to function, the authors and designers of its component parts must communicate with each other in order to ensure compatibility between the components. Therefore, the technical structure of a system will reflect the social boundaries of the organizations that produced it, across which communication is more difficult. In colloquial terms, it means complex products end up \"shaped like\" the organizational structure they are designed in or designed for. The law is applied primarily in the field of software architecture, though Conway directed it more broadly and its assumptions and conclusions apply to most technical fields.\n\nEric S. Raymond, an open-source advocate, restated Conway's law in The New Hacker's Dictionary, a reference work based on the Jargon File. The organization of the software and the organization of the software team will be congruent, he said. Summarizing an example in Conway's paper, Raymond wrote:\n\nIf you have four groups working on a compiler, you'll get a 4-pass compiler.[4][5]\n\nRaymond further presents Tom Cheatham's amendment of Conway's Law, stated as:\n\nIf a group of N persons implements a COBOL compiler, there will be N−1 passes. Someone in the group has to be the manager.[4]\n\nYourdon and Constantine, in their 1979 book on Structured Design, gave a more strongly stated variation of Conway's Law:\n\nThe structure of any system designed by an organization is isomorphic to the structure of the organization.[6]\n\nJames O. Coplien and Neil B. Harrison stated in a 2004 book concerned with organizational patterns of Agile software development:\n\nIf the parts of an organization (e.g., teams, departments, or subdivisions) do not closely reflect the essential parts of the product, or if the relationships between organizations do not reflect the relationships between product parts, then the project will be in trouble ... Therefore: Make sure the organization is compatible with the product architecture.[7]\n\nMore recent commentators have noted a corollary - for software projects with a long lifetime of code reuse, such as Microsoft Windows, the structure of the code mirrors not only the communication structure of the organization which created the most recent release, but also the communication structures of every previous team which worked on that code.[8]\n\nYou can see the organization chart of a car company in the dashboard, and also see whether the steering wheel team hates the gear stick team.\n\nThe law is, in a strict sense, only about correspondence; it does not state that communication structure is the cause of system structure, merely describes the connection. Different commentators have taken various positions on the direction of causality; that technical design causes the organization to restructure to fit,[10] that the organizational structure dictates the technical design,[11] or both.[12][13][14] Conway's law was intended originally as a sociological observation[citation needed], but many other interpretations are possible. The New Hacker's Dictionary entry uses it in a primarily humorous context,[15] while participants at the 1968 National Symposium on Modular Programming considered it sufficiently serious and universal to dub it 'Conway's Law'.[6] Opinions also vary on the desirability of the phenomenon; some say that the mirroring pattern is a helpful feature of such systems, while other interpretations say it's an undesirable result of organizational bias.[citation needed] Middle positions describe it as a necessary feature of compromise, undesirable in the abstract but necessary to handle human limitations.[8]\n\nAn example of the impact of Conway's Law can be found in the design of some organization websites. Nigel Bevan stated in a 1997 paper, regarding usability issues in websites: \"Organizations often produce web sites with a content and structure which mirrors the internal concerns of the organization rather than the needs of the users of the site.\"[16]\n\nEvidence in support of Conway's law has been published by a team of Massachusetts Institute of Technology (MIT) and Harvard Business School researchers who, using \"the mirroring hypothesis\" as an equivalent term for Conway's law, found \"strong evidence to support the mirroring hypothesis\", and that the \"product developed by the loosely-coupled organization is significantly more modular than the product from the tightly-coupled organization\". The authors highlight the impact of \"organizational design decisions on the technical structure of the artifacts that these organizations subsequently develop\".[17]\n\nAdditional and likewise supportive case studies of Conway's law have been conducted by Nagappan, Murphy and Basili at the University of Maryland in collaboration with Microsoft,[18] and by Syeed and Hammouda at Tampere University of Technology in Finland.[19]",
        pageTitle: "Conway's law",
    },
    {
        title: "Cooper's law",
        link: "https://en.wikipedia.org/wiki/Martin_Cooper_(inventor)#Cooper's_law",
        content:
            'Martin Cooper (born December 26, 1928) is an American engineer. He is a pioneer in the wireless communications industry, especially in radio spectrum management, with eleven patents in the field.[2][3]\n\nOn April 3, 1973, Cooper placed the first public call from a handheld portable cell phone while working at Motorola, from a Manhattan sidewalk to his counterpart at competitor Bell Labs.[4][5] Cooper reprised the first handheld cellular mobile phone (distinct from the car phone) in 1973 and led the team that redeveloped it and brought it to market in 1983.[6][7] He is considered the "father of the (handheld) cell phone".[2][6][8][9]\n\nCooper is co-founder of numerous communications companies with his wife and business partner Arlene Harris;[10] He is co-founder and current Chairman of Dyna LLC, in Del Mar, California. Cooper also sits on committees supporting the U.S. Federal Communications Commission[11] and the United States Department of Commerce.\n\nIn 2010, Cooper was elected a member of the National Academy of Engineering for leadership in the creation and deployment of the cellular portable hand-held telephone.\n\nCooper was born in Chicago to Ukrainian Jewish immigrants.[12][13][14] He graduated from Illinois Institute of Technology (IIT) in 1950 and served as a submarine officer during the Korean War.[2]\n\nIn 1957, he earned his master\'s degree from IIT in electrical engineering and in 2004 received an honorary doctorate degree from IIT. He serves on the university\'s board of trustees.\n\nCooper left his first job at Teletype Corporation in Chicago in 1954 and joined Motorola, Inc. (Schaumburg, Illinois) as a senior development engineer in the mobile equipment group. He developed products including the first cellular-like portable handheld police radio system, produced for the Chicago police department in 1967.[15][16]\n\nBy the early 1970s, Cooper headed Motorola\'s communications systems division.[4] Here he conceived of the first portable cellular phone in 1973 and led the 10-year process of bringing it to market.[8] Car phones had been in limited use in large U.S. cities since the 1930s but Cooper championed cellular telephony for more general personal, portable communications.[17] He believed the cellular phone should be a "personal telephone – something that would represent an individual so you could assign a number; not to a place, not to a desk, not to a home, but to a person."[4]\n\nTop management at Motorola supported Cooper\'s mobile phone concept, investing $100 million between 1973 and 1993 before any revenues were realized.[18] Cooper assembled a team that designed and assembled a product in less than 90 days. That original handset, called the DynaTAC 8000x (DYNamic Adaptive Total Area Coverage) weighed 2.5 pounds (1.1 kg), measured 10 inches (25 cm) long and was dubbed "the brick" or "the shoe" phone.[19] A very substantial part of the DynaTAC was the battery, which weighed four to five times more than a modern cell phone.[7] The phone had only 30 minutes of talk time before requiring a 10-hour recharge but according to Cooper, "The battery lifetime wasn\'t really a problem because you couldn\'t hold that phone up for that long!" By 1983 and after four iterations, the handset was reduced to half its original weight.\n\nCooper is the lead inventor named on "radio telephone system" filed on October 17, 1973, with the U.S. Patent Office and later issued as U.S. Patent 3,906,166.[20] John Francis Mitchell, Motorola\'s Chief of Portable Communication Products (and Cooper\'s Manager and Mentor) and the engineers who worked for Cooper and Mitchell are also named on the patent.\n\nOn April 3, 1973, Cooper and Mitchell demonstrated two working phones to the media and to passers-by prior to walking into a scheduled press conference at the New York City Hilton in midtown Manhattan. Standing on Sixth avenue near the Hilton, Cooper made the first handheld cellular phone call in public from the prototype DynaTAC. The call connected him to a base station Motorola had installed on the roof of the Burlington House (now the AllianceBernstein Building) and into the AT&T land-line telephone system.[15] Reporters and onlookers watched as Cooper dialed the number of his chief competitor Dr. Joel S. Engel at AT&T.[21] "Joel, this is Marty. I\'m calling you from a cell phone, a real handheld portable cell phone."[22] That public demonstration landed the DynaTAC on the July 1973 cover of Popular Science Magazine.[15] As Cooper recalls from the experience:  "I made numerous calls, including one where I crossed the street while talking to a New York radio reporter – probably one of the most dangerous things I have ever done in my life."\n\nThat first cell phone began a fundamental technology and communications market shift to making phone calls to a person instead of to a place.[6][19] Bell Labs had introduced the idea of cellular communications in 1947, but their first systems were limited to car phones which required roughly 30 pounds (12 kg) of equipment in the trunk.[21] Motorola gained Federal Communications Commission (FCC) approval for cellular licenses to be assigned to competing entities and prevented an AT&T monopoly on cellular service.[15]\n\nCooper worked at Motorola for 29 years; building and managing both its paging and cellular businesses. He also led the creation of trunked mobile radio, quartz crystal oscillators, liquid crystal displays, piezo-electric components, Motorola A.M. stereo technology and various mobile and portable two-way radio product lines.\n\nCooper rose to Vice-President and Corporate Director of Research and Development at Motorola.[2] In addition to his work on the mobile cellular phone, he was instrumental in expanding the technology of pagers from use within a single building to use across multiple cities.[8] Cooper also worked with inventor Clifford L. Rose to fix a flaw in quartz crystals used in Motorola\'s radios which encouraged the company to mass-produce the first crystals used in wrist watches.[8]\n\nCooper and his wife Arlene Harris founded Dyna LLC in 1986 as a home base for their developmental and support activities for the new companies, Subscriber Computing Inc., Cellular Pay Phone, Inc. (CPPI), SOS Wireless Communications and Accessible Wireless; the later two of which together created the underpinning for the creation of GreatCall, were all launched from Dyna LLC.\n\nFrom his Dyna headquarters Cooper continues to write and lecture about wireless communications, technological innovation, the Internet and R&D management. He serves on industry, civic and national governmental groups including the U.S. Department of Commerce Spectrum Advisory Committee that advises the Secretary of Commerce of the United States on spectrum policy and the Federal Communications Commission\'s (FCC) Technological Advisory Council.\n\nIn 1986 Cooper co-founded Cellular Payphone Inc. (CPPI), the parent company of GreatCall, Inc., Innovator of the Jitterbug cell phone (in partnership with Samsung).[23] GreatCall is the first complete end-to-end value-added service provider in the cellular industry to focus on simplicity with its primary emphasis on senior citizens.\n\nIn 1992 Cooper co-founded Arraycomm a developer of software for mobile antenna technologies. Under his leadership, the Company grew from a seed-funded startup in San Jose, California, into the world leader in smart antenna technology with 400 patents issued or pending, worldwide.[24]\n\nCooper joined the board of directors from 2015 to 2019.\n\nCooper found that the ability to transmit different radio communications simultaneously and in the same place has grown at the same pace since Guglielmo Marconi\'s first transmissions in 1895. This led Cooper to formulate the Law of Spectral Efficiency, otherwise known as Cooper\'s Law. The law states that the maximum number of voice conversations or equivalent data transactions that can be conducted in all of the useful radio spectrum over a given area doubles every 30 months.[8][24]\n\n"The Myth of Spectrum Scarcity" Position Paper, March 2010.\n\n"Mobile WiMax – Fourth-Generation Wireless," Bechtel Communications Technical Journal, September 2007.\n\n"The Need for Simplicity," in the anthology "Mobile Persuasion: 20 Perspectives on the Future of Behavior Change," published by Stanford University in 2007.\n\n"Personal Communications in 2025" for Eta Kappa Nu Electrical and Computer Engineering Honor Society, Autumn 2005.\n\n"Antennas Get Smart" in Scientific American, July 2003.\n\n"Everyone is Wrong" in Technology Review, June 2001.[25]\n\nMedia related to Martin Cooper at Wikimedia Commons',
        pageTitle: "Martin Cooper (inventor)",
    },
    {
        title: "Coulomb's law",
        link: "https://en.wikipedia.org/wiki/Coulomb%27s_law",
        content:
            "Coulomb's inverse-square law, or simply Coulomb's law, is an experimental law[1] of physics that calculates the amount of force between two electrically charged particles at rest. This electric force is conventionally called the electrostatic force or Coulomb force.[2] Although the law was known earlier, it was first published in 1785 by French physicist Charles-Augustin de Coulomb. Coulomb's law was essential to the development of the theory of electromagnetism and maybe even its starting point,[1] as it allowed meaningful discussions of the amount of electric charge in a particle.[3]\n\nThe law states that the magnitude, or absolute value, of the attractive or repulsive electrostatic force between two point charges is directly proportional to the product of the magnitudes of their charges and inversely proportional to the square of the distance between them.[4] Coulomb discovered that bodies with like electrical charges repel:\n\nIt follows therefore from these three tests, that the repulsive force that the two balls – [that were] electrified with the same kind of electricity – exert on each other, follows the inverse proportion of the square of the distance.[5]\n\nCoulomb also showed that oppositely charged bodies attract according to an inverse-square law:\n\n  \n    \n      \n        \n          |\n        \n        F\n        \n          |\n        \n        =\n        \n          k\n          \n            e\n          \n        \n        \n          \n            \n              \n                |\n              \n              \n                q\n                \n                  1\n                \n              \n              \n                |\n              \n              \n                |\n              \n              \n                q\n                \n                  2\n                \n              \n              \n                |\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle |F|=k_{\\text{e}}{\\frac {|q_{1}||q_{2}|}{r^{2}}}}\n\nHere, ke is a constant, q1 and q2 are the quantities of each charge, and the scalar r is the distance between the charges.\n\nThe force is along the straight line joining the two charges. If the charges have the same sign, the electrostatic force between them makes them repel; if they have different signs, the force between them makes them attract.\n\nBeing an inverse-square law, the law is similar to Isaac Newton's inverse-square law of universal gravitation, but gravitational forces always make things attract, while electrostatic forces make charges attract or repel. Also, gravitational forces are much weaker than electrostatic forces.[2] Coulomb's law can be used to derive Gauss's law, and vice versa. In the case of a single point charge at rest, the two laws are equivalent, expressing the same physical law in different ways.[6] The law has been tested extensively, and observations have upheld the law on the scale from 10−16 m to 108 m.[6]\n\nAncient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers and pieces of paper. Thales of Miletus made the first recorded description of static electricity around 600 BC,[7] when he noticed that friction could make a piece of amber attract small objects.[8][9]\n\nIn 1600, English scientist William Gilbert made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber.[8] He coined the Neo-Latin word electricus (\"of amber\" or \"like amber\", from ἤλεκτρον [elektron], the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed.[10] This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.[11]\n\nEarly investigators of the 18th century who suspected that the electrical force diminished with distance as the force of gravity did (i.e., as the inverse square of the distance) included Daniel Bernoulli[12] and Alessandro Volta, both of whom measured the force between plates of a capacitor, and Franz Aepinus who supposed the inverse-square law in 1758.[13]\n\nBased on experiments with electrically charged spheres, Joseph Priestley of England was among the first to propose that electrical force followed an inverse-square law, similar to Newton's law of universal gravitation. However, he did not generalize or elaborate on this.[14] In 1767, he conjectured that the force between charges varied as the inverse square of the distance.[15][16]\n\nIn 1769, Scottish physicist John Robison announced that, according to his measurements, the force of repulsion between two spheres with charges of the same sign varied as x−2.06.[17]\n\nIn the early 1770s, the dependence of the force between charged bodies upon both distance and charge had already been discovered, but not published, by Henry Cavendish of England.[18] In his notes, Cavendish wrote, \"We may therefore conclude that the electric attraction and repulsion must be inversely as some power of the distance between that of the 2 + .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠1/50⁠th and that of the 2 − ⁠1/50⁠th, and there is no reason to think that it differs at all from the inverse duplicate ratio\".\n\nFinally, in 1785, the French physicist Charles-Augustin de Coulomb published his first three reports of electricity and magnetism where he stated his law. This publication was essential to the development of the theory of electromagnetism.[4] He used a torsion balance to study the repulsion and attraction forces of charged particles, and determined that the magnitude of the electric force between two point charges is directly proportional to the product of the charges and inversely proportional to the square of the distance between them.\n\nThe torsion balance consists of a bar suspended from its middle by a thin fiber. The fiber acts as a very weak torsion spring. In Coulomb's experiment, the torsion balance was an insulating rod with a metal-coated ball attached to one end, suspended by a silk thread. The ball was charged with a known charge of static electricity, and a second charged ball of the same polarity was brought near it. The two charged balls repelled one another, twisting the fiber through a certain angle, which could be read from a scale on the instrument. By knowing how much force it took to twist the fiber through a given angle, Coulomb was able to calculate the force between the balls and derive his inverse-square proportionality law.\n\nCoulomb's law states that the electrostatic force \n  \n    \n      \n        \n          \n            F\n          \n          \n            1\n          \n        \n      \n    \n    {\\textstyle \\mathbf {F} _{1}}\n  \n experienced by a charge, \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n at position \n  \n    \n      \n        \n          \n            r\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{1}}\n  \n, in the vicinity of another charge, \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n at position \n  \n    \n      \n        \n          \n            r\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{2}}\n  \n, in a vacuum is equal to[19]\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            1\n          \n        \n        =\n        \n          \n            \n              \n                q\n                \n                  1\n                \n              \n              \n                q\n                \n                  2\n                \n              \n            \n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                      r\n                    \n                    ^\n                  \n                \n              \n              \n                12\n              \n            \n            \n              \n                \n                  |\n                \n                \n                  \n                    r\n                  \n                  \n                    12\n                  \n                \n                \n                  |\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} _{1}={\\frac {q_{1}q_{2}}{4\\pi \\varepsilon _{0}}}{{\\hat {\\mathbf {r} }}_{12} \\over {|\\mathbf {r} _{12}|}^{2}}}\n\nwhere \n  \n    \n      \n        \n          \n            r\n            \n              12\n            \n          \n          =\n          \n            r\n            \n              1\n            \n          \n          −\n          \n            r\n            \n              2\n            \n          \n        \n      \n    \n    {\\textstyle \\mathbf {r_{12}=r_{1}-r_{2}} }\n  \n is the displacement vector between the charges, \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            12\n          \n        \n      \n    \n    {\\textstyle {\\hat {\\mathbf {r} }}_{12}}\n  \n a unit vector pointing from \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\textstyle q_{2}}\n  \n to \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\textstyle q_{1}}\n  \n, and \n  \n    \n      \n        \n          ε\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n the electric constant. Here, \n  \n    \n      \n        \n          \n            \n              \n                r\n                ^\n              \n            \n          \n          \n            12\n          \n        \n      \n    \n    {\\textstyle \\mathbf {\\hat {r}} _{12}}\n  \n is used for the vector notation. The electrostatic force \n  \n    \n      \n        \n          \n            F\n          \n          \n            2\n          \n        \n      \n    \n    {\\textstyle \\mathbf {F} _{2}}\n  \n experienced by \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n, according to Newton's third law, is \n  \n    \n      \n        \n          \n            F\n          \n          \n            2\n          \n        \n        =\n        −\n        \n          \n            F\n          \n          \n            1\n          \n        \n      \n    \n    {\\textstyle \\mathbf {F} _{2}=-\\mathbf {F} _{1}}\n  \n.\n\nIf both charges have the same sign (like charges) then the product \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{1}q_{2}}\n  \n is positive and the direction of the force on \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n is given by \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            12\n          \n        \n      \n    \n    {\\textstyle {\\widehat {\\mathbf {r} }}_{12}}\n  \n; the charges repel each other. If the charges have opposite signs then the product \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{1}q_{2}}\n  \n is negative and the direction of the force on \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n is \n  \n    \n      \n        −\n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            12\n          \n        \n      \n    \n    {\\textstyle -{\\hat {\\mathbf {r} }}_{12}}\n  \n; the charges attract each other.[20]\n\nThe law of superposition allows Coulomb's law to be extended to include any number of point charges. The force acting on a point charge due to a system of point charges is simply the vector addition of the individual forces acting alone on that point charge due to each one of the charges. The resulting force vector is parallel to the electric field vector at that point, with that point charge removed.\n\nForce \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\textstyle \\mathbf {F} }\n  \n on a small charge \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n at position \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n, due to a system of \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n  \n discrete charges in vacuum is[19]\n\nF\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          q\n          \n            i\n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                      r\n                    \n                    ^\n                  \n                \n              \n              \n                i\n              \n            \n            \n              \n                \n                  |\n                \n                \n                  \n                    r\n                  \n                  \n                    i\n                  \n                \n                \n                  |\n                \n              \n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} (\\mathbf {r} )={q \\over 4\\pi \\varepsilon _{0}}\\sum _{i=1}^{n}q_{i}{{\\hat {\\mathbf {r} }}_{i} \\over {|\\mathbf {r} _{i}|}^{2}},}\n\nwhere \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n is the magnitude of the ith charge, \n  \n    \n      \n        \n          \n            r\n          \n          \n            i\n          \n        \n      \n    \n    {\\textstyle \\mathbf {r} _{i}}\n  \n is the vector from its position to \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\textstyle {\\hat {\\mathbf {r} }}_{i}}\n  \n is the unit vector in the direction of \n  \n    \n      \n        \n          \n            r\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{i}}\n  \n.\n\nIn this case, the principle of linear superposition is also used. For a continuous charge distribution, an integral over the region containing the charge is equivalent to an infinite summation, treating each infinitesimal element of space as a point charge \n  \n    \n      \n        d\n        q\n      \n    \n    {\\displaystyle dq}\n  \n. The distribution of charge is usually linear, surface or volumetric.\n\nFor a linear charge distribution (a good approximation for charge in a wire) where \n  \n    \n      \n        λ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\lambda (\\mathbf {r} ')}\n  \n gives the charge per unit length at position \n  \n    \n      \n        \n          \n            r\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mathbf {r} '}\n  \n, and \n  \n    \n      \n        d\n        \n          ℓ\n          ′\n        \n      \n    \n    {\\displaystyle d\\ell '}\n  \n is an infinitesimal element of length,[21]\n\n  \n    \n      \n        d\n        \n          q\n          ′\n        \n        =\n        λ\n        (\n        \n          \n            r\n            ′\n          \n        \n        )\n        \n        d\n        \n          ℓ\n          ′\n        \n        .\n      \n    \n    {\\displaystyle dq'=\\lambda (\\mathbf {r'} )\\,d\\ell '.}\n\nFor a surface charge distribution (a good approximation for charge on a plate in a parallel plate capacitor) where \n  \n    \n      \n        σ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\sigma (\\mathbf {r} ')}\n  \n gives the charge per unit area at position \n  \n    \n      \n        \n          \n            r\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mathbf {r} '}\n  \n, and \n  \n    \n      \n        d\n        \n          A\n          ′\n        \n      \n    \n    {\\displaystyle dA'}\n  \n is an infinitesimal element of area,\n\n  \n    \n      \n        d\n        \n          q\n          ′\n        \n        =\n        σ\n        (\n        \n          \n            r\n            ′\n          \n        \n        )\n        \n        d\n        \n          A\n          ′\n        \n        .\n      \n    \n    {\\displaystyle dq'=\\sigma (\\mathbf {r'} )\\,dA'.}\n\nFor a volume charge distribution (such as charge within a bulk metal) where \n  \n    \n      \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\rho (\\mathbf {r} ')}\n  \n gives the charge per unit volume at position \n  \n    \n      \n        \n          \n            r\n          \n          ′\n        \n      \n    \n    {\\displaystyle \\mathbf {r} '}\n  \n, and \n  \n    \n      \n        d\n        \n          V\n          ′\n        \n      \n    \n    {\\displaystyle dV'}\n  \n is an infinitesimal element of volume,[20]\n\n  \n    \n      \n        d\n        \n          q\n          ′\n        \n        =\n        ρ\n        (\n        \n          \n            r\n            ′\n          \n        \n        )\n        \n        d\n        \n          V\n          ′\n        \n        .\n      \n    \n    {\\displaystyle dq'=\\rho ({\\boldsymbol {r'}})\\,dV'.}\n\nThe force on a small test charge \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n at position \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {r}}}\n  \n in vacuum is given by the integral over the distribution of charge\n\n  \n    \n      \n        \n          F\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        ∫\n        d\n        \n          q\n          ′\n        \n        \n          \n            \n              \n                r\n              \n              −\n              \n                \n                  r\n                  ′\n                \n              \n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                \n                  r\n                  ′\n                \n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} (\\mathbf {r} )={\\frac {q}{4\\pi \\varepsilon _{0}}}\\int dq'{\\frac {\\mathbf {r} -\\mathbf {r'} }{|\\mathbf {r} -\\mathbf {r'} |^{3}}}.}\n\nThe \"continuous charge\" version of Coulomb's law is never supposed to be applied to locations for which \n  \n    \n      \n        \n          |\n        \n        \n          r\n        \n        −\n        \n          \n            r\n            ′\n          \n        \n        \n          |\n        \n        =\n        0\n      \n    \n    {\\displaystyle |\\mathbf {r} -\\mathbf {r'} |=0}\n  \n because that location would directly overlap with the location of a charged particle (e.g. electron or proton) which is not a valid location to analyze the electric field or potential classically. Charge is always discrete in reality, and the \"continuous charge\" assumption is just an approximation that is not supposed to allow \n  \n    \n      \n        \n          |\n        \n        \n          r\n        \n        −\n        \n          \n            r\n            ′\n          \n        \n        \n          |\n        \n        =\n        0\n      \n    \n    {\\displaystyle |\\mathbf {r} -\\mathbf {r'} |=0}\n  \n to be analyzed.\n\nThe constant of proportionality, \n  \n    \n      \n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{4\\pi \\varepsilon _{0}}}}\n  \n, in Coulomb's law:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            1\n          \n        \n        =\n        \n          \n            \n              \n                q\n                \n                  1\n                \n              \n              \n                q\n                \n                  2\n                \n              \n            \n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                      r\n                    \n                    ^\n                  \n                \n              \n              \n                12\n              \n            \n            \n              \n                \n                  |\n                \n                \n                  \n                    r\n                  \n                  \n                    12\n                  \n                \n                \n                  |\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} _{1}={\\frac {q_{1}q_{2}}{4\\pi \\varepsilon _{0}}}{{\\hat {\\mathbf {r} }}_{12} \\over {|\\mathbf {r} _{12}|}^{2}}}\n  \n\nis a consequence of historical choices for units.[19]: 4–2\n\nThe constant \n  \n    \n      \n        \n          ε\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n is the vacuum electric permittivity.[22] Using the CODATA 2022 recommended value for \n  \n    \n      \n        \n          ε\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n,[23] the Coulomb constant[24] is  \n\n  \n    \n      \n        \n          k\n          \n            e\n          \n        \n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        =\n        8.987\n         \n        551\n         \n        7862\n        (\n        14\n        )\n        ×\n        \n          10\n          \n            9\n          \n        \n         \n        \n          N\n          \n            ⋅\n          \n          \n            m\n            \n              2\n            \n          \n          \n            ⋅\n          \n          \n            C\n            \n              −\n              2\n            \n          \n        \n      \n    \n    {\\displaystyle k_{\\text{e}}={\\frac {1}{4\\pi \\varepsilon _{0}}}=8.987\\ 551\\ 7862(14)\\times 10^{9}\\ \\mathrm {N{\\cdot }m^{2}{\\cdot }C^{-2}} }\n\nThere are three conditions to be fulfilled for the validity of Coulomb's inverse square law:[25]\n\nThe last of these is known as the electrostatic approximation. When movement takes place, an extra factor is introduced, which alters the force produced on the two objects. This extra part of the force is called the magnetic force. For slow movement, the magnetic force is minimal and Coulomb's law can still be considered approximately correct. A more accurate approximation in this case is, however, the Weber force. When the charges are moving more quickly in relation to each other or accelerations occur, Maxwell's equations and Einstein's theory of relativity must be taken into consideration.\n\nAn electric field is a vector field that associates to each point in space the Coulomb force experienced by a unit test charge.[19] The strength and direction of the Coulomb force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\textstyle \\mathbf {F} }\n  \n on a charge \n  \n    \n      \n        \n          q\n          \n            t\n          \n        \n      \n    \n    {\\textstyle q_{t}}\n  \n depends on the electric field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\textstyle \\mathbf {E} }\n  \n established by other charges that it finds itself in, such that \n  \n    \n      \n        \n          F\n        \n        =\n        \n          q\n          \n            t\n          \n        \n        \n          E\n        \n      \n    \n    {\\textstyle \\mathbf {F} =q_{t}\\mathbf {E} }\n  \n. In the simplest case, the field is considered to be generated solely by a single source point charge. More generally, the field can be generated by a distribution of charges who contribute to the overall by the principle of superposition.\n\nIf the field is generated by a positive source point charge \n  \n    \n      \n        q\n      \n    \n    {\\textstyle q}\n  \n, the direction of the electric field points along lines directed radially outwards from it, i.e. in the direction that a positive point test charge \n  \n    \n      \n        \n          q\n          \n            t\n          \n        \n      \n    \n    {\\textstyle q_{t}}\n  \n would move if placed in the field. For a negative point source charge, the direction is radially inwards.\n\nThe magnitude of the electric field E can be derived from Coulomb's law. By choosing one of the point charges to be the source, and the other to be the test charge, it follows from Coulomb's law that the magnitude of the electric field E created by a single source point charge Q at a certain distance from it r in vacuum is given by\n\n  \n    \n      \n        \n          |\n        \n        \n          E\n        \n        \n          |\n        \n        =\n        \n          k\n          \n            e\n          \n        \n        \n          \n            \n              \n                |\n              \n              q\n              \n                |\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle |\\mathbf {E} |=k_{\\text{e}}{\\frac {|q|}{r^{2}}}}\n\nA system of n discrete charges \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n stationed at \n  \n    \n      \n        \n          \n            r\n          \n          \n            i\n          \n        \n        =\n        \n          r\n        \n        −\n        \n          \n            r\n          \n          \n            i\n          \n        \n      \n    \n    {\\textstyle \\mathbf {r} _{i}=\\mathbf {r} -\\mathbf {r} _{i}}\n  \n produces an electric field whose magnitude and direction is, by superposition\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          q\n          \n            i\n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                      r\n                    \n                    ^\n                  \n                \n              \n              \n                i\n              \n            \n            \n              \n                \n                  |\n                \n                \n                  \n                    r\n                  \n                  \n                    i\n                  \n                \n                \n                  |\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={1 \\over 4\\pi \\varepsilon _{0}}\\sum _{i=1}^{n}q_{i}{{\\hat {\\mathbf {r} }}_{i} \\over {|\\mathbf {r} _{i}|}^{2}}}\n\nCoulomb's law holds even within atoms, correctly describing the force between the positively charged atomic nucleus and each of the negatively charged electrons. This simple law also correctly accounts for the forces that bind atoms together to form molecules and for the forces that bind atoms and molecules together to form solids and liquids. Generally, as the distance between ions increases, the force of attraction, and binding energy, approach zero and ionic bonding is less favorable. As the magnitude of opposing charges increases, energy increases and ionic bonding is more favorable.\n\n[citation needed]\nStrictly speaking, Gauss's law cannot be derived from Coulomb's law alone, since Coulomb's law gives the electric field due to an individual, electrostatic point charge only. However, Gauss's law can be proven from Coulomb's law if it is assumed, in addition, that the electric field obeys the superposition principle. The superposition principle states that the resulting field is the vector sum of fields generated by each particle (or the integral, if the charges are distributed smoothly in space).\n\nCoulomb's law states that the electric field due to a stationary point charge is:\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                e\n              \n              \n                r\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {q}{4\\pi \\varepsilon _{0}}}{\\frac {\\mathbf {e} _{r}}{r^{2}}}}\n  \n\nwhere\n\nUsing the expression from Coulomb's law, we get the total field at r by using an integral to sum the field at r due to the infinitesimal charge at each other point s in space, to give\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        ∫\n        \n          \n            \n              ρ\n              (\n              \n                s\n              \n              )\n              (\n              \n                r\n              \n              −\n              \n                s\n              \n              )\n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                s\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        \n        \n          \n            d\n          \n          \n            3\n          \n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int {\\frac {\\rho (\\mathbf {s} )(\\mathbf {r} -\\mathbf {s} )}{|\\mathbf {r} -\\mathbf {s} |^{3}}}\\,\\mathrm {d} ^{3}\\mathbf {s} }\n  \n\nwhere ρ is the charge density. If we take the divergence of both sides of this equation with respect to r, and use the known theorem[26]\n\n∇\n        ⋅\n        \n          (\n          \n            \n              \n                r\n              \n              \n                \n                  |\n                \n                \n                  r\n                \n                \n                  \n                    |\n                  \n                  \n                    3\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        4\n        π\n        δ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle \\nabla \\cdot \\left({\\frac {\\mathbf {r} }{|\\mathbf {r} |^{3}}}\\right)=4\\pi \\delta (\\mathbf {r} )}\n  \n\nwhere δ(r) is the Dirac delta function, the result is\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ∫\n        ρ\n        (\n        \n          s\n        \n        )\n        \n        δ\n        (\n        \n          r\n        \n        −\n        \n          s\n        \n        )\n        \n        \n          \n            d\n          \n          \n            3\n          \n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {1}{\\varepsilon _{0}}}\\int \\rho (\\mathbf {s} )\\,\\delta (\\mathbf {r} -\\mathbf {s} )\\,\\mathrm {d} ^{3}\\mathbf {s} }\n\nUsing the \"sifting property\" of the Dirac delta function, we arrive at\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              ρ\n              (\n              \n                r\n              \n              )\n            \n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {\\rho (\\mathbf {r} )}{\\varepsilon _{0}}},}\n  \n\nwhich is the differential form of Gauss's law, as desired.\n\nSince Coulomb's law only applies to stationary charges, there is no reason to expect Gauss's law to hold for moving charges based on this derivation alone. In fact, Gauss's law does hold for moving charges, and, in this respect, Gauss's law is more general than Coulomb's law.\n\nLet \n  \n    \n      \n        Ω\n        ⊆\n        \n          R\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\Omega \\subseteq R^{3}}\n  \n be a bounded open set, and  \n  \n    \n      \n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n        \n          \n            \n              \n                r\n              \n              −\n              \n                \n                  r\n                \n                ′\n              \n            \n            \n              \n                ‖\n                \n                  \n                    r\n                  \n                  −\n                  \n                    \n                      r\n                    \n                    ′\n                  \n                \n                ‖\n              \n              \n                3\n              \n            \n          \n        \n        \n          d\n        \n        \n          \n            r\n          \n          ′\n        \n        ≡\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{0}(\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }\\rho (\\mathbf {r} '){\\frac {\\mathbf {r} -\\mathbf {r} '}{\\left\\|\\mathbf {r} -\\mathbf {r} '\\right\\|^{3}}}\\mathrm {d} \\mathbf {r} '\\equiv {\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }e(\\mathbf {r,\\mathbf {r} '} ){\\mathrm {d} \\mathbf {r} '}}\n  \n be the electric field, with \n  \n    \n      \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\rho (\\mathbf {r} ')}\n  \n a continuous function (density of charge).\n\nIt is true for all \n  \n    \n      \n        \n          r\n        \n        ≠\n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} \\neq \\mathbf {r'} }\n  \n that \n  \n    \n      \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        \n          e\n        \n        (\n        \n          r\n          ,\n          \n            r\n            ′\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {r} }\\cdot \\mathbf {e} (\\mathbf {r,r'} )=0}\n  \n.\n\nConsider now a compact set \n  \n    \n      \n        V\n        ⊆\n        \n          R\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle V\\subseteq R^{3}}\n  \n having a piecewise smooth boundary \n  \n    \n      \n        ∂\n        V\n      \n    \n    {\\displaystyle \\partial V}\n  \n such that \n  \n    \n      \n        Ω\n        ∩\n        V\n        =\n        ∅\n      \n    \n    {\\displaystyle \\Omega \\cap V=\\emptyset }\n  \n. It follows that \n  \n    \n      \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        ∈\n        \n          C\n          \n            1\n          \n        \n        (\n        V\n        ×\n        Ω\n        )\n      \n    \n    {\\displaystyle e(\\mathbf {r,\\mathbf {r} '} )\\in C^{1}(V\\times \\Omega )}\n  \n and so, for the divergence theorem:\n\n∮\n          \n            ∂\n            V\n          \n        \n        \n          \n            E\n          \n          \n            0\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∫\n          \n            V\n          \n        \n        \n          ∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        \n        d\n        V\n      \n    \n    {\\displaystyle \\oint _{\\partial V}\\mathbf {E} _{0}\\cdot d\\mathbf {S} =\\int _{V}\\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}\\,dV}\n\nBut because \n  \n    \n      \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        ∈\n        \n          C\n          \n            1\n          \n        \n        (\n        V\n        ×\n        Ω\n        )\n      \n    \n    {\\displaystyle e(\\mathbf {r,\\mathbf {r} '} )\\in C^{1}(V\\times \\Omega )}\n  \n,\n\n∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}(\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }\\nabla _{\\mathbf {r} }\\cdot e(\\mathbf {r,\\mathbf {r} '} ){\\mathrm {d} \\mathbf {r} '}=0}\n  \n  for the argument above (\n  \n    \n      \n        Ω\n        ∩\n        V\n        =\n        ∅\n        \n        ⟹\n        \n        ∀\n        \n          r\n        \n        ∈\n        V\n         \n         \n        ∀\n        \n          \n            r\n            ′\n          \n        \n        ∈\n        Ω\n         \n         \n         \n        \n          r\n        \n        ≠\n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\Omega \\cap V=\\emptyset \\implies \\forall \\mathbf {r} \\in V\\ \\ \\forall \\mathbf {r'} \\in \\Omega \\ \\ \\ \\mathbf {r} \\neq \\mathbf {r'} }\n  \n and then \n  \n    \n      \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        \n          e\n        \n        (\n        \n          r\n          ,\n          \n            r\n            ′\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {r} }\\cdot \\mathbf {e} (\\mathbf {r,r'} )=0}\n  \n)\n\nTherefore the flux through a closed surface generated by some charge density outside (the surface) is null.\n\nNow consider \n  \n    \n      \n        \n          \n            r\n          \n          \n            0\n          \n        \n        ∈\n        Ω\n      \n    \n    {\\displaystyle \\mathbf {r} _{0}\\in \\Omega }\n  \n, and \n  \n    \n      \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        ⊆\n        Ω\n      \n    \n    {\\displaystyle B_{R}(\\mathbf {r} _{0})\\subseteq \\Omega }\n  \n  as the sphere centered in \n  \n    \n      \n        \n          \n            r\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{0}}\n  \n having \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n as radius (it exists because \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n is an open set).\n\nLet \n  \n    \n      \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{B_{R}}}\n  \n and \n  \n    \n      \n        \n          \n            E\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{C}}\n  \n be the electric field created inside and outside the sphere respectively. Then,\n\nΦ\n        (\n        R\n        )\n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            0\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n        ⋅\n        d\n        \n          S\n        \n        +\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            C\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n        ⋅\n        d\n        \n          S\n        \n      \n    \n    {\\displaystyle \\Phi (R)=\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{0}\\cdot d\\mathbf {S} =\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{B_{R}}\\cdot d\\mathbf {S} +\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{C}\\cdot d\\mathbf {S} =\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{B_{R}}\\cdot d\\mathbf {S} }\n\nThe last equality follows by observing that \n  \n    \n      \n        (\n        Ω\n        ∖\n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        )\n        ∩\n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        =\n        ∅\n      \n    \n    {\\displaystyle (\\Omega \\setminus B_{R}(\\mathbf {r} _{0}))\\cap B_{R}(\\mathbf {r} _{0})=\\emptyset }\n  \n, and the argument above.\n\nThe RHS is the electric flux generated by a charged sphere, and so:\n\nΦ\n        (\n        R\n        )\n        =\n        \n          \n            \n              Q\n              (\n              R\n              )\n            \n            \n              ε\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        \n          ∫\n          \n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          \n            c\n          \n          ′\n        \n        )\n        \n          |\n        \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle \\Phi (R)={\\frac {Q(R)}{\\varepsilon _{0}}}={\\frac {1}{\\varepsilon _{0}}}\\int _{B_{R}(\\mathbf {r} _{0})}\\rho (\\mathbf {r} '){\\mathrm {d} \\mathbf {r} '}={\\frac {1}{\\varepsilon _{0}}}\\rho (\\mathbf {r} '_{c})|B_{R}(\\mathbf {r} _{0})|}\n  \n with \n  \n    \n      \n        \n          r\n          \n            c\n          \n          ′\n        \n        ∈\n         \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle r'_{c}\\in \\ B_{R}(\\mathbf {r} _{0})}\n\nWhere the last equality follows by the mean value theorem for integrals. Using the squeeze theorem and the continuity of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n, one arrives at:\n\n∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        =\n        \n          lim\n          \n            R\n            →\n            0\n          \n        \n        \n          \n            1\n            \n              \n                |\n              \n              \n                B\n                \n                  R\n                \n              \n              (\n              \n                \n                  r\n                \n                \n                  0\n                \n              \n              )\n              \n                |\n              \n            \n          \n        \n        Φ\n        (\n        R\n        )\n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}(\\mathbf {r} _{0})=\\lim _{R\\to 0}{\\frac {1}{|B_{R}(\\mathbf {r} _{0})|}}\\Phi (R)={\\frac {1}{\\varepsilon _{0}}}\\rho (\\mathbf {r} _{0})}\n\nStrictly speaking, Coulomb's law cannot be derived from Gauss's law alone, since Gauss's law does not give any information regarding the curl of E (see Helmholtz decomposition and Faraday's law). However, Coulomb's law can be proven from Gauss's law if it is assumed, in addition, that the electric field from a point charge is spherically symmetric (this assumption, like Coulomb's law itself, is exactly true if the charge is stationary, and approximately true if the charge is in motion).\n\nTaking S in the integral form of Gauss's law to be a spherical surface of radius r, centered at the point charge Q, we have\n\n  \n    \n      \n        \n          ∮\n          \n            S\n          \n        \n        \n          E\n        \n        ⋅\n        d\n        \n          A\n        \n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\oint _{S}\\mathbf {E} \\cdot d\\mathbf {A} ={\\frac {Q}{\\varepsilon _{0}}}}\n\nBy the assumption of spherical symmetry, the integrand is a constant which can be taken out of the integral. The result is\n\n  \n    \n      \n        4\n        π\n        \n          r\n          \n            2\n          \n        \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle 4\\pi r^{2}{\\hat {\\mathbf {r} }}\\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {Q}{\\varepsilon _{0}}}}\n  \n\nwhere r̂ is a unit vector pointing radially away from the charge. Again by spherical symmetry, E points in the radial direction, and so we get\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            Q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {Q}{4\\pi \\varepsilon _{0}}}{\\frac {\\hat {\\mathbf {r} }}{r^{2}}}}\n  \n\nwhich is essentially equivalent to Coulomb's law. Thus the inverse-square law dependence of the electric field in Coulomb's law follows from Gauss's law.\n\nCoulomb's law can be used to gain insight into the form of the magnetic field generated by moving charges since by special relativity, in certain cases the magnetic field can be shown to be a transformation of forces caused by the electric field. When no acceleration is involved in a particle's history, Coulomb's law can be assumed on any test particle in its own inertial frame, supported by symmetry arguments in solving Maxwell's equation, shown above. Coulomb's law can be expanded to moving test particles to be of the same form. This assumption is supported by Lorentz force law which, unlike Coulomb's law is not limited to stationary test charges. Considering the charge to be invariant of observer, the electric and magnetic fields of a uniformly moving point charge can hence be derived by the Lorentz transformation of the four force on the test charge in the charge's frame of reference given by Coulomb's law and attributing magnetic and electric fields by their definitions given by the form of Lorentz force.[27] The fields hence found for uniformly moving point charges are given by:[28]\n  \n    \n      \n        \n          E\n        \n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ϵ\n                \n                  0\n                \n              \n              \n                r\n                \n                  3\n                \n              \n            \n          \n        \n        \n          \n            \n              1\n              −\n              \n                β\n                \n                  2\n                \n              \n            \n            \n              (\n              1\n              −\n              \n                β\n                \n                  2\n                \n              \n              \n                sin\n                \n                  2\n                \n              \n              ⁡\n              θ\n              \n                )\n                \n                  3\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {E} ={\\frac {q}{4\\pi \\epsilon _{0}r^{3}}}{\\frac {1-\\beta ^{2}}{(1-\\beta ^{2}\\sin ^{2}\\theta )^{3/2}}}\\mathbf {r} }\n  \n\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ϵ\n                \n                  0\n                \n              \n              \n                r\n                \n                  3\n                \n              \n            \n          \n        \n        \n          \n            \n              1\n              −\n              \n                β\n                \n                  2\n                \n              \n            \n            \n              (\n              1\n              −\n              \n                β\n                \n                  2\n                \n              \n              \n                sin\n                \n                  2\n                \n              \n              ⁡\n              θ\n              \n                )\n                \n                  3\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                v\n              \n              ×\n              \n                r\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                v\n              \n              ×\n              \n                E\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} ={\\frac {q}{4\\pi \\epsilon _{0}r^{3}}}{\\frac {1-\\beta ^{2}}{(1-\\beta ^{2}\\sin ^{2}\\theta )^{3/2}}}{\\frac {\\mathbf {v} \\times \\mathbf {r} }{c^{2}}}={\\frac {\\mathbf {v} \\times \\mathbf {E} }{c^{2}}}}\n  \nwhere \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n is the charge of the point source, \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n is the position vector from the point source to the point in space, \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n is the velocity vector of the charged particle, \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n is the ratio of speed of the charged particle divided by the speed of light and \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is the angle between \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n.\n\nThis form of solutions need not obey Newton's third law as is the case in the framework of special relativity (yet without violating relativistic-energy momentum conservation).[29] Note that the expression for electric field reduces to Coulomb's law for non-relativistic speeds of the point charge and that the magnetic field in non-relativistic limit (approximating \n  \n    \n      \n        β\n        ≪\n        1\n      \n    \n    {\\displaystyle \\beta \\ll 1}\n  \n) can be applied to electric currents to get the Biot–Savart law. These solutions, when expressed in retarded time also correspond to the general solution of Maxwell's equations given by solutions of Liénard–Wiechert potential, due to the validity of Coulomb's law within its specific range of application. Also note that the spherical symmetry for gauss law on stationary charges is not valid for moving charges owing to the breaking of symmetry by the specification of direction of velocity in the problem. Agreement with Maxwell's equations can also be manually verified for the above two equations.[30]\n\nThe Coulomb potential admits continuum states (with E > 0), describing electron-proton scattering, as well as discrete bound states, representing the hydrogen atom.[31] It can also be derived within the non-relativistic limit between two charged particles, as follows:\n\nUnder Born approximation, in non-relativistic quantum mechanics, the scattering amplitude \n  \n    \n      \n        \n          \n            A\n          \n        \n        (\n        \n          |\n        \n        \n          p\n        \n        ⟩\n        →\n        \n          |\n        \n        \n          \n            p\n          \n          ′\n        \n        ⟩\n        )\n      \n    \n    {\\textstyle {\\mathcal {A}}(|\\mathbf {p} \\rangle \\to |\\mathbf {p} '\\rangle )}\n  \n is:\n\n  \n    \n      \n        \n          \n            A\n          \n        \n        (\n        \n          |\n        \n        \n          p\n        \n        ⟩\n        →\n        \n          |\n        \n        \n          \n            p\n          \n          ′\n        \n        ⟩\n        )\n        −\n        1\n        =\n        2\n        π\n        δ\n        (\n        \n          E\n          \n            p\n          \n        \n        −\n        \n          E\n          \n            \n              p\n              ′\n            \n          \n        \n        )\n        (\n        −\n        i\n        )\n        ∫\n        \n          d\n          \n            3\n          \n        \n        \n          r\n        \n        \n        V\n        (\n        \n          r\n        \n        )\n        \n          e\n          \n            −\n            i\n            (\n            \n              p\n            \n            −\n            \n              \n                p\n              \n              ′\n            \n            )\n            \n              r\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}(|\\mathbf {p} \\rangle \\to |\\mathbf {p} '\\rangle )-1=2\\pi \\delta (E_{p}-E_{p'})(-i)\\int d^{3}\\mathbf {r} \\,V(\\mathbf {r} )e^{-i(\\mathbf {p} -\\mathbf {p} ')\\mathbf {r} }}\n  \n\nThis is to be compared to the:\n\n  \n    \n      \n        ∫\n        \n          \n            \n              \n                d\n                \n                  3\n                \n              \n              k\n            \n            \n              (\n              2\n              π\n              \n                )\n                \n                  3\n                \n              \n            \n          \n        \n        \n          e\n          \n            i\n            k\n            \n              r\n              \n                0\n              \n            \n          \n        \n        ⟨\n        \n          p\n          ′\n        \n        ,\n        k\n        \n          |\n        \n        S\n        \n          |\n        \n        p\n        ,\n        k\n        ⟩\n      \n    \n    {\\displaystyle \\int {\\frac {d^{3}k}{(2\\pi )^{3}}}e^{ikr_{0}}\\langle p',k|S|p,k\\rangle }\n  \n\nwhere we look at the (connected) S-matrix entry for two electrons scattering off each other, treating one with \"fixed\" momentum as the source of the potential, and the other scattering off that potential.\n\nUsing the Feynman rules to compute the S-matrix element, we obtain in the non-relativistic limit with \n  \n    \n      \n        \n          m\n          \n            0\n          \n        \n        ≫\n        \n          |\n        \n        \n          p\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle m_{0}\\gg |\\mathbf {p} |}\n  \n\n\n  \n    \n      \n        ⟨\n        \n          p\n          ′\n        \n        ,\n        k\n        \n          |\n        \n        S\n        \n          |\n        \n        p\n        ,\n        k\n        ⟩\n        \n          \n            |\n          \n          \n            c\n            o\n            n\n            n\n          \n        \n        =\n        −\n        i\n        \n          \n            \n              e\n              \n                2\n              \n            \n            \n              \n                |\n              \n              \n                p\n              \n              −\n              \n                \n                  p\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n              −\n              i\n              ε\n            \n          \n        \n        (\n        2\n        m\n        \n          )\n          \n            2\n          \n        \n        δ\n        (\n        \n          E\n          \n            p\n            ,\n            k\n          \n        \n        −\n        \n          E\n          \n            \n              p\n              ′\n            \n            ,\n            k\n          \n        \n        )\n        (\n        2\n        π\n        \n          )\n          \n            4\n          \n        \n        δ\n        (\n        \n          p\n        \n        −\n        \n          \n            p\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\langle p',k|S|p,k\\rangle |_{conn}=-i{\\frac {e^{2}}{|\\mathbf {p} -\\mathbf {p} '|^{2}-i\\varepsilon }}(2m)^{2}\\delta (E_{p,k}-E_{p',k})(2\\pi )^{4}\\delta (\\mathbf {p} -\\mathbf {p} ')}\n\nComparing with the QM scattering, we have to discard the \n  \n    \n      \n        (\n        2\n        m\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (2m)^{2}}\n  \n as they arise due to differing normalizations of momentum eigenstate in QFT compared to QM and obtain:\n\n  \n    \n      \n        ∫\n        V\n        (\n        \n          r\n        \n        )\n        \n          e\n          \n            −\n            i\n            (\n            \n              p\n            \n            −\n            \n              \n                p\n              \n              ′\n            \n            )\n            \n              r\n            \n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          r\n        \n        =\n        \n          \n            \n              e\n              \n                2\n              \n            \n            \n              \n                |\n              \n              \n                p\n              \n              −\n              \n                \n                  p\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  2\n                \n              \n              −\n              i\n              ε\n            \n          \n        \n      \n    \n    {\\displaystyle \\int V(\\mathbf {r} )e^{-i(\\mathbf {p} -\\mathbf {p} ')\\mathbf {r} }d^{3}\\mathbf {r} ={\\frac {e^{2}}{|\\mathbf {p} -\\mathbf {p} '|^{2}-i\\varepsilon }}}\n  \n\nwhere Fourier transforming both sides, solving the integral and taking \n  \n    \n      \n        ε\n        →\n        0\n      \n    \n    {\\displaystyle \\varepsilon \\to 0}\n  \n at the end will yield\n\n  \n    \n      \n        V\n        (\n        r\n        )\n        =\n        \n          \n            \n              e\n              \n                2\n              \n            \n            \n              4\n              π\n              r\n            \n          \n        \n      \n    \n    {\\displaystyle V(r)={\\frac {e^{2}}{4\\pi r}}}\n  \n\nas the Coulomb potential.[32]\n\nHowever, the equivalent results of the classical Born derivations for the Coulomb problem are thought to be strictly accidental.[33][34]\n\nThe Coulomb potential, and its derivation, can be seen as a special case of the Yukawa potential, which is the case where the exchanged boson – the photon – has no rest mass.[31]\n\nIt is possible to verify Coulomb's law with a simple experiment. Consider two small spheres of mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n and same-sign charge \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n, hanging from two ropes of negligible mass of length \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n. The forces acting on each sphere are three: the weight \n  \n    \n      \n        m\n        g\n      \n    \n    {\\displaystyle mg}\n  \n, the rope tension \n  \n    \n      \n        \n          T\n        \n      \n    \n    {\\displaystyle \\mathbf {T} }\n  \n and the electric force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n. In the equilibrium state:\n\nLet \n  \n    \n      \n        \n          \n            L\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {L} _{1}}\n  \n be the distance between the charged spheres; the repulsion force between them \n  \n    \n      \n        \n          \n            F\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} _{1}}\n  \n, assuming Coulomb's law is correct, is equal to\n\nIf we now discharge one of the spheres, and we put it in contact with the charged sphere, each one of them acquires a charge \n  \n    \n      \n        \n          \n            q\n            2\n          \n        \n      \n    \n    {\\textstyle {\\frac {q}{2}}}\n  \n. In the equilibrium state, the distance between the charges will be \n  \n    \n      \n        \n          \n            L\n          \n          \n            2\n          \n        \n        <\n        \n          \n            L\n          \n          \n            1\n          \n        \n      \n    \n    {\\textstyle \\mathbf {L} _{2}<\\mathbf {L} _{1}}\n  \n and the repulsion force between them will be:\n\nWe know that \n  \n    \n      \n        \n          \n            F\n          \n          \n            2\n          \n        \n        =\n        m\n        g\n        tan\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} _{2}=mg\\tan \\theta _{2}}\n  \n and:\n\n  \n    \n      \n        \n          \n            \n              \n                q\n                \n                  2\n                \n              \n              4\n            \n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n              \n                L\n                \n                  2\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        m\n        g\n        tan\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\frac {q^{2}}{4}}{4\\pi \\varepsilon _{0}L_{2}^{2}}}=mg\\tan \\theta _{2}}\n  \n\nDividing (4) by (5), we get:\n\nMeasuring the angles \n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \n and \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n and the distance between the charges \n  \n    \n      \n        \n          \n            L\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {L} _{1}}\n  \n and \n  \n    \n      \n        \n          \n            L\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {L} _{2}}\n  \n is sufficient to verify that the equality is true taking into account the experimental error. In practice, angles can be difficult to measure, so if the length of the ropes is sufficiently great, the angles will be small enough to make the following approximation:\n\nUsing this approximation, the relationship (6) becomes the much simpler expression:\n\nIn this way, the verification is limited to measuring the distance between the charges and checking that the division approximates the theoretical value.\n\nSpavieri, G., Gillies, G. T., & Rodriguez, M. (2004). Physical implications of Coulomb’s Law. Metrologia, 41(5), S159–S170. doi:10.1088/0026-1394/41/5/s06",
        pageTitle: "Coulomb's law",
    },
    {
        title: "inverse-square law",
        link: "https://en.wikipedia.org/wiki/Inverse-square_law",
        content:
            'In science, an inverse-square law is any scientific law stating that the observed "intensity" of a specified physical quantity is inversely proportional to the square of the distance from the source of that physical quantity. The fundamental cause for this can be understood as geometric dilution corresponding to point-source radiation into three-dimensional space.\n\nRadar energy expands during both the signal transmission and the reflected return, so the inverse square for both paths means that the radar will receive energy according to the inverse fourth power of the range.\n\nTo prevent dilution of energy while propagating a signal, certain methods can be used such as a waveguide, which acts like a canal does for water, or how a gun barrel restricts hot gas expansion to one dimension in order to prevent loss of energy transfer to a bullet.\n\nIn mathematical notation the inverse square law can be expressed as an intensity (I) varying as a function of distance (d) from some centre. The intensity is proportional (see ∝) to the reciprocal of the square of the distance thus: \n\n  \n    \n      \n        \n          intensity\n        \n         \n        ∝\n         \n        \n          \n            1\n            \n              \n                distance\n              \n              \n                2\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle {\\text{intensity}}\\ \\propto \\ {\\frac {1}{{\\text{distance}}^{2}}}\\,}\n\nIt can also be mathematically expressed as :\n\n  \n    \n      \n        \n          \n            \n              \n                intensity\n              \n              \n                1\n              \n            \n            \n              \n                intensity\n              \n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                distance\n              \n              \n                2\n              \n              \n                2\n              \n            \n            \n              \n                distance\n              \n              \n                1\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {{\\text{intensity}}_{1}}{{\\text{intensity}}_{2}}}={\\frac {{\\text{distance}}_{2}^{2}}{{\\text{distance}}_{1}^{2}}}}\n\nor as the formulation of a constant quantity:   \n\n  \n    \n      \n        \n          \n            intensity\n          \n          \n            1\n          \n        \n        ×\n        \n          \n            distance\n          \n          \n            1\n          \n          \n            2\n          \n        \n        =\n        \n          \n            intensity\n          \n          \n            2\n          \n        \n        ×\n        \n          \n            distance\n          \n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\text{intensity}}_{1}\\times {\\text{distance}}_{1}^{2}={\\text{intensity}}_{2}\\times {\\text{distance}}_{2}^{2}}\n\nThe divergence of a vector field which is the resultant of radial inverse-square law fields with respect to one or more sources is proportional to the strength of the local sources, and hence zero outside sources. Newton\'s law of universal gravitation follows an inverse-square law, as do the effects of electric, light, sound, and radiation phenomena.\n\nThe inverse-square law generally applies when some force, energy, or other conserved quantity is evenly radiated outward from a point source in three-dimensional space.  Since the surface area of a sphere (which is 4πr2) is proportional to the square of the radius, as the emitted radiation gets farther from the source, it is spread out over an area that is increasing in proportion to the square of the distance from the source. Hence, the intensity of radiation passing through any unit area (directly facing the point source) is inversely proportional to the square of the distance from the point source. Gauss\'s law for gravity is similarly applicable, and can be used with any physical quantity that acts in accordance with the inverse-square relationship.\n\nGravitation is the attraction between objects that have mass. Newton\'s law states:\n\nThe gravitational attraction force between two point masses is directly proportional to the product of their masses and inversely proportional to the square of their separation distance. The force is always attractive and acts along the line joining them.[1]\n\nF\n        =\n        G\n        \n          \n            \n              \n                m\n                \n                  1\n                \n              \n              \n                m\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F=G{\\frac {m_{1}m_{2}}{r^{2}}}}\n\nIf the distribution of matter in each body is spherically symmetric, then the objects can be treated as point masses without approximation, as shown in the shell theorem.  Otherwise, if we want to calculate the attraction between massive bodies, we need to add all the point-point attraction forces vectorially and the net attraction might not be exact inverse square. However, if the separation between the massive bodies is much larger compared to their sizes, then to a good approximation, it is reasonable to treat the masses as a point mass located at the object\'s center of mass while calculating the gravitational force.\n\nAs the law of gravitation, this law was suggested in 1645 by Ismaël Bullialdus. But Bullialdus did not accept Kepler\'s second and third laws, nor did he appreciate Christiaan Huygens\'s solution for circular motion (motion in a straight line pulled aside by the central force).  Indeed, Bullialdus maintained the sun\'s force was attractive at aphelion and repulsive at perihelion. Robert Hooke and Giovanni Alfonso Borelli both expounded gravitation in 1666 as an attractive force.[2] Hooke\'s lecture "On gravity" was at the Royal Society, in London, on 21 March.[3] Borelli\'s "Theory of the Planets" was published later in 1666.[4]  Hooke\'s 1670 Gresham lecture explained that gravitation applied to "all celestiall bodys" and added the principles that the gravitating power decreases with distance and that in the absence of any such power bodies move in straight lines. By 1679, Hooke thought gravitation had inverse square dependence and communicated this in a letter to Isaac Newton:[5] \nmy supposition is that the attraction always is in duplicate proportion to the distance from the center reciprocall.[6]\n\nHooke remained bitter about Newton claiming the invention of this principle, even though Newton\'s 1686 Principia acknowledged that Hooke, along with Wren and Halley, had separately appreciated the inverse square law in the Solar System,[7] as well as giving some credit to Bullialdus.[8]\n\nThe force of attraction or repulsion between two electrically charged particles, in addition to being directly proportional to the product of the electric charges, is inversely proportional to the square of the distance between them; this is known as Coulomb\'s law. The deviation of the exponent from 2 is less than one part in 1015.[9]\n\nF\n        =\n        \n          k\n          \n            e\n          \n        \n        \n          \n            \n              \n                q\n                \n                  1\n                \n              \n              \n                q\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F=k_{\\text{e}}{\\frac {q_{1}q_{2}}{r^{2}}}}\n\nThe intensity (or illuminance or irradiance) of light or other linear waves radiating from a point source (energy per unit of area perpendicular to the source) is inversely proportional to the square of the distance from the source, so an object (of the same size) twice as far away receives only one-quarter the energy (in the same time period).\n\nMore generally, the irradiance, i.e., the intensity (or power per unit area in the direction of propagation), of a spherical wavefront varies inversely with the square of the distance from the source (assuming there are no losses caused by absorption or scattering).\n\nFor example, the intensity of radiation from the Sun is 9126 watts per square meter at the distance of Mercury (0.387 AU); but only 1367 watts per square meter at the distance of Earth (1 AU)—an approximate threefold increase in distance results in an approximate ninefold decrease in intensity of radiation.\n\nFor non-isotropic radiators such as parabolic antennas, headlights, and lasers, the effective origin is located far behind the beam aperture. If you are close to the origin, you don\'t have to go far to double the radius, so the signal drops quickly. When you are far from the origin and still have a strong signal, like with a laser, you have to travel very far to double the radius and reduce the signal. This means you have a stronger signal or have antenna gain in the direction of the narrow beam relative to a wide beam in all directions of an isotropic antenna.\n\nIn photography and stage lighting, the inverse-square law is used to determine the “fall off” or the difference in illumination on a subject as it moves closer to or further from the light source. For quick approximations, it is enough to remember that doubling the distance reduces illumination to one quarter;[10] or similarly, to halve the illumination increase the distance by a factor of 1.4 (the square root of 2), and to double illumination, reduce the distance to 0.7 (square root of 1/2). When the illuminant is not a point source, the inverse square rule is often still a useful approximation; when the size of the light source is less than one-fifth of the distance to the subject, the calculation error is less than 1%.[11]\n\nThe fractional reduction in electromagnetic fluence (Φ) for indirectly ionizing radiation with increasing distance from a point source can be calculated using the inverse-square law. Since emissions from a point source have radial directions, they intercept at a perpendicular incidence. The area of such a shell is 4πr 2 where r is the radial distance from the center. The law is particularly important in diagnostic radiography and radiotherapy treatment planning, though this proportionality does not hold in practical situations unless source dimensions are much smaller than the distance.  As stated in Fourier theory of heat “as the point source is magnification by distances, its radiation is dilute proportional to the sin of the angle, of the increasing circumference arc from the point of origin”.\n\nLet P  be the total power radiated from a point source (for example, an omnidirectional isotropic radiator). At large distances from the source (compared to the size of the source), this power is distributed over larger and larger spherical surfaces as the distance from the source increases.  Since the surface area of a sphere of radius r is A = 4πr 2, the intensity I (power per unit area) of radiation at distance r is\n\n  \n    \n      \n        I\n        =\n        \n          \n            P\n            A\n          \n        \n        =\n        \n          \n            P\n            \n              4\n              π\n              \n                r\n                \n                  2\n                \n              \n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle I={\\frac {P}{A}}={\\frac {P}{4\\pi r^{2}}}.\\,}\n\nThe energy or intensity decreases (divided by 4) as the distance r is doubled; if measured in dB would decrease by 6.02 dB per doubling of distance. When referring to measurements of power quantities, a ratio can be expressed as a level in decibels by evaluating ten times the base-10 logarithm of the ratio of the measured quantity to the reference value.\n\nIn acoustics, the sound pressure of a spherical wavefront radiating from a point source decreases by 50% as the distance r is doubled; measured in dB, the decrease is still 6.02 dB, since dB represents an intensity ratio. The pressure ratio (as opposed to power ratio) is not inverse-square, but is inverse-proportional (inverse distance law):\n\np\n         \n        ∝\n         \n        \n          \n            1\n            r\n          \n        \n        \n      \n    \n    {\\displaystyle p\\ \\propto \\ {\\frac {1}{r}}\\,}\n\nThe same is true for the component of particle velocity \n  \n    \n      \n        v\n        \n      \n    \n    {\\displaystyle v\\,}\n  \n that is in-phase with the instantaneous sound pressure \n  \n    \n      \n        p\n        \n      \n    \n    {\\displaystyle p\\,}\n  \n:\n\nv\n         \n        ∝\n        \n          \n            1\n            r\n          \n        \n         \n        \n      \n    \n    {\\displaystyle v\\ \\propto {\\frac {1}{r}}\\ \\,}\n\nIn the near field is a quadrature component of the particle velocity that is 90° out of phase with the sound pressure and does not contribute to the time-averaged energy or the intensity of the sound.  The sound intensity is the product of the RMS sound pressure and the in-phase component of the RMS particle velocity, both of which are inverse-proportional. Accordingly, the intensity follows an inverse-square behaviour:\n\nI\n         \n        =\n         \n        p\n        v\n         \n        ∝\n         \n        \n          \n            1\n            \n              r\n              \n                2\n              \n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle I\\ =\\ pv\\ \\propto \\ {\\frac {1}{r^{2}}}.\\,}\n\nFor an irrotational vector field in three-dimensional space, the inverse-square law corresponds to the property that the divergence is zero outside the source. This can be generalized to higher dimensions. Generally, for an irrotational vector field in n-dimensional Euclidean space, the intensity "I" of the vector field falls off with the distance "r" following the inverse (n − 1)th power law\n\nI\n        ∝\n        \n          \n            1\n            \n              r\n              \n                n\n                −\n                1\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I\\propto {\\frac {1}{r^{n-1}}},}\n\ngiven that the space outside the source is divergence free. [citation needed]\n\nThe inverse-square law, fundamental in Euclidean spaces, also applies to non-Euclidean geometries, including hyperbolic space. The curvature present in these spaces alters physical laws, influencing a variety of fields such as cosmology, general relativity, and string theory.[12]\n\nJohn D. Barrow, in his 2020 paper "Non-Euclidean Newtonian Cosmology," expands on the behavior of force (F) and potential (Φ) within hyperbolic 3-space (H3). He explains that F and Φ obey the relationships F ∝ 1 / R² sinh²(r/R) and Φ ∝ coth(r/R), where R represents the curvature radius and r represents the distance from the focal point.\n\nThe concept of spatial dimensionality, first proposed by Immanuel Kant, remains a topic of debate concerning the inverse-square law.[13] Dimitria Electra Gatzia and Rex D. Ramsier, in their 2021 paper, contend that the inverse-square law is more closely related to force distribution symmetry than to the dimensionality of space.\n\nIn the context of non-Euclidean geometries and general relativity, deviations from the inverse-square law do not arise from the law itself but rather from the assumption that the force between two bodies is instantaneous, which contradicts special relativity. General relativity reinterprets gravity as the curvature of spacetime, leading particles to move along geodesics in this curved spacetime.[14]\n\nJohn Dumbleton of the 14th-century Oxford Calculators, was one of the first to express functional relationships in graphical form. He gave a proof of the mean speed theorem stating that "the latitude of a uniformly difform movement corresponds to the degree of the midpoint" and used this method to study the quantitative decrease in intensity of illumination in his Summa logicæ et philosophiæ naturalis (ca. 1349), stating that it was not linearly proportional to the distance, but was unable to expose the Inverse-square law.[15]\n\nIn proposition 9 of Book 1 in his book Ad Vitellionem paralipomena, quibus astronomiae pars optica traditur (1604), the astronomer Johannes Kepler argued that the spreading of light from a point source obeys an inverse square law:[16][17]\n\nSicut se habent spharicae superificies, quibus origo lucis pro centro est, amplior ad angustiorem:  ita se habet fortitudo seu densitas lucis radiorum in angustiori, ad illamin in laxiori sphaerica, hoc est, conversim.  Nam per 6. 7. tantundem lucis est in angustiori sphaerica superficie, quantum in fusiore, tanto ergo illie stipatior & densior quam hic.\n\nJust as [the ratio of] spherical surfaces, for which the source of light is the center, [is] from the wider to the narrower, so the density or fortitude of the rays of light in the narrower [space], towards the more spacious spherical surfaces, that is, inversely.  For according to [propositions] 6 & 7, there is as much light in the narrower spherical surface, as in the wider, thus it is as much more compressed and dense here than there.\n\nIn 1645, in his book Astronomia Philolaica ..., the French astronomer Ismaël Bullialdus (1605–1694) refuted Johannes Kepler\'s suggestion that "gravity"[18] weakens as the inverse of the distance; instead, Bullialdus argued, "gravity" weakens as the inverse square of the distance:[19][20]\n\nVirtus autem illa, qua Sol prehendit seu harpagat planetas, corporalis quae ipsi pro manibus est, lineis rectis in omnem mundi amplitudinem emissa quasi species solis cum illius corpore rotatur:  cum ergo sit corporalis imminuitur, & extenuatur in maiori spatio & intervallo, ratio autem huius imminutionis eadem est, ac luminus, in ratione nempe dupla intervallorum, sed eversa.\n\nAs for the power by which the Sun seizes or holds the planets, and which, being corporeal, functions in the manner of hands, it is emitted in straight lines throughout the whole extent of the world, and like the species of the Sun, it turns with the body of the Sun; now, seeing that it is corporeal, it becomes weaker and attenuated at a greater distance or interval, and the ratio of its decrease in strength is the same as in the case of light, namely, the duplicate proportion, but inversely, of the distances [that is, 1/d²].\n\nIn England, the Anglican bishop Seth Ward (1617–1689) publicized the ideas of Bullialdus in his critique In Ismaelis Bullialdi astronomiae philolaicae fundamenta inquisitio brevis (1653) and publicized the planetary astronomy of Kepler in his book Astronomia geometrica (1656).\n\nIn 1663–1664, the English scientist Robert Hooke was writing his book Micrographia (1666) in which he discussed, among other things, the relation between the height of the atmosphere and the barometric pressure at the surface.  Since the atmosphere surrounds the Earth, which itself is a sphere, the volume of atmosphere bearing on any unit area of the Earth\'s surface is a truncated cone (which extends from the Earth\'s center to the vacuum of space; obviously only the section of the cone from the Earth\'s surface to space bears on the Earth\'s surface).  Although the volume of a cone is proportional to the cube of its height, Hooke argued that the air\'s pressure at the Earth\'s surface is instead proportional to the height of the atmosphere because gravity diminishes with altitude.  Although Hooke did not explicitly state so, the relation that he proposed would be true only if gravity decreases as the inverse square of the distance from the Earth\'s center.[21][22]\n\n.mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)} This article incorporates public domain material from .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}Federal Standard 1037C. General Services Administration. Archived from the original on 22 January 2022.',
        pageTitle: "Inverse-square law",
    },
    {
        title: "Crane's law",
        link: "https://en.wikipedia.org/wiki/Crane%27s_law",
        content:
            '"No such thing as a free lunch" (also written as "There ain\'t no such thing as a free lunch" and sometimes called Crane\'s law[1]) is a popular adage communicating the idea that it is impossible to get something for nothing.  The acronyms TANSTAAFL, TINSTAAFL, and TNSTAAFL are also used.  The phrase was in use by the 1930s, but its first appearance is unknown.[2] The "free lunch" in the saying refers to the formerly common practice in American bars of offering a "free lunch" in order to entice drinking customers.\n\nThe phrase and the acronym are central to Robert A. Heinlein\'s 1966 science-fiction novel The Moon is a Harsh Mistress, which helped popularize it.[3][4] The free-market economist Milton Friedman also increased its exposure and use[2] by paraphrasing it as the title of a 1975 book;[5] it is used in economics literature to describe opportunity cost.[6] Campbell McConnell writes that the idea is "at the core of economics".[7]\n\nThe "free lunch" refers to the once-common tradition of saloons in the United States providing a "free" lunch to patrons who had purchased at least one drink. Many foods on offer were high in salt (e.g., ham, cheese, and salted crackers), so those who ate them ended up buying a lot of beer. Rudyard Kipling, writing in 1891, noted how he\n\n...came upon a bar-room full of bad Salon pictures, in which men with hats on the backs of their heads were wolfing food from a counter. It was the institution of the "free lunch" I had struck. You paid for a drink and got as much as you wanted to eat. For something less than a rupee a day a man can feed himself sumptuously in San Francisco, even though he be a bankrupt. Remember this if ever you are stranded in these parts.[8]\n\nSome quotes exist from the time, arguing that these free lunches were not really free, such as in the Columbia Daily Phoenix of 1873: "One of the most expensive things in this city—Free lunch.",[9] L. A. W. Bulletin 1897: "If no one ever paid for drinks, there would be no \'free lunch\', and the man who confines his attention to the free lunch, alone, is getting what he knows others pay for."[10] and the Washington Herald 1909: "as a matter of fact, there is no such thing as free lunch. Somebody has to pay for it."[11] When Chicago attempted to ban free lunches in 1917, Michael Montague, a saloon owner, made the case that "There is no such thing as free lunch. First of all, you have to buy something from the saloonkeeper before you can partake of the lunch. Lunch is the greatest tempering influence in the saloon. If a man takes a two-ounce drink of whisky and then takes a bite of lunch, he probably does not take a second drink. Whisky taken alone creates an appetite. If you want to create the use of whisky, pass this ordinance."[12]\n\nTANSTAAFL, on the other hand, applies this more generally, and indicates an acknowledgement that in reality a person or a society cannot get "something for nothing". Even if something appears to be free, there is always a cost to the person or to society as a whole, although that may be a hidden cost or an externality. For example, as Heinlein has one of his characters point out, a bar offering a free lunch will likely charge more for its drinks.[13]\n\nThe earliest known use of the phrase in its current sense is as the punchline of the article "Economics in Eight Words" by Walter Morrow,[14] published in the El Paso Herald-Post of June 27, 1938 (and other Scripps-Howard newspapers about the same time).[15][16] The article is a fable about a king who seeks advice from his economic advisors. He asks for ever-simplified advice following their original "eighty-seven volumes of six hundred pages", executing half the economists each time. The last surviving economist distills all the advice to eight words: "There ain\'t no such thing as free lunch."[17]\n\nIn 1942, "There ain\'t no such thing as a free lunch" (with the word "a" before "free lunch") appeared in Public Utilities Fortnightly,[18] and the Columbia Law Review in 1945. A shortened version of the phrase, "there is no free lunch" appeared in a 1942 article in the Oelwein Daily Register (in a quote attributed to economist Harley L. Lutz) and in a 1947 column by economist Merryle S. Rukeyser.[3][19]\n\nIn 1949, the phrase appeared in Pierre Dos Utt\'s monograph TANSTAAFL: A Plan for a New Economic World Order,[20] which describes an oligarchic political system based on his conclusions from "no free lunch" principles.\n\nIn 1950, a New York Times columnist ascribed the phrase to economist (and army general) Leonard P. Ayres of the Cleveland Trust Company: "It seems that shortly before the General\'s death [in 1946]... a group of reporters approached the general with the request that perhaps he might give them one of several immutable economic truisms that he had gathered from his long years of economic study... \'It is an immutable economic fact,\' said the general, \'that there is no such thing as a free lunch.\'"[21]\n\nThe September 8, 1961, issue of LIFE magazine has an editorial on page 4, "\'TANSTAFL\', It\'s the Truth", that closes with an anecdotal farmer explaining this slight variant of TANSTAAFL.\n\nBy the late 1960s, the phrase had also been given the name "Crane\'s law", for example in an article by Henry D. Harral in the Pennsylvanian (1969).[22]\n\nIn 1966, author Robert A. Heinlein published his novel The Moon Is a Harsh Mistress, in which TANSTAAFL was a central, libertarian theme, mentioned by name and explained. This increased its use in the mainstream.[3][4]\n\nEdwin G. Dolan used the phrase as the title of his 1971 book TANSTAAFL (There Ain\'t No Such Thing As A Free Lunch) – A Libertarian Perspective on Environmental Policy.[23]\n\nIn the sciences, no free lunch means that the universe as a whole is ultimately a closed system. There is no source of matter, energy, or light that draws resources from something else which will not eventually be exhausted. Therefore, the no free lunch argument may also be applied to natural physical processes in a closed system (either the universe as a whole, or any system that does not receive energy or matter from outside). (See Second law of thermodynamics.) The bio-ecologist Barry Commoner used this concept as the last of his famous "Four Laws of Ecology".\n\nAccording to American theoretical physicist and cosmologist Alan Guth "the universe is the ultimate free lunch", given that in the early stage of its expansion the total amount of energy available to make particles was very large.[24]\n\nIn economics, no free lunch demonstrates opportunity cost. Greg Mankiw described the concept as follows: "To get one thing that we like, we usually have to give up another thing that we like. Making decisions requires trading off one goal against another."[25] The idea that there is no free lunch at the societal level applies only when all resources are being used completely and appropriately – i.e., when economic efficiency prevails. If not, a \'free lunch\' can be had through a more efficient utilization of resources. Or, as Fred Brooks put it, "You can only get something for nothing if you have previously gotten nothing for something."  If one individual or group gets something at no cost, somebody else ends up paying for it. If there appears to be no direct cost to any single individual, there is a social cost.  Similarly, someone can benefit for "free" from an externality or from a public good, but someone has to pay the cost of producing these benefits. (See Free rider problem and Tragedy of the commons.)\n\nIn mathematical finance, the term is also used as an informal synonym for the principle of no-arbitrage. This principle states that a combination of securities that has the same cash-flows as another security must have the same net price in equilibrium.\n\nIn statistics, the term has been used to describe the tradeoffs of statistical learners (e.g., in machine learning) which are unavoidable according to the "No free lunch" theorem. That is, any model that claims to offer superior flexibility in analyzing data patterns usually does so at the cost of introducing extra assumptions, or by sacrificing generalizability in important situations.[26]\n\nNo free lunch is sometimes used as a response to claims of the virtues of free software. Supporters of free software often counter that the use of the term "free" in this context is primarily a reference to a lack of constraint ("libre") rather than a lack of cost ("gratis"). Richard Stallman has described it as "\'free\' as in \'free speech\', not as in \'free beer\'".\n\nThe prefix "TANSTAA-" (or "TINSTAA-") is used in numerous other contexts as well to denote some immutable property of the system being discussed. For example, "TANSTAANFS" is used by electrical engineering professors to stand for "There Ain\'t No Such Thing As A Noise-Free System".[citation needed]\n\nBaseball Prospectus coined the abbreviation "TINSTAAPP", for "There Is No Such Thing As A Pitching Prospect",[27] as many young pitchers hurt their arms before they can be effective at a major league level.\n\nHungarian prime minister Ferenc Gyurcsány used this adage to justify his social reforms in the mid-2000s. As a post-socialist country, Hungary struggled with the illusion of the state as a caring and giving, independent entity, rather than being the embodiment of the community. The saying "there is no free lunch" represented that even if the state provides welfare or something else for the people in need, it is in fact bought or provided by other people of the same community through taxes. Therefore, the state cannot provide everything for everyone, and increased provisions given by the state can only be financed by economic growth, increased taxes or public debt.\n\nSome exceptions from the "no free lunch" tenet have been put forward, such as the Sun and carbon dioxide.[28] It was argued in particular that metabolism evolved to take advantage of the free lunch provided by the Sun, which also triggers production of vital oxygen in plants.[28]  However, these too fall short in that the viewpoint is an open system, Earth, with "free" inputs from the Sun.  When viewed from the larger system context, the Sun/Earth or Solar System, there is no net energy exchange, and still "no free lunch".[29]',
        pageTitle: "No such thing as a free lunch",
    },
    {
        title: "Cunningham's law",
        link: "https://en.wikipedia.org/wiki/Cunningham%27s_law",
        content:
            'Howard G. Cunningham (born May 26, 1949) is an American computer programmer who developed the first wiki[1][2] and was a co-author of the Manifesto for Agile Software Development. Termed a pioneer,[3] and innovator,[1][2] he also helped create both software design patterns and extreme programming. He began coding the WikiWikiWeb in 1994, and installed it on c2.com (the website of his software consulting firm) on March 25, 1995, as an add-on to the Portland Pattern Repository. He co-authored (with Bo Leuf) a book about wikis, entitled The Wiki Way, and invented the Framework for Integrated Test.\n\nCunningham was a keynote speaker at the first three instances of the WikiSym conference series on wiki research and practice, and also at the Wikimedia Developer Summit 2017.[4] He was a keynote speaker at the MediaWiki Users and Developers Conference, Spring 2024.[5]\n\nCunningham was born in Michigan City, Indiana, on May 26, 1949.[6] He grew up in Highland, Indiana, where he completed high school.[7]\n\nCunningham received his bachelor\'s degree in interdisciplinary engineering (electrical engineering and computer science) and his master\'s degree in computer science from Purdue University, graduating in 1978.[8] He is a co-founder of Cunningham & Cunningham, a software consultancy he started with his wife.[9]\n\nCunningham has also served as Director of R&D at Wyatt Software and as Principal Engineer in the Tektronix Computer Research Laboratory. He is founder of The Hillside Group and has served as program chair of the Pattern Languages of Programming conference which it sponsors.\n\nFrom December 2003 until October 2005, Cunningham worked for Microsoft in the "Patterns & Practices" group. From October 2005 to May 2007, he held the position of Director of Committer Community Development at the Eclipse Foundation. In May 2009, he joined AboutUs as its chief technology officer.[3][10] On March 24, 2011 The Oregonian reported that Cunningham had departed AboutUs to join the Venice Beach-based CitizenGlobal, a startup working on crowd-sourced video content, as their chief technology officer and the Co-Creation Czar.[11] He remains "an adviser" with AboutUs.[12][13] In April 2013, Cunningham left CitizenGlobal to work as a programmer at New Relic.[14]\n\nCunningham is well known for a few widely disseminated ideas which he originated and developed. The most famous among these are the wiki and many ideas in the field of software design patterns, made popular by the Gang of Four (GoF). He owns the company Cunningham & Cunningham Inc., a consultancy that has specialized in object-oriented programming. He coined the concept of technical debt and expanded on the idea in 1992.[15][16]\nHe created the site (and software) WikiWikiWeb, the first internet wiki, in 1995.\n\nIn 2001, he signed the Manifesto for Agile Software Development as a co-author.[17][better source needed]\n\nWhen asked in a 2006 interview with internetnews.com whether he considered patenting the wiki concept, he explained that he thought the idea "just sounded like something that no one would want to pay money for."[18]\n\nCunningham is interested in tracking the number and location of wiki page edits as a sociological experiment and may even consider the degradation of a wiki page as part of its process to stability. "There are those who give and those who take. You can tell by reading what they write."[19]\n\nIn 2011, Cunningham created Smallest Federated Wiki, a tool for wiki federation, which applies aspects of software development such as forking to wiki pages.\n\nCunningham has contributed to the practice of object-oriented programming, in particular the use of pattern languages and (with Kent Beck) the class-responsibility-collaboration cards. He also contributes to the extreme programming software development methodology. Much of this work was done collaboratively on the first wiki site.\n\nCunningham is credited with the idea: "The best way to get the right answer on the Internet is not to ask a question; it\'s to post the wrong answer."[20] This refers to the observation that people are quicker to correct a wrong answer than to answer a question. According to Steven McGeady, Cunningham advised him of this on a whim in the early 1980s, and McGeady dubbed this Cunningham\'s Law.[21] Although originally referring to interactions on Usenet, the law has been used to describe how other online communities work, such as Wikipedia.[22] Cunningham relativises his ownership of the law, calling it a "misquote that disproves itself by propagating through the internet" and by saying that he "never suggested asking questions by posting wrong answers".[23]\n\nCunningham lives in Beaverton, Oregon.[14] He holds an amateur radio extra class license issued by the Federal Communications Commission. His call sign is K9OX.[24][25][26][27]\n\nCunningham is Nike\'s first "Code for a Better World" Fellow.[28]',
        pageTitle: "Ward Cunningham",
    },
    {
        title: "Curie's law",
        link: "https://en.wikipedia.org/wiki/Curie%27s_law",
        content:
            "For many paramagnetic materials, the magnetization of the material is directly proportional to an applied magnetic field, for sufficiently high temperatures and small fields. However, if the material is heated, this proportionality is reduced. For a fixed value of the field, the magnetic susceptibility is inversely proportional to temperature, that is\n\nPierre Curie discovered this relation, now known as Curie's law, by fitting data from experiment. It only holds for high temperatures and weak magnetic fields. As the derivations below show, the magnetization saturates in the opposite limit of low temperatures and strong fields. If the Curie constant is null, other magnetic effects dominate, like Langevin diamagnetism or Van Vleck paramagnetism.\n\nA simple model of a paramagnet concentrates on the particles which compose it which do not interact with each other. Each particle has a magnetic moment given by \n  \n    \n      \n        \n          \n            \n              μ\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\mu }}}\n  \n. The energy of a magnetic moment in a magnetic field is given by\n\nwhere \n  \n    \n      \n        \n          B\n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        (\n        \n          H\n        \n        +\n        \n          M\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {B} =\\mu _{0}(\\mathbf {H} +\\mathbf {M} )}\n  \n is the magnetic field density, measured in teslas (T).\n\nTo simplify the calculation, we are going to work with a 2-state particle: it may either align its magnetic moment with the magnetic field or against it. So the only possible values of magnetic moment are then \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n and \n  \n    \n      \n        −\n        μ\n      \n    \n    {\\displaystyle -\\mu }\n  \n. If so, then such a particle has only two possible energies, \n  \n    \n      \n        −\n        μ\n        B\n      \n    \n    {\\displaystyle -\\mu B}\n  \n when it is aligned with the field and \n  \n    \n      \n        +\n        μ\n        B\n      \n    \n    {\\displaystyle +\\mu B}\n  \n when it is oriented opposite to the field.\n\nThe extent to which the magnetic moments are aligned with the field can be calculated from the partition function. For a single particle, this is\n\nThe partition function for a set of N such particles, if they do not interact with each other, is\n\nThe magnetization is the negative derivative of the free energy with respect to the applied field, and so the magnetization per unit volume is\n\nwhere n is the number density of magnetic moments.[1]: 117  The formula above is known as the Langevin paramagnetic equation.\nPierre Curie found an approximation to this law that applies to the relatively high temperatures and low magnetic fields used in his experiments. As temperature increases and magnetic field decreases, the argument of the hyperbolic tangent decreases. In the Curie regime,\n\nMoreover, if \n  \n    \n      \n        \n          |\n        \n        x\n        \n          |\n        \n        ≪\n        1\n      \n    \n    {\\displaystyle |x|\\ll 1}\n  \n, then\n\nso the magnetization is small, and we can write \n  \n    \n      \n        B\n        ≈\n        \n          μ\n          \n            0\n          \n        \n        H\n      \n    \n    {\\displaystyle B\\approx \\mu _{0}H}\n  \n, and thus\n\nIn this regime, the magnetic susceptibility given by\n\nwith a Curie constant given by \n  \n    \n      \n        C\n        =\n        \n          μ\n          \n            0\n          \n        \n        n\n        \n          μ\n          \n            2\n          \n        \n        \n          /\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle C=\\mu _{0}n\\mu ^{2}/k_{\\rm {B}}}\n  \n, in kelvins (K).[2]\n\nIn the regime of low temperatures or high fields, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n tends to a maximum value of \n  \n    \n      \n        n\n        μ\n      \n    \n    {\\displaystyle n\\mu }\n  \n, corresponding to all the particles being completely aligned with the field. Since this calculation doesn't describe the electrons embedded deep within the Fermi surface, forbidden by the Pauli exclusion principle to flip their spins, it does not exemplify the quantum statistics of the problem at low temperatures. Using the Fermi–Dirac distribution, one will find that at low temperatures \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is linearly dependent on the magnetic field, so that the magnetic susceptibility saturates to a constant.\n\nWhen the particles have an arbitrary spin (any number of spin states), the formula is a bit more complicated.\nAt low magnetic fields or high temperature, the spin follows Curie's law, with[3]\n\nwhere \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is the total angular momentum quantum number, and \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n is the g-factor (such that \n  \n    \n      \n        μ\n        =\n        g\n        J\n        \n          μ\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\mu =gJ\\mu _{\\text{B}}}\n  \n is the magnetic moment). For a two-level system with magnetic moment \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, the formula reduces to\n\n  \n    \n      \n        C\n        =\n        \n          \n            1\n            \n              k\n              \n                \n                  B\n                \n              \n            \n          \n        \n        n\n        \n          μ\n          \n            0\n          \n        \n        \n          μ\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle C={\\frac {1}{k_{\\rm {B}}}}n\\mu _{0}\\mu ^{2},}\n  \n\nas above, while the corresponding expressions in Gaussian units are\n\n  \n    \n      \n        C\n        =\n        \n          \n            \n              μ\n              \n                \n                  B\n                \n              \n              \n                2\n              \n            \n            \n              3\n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n            \n          \n        \n        n\n        \n          g\n          \n            2\n          \n        \n        J\n        (\n        J\n        +\n        1\n        )\n        ,\n      \n    \n    {\\displaystyle C={\\frac {\\mu _{\\rm {B}}^{2}}{3k_{\\rm {B}}}}ng^{2}J(J+1),}\n  \n\n\n  \n    \n      \n        C\n        =\n        \n          \n            1\n            \n              k\n              \n                \n                  B\n                \n              \n            \n          \n        \n        n\n        \n          μ\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle C={\\frac {1}{k_{\\rm {B}}}}n\\mu ^{2}.}\n\nFor this more general formula and its derivation (including high field, low temperature) see the article Brillouin function.\nAs the spin approaches infinity, the formula for the magnetization approaches the classical value derived in the following section.\n\nAn alternative treatment applies when the paramagnets are imagined to be classical, freely-rotating magnetic moments. In this case, their position will be determined by their angles in spherical coordinates, and the energy for one of them will be:\n\nwhere \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is the angle between the magnetic moment and\nthe magnetic field (which we take to be pointing in the \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n\ncoordinate.) The corresponding partition function is\n\nWe see there is no dependence on the \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n angle, and also we can\nchange variables to \n  \n    \n      \n        y\n        =\n        cos\n        ⁡\n        θ\n      \n    \n    {\\displaystyle y=\\cos \\theta }\n  \n to obtain\n\nNow, the expected value of the \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n component of the magnetization (the other two are seen to be null (due to integration over \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n), as they should) will be given by\n\nTo simplify the calculation, we see this can be written as a differentiation of \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  \n:\n\n(This approach can also be used for the model above, but the calculation was so simple this\nis not so helpful.)\n\nwhere \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the Langevin function:\n\nThis function would appear to be singular for small \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, but it is not, \nsince the two singular terms cancel each other. In fact, its behavior for small arguments is\n\n  \n    \n      \n        L\n        (\n        x\n        )\n        ≈\n        x\n        \n          /\n        \n        3\n      \n    \n    {\\displaystyle L(x)\\approx x/3}\n  \n, so the Curie limit also applies, but with a Curie constant\nthree times smaller in this case. Similarly, the function saturates at \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  \n for large values of its argument, and the opposite limit is likewise recovered.\n\nPierre Curie observed in 1895 that the magnetic susceptibility of oxygen is inversely proportional to temperature. Paul Langevin presented a classical derivation of this relationship ten years later.[4]",
        pageTitle: "Curie's law",
    },
    {
        title: "Curie-Weiss law",
        link: "https://en.wikipedia.org/wiki/Curie-Weiss_law",
        content:
            "In magnetism, the Curie–Weiss law describes the magnetic susceptibility  χ of a ferromagnet in the paramagnetic region above the Curie temperature:\n\nwhere  C is a material-specific Curie constant,  T is the absolute temperature, and  TC is the Curie temperature, both measured in kelvin. The law predicts a singularity in the susceptibility at  T = TC. Below this temperature, the ferromagnet has a spontaneous magnetization. The name is given after Pierre Curie and Pierre Weiss.\n\nA magnetic moment which is present even in the absence of the external magnetic field is called spontaneous magnetization. Materials with this property are known as ferromagnets, such as iron, nickel, and magnetite. However, when these materials are heated up, at a certain temperature they lose their spontaneous magnetization, and become paramagnetic. This threshold temperature below which a material is ferromagnetic is called the Curie temperature and is different for each material.\n\nThe Curie–Weiss law describes the changes in a material's magnetic susceptibility, \n  \n    \n      \n        χ\n      \n    \n    {\\displaystyle \\chi }\n  \n, near its Curie temperature. The magnetic susceptibility is the ratio between the material's magnetization and the applied magnetic field.\n\nIn many materials, the Curie–Weiss law fails to describe the susceptibility in the immediate vicinity of the Curie point, since it is based on a mean-field approximation. Instead, there is a critical behavior of the form\n\nwith the critical exponent  γ. However, at temperatures  T ≫ TC the expression of the Curie–Weiss law still holds true, but with  TC replaced by a temperature Θ that is somewhat higher than the actual Curie temperature. Some authors call  Θ the Weiss constant to distinguish it from the temperature of the actual Curie point.\n\nAccording to the Bohr–van Leeuwen theorem, when statistical mechanics and classical mechanics are applied consistently, the thermal average of the magnetization is always zero. Magnetism cannot be explained without quantum mechanics. That means that it can not be explained without taking into account that matter consists of atoms. Next are listed some semi-classical approaches to it, using a simple atom model, as they are easy to understand and relate to even though they are not perfectly correct.\n\nThe magnetic moment of a free atom is due to the orbital angular momentum and spin of its electrons and nucleus. When the atoms are such that their shells are completely filled, they do not have any net magnetic dipole moment in the absence of an external magnetic field. When present, such a field distorts the trajectories (classical concept) of the electrons so that the applied field could be opposed as predicted by the Lenz's law. In other words, the net magnetic dipole induced by the external field is in the opposite direction, and such materials are repelled by it. These are called diamagnetic materials.\n\nSometimes an atom has a net magnetic dipole moment even in the absence of an external magnetic field. The contributions of the individual electrons and nucleus to the total angular momentum do not cancel each other. This happens when the shells of the atoms are not fully filled up (Hund's Rule). A collection of such atoms however, may not have any net magnetic moment as these dipoles are not aligned. An external magnetic field may serve to align them to some extent and develop a net magnetic moment per volume. Such alignment is temperature dependent as thermal agitation acts to disorient the dipoles. Such materials are called paramagnetic.\n\nIn some materials, the atoms (with net magnetic dipole moments) can interact with each other to align themselves even in the absence of any external magnetic field when the thermal agitation is low enough. Alignment could be parallel (ferromagnetism) or anti-parallel. In the case of anti-parallel, the dipole moments may or may not cancel each other (antiferromagnetism, ferrimagnetism).\n\nWe take a very simple situation in which each atom can be approximated as a two state system. The thermal energy is so low that the atom is in the ground state. In this ground state, the atom is assumed to have no net orbital angular momentum but only one unpaired electron to give it a spin of the half. In the presence of an external magnetic field, the ground state will split into two states having an energy difference proportional to the applied field. The spin of the unpaired electron is parallel to the field in the higher energy state and anti-parallel in the lower one.\n\nA density matrix, \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n, is a matrix that describes a quantum system in a mixed state, a statistical ensemble of several quantum states (here several similar 2-state atoms). This should be contrasted with a single state vector that describes a quantum system in a pure state. The expectation value of a measurement, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, over the ensemble is \n  \n    \n      \n        ⟨\n        A\n        ⟩\n        =\n        Tr\n        ⁡\n        (\n        A\n        ρ\n        )\n      \n    \n    {\\displaystyle \\langle A\\rangle =\\operatorname {Tr} (A\\rho )}\n  \n. In terms of a complete set of states, \n  \n    \n      \n        \n          |\n        \n        i\n        ⟩\n      \n    \n    {\\displaystyle |i\\rangle }\n  \n, one can write\n\nVon Neumann's equation tells us how the density matrix evolves with time.\n\nIn equilibrium,\none has \n  \n    \n      \n        [\n        H\n        ,\n        ρ\n        ]\n        =\n        0\n      \n    \n    {\\displaystyle [H,\\rho ]=0}\n  \n, and the allowed density matrices are \n  \n    \n      \n        f\n        (\n        H\n        )\n      \n    \n    {\\displaystyle f(H)}\n  \n.\nThe canonical ensemble has \n  \n    \n      \n        ρ\n        =\n        exp\n        ⁡\n        (\n        −\n        H\n        \n          /\n        \n        T\n        )\n        \n          /\n        \n        Z\n      \n    \n    {\\displaystyle \\rho =\\exp(-H/T)/Z}\n  \n, where \n  \n    \n      \n        Z\n        =\n        Tr\n        ⁡\n        exp\n        ⁡\n        (\n        −\n        H\n        \n          /\n        \n        T\n        )\n      \n    \n    {\\displaystyle Z=\\operatorname {Tr} \\exp(-H/T)}\n  \n.\n\nFor the 2-state system, we can write\n\n  \n    \n      \n        H\n        =\n        −\n        γ\n        ℏ\n        B\n        \n          σ\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle H=-\\gamma \\hbar B\\sigma _{3}}\n  \n.\nHere \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is the gyromagnetic ratio.\nHence \n  \n    \n      \n        Z\n        =\n        2\n        cosh\n        ⁡\n        (\n        γ\n        ℏ\n        B\n        \n          /\n        \n        (\n        2\n        T\n        )\n        )\n      \n    \n    {\\displaystyle Z=2\\cosh(\\gamma \\hbar B/(2T))}\n  \n, and\n\nIn the presence of a uniform external magnetic field \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n along the z-direction, the Hamiltonian of the atom changes by\n\nwhere \n  \n    \n      \n        α\n        ,\n        β\n      \n    \n    {\\displaystyle \\alpha ,\\beta }\n  \n are positive real numbers which are independent of which atom we are looking at but depend on the mass and the charge of the electron. \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n corresponds to individual electrons of the atom.\n\nWe apply second order perturbation theory to this situation. This is justified by the fact that even for highest presently attainable field strengths, the shifts in the energy level due to \n  \n    \n      \n        Δ\n        H\n      \n    \n    {\\displaystyle \\Delta H}\n  \n is quite small w.r.t. atomic excitation energies. Degeneracy of the original Hamiltonian is handled by choosing a basis which diagonalizes \n  \n    \n      \n        Δ\n        H\n      \n    \n    {\\displaystyle \\Delta H}\n  \n in the degenerate subspaces. Let \n  \n    \n      \n        \n          |\n        \n        n\n        ⟩\n      \n    \n    {\\displaystyle |n\\rangle }\n  \n be such a basis for the state of the atom (rather the electrons in the atom). Let \n  \n    \n      \n        Δ\n        \n          E\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\Delta E_{n}}\n  \n be the change in energy in \n  \n    \n      \n        \n          |\n        \n        n\n        ⟩\n      \n    \n    {\\displaystyle |n\\rangle }\n  \n. So we get\n\nIn our case we can ignore \n  \n    \n      \n        \n          B\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle B^{3}}\n  \n and higher order terms. We get\n\nIn case of diamagnetic material, the first two terms are absent as they don't have any angular momentum in their ground state. In case of paramagnetic material all the three terms contribute.\n\nSo far, we have assumed that the atoms do not interact with each other. Even though this is a reasonable assumption in the case of diamagnetic and paramagnetic substances, this assumption fails in the case of ferromagnetism, where the spins of the atom try to align with each other to the extent permitted by the thermal agitation. In this case, we have to consider the Hamiltonian of the ensemble of the atom. Such a Hamiltonian will contain all the terms described above for individual atoms and terms corresponding to the interaction among the pairs of the atom. Ising model is one of the simplest approximations of such pairwise interaction.\n\nHere the two atoms of a pair are at \n  \n    \n      \n        R\n        ,\n        \n          R\n          ′\n        \n      \n    \n    {\\displaystyle R,R'}\n  \n. Their interaction \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is determined by their distance vector \n  \n    \n      \n        R\n        −\n        \n          R\n          ′\n        \n      \n    \n    {\\displaystyle R-R'}\n  \n. In order to simplify the calculation, it is often assumed that interaction happens between neighboring atoms only and \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is a constant. The effect of such interaction is often approximated as a mean field and, in our case, the Weiss field.\n\nThe Curie–Weiss law is an adapted version of Curie's law, which for a paramagnetic material may be written in SI units as follows,[1] assuming \n  \n    \n      \n        χ\n        ≪\n        1\n      \n    \n    {\\displaystyle \\chi \\ll 1}\n  \n:\n\n  \n    \n      \n        χ\n        =\n        \n          \n            M\n            H\n          \n        \n        ≈\n        \n          \n            \n              M\n              \n                μ\n                \n                  0\n                \n              \n            \n            B\n          \n        \n        =\n        \n          \n            C\n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle \\chi ={\\frac {M}{H}}\\approx {\\frac {M\\mu _{0}}{B}}={\\frac {C}{T}}.}\n\nHere μ0 is the permeability of free space;  M the magnetization (magnetic moment per unit volume), B = μ0H is the magnetic field, and C the material-specific Curie constant:\n\n  \n    \n      \n        C\n        =\n        \n          \n            \n              \n                μ\n                \n                  0\n                \n              \n              \n                μ\n                \n                  \n                    B\n                  \n                \n                \n                  2\n                \n              \n            \n            \n              3\n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n            \n          \n        \n        N\n        \n          g\n          \n            2\n          \n        \n        J\n        (\n        J\n        +\n        1\n        )\n        ,\n      \n    \n    {\\displaystyle C={\\frac {\\mu _{0}\\mu _{\\rm {B}}^{2}}{3k_{\\rm {B}}}}Ng^{2}J(J+1),}\n  \n\nwhere kB is the Boltzmann constant, N the number of magnetic atoms (or molecules) per unit volume, g the Landé g-factor, μB the Bohr magneton,  J the angular momentum quantum number.[2]\n\nFor the Curie-Weiss Law the total magnetic field is B + λM where λ is the Weiss molecular field constant and then\n\n  \n    \n      \n        χ\n        =\n        \n          \n            \n              M\n              \n                μ\n                \n                  0\n                \n              \n            \n            B\n          \n        \n        →\n        \n          \n            \n              M\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              B\n              +\n              λ\n              M\n            \n          \n        \n        =\n        \n          \n            C\n            T\n          \n        \n      \n    \n    {\\displaystyle \\chi ={\\frac {M\\mu _{0}}{B}}\\rightarrow {\\frac {M\\mu _{0}}{B+\\lambda M}}={\\frac {C}{T}}}\n  \n\nwhich can be rearranged to get\n\n  \n    \n      \n        χ\n        =\n        \n          \n            C\n            \n              T\n              −\n              \n                \n                  \n                    C\n                    λ\n                  \n                  \n                    μ\n                    \n                      0\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\chi ={\\frac {C}{T-{\\frac {C\\lambda }{\\mu _{0}}}}}}\n  \n\nwhich is the Curie-Weiss Law\n\n  \n    \n      \n        χ\n        =\n        \n          \n            C\n            \n              T\n              −\n              \n                T\n                \n                  \n                    C\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\chi ={\\frac {C}{T-T_{\\rm {C}}}}}\n  \n\nwhere the Curie temperature TC is\n\n  \n    \n      \n        \n          T\n          \n            \n              C\n            \n          \n        \n        =\n        \n          \n            \n              C\n              λ\n            \n            \n              μ\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle T_{\\rm {C}}={\\frac {C\\lambda }{\\mu _{0}}}}",
        pageTitle: "Curie–Weiss law",
    },
    {
        title: "Dahl's law",
        link: "https://en.wikipedia.org/wiki/Dahl%27s_law",
        content:
            "Dahl's law (German: das Dahlsche Gesetz[1]) is a sound rule in some of the Northeast Bantu languages that illustrates a case of voicing dissimilation. In the history of these languages, a voiceless stop, such as /p t k/, became voiced (/b d ɡ/) when immediately followed by a syllable with another voiceless stop. For example, Nyamwezi has -datu \"three\" where Swahili, a Bantu language that did not undergo Dahl's law, has -tatu, and Shambala has mgate \"bread\" where Swahili has mkate. Dahl's law is the reason for the name Gikuyu when the language prefix normally found in that language is ki- .\n\nThe law was named in 1903 by Carl Meinhof in his paper \"Das Dahlsche Gesetz\": in the paper, Meinhof explains that he named the rule after his pupil, the Moravian missionary Edmund Dahl, who reported it in 1897 when visiting the Wanyamwezi tribe in Urambo.[1][2] It is productive in Sukuma, in the Nyanyembe dialect of Nyamwezi, most E50 languages (such as Kikuyu, Embu and Meru) and some J languages (such as Rwanda, Gusii and Kuria). In other languages the law is no longer productive, but there are indications that it once was (such as in Taita, Kamba/Daisũ, Taveta and Luhya/Logooli). In some neighboring languages (and in other dialects of Nyamwezi) words reflecting Dahl's law are found, but they appear to be transfers from languages in which the law is productive.[3]\n\nDahl's law is often portrayed as the African equivalent of Grassmann's law in Indo-European languages.[2] However, an analogue of Grassmann's law (which is aspiration, not voicing, dissimilation) has taken place in the Bantu language Makhuwa, where it is called Katupha's law.\n\nThis phonology article is a stub. You can help Wikipedia by expanding it.\n\nThis Bantu language-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Dahl's law",
    },
    {
        title: "Dalton's law",
        link: "https://en.wikipedia.org/wiki/Dalton%27s_law",
        content:
            "Dalton's law (also called Dalton's law of partial pressures) states that in a mixture of non-reacting gases, the total  pressure exerted is equal to the sum of the partial pressures of the individual gases.[1] This empirical law was observed by John Dalton in 1801 and published in 1802.[2] Dalton's law is related to the ideal gas laws.\n\nMathematically, the pressure of a mixture of non-reactive gases can be defined as the summation:\n\n  \n    \n      \n        \n          p\n          \n            total\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            1\n          \n        \n        +\n        \n          p\n          \n            2\n          \n        \n        +\n        \n          p\n          \n            3\n          \n        \n        +\n        ⋯\n        +\n        \n          p\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{total}}=\\sum _{i=1}^{n}p_{i}=p_{1}+p_{2}+p_{3}+\\cdots +p_{n}}\n  \n\nwhere p1, p2, ..., pn represent the partial pressures of each component.[1]\n\np\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            total\n          \n        \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}=p_{\\text{total}}x_{i}}\n  \n\nwhere xi is the mole fraction of the ith component in the total mixture of n components .\n\nThe relationship below provides a way to determine the volume-based concentration of any individual gaseous component\n\n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            total\n          \n        \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}=p_{\\text{total}}c_{i}}\n  \n\nwhere ci is the concentration of component i.\n\nDalton's law is not strictly followed by real gases, with the deviation increasing with pressure. Under such conditions the volume occupied by the molecules becomes significant compared to the free space between them. In particular, the short average distances between molecules increases intermolecular forces between gas molecules enough to substantially change the pressure exerted by them, an effect not included in the ideal gas model.",
        pageTitle: "Dalton's law",
    },
    {
        title: "Dalton's law of partial pressure",
        link: "https://en.wikipedia.org/wiki/Dalton%27s_law_of_partial_pressure",
        content:
            "Dalton's law (also called Dalton's law of partial pressures) states that in a mixture of non-reacting gases, the total  pressure exerted is equal to the sum of the partial pressures of the individual gases.[1] This empirical law was observed by John Dalton in 1801 and published in 1802.[2] Dalton's law is related to the ideal gas laws.\n\nMathematically, the pressure of a mixture of non-reactive gases can be defined as the summation:\n\n  \n    \n      \n        \n          p\n          \n            total\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            1\n          \n        \n        +\n        \n          p\n          \n            2\n          \n        \n        +\n        \n          p\n          \n            3\n          \n        \n        +\n        ⋯\n        +\n        \n          p\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{total}}=\\sum _{i=1}^{n}p_{i}=p_{1}+p_{2}+p_{3}+\\cdots +p_{n}}\n  \n\nwhere p1, p2, ..., pn represent the partial pressures of each component.[1]\n\np\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            total\n          \n        \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}=p_{\\text{total}}x_{i}}\n  \n\nwhere xi is the mole fraction of the ith component in the total mixture of n components .\n\nThe relationship below provides a way to determine the volume-based concentration of any individual gaseous component\n\n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          p\n          \n            total\n          \n        \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}=p_{\\text{total}}c_{i}}\n  \n\nwhere ci is the concentration of component i.\n\nDalton's law is not strictly followed by real gases, with the deviation increasing with pressure. Under such conditions the volume occupied by the molecules becomes significant compared to the free space between them. In particular, the short average distances between molecules increases intermolecular forces between gas molecules enough to substantially change the pressure exerted by them, an effect not included in the ideal gas model.",
        pageTitle: "Dalton's law",
    },
    {
        title: "gas laws",
        link: "https://en.wikipedia.org/wiki/Gas_laws",
        content:
            "The laws describing the behaviour of gases under fixed pressure, volume, amount of gas, and absolute temperature conditions are called gas laws. The basic gas laws were discovered by the end of the 18th century when scientists found out that relationships between pressure, volume and temperature of a sample of gas could be obtained which would hold to approximation for all gases. The combination of several empirical gas laws led to the development of the ideal gas law.\n\nThe ideal gas law was later found to be consistent with atomic and kinetic theory.\n\nIn 1643, the Italian physicist and mathematician, Evangelista Torricelli, who for a few months had acted as Galileo Galilei's secretary, conducted a celebrated experiment in Florence.[1] He demonstrated that a column of mercury in an inverted tube can be supported by the pressure of air outside of the tube, with the creation of a small section of vacuum above the mercury.[2] This experiment essentially paved the way towards the invention of the barometer, as well as drawing the attention of Robert Boyle, then a \"skeptical\" scientist working in England. Boyle was inspired by Torricelli's experiment to investigate how the elasticity of air responds to varying pressure, and he did this through a series of experiments with a setup reminiscent of that used by Torricelli.[3] Boyle published his results in 1662.\n\nLater on, in 1676, the French physicist Edme Mariotte, independently arrived at the same conclusions of Boyle, while also noting some dependency of air volume on temperature.[4] However it took another century and a half for the development of thermometry and recognition of the absolute zero temperature scale, which eventually allowed the discovery of temperature-dependent gas laws.\n\nIn 1662, Robert Boyle systematically studied the relationship between the volume and pressure of a fixed amount of gas at a constant temperature. He observed that the volume of a given mass of a gas is inversely proportional to its pressure at a constant temperature.\nBoyle's law, published in 1662, states that, at a constant temperature, the product of the pressure and volume of a given mass of an ideal gas in a closed system is always constant. It can be verified experimentally using a pressure gauge and a variable volume container. It can also be derived from the kinetic theory of gases: if a container, with a fixed number of molecules inside, is reduced in volume, more molecules will strike a given area of the sides of the container per unit time, causing a greater pressure.\n\nThe concept can be represented with these formulae:\n\nP\n          \n            1\n          \n        \n        \n          V\n          \n            1\n          \n        \n        =\n        \n          P\n          \n            2\n          \n        \n        \n          V\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{1}V_{1}=P_{2}V_{2}}\n  \n\nwhere P is the pressure, V is the volume of a gas, and k1 is the constant in this equation (and is not the same as the proportionality constants in the other equations).\n\nCharles' law, or the law of volumes, was founded in 1787 by Jacques Charles. It states that, for a given mass of an ideal gas at constant pressure, the volume is directly proportional to its absolute temperature, assuming in a closed system.\nThe statement of Charles' law is as follows:\nthe volume (V) of a given mass of a gas, at constant pressure (P), is directly proportional to its temperature (T).\n\nwhere \"V\" is the volume of a gas, \"T\" is the absolute temperature and k2 is a proportionality constant (which is not the same as the proportionality constants in the other equations in this article).\n\nGay-Lussac's law, Amontons' law or the pressure law was founded by Joseph Louis Gay-Lussac in 1808.\n\nP\n              \n                1\n              \n            \n            \n              T\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              P\n              \n                2\n              \n            \n            \n              T\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {P_{1} \\over T_{1}}={P_{2} \\over T_{2}}}\n  \n,\n\nAvogadro's law, Avogadro's hypothesis, Avogadro's principle or Avogadro-Ampère's hypothesis is an experimental gas law which was hypothesized by Amedeo Avogadro in 1811. It related the volume of a gas to the amount of substance of gas present.[5]\n\nThis statement gives rise to the molar volume of a gas, which at STP (273.15 K, 1 atm) is about 22.4 L. The relation is given by:\n\nThe combined gas law or general gas equation is obtained by combining Boyle's law, Charles's law, and Gay-Lussac's law. It shows the relationship between the pressure, volume, and temperature for a fixed mass of gas:\n\nWith the addition of Avogadro's law, the combined gas law develops into the ideal gas law:\n\nThese equations are exact only for an ideal gas, which neglects various intermolecular effects (see real gas). However, the ideal gas law is a good approximation for most gases under moderate pressure and temperature.",
        pageTitle: "Gas laws",
    },
    {
        title: "Darcy's law",
        link: "https://en.wikipedia.org/wiki/Darcy%27s_law",
        content:
            "Darcy's law is an equation that describes the flow of a fluid through a porous medium and through a Hele-Shaw cell. The law was formulated by Henry Darcy based on results of experiments[1] on the flow of water through beds of sand, forming the basis of hydrogeology, a branch of earth sciences. It is analogous to Ohm's law in electrostatics, linearly relating the volume flow rate of the fluid to the hydraulic head difference (which is often just proportional to the pressure difference) via the hydraulic conductivity. In fact, the Darcy's law is a special case of the Stokes equation for the momentum flux, in turn deriving from the momentum Navier–Stokes equation.\n\nDarcy's law was first determined experimentally by Darcy, but has since been derived from the Navier–Stokes equations via homogenization methods.[2][3] It is analogous to Fourier's law in the field of heat conduction, Ohm's law in the field of electrical networks, and Fick's law in diffusion theory.\n\nOne application of Darcy's law is in the analysis of water flow through an aquifer; Darcy's law along with the equation of conservation of mass simplifies to the groundwater flow equation, one of the basic relationships of hydrogeology.\n\nMorris Muskat first[4] refined Darcy's equation for a single-phase flow by including viscosity in the single (fluid) phase equation of Darcy. It can be understood that viscous fluids have more difficulty permeating through a porous medium than less viscous fluids. This change made it suitable for researchers in the petroleum industry. Based on experimental results by his colleagues Wyckoff and Botset, Muskat and Meres also generalized Darcy's law to cover a multiphase flow of water, oil and gas in the porous medium of a petroleum reservoir. The generalized multiphase flow equations by Muskat and others provide the analytical foundation for reservoir engineering that exists to this day.\n\nIn the integral form, Darcy's law, as refined by Morris Muskat, in the absence of gravitational forces and in a homogeneously permeable medium, is given by a simple proportionality relationship between the volumetric flow rate \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, and the pressure drop \n  \n    \n      \n        Δ\n        p\n      \n    \n    {\\displaystyle \\Delta p}\n  \n  through a porous medium. The proportionality constant is linked to the permeability \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n of the medium, the dynamic viscosity of the fluid \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, the given distance \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n over which the pressure drop is computed, and the cross-sectional area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, in the form:\n\n  \n    \n      \n        Q\n        =\n        \n          \n            \n              k\n              A\n            \n            \n              μ\n              L\n            \n          \n        \n        Δ\n        p\n      \n    \n    {\\displaystyle Q={\\frac {kA}{\\mu L}}\\Delta p}\n\nR\n        =\n        \n          \n            \n              μ\n              L\n            \n            \n              k\n              A\n            \n          \n        \n      \n    \n    {\\displaystyle R={\\frac {\\mu L}{kA}}}\n\ncan be defined as the Darcy's law hydraulic resistance.\n\nThe Darcy's law can be generalised to a local form:\n\nwhere \n  \n    \n      \n        ∇\n        p\n      \n    \n    {\\displaystyle \\nabla p}\n  \n is the hydraulic gradient and \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n is the volumetric flux which here is called also superficial velocity.\nNote that the ratio:\n\nσ\n        =\n        \n          \n            k\n            μ\n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\frac {k}{\\mu }}}\n\ncan be thought as the Darcy's law hydraulic conductivity.\n\nIn the (less general) integral form, the volumetric flux and the pressure gradient correspond to the ratios:\n\nq\n        =\n        \n          \n            Q\n            A\n          \n        \n      \n    \n    {\\displaystyle q={\\frac {Q}{A}}}\n\n∇\n        p\n        =\n        \n          \n            \n              Δ\n              p\n            \n            L\n          \n        \n      \n    \n    {\\displaystyle \\nabla p={\\frac {\\Delta p}{L}}}\n  \n.\n\nIn case of an anisotropic porous media, the permeability is a second order tensor, and in tensor notation one can write the more general law:\n\nNotice that the quantity \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n, often referred to as the Darcy flux or Darcy velocity, is not the velocity at which the fluid is travelling through the pores. It is the specific discharge, or flux per unit area. The flow velocity (u) is related to the flux (q) by the porosity (φ) with the following equation:\n\nThe Darcy's constitutive equation, for single phase (fluid) flow, is the defining equation for absolute permeability (single phase permeability).\n\nWith reference to the diagram to the right, the flow velocity is in SI units \n  \n    \n      \n        \n          (\n          m\n          \n            /\n          \n          s\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m/s)} }\n  \n, and since the porosity φ is a nondimensional number, the Darcy flux \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n, or discharge per unit area, is also defined in units \n  \n    \n      \n        \n          (\n          m\n          \n            /\n          \n          s\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m/s)} }\n  \n; the permeability \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n in units \n  \n    \n      \n        \n          (\n          \n            m\n            \n              2\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m^{2})} }\n  \n, the dynamic viscosity \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n in units \n  \n    \n      \n        \n          (\n          P\n          a\n          ⋅\n          s\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(Pa\\cdot s)} }\n  \n and the hydraulic gradient is in units \n  \n    \n      \n        \n          (\n          P\n          a\n          \n            /\n          \n          m\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(Pa/m)} }\n  \n.\n\nIn the integral form, the total pressure drop \n  \n    \n      \n        Δ\n        p\n        =\n        \n          p\n          \n            b\n          \n        \n        −\n        \n          p\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle \\Delta p=p_{b}-p_{a}}\n  \n is in units \n  \n    \n      \n        \n          (\n          P\n          a\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(Pa)} }\n  \n, and \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the length of the sample in units \n  \n    \n      \n        \n          (\n          m\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m)} }\n  \n, the Darcy's volumetric flow rate \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, or discharge, is also defined in units \n  \n    \n      \n        \n          (\n          \n            m\n            \n              3\n            \n          \n          \n            /\n          \n          s\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m^{3}/s)} }\n  \nand the cross-sectional area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n in units \n  \n    \n      \n        \n          (\n          \n            m\n            \n              2\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m^{2})} }\n  \n. A number of these parameters are used in alternative definitions below. A negative sign is used in the definition of the flux following the standard physics convention that fluids flow from regions of high pressure to regions of low pressure. Note that the elevation head must be taken into account if the inlet and outlet are at different elevations. If the change in pressure is negative, then the flow will be in the positive x direction. There have been several proposals for a constitutive equation for absolute permeability, and the most famous one is probably the Kozeny equation (also called Kozeny–Carman equation).\n\nBy considering the relation for static fluid pressure (Stevin's law):\n\np\n        =\n        ρ\n        g\n        h\n      \n    \n    {\\displaystyle p=\\rho gh}\n  \n\none can decline the integral form also into the equation:\n\n  \n    \n      \n        Q\n        =\n        \n          \n            \n              k\n              A\n              g\n            \n            \n              ν\n              L\n            \n          \n        \n        \n        \n          Δ\n          h\n        \n      \n    \n    {\\displaystyle Q={\\frac {kAg}{\\nu L}}\\,{\\Delta h}}\n  \n\nwhere ν is the kinematic viscosity.\nThe corresponding hydraulic conductivity is therefore:\n\nDarcy's law is a simple mathematical statement which neatly summarizes several familiar properties that groundwater flowing in aquifers exhibits, including:\n\nA graphical illustration of the use of the steady-state groundwater flow equation (based on Darcy's law and the conservation of mass) is in the construction of flownets, to quantify the amount of groundwater flowing under a dam.\n\nDarcy's law is only valid for slow, viscous flow; however, most groundwater flow cases fall in this category. Typically any flow with a Reynolds number less than one is clearly laminar, and it would be valid to apply Darcy's law.  Experimental tests have shown that flow regimes with Reynolds numbers up to 10 may still be Darcian, as in the case of groundwater flow.  The Reynolds number (a dimensionless parameter) for porous media flow is typically expressed as\n\nwhere ν is the kinematic viscosity of water, q is the specific discharge (not the pore velocity — with units of length per time),  d is a representative grain diameter for the porous media (the standard choice is math|d30, which is the 30% passing size from a grain size analysis using sieves — with units of length).\n\nFor stationary, creeping, incompressible flow, i.e. .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠D(ρui)/Dt⁠ ≈ 0, the Navier–Stokes equation simplifies to the Stokes equation, which by neglecting the bulk term is:\n\nwhere μ is the viscosity, ui is the velocity in the i direction, and p is the pressure. Assuming the viscous resisting force is linear with the velocity we may write:\n\nwhere φ is the porosity, and kij is the second order permeability tensor. This gives the velocity in the n direction,\n\nwhich gives Darcy's law for the volumetric flux density in the n direction,\n\nIn isotropic porous media the off-diagonal elements in the permeability tensor are zero, kij = 0 for i ≠ j and the diagonal elements are identical, kii = k, and the common form is obtained as below, which enables the determination of the liquid flow velocity by solving a set of equations in a given region.[5]\n\nThe above equation is a governing equation for single-phase fluid flow in a porous medium.\n\nAnother derivation of Darcy's law is used extensively in petroleum engineering to determine the flow through permeable media — the most simple of which is for a one-dimensional, homogeneous rock formation with a single fluid phase and constant fluid viscosity.\n\nAlmost all oil reservoirs have a water zone below the oil leg, and some also have a gas cap above the oil leg. When the reservoir pressure drops due to oil production, water flows into the oil zone from below, and gas flows into the oil zone from above (if the gas cap exists), and we get a simultaneous flow and immiscible mixing of all fluid phases in the oil zone. The oil field operator may also inject water (or gas) to improve oil production. The petroleum industry is, therefore, using a generalized Darcy equation for multiphase flow developed by Muskat et alios. Because Darcy's name is so widespread and strongly associated with flow in porous media, the multiphase equation is denoted Darcy's law for multiphase flow or generalized Darcy equation (or law) or simply Darcy's equation (or law) or flow equation if the context says that the text is discussing the multiphase equation of Muskat. Multiphase flow in oil and gas reservoirs is a comprehensive topic, and one of many articles about this topic is Darcy's law for multiphase flow.\n\nA number of papers have utilized Darcy's law to model the physics of brewing in a moka pot, specifically how the hot water percolates through the coffee grinds under pressure, starting with a 2001 paper by Varlamov and Balestrino,[6] and continuing with a 2007 paper by Gianino,[7] a 2008 paper by Navarini et al.,[8] and a 2008 paper by W. King.[9] The papers will either take the coffee permeability to be constant as a simplification or will measure change through the brewing process.\n\nwhere q is the volume flux vector of the fluid at a particular point in the medium, h is the total hydraulic head, and K is the hydraulic conductivity tensor, at that point. The hydraulic conductivity can often be approximated as a scalar. (Note the analogy to Ohm's law in electrostatics. The flux vector is analogous to the current density, head is analogous to voltage, and hydraulic conductivity is analogous to electrical conductivity.)\n\nFor flows in porous media with Reynolds numbers greater than about 1 to 10, inertial effects can also become significant. Sometimes an inertial term is added to the Darcy's equation, known as Forchheimer term. This term is able to account for the non-linear behavior of the pressure difference vs flow data.[10]\n\nwhere the additional term k1 is known as inertial permeability, in units of length \n  \n    \n      \n        \n          (\n          m\n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {(m)} }\n  \n.\n\nThe flow in the middle of a sandstone reservoir is so slow that Forchheimer's equation is usually not needed, but the gas flow into a gas production well may be high enough to justify using it. In this case, the inflow performance calculations for the well, not the grid cell of the 3D model, are based on the Forchheimer equation. The effect of this is that an additional rate-dependent skin appears in the inflow performance formula.\n\nSome carbonate reservoirs have many fractures, and Darcy's equation for multiphase flow is generalized in order to govern both flow in fractures and flow in the matrix (i.e. the traditional porous rock). The irregular surface of the fracture walls and high flow rate in the fractures may justify the use of Forchheimer's equation.\n\nFor gas flow in small characteristic dimensions (e.g., very fine sand, nanoporous structures etc.), the particle-wall interactions become more frequent, giving rise to additional wall friction (Knudsen friction). For a flow in this region, where both viscous and Knudsen friction are present, a new formulation needs to be used. Knudsen presented a semi-empirical model for flow in transition regime based on his experiments on small capillaries.[11][12] For a porous medium, the Knudsen equation can be given as[12]\n\nwhere N is the molar flux, R is the gas constant, T is the temperature, DeffK is the effective Knudsen diffusivity of the porous media. The model can also be derived from the first-principle-based binary friction model (BFM).[13][14] The differential equation of transition flow in porous media based on BFM is given as[13]\n\nThis equation is valid for capillaries as well as porous media. The terminology of the Knudsen effect and Knudsen diffusivity is more common in mechanical and chemical engineering. In geological and petrochemical engineering, this effect is known as the Klinkenberg effect. Using the definition of molar flux, the above equation can be rewritten as\n\nThis equation can be rearranged into the following equation\n\nComparing this equation with conventional Darcy's law, a new formulation can be given as\n\nThis is equivalent to the effective permeability formulation proposed by Klinkenberg:[15]\n\nwhere b is known as the Klinkenberg parameter, which depends on the gas and the porous medium structure. This is quite evident if we compare the above formulations. The Klinkenberg parameter b is dependent on permeability, Knudsen diffusivity and viscosity (i.e., both gas and porous medium properties).\n\nFor very short time scales, a time derivative of flux may be added to Darcy's law, which results in valid solutions at very small times (in heat transfer, this is called the modified form of Fourier's law),\n\nwhere τ is a very small time constant which causes this equation to reduce to the normal form of Darcy's law at \"normal\" times (> nanoseconds). The main reason for doing this is that the regular groundwater flow equation (diffusion equation) leads to singularities at constant head boundaries at very small times. This form is more mathematically rigorous but leads to a hyperbolic groundwater flow equation, which is more difficult to solve and is only useful at very small times, typically out of the realm of practical use.\n\nAnother extension to the traditional form of Darcy's law is the Brinkman term, which is used to account for transitional flow between boundaries (introduced by Brinkman in 1949[16]),\n\nwhere β is an effective viscosity term.  This correction term accounts for flow through medium where the grains of the media are porous themselves, but is difficult to use, and is typically neglected.\n\nDarcy's law is valid for laminar flow through sediments. In fine-grained sediments, the dimensions of interstices are small; thus, the flow is laminar. Coarse-grained sediments also behave similarly, but in very coarse-grained sediments, the flow may be turbulent.[17] Hence Darcy's law is not always valid in such sediments.\nFor flow through commercial circular pipes, the flow is laminar when the Reynolds number is less than 2000 and turbulent when it is more than 4000, but in some sediments, it has been found that flow is laminar when the value of the Reynolds number is less than 1.[18]",
        pageTitle: "Darcy's law",
    },
    {
        title: "Davis's law",
        link: "https://en.wikipedia.org/wiki/Davis%27s_law",
        content:
            "Davis's law is used in anatomy and physiology to describe how soft tissue models along imposed demands. It is similar to Wolff's law, which applies to osseous tissue. It is a physiological principle stating that soft tissue heal according to the manner in which they are mechanically stressed.[1]\n\nIt is also an application of the Mechanostat model of Harold Frost which was originally developed to describe the adaptational response of bones; however – as outlined by Harold Frost himself – it also applies to fibrous collagenous connective tissues, such as ligaments, tendons and fascia.[2][3] The \"stretch-hypertrophy rule\" of that model states: \"Intermittent stretch causes collagenous tissues to hypertrophy until the resulting increase in strength reduces elongation in tension to some minimum level\".[4] Similar to the behavior of bony tissues this adaptational response occurs only if the mechanical strain exceeds a certain threshold value. Harold Frost proposed that for dense collagenous connective tissues the related threshold values are around 23 Newton/mm2  or  4% strain elongation.[5]\n\nThe term Davis's law is named after Henry Gassett Davis, an American orthopedic surgeon known for his work in developing traction methods. Its earliest known appearance is in John Joseph Nutt's 1913 book Diseases and Deformities of the Foot, where Nutt outlines the law by quoting a passage from Davis's 1867 book, Conservative Surgery:\n\nDavis's writing on the subject exposes a long chain of competing theories on the subject of soft tissue contracture and the causes of scoliosis. Davis's comments in Conservative Surgery were in the form of a sharp rebuke of lectures published by Louis Bauer of the Brooklyn Medical and Surgical Institute in 1862.[7] In his writing, Bauer claimed that \"a contraction of ligaments is a physiological impossibility\".[8] Bauer sided with work published in 1851 by Julius Konrad Werner, director of the Orthopedic Institute of Konigsberg, Prussia. Bauer and Werner, in turn, were contradicting research published by Jacques Mathieu Delpech in 1823.[9][10]\n\nTendons are soft tissue structures that respond to changes in mechanical loading. Bulk mechanical properties, such as modulus, failure strain, and ultimate tensile strength, decrease over long periods of disuse as a result of micro-structural changes on the collagen fiber level. In micro-gravity simulations, human test subjects can experience gastrocnemius tendon strength loss of up to 58% over a 90-day period.[11]\nTest subjects who were allowed to engage in resistance training displayed a smaller magnitude of tendon strength loss in the same micro-gravity environment, but modulus strength decrease was still significant.\n\nConversely, tendons that have lost their original strength due to extended periods of inactivity can regain most of their mechanical properties through gradual re-loading of the tendon,[12] due to the tendon's response to mechanical loading. Biological signaling events initiate re-growth at the site, while mechanical stimuli further promote rebuilding. This 6-8 week process results in an increase of the tendon's mechanical properties until it recovers its original strength.[13]\nHowever, excessive loading during the recovery process may lead to material failure, i.e. partial tears or complete rupture. Additionally, studies show that tendons have a maximum modulus of approximately 800 MPa; thus, any additional loading will not result in a significant increase in modulus strength.[12] These results may change current physical therapy practices, since aggressive training of the tendon does not strengthen the structure beyond its baseline mechanical properties; therefore, patients are still as susceptible to tendon overuse and injuries.",
        pageTitle: "Davis's law",
    },
    {
        title: "Wolff's law",
        link: "https://en.wikipedia.org/wiki/Wolff%27s_law",
        content:
            "Wolff's law, developed by the German anatomist and surgeon Julius Wolff (1836–1902) in the 19th century, states that bone in a healthy animal will adapt to the loads under which it is placed.[1]  If loading on a particular bone increases, the bone will remodel itself over time to become stronger to resist that sort of loading.[2][3] The internal architecture of the trabeculae undergoes adaptive changes, followed by secondary changes to the external cortical portion of the bone,[4] perhaps becoming thicker as a result. The inverse is true as well: if the loading on a bone decreases, the bone will become less dense and weaker due to the lack of the stimulus required for continued remodeling.[5] This reduction in bone density (osteopenia) is known as stress shielding and can occur as a result of a hip replacement (or other prosthesis).[citation needed]  The normal stress on a bone is shielded from that bone by being placed on a prosthetic implant.\n\nThe remodeling of bone in response to loading is achieved via mechanotransduction, a process through which forces or other mechanical signals are converted to biochemical signals in cellular signaling.[6] Mechanotransduction leading to bone remodeling involves the steps of mechanocoupling, biochemical coupling, signal transmission, and cell response.[7] The specific effects on bone structure depend on the duration, magnitude, and rate of loading, and it has been found that only cyclic loading can induce bone formation.[7] When loaded, fluid flows away from areas of high compressive loading in the bone matrix.[8] Osteocytes are the most abundant cells in bone and are also the most sensitive to such fluid flow caused by mechanical loading.[6] Upon sensing a load, osteocytes regulate bone remodeling by signaling to other cells with signaling molecules or direct contact.[9] Additionally, osteoprogenitor cells, which may differentiate into osteoblasts or osteoclasts, are also mechanosensors and will differentiate depending on the loading condition.[9]\n\nComputational models suggest that mechanical feedback loops can stably regulate bone remodeling by reorienting trabeculae in the direction of the mechanical loads.[10]",
        pageTitle: "Wolff's law",
    },
    {
        title: "De Morgan's laws",
        link: "https://en.wikipedia.org/wiki/De_Morgan%27s_laws",
        content:
            'In propositional logic and Boolean algebra, De Morgan\'s laws,[1][2][3] also known as De Morgan\'s theorem,[4] are a pair of transformation rules that are both valid rules of inference. They are named after Augustus De Morgan, a 19th-century British mathematician. The rules allow the expression of conjunctions and disjunctions purely in terms of each other via negation.\n\nwhere "A or B" is an "inclusive or" meaning at least one of A or B rather than an "exclusive or" that means exactly one of A or B.\n\nAnother form of De Morgan\'s law is the following as seen below.\n\nApplications of the rules include simplification of logical expressions in computer programs and digital circuit designs. De Morgan\'s laws are an example of a more general concept of mathematical duality.\n\nThe negation of conjunction rule may be written in sequent notation:\n\nThe negation of disjunction rule may be written as:\n\n¬\n              (\n              P\n              ∧\n              Q\n              )\n            \n            \n              ∴\n              ¬\n              P\n              ∨\n              ¬\n              Q\n            \n          \n        \n        \n        \n          \n            \n              ¬\n              P\n              ∨\n              ¬\n              Q\n            \n            \n              ∴\n              ¬\n              (\n              P\n              ∧\n              Q\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\neg (P\\land Q)}{\\therefore \\neg P\\lor \\neg Q}}\\qquad {\\frac {\\neg P\\lor \\neg Q}{\\therefore \\neg (P\\land Q)}}}\n\n¬\n              (\n              P\n              ∨\n              Q\n              )\n            \n            \n              ∴\n              ¬\n              P\n              ∧\n              ¬\n              Q\n            \n          \n        \n        \n        \n          \n            \n              ¬\n              P\n              ∧\n              ¬\n              Q\n            \n            \n              ∴\n              ¬\n              (\n              P\n              ∨\n              Q\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\neg (P\\lor Q)}{\\therefore \\neg P\\land \\neg Q}}\\qquad {\\frac {\\neg P\\land \\neg Q}{\\therefore \\neg (P\\lor Q)}}}\n\nand expressed as truth-functional tautologies or theorems of propositional logic:\n\nwhere \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n and \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n are propositions expressed in some formal system.\n\nThe generalized De Morgan\'s laws provide an equivalence for negating a conjunction or disjunction involving multiple terms.For a set of propositions \n  \n    \n      \n        \n          P\n          \n            1\n          \n        \n        ,\n        \n          P\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          P\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle P_{1},P_{2},\\dots ,P_{n}}\n  \n, the generalized De Morgan\'s Laws are as follows:\n\nThese laws generalize De Morgan\'s original laws for negating conjunctions and disjunctions.\n\nDe Morgan\'s laws are normally shown in the compact form above, with the negation of the output on the left and negation of the inputs on the right. A clearer form for substitution can be stated as:\n\nThis emphasizes the need to invert both the inputs and the output, as well as change the operator when doing a substitution.\n\nIn set theory, it is often stated as "union and intersection interchange under complementation",[5] which can be formally expressed as:\n\nwhere I is some, possibly countably or uncountably infinite, indexing set.\n\nIn set notation, De Morgan\'s laws can be remembered using the mnemonic "break the line, change the sign".[6]\n\nIn Boolean algebra, similarly, this law which can be formally expressed as:\n\nIn electrical and computer engineering, De Morgan\'s laws are commonly written as:\n\nDe Morgan\'s laws commonly apply to text searching using Boolean operators AND, OR, and NOT. Consider a set of documents containing the words "cats" and "dogs". De Morgan\'s laws hold that these two searches will return the same set of documents:\n\nThe corpus of documents containing "cats" or "dogs" can be represented by four documents:\n\nTo evaluate Search A, clearly the search "(cats OR dogs)" will hit on Documents 1, 2, and 3. So the negation of that search (which is Search A) will hit everything else, which is Document 4.\n\nEvaluating Search B, the search "(NOT cats)" will hit on documents that do not contain "cats", which is Documents 2 and 4. Similarly the search "(NOT dogs)" will hit on Documents 1 and 4. Applying the AND operator to these two searches (which is Search B) will hit on the documents that are common to these two searches, which is Document 4.\n\nA similar evaluation can be applied to show that the following two searches will both return Documents 1, 2, and 4:\n\nThe laws are named after Augustus De Morgan (1806–1871),[7] who introduced a formal version of the laws to classical propositional logic. De Morgan\'s formulation was influenced by the algebraization of logic undertaken by George Boole, which later cemented De Morgan\'s claim to the find. Nevertheless, a similar observation was made by Aristotle, and was known to Greek and Medieval logicians.[8] For example, in the 14th century, William of Ockham wrote down the words that would result by reading the laws out.[9] Jean Buridan, in his Summulae de Dialectica, also describes rules of conversion that follow the lines of De Morgan\'s laws.[10] Still, De Morgan is given credit for stating the laws in the terms of modern formal logic, and incorporating them into the language of logic. De Morgan\'s laws can be proved easily, and may even seem trivial.[11] Nonetheless, these laws are helpful in making valid inferences in proofs and deductive arguments.\n\nDe Morgan\'s theorem may be applied to the negation of a disjunction or the negation of a conjunction in all or part of a formula.\n\nIn the case of its application to a disjunction, consider the following claim: "it is false that either of A or B is true", which is written as:\n\nIn that it has been established that neither A nor B is true, then it must follow that both A is not true and B is not true, which may be written directly as:\n\nIf either A or B were true, then the disjunction of A and B would be true, making its negation false. Presented in English, this follows the logic that "since two things are both false, it is also false that either of them is true".\n\nWorking in the opposite direction, the second expression asserts that A is false and B is false (or equivalently that "not A" and "not B" are true). Knowing this, a disjunction of A and B must be false also. The negation of said disjunction must thus be true, and the result is identical to the first claim.\n\nThe application of De Morgan\'s theorem to conjunction is very similar to its application to a disjunction both in form and rationale. Consider the following claim: "it is false that A and B are both true", which is written as:\n\nIn order for this claim to be true, either or both of A or B must be false, for if they both were true, then the conjunction of A and B would be true, making its negation false. Thus, one (at least) or more of A and B must be false (or equivalently, one or more of "not A" and "not B" must be true). This may be written directly as,\n\nPresented in a natural language like English, it is expressed as "since it is false that two things are both true, at least one of them must be false".\n\nWorking in the opposite direction again, the second expression asserts that at least one of "not A" and "not B" must be true, or equivalently that at least one of A and B must be false. Since at least one of them must be false, then their conjunction would likewise be false. Negating said conjunction thus results in a true expression, and this expression is identical to the first claim.\n\nHere we use \n  \n    \n      \n        \n          \n            A\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A}}}\n  \n to denote the complement of A, as above in § Set theory and Boolean algebra. The proof that \n  \n    \n      \n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        =\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cap B}}={\\overline {A}}\\cup {\\overline {B}}}\n  \n is completed in 2 steps by proving both \n  \n    \n      \n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        ⊆\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cap B}}\\subseteq {\\overline {A}}\\cup {\\overline {B}}}\n  \n and \n  \n    \n      \n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n        ⊆\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A}}\\cup {\\overline {B}}\\subseteq {\\overline {A\\cap B}}}\n  \n.\n\nLet \n  \n    \n      \n        x\n        ∈\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A\\cap B}}}\n  \n. Then, \n  \n    \n      \n        x\n        ∉\n        A\n        ∩\n        B\n      \n    \n    {\\displaystyle x\\not \\in A\\cap B}\n  \n.\n\nBecause \n  \n    \n      \n        A\n        ∩\n        B\n        =\n        {\n        \n        y\n         \n        \n          |\n        \n         \n        y\n        ∈\n        A\n        ∧\n        y\n        ∈\n        B\n        \n        }\n      \n    \n    {\\displaystyle A\\cap B=\\{\\,y\\ |\\ y\\in A\\wedge y\\in B\\,\\}}\n  \n, it must be the case that \n  \n    \n      \n        x\n        ∉\n        A\n      \n    \n    {\\displaystyle x\\not \\in A}\n  \n or \n  \n    \n      \n        x\n        ∉\n        B\n      \n    \n    {\\displaystyle x\\not \\in B}\n  \n.\n\nIf \n  \n    \n      \n        x\n        ∉\n        A\n      \n    \n    {\\displaystyle x\\not \\in A}\n  \n, then \n  \n    \n      \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A}}}\n  \n, so \n  \n    \n      \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A}}\\cup {\\overline {B}}}\n  \n.\n\nSimilarly, if \n  \n    \n      \n        x\n        ∉\n        B\n      \n    \n    {\\displaystyle x\\not \\in B}\n  \n, then \n  \n    \n      \n        x\n        ∈\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {B}}}\n  \n, so \n  \n    \n      \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A}}\\cup {\\overline {B}}}\n  \n.\n\nThus, \n  \n    \n      \n        ∀\n        x\n        \n          \n            (\n          \n        \n        x\n        ∈\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        \n        ⟹\n        \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle \\forall x{\\Big (}x\\in {\\overline {A\\cap B}}\\implies x\\in {\\overline {A}}\\cup {\\overline {B}}{\\Big )}}\n  \n;\n\nthat is, \n  \n    \n      \n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        ⊆\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cap B}}\\subseteq {\\overline {A}}\\cup {\\overline {B}}}\n  \n.\n\nTo prove the reverse direction, let \n  \n    \n      \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A}}\\cup {\\overline {B}}}\n  \n, and for contradiction assume \n  \n    \n      \n        x\n        ∉\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\not \\in {\\overline {A\\cap B}}}\n  \n.\n\nUnder that assumption, it must be the case that \n  \n    \n      \n        x\n        ∈\n        A\n        ∩\n        B\n      \n    \n    {\\displaystyle x\\in A\\cap B}\n  \n,\n\nso it follows that \n  \n    \n      \n        x\n        ∈\n        A\n      \n    \n    {\\displaystyle x\\in A}\n  \n and \n  \n    \n      \n        x\n        ∈\n        B\n      \n    \n    {\\displaystyle x\\in B}\n  \n, and thus \n  \n    \n      \n        x\n        ∉\n        \n          \n            A\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\not \\in {\\overline {A}}}\n  \n and \n  \n    \n      \n        x\n        ∉\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\not \\in {\\overline {B}}}\n  \n.\n\nHowever, that means \n  \n    \n      \n        x\n        ∉\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\not \\in {\\overline {A}}\\cup {\\overline {B}}}\n  \n, in contradiction to the hypothesis that \n  \n    \n      \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A}}\\cup {\\overline {B}}}\n  \n,\n\ntherefore, the assumption \n  \n    \n      \n        x\n        ∉\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\not \\in {\\overline {A\\cap B}}}\n  \n must not be the case, meaning that \n  \n    \n      \n        x\n        ∈\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle x\\in {\\overline {A\\cap B}}}\n  \n.\n\nHence, \n  \n    \n      \n        ∀\n        x\n        \n          \n            (\n          \n        \n        x\n        ∈\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n        \n        ⟹\n        \n        x\n        ∈\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle \\forall x{\\Big (}x\\in {\\overline {A}}\\cup {\\overline {B}}\\implies x\\in {\\overline {A\\cap B}}{\\Big )}}\n  \n,\n\nthat is, \n  \n    \n      \n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n        ⊆\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A}}\\cup {\\overline {B}}\\subseteq {\\overline {A\\cap B}}}\n  \n.\n\nIf \n  \n    \n      \n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n        ⊆\n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A}}\\cup {\\overline {B}}\\subseteq {\\overline {A\\cap B}}}\n  \n and \n  \n    \n      \n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        ⊆\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cap B}}\\subseteq {\\overline {A}}\\cup {\\overline {B}}}\n  \n, then \n  \n    \n      \n        \n          \n            \n              A\n              ∩\n              B\n            \n            ¯\n          \n        \n        =\n        \n          \n            A\n            ¯\n          \n        \n        ∪\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cap B}}={\\overline {A}}\\cup {\\overline {B}}}\n  \n; this concludes the proof of De Morgan\'s law.\n\nThe other De Morgan\'s law, \n  \n    \n      \n        \n          \n            \n              A\n              ∪\n              B\n            \n            ¯\n          \n        \n        =\n        \n          \n            A\n            ¯\n          \n        \n        ∩\n        \n          \n            B\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {A\\cup B}}={\\overline {A}}\\cap {\\overline {B}}}\n  \n, is proven similarly.\n\nIn extensions of classical propositional logic, the duality still holds (that is, to any logical operator one can always find its dual), since in the presence of the identities governing negation, one may always introduce an operator that is the De Morgan dual of another. This leads to an important property of logics based on classical logic, namely the existence of negation normal forms: any formula is equivalent to another formula where negations only occur applied to the non-logical atoms of the formula. The existence of negation normal forms drives many applications, for example in digital circuit design, where it is used to manipulate the types of logic gates, and in formal logic, where it is needed to find the conjunctive normal form and disjunctive normal form of a formula. Computer programmers use them to simplify or properly negate complicated logical conditions. They are also often useful in computations in elementary probability theory.\n\nLet one define the dual of any propositional operator P(p, q, ...) depending on elementary propositions p, q, ... to be the operator \n  \n    \n      \n        \n          \n            \n              P\n            \n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{P}}^{d}}\n  \n defined by\n\nThis duality can be generalised to quantifiers, so for example the universal quantifier and existential quantifier are duals:\n\nTo relate these quantifier dualities to the De Morgan laws, consider a domain of discourse D (with some small number of entities) to which properties are ascribed universally and existentially, such as\n\nThen express universal quantifier equivalently by conjunction of individual statements\n\nand existential quantifier by disjunction of individual statements\n\nThen, the quantifier dualities can be extended further to modal logic, relating the box ("necessarily") and diamond ("possibly") operators:\n\nIn its application to the alethic modalities of possibility and necessity, Aristotle observed this case, and in the case of normal modal logic, the relationship of these modal operators to the quantification can be understood by setting up models using Kripke semantics.\n\nThree out of the four implications of de Morgan\'s laws hold in intuitionistic logic. Specifically, we have\n\nThe converse of the last implication does not hold in pure intuitionistic logic. That is, the failure of the joint proposition \n  \n    \n      \n        P\n        ∧\n        Q\n      \n    \n    {\\displaystyle P\\land Q}\n  \n cannot necessarily be resolved to the failure of either of the two conjuncts. For example, from knowing it not to be the case that both Alice and Bob showed up to their date, it does not follow who did not show up. The latter principle is equivalent to the principle of the weak excluded middle \n  \n    \n      \n        \n          \n            W\n            P\n            E\n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathrm {WPEM} }}\n  \n,\n\nThis weak form can be used as a foundation for an intermediate logic.\nFor a refined version of the failing law concerning existential statements, see the lesser limited principle of omniscience \n  \n    \n      \n        \n          \n            L\n            L\n            P\n            O\n          \n        \n      \n    \n    {\\displaystyle {\\mathrm {LLPO} }}\n  \n, which however is different from \n  \n    \n      \n        \n          \n            W\n            L\n            P\n            O\n          \n        \n      \n    \n    {\\displaystyle {\\mathrm {WLPO} }}\n  \n.\n\nThe validity of the other three De Morgan\'s laws remains true if negation \n  \n    \n      \n        ¬\n        P\n      \n    \n    {\\displaystyle \\neg P}\n  \n is replaced by implication \n  \n    \n      \n        P\n        →\n        C\n      \n    \n    {\\displaystyle P\\to C}\n  \n for some arbitrary constant predicate C, meaning that the above laws are still true in minimal logic.\n\nare tautologies even in minimal logic with negation replaced with implying a fixed \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, while the converse of the last law does not have to be true in general.\n\nbut their inversion implies excluded middle, \n  \n    \n      \n        \n          \n            P\n            E\n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathrm {PEM} }}\n  \n.',
        pageTitle: "De Morgan's laws",
    },
    {
        title: "Dermott's law",
        link: "https://en.wikipedia.org/wiki/Dermott%27s_law",
        content:
            "Dermott's law is an empirical formula for the orbital period of major satellites orbiting planets in the Solar System. It was identified by the celestial mechanics researcher Stanley Dermott in the 1960s and takes the form:\n\nfor \n  \n    \n      \n        n\n        =\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n        …\n      \n    \n    {\\displaystyle n=1,2,3,4\\ldots }\n\nWhere T(n) is the orbital period of the nth satellite, T(0) is of the order of days and C is a constant of the satellite system in question. Specific values are:\n\nSuch power-laws may be a consequence of collapsing-cloud models of planetary and satellite systems possessing various symmetries; see Titius-Bode law. They may also reflect the effect of resonance-driven commensurabilities in the various systems.\n\nThis astronomy-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Dermott's law",
    },
    {
        title: "De Vaucouleurs' law",
        link: "https://en.wikipedia.org/wiki/De_Vaucouleurs%27_law",
        content:
            "In astronomy, de Vaucouleurs's law, also known as the de Vaucouleurs profile or de Vaucouleurs model, describes how the surface brightness \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n of an elliptical galaxy varies as a function of apparent distance \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n from the center of the galaxy:[1]\n\n  \n    \n      \n        ln\n        ⁡\n        I\n        (\n        R\n        )\n        =\n        ln\n        ⁡\n        \n          I\n          \n            0\n          \n        \n        −\n        k\n        \n          R\n          \n            1\n            \n              /\n            \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle \\ln I(R)=\\ln I_{0}-kR^{1/4}.}\n\nBy defining Re as the radius of the isophote containing half of the total luminosity of the galaxy, the half-light radius, de Vaucouleurs profile may be expressed as:\n\n  \n    \n      \n        ln\n        ⁡\n        I\n        (\n        R\n        )\n        =\n        ln\n        ⁡\n        \n          I\n          \n            e\n          \n        \n        +\n        7.669\n        \n          [\n          \n            1\n            −\n            \n              \n                (\n                \n                  \n                    R\n                    \n                      R\n                      \n                        e\n                      \n                    \n                  \n                \n                )\n              \n              \n                1\n                \n                  /\n                \n                4\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\ln I(R)=\\ln I_{e}+7.669\\left[1-\\left({\\frac {R}{R_{e}}}\\right)^{1/4}\\right]}\n  \n\nor\n\n  \n    \n      \n        I\n        (\n        R\n        )\n        =\n        \n          I\n          \n            e\n          \n        \n        \n          e\n          \n            −\n            7.669\n            \n              [\n              \n                \n                  \n                    (\n                    \n                      \n                        R\n                        \n                          R\n                          \n                            e\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    1\n                    \n                      /\n                    \n                    4\n                  \n                \n                −\n                1\n              \n              ]\n            \n          \n        \n      \n    \n    {\\displaystyle I(R)=I_{e}e^{-7.669\\left[\\left({\\frac {R}{R_{e}}}\\right)^{1/4}-1\\right]}}\n  \n\nwhere Ie is the surface brightness at Re. This can be confirmed by noting\n\n  \n    \n      \n        \n          ∫\n          \n            0\n          \n          \n            \n              R\n              \n                e\n              \n            \n          \n        \n        I\n        (\n        r\n        )\n        2\n        π\n        r\n        \n        d\n        r\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        I\n        (\n        r\n        )\n        2\n        π\n        r\n        \n        d\n        r\n        .\n      \n    \n    {\\displaystyle \\int _{0}^{R_{e}}I(r)2\\pi r\\,dr={\\frac {1}{2}}\\int _{0}^{\\infty }I(r)2\\pi r\\,dr.}\n\nDe Vaucouleurs model is a special case of Sersic's model, with a Sersic index of n = 4. A number of (internal) density profiles that approximately reproduce de Vaucouleurs's law after projection onto the plane of the sky include Jaffe's model and Dehnen's model.\n\nThe model is named after Gérard de Vaucouleurs who first formulated it in 1948.[2][3]  Although an empirical model rather than a law of physics, it was so entrenched in astronomy during the 20th century that it was referred to as a \"law\".\n\nThis astrophysics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "De Vaucouleurs's law",
    },
    {
        title: "Doctorow's law",
        link: "https://en.wikipedia.org/wiki/Doctorow%27s_law",
        content:
            'Cory Efram Doctorow (/ˈkɔːri ˈdɒktəroʊ/; born 17 July 1971) is a Canadian-British blogger, journalist, and science fiction author who served as co-editor of the blog Boing Boing. He is an activist in favour of liberalising copyright laws and a proponent of the Creative Commons organization, using some of its licences for his books. Some common themes of his work include digital rights management, file sharing, and post-scarcity economics.[1][2]\n\nCory Efram Doctorow was born in Toronto, Ontario, on 17 July 1971.[3] He is of Ashkenazi Jewish descent.[4] His paternal grandfather was born in what is now Poland and his paternal grandmother was from Leningrad, Russia. Both fled Nazi Germany\'s advance eastward during World War II, and as a result Doctorow\'s father was born in a displaced persons camp near Baku, Azerbaijan.[5] His grandparents and father emigrated to Canada from the Soviet Union.[6] Doctorow\'s mother\'s family were Ukrainian-Russian Romanians.[6]\n\nDoctorow is a friend of Columbia law professor Tim Wu, dating to their time together to elementary school.[7] Doctorow went to summer camp as a young teenager at what he has described as a "hippy summer camp" at Grindstone Island, near Portland, Ontario, that was influential on his intellectual life and development.[8] He quit high school,[9] received his Ontario Academic Credit (high school diploma) from the SEED School in Toronto,[10] and attended four universities without obtaining a degree.[11]\n\nCory Doctorow has stated both that he is not related to the American novelist E. L. Doctorow,[12] and that he may be a third cousin once removed of the novelist.[13]  Thomas Rankin in Guide to Literary Masters & Their Works (2007) describes Doctorow as "a distant cousin of author E.L. Doctorow".[14]\n\nIn June 1999, Doctorow co-founded the free software P2P company Opencola[15] with John Henson and Grad Conn, which was sold to the Open Text Corporation of Waterloo, Ontario, in the summer of 2003.[1] The company used a drink called OpenCola as part of its promotional campaign.[16]\n\nDoctorow later relocated to London and worked as European Affairs Coordinator for the Electronic Frontier Foundation (EFF) for four years,[1] helping to establish the Open Rights Group, before leaving the EFF to pursue writing full-time in January 2006; Doctorow remained a Fellow of the EFF for some time after his departure from the EFF Staff.[1][17] He was named the 2006–2007 Canadian Fulbright Chair for Public Diplomacy at the USC Center on Public Diplomacy, sponsored jointly by the Royal Fulbright Commission,[18] the Integrated Media Systems Center, and the University of Southern California (USC) Center on Public Diplomacy. The professorship included a one-year writing and teaching residency at the University of Southern California in Los Angeles, United States.[1][19] He then returned to London, but remained a frequent public speaker on copyright issues.\n\nIn 2009, Doctorow became the first Independent Studies Scholar in Virtual Residence at the University of Waterloo in Ontario.[20] He was a student in the program during 1993–94, but left without completing a thesis. Doctorow was also a visiting professor at the Open University in the United Kingdom from September 2009 to August 2010.[20] In 2012 he was awarded an honorary doctorate from The Open University.[21]\n\nDoctorow married Alice Taylor in October 2008;[22] they have a daughter named Poesy Emmeline Fibonacci Nautilus Taylor Doctorow, who was born in 2008.[23] Doctorow became a British citizen by naturalisation on 12 August 2011.[24]\n\nIn 2015, Doctorow decided to leave London and move to Los Angeles, expressing disappointment at London\'s "death" after Britain\'s choice of Conservative government; he stated at the time, "London is a city whose two priorities are being a playground for corrupt global elites who turn neighbourhoods into soulless collections of empty safe-deposit boxes in the sky, and encouraging the feckless criminality of the finance industry. These two facts are not unrelated."[25] He rejoined the EFF in January 2015 to campaign for the eradication of digital rights management (DRM).[26]\n\nDoctorow left Boing Boing in January 2020, and soon started a solo blogging project titled Pluralistic.[27] The circumstances surrounding Doctorow\'s exit from the website were unclear at the time, although Doctorow acknowledged that he remained a co-owner of Boing Boing.[27][28] Given the end of the 19-year association between Doctorow and Boing Boing, MetaFilter described this news as "the equivalent of the Beatles breaking up" for the blog world.[28] Doctorow\'s exit was not acknowledged by Boing Boing, with his name being quietly removed from the list of editors on 29 January 2020.[29]\n\nDoctorow served as Canadian Regional Director of the Science Fiction and Fantasy Writers of America in 1999.\n\nIn 2007, together with Austrian art group monochrom, he initiated the Instant Blitz Copy Fight project, which asks people from all over the world to take flash pictures of copyright warnings in movie theaters.[30][31]\n\nOn 31 October 2005, Doctorow was involved in a controversy concerning digital rights management with Sony-BMG, as told in Wikinomics, a book by Don Tapscott and Anthony D. Williams.[32]\n\nAs a user of the Tor anonymity network for more than a decade during his global travels, Doctorow publicly supports the network; furthermore, Boing Boing operates a "high speed, high-quality exit node."[33]\n\nDoctorow was the keynote speaker at the July 2016 Hackers on Planet Earth conference.[34] He also presented on enshittification at the 2024 conference, HOPE XV.[35]\n\nHe is a member of the Democratic Socialists of America.[36]\n\nDoctorow began selling fiction when he was 17 years old, and sold several stories, followed by publication of the story "Craphound" in 1998.[9]\n\nDown and Out in the Magic Kingdom, Doctorow\'s first novel, was published in January 2003, and was the first novel released under one of the Creative Commons licences, allowing readers to circulate the electronic edition as long as they neither made money from it nor used it to create derived works.[37][2] The electronic edition was released simultaneously with the print edition.[2] In February 2004, it was re-released with a different Creative Commons license that allowed derivative works such as fan fiction, but still prohibited commercial usage.[38]\n\nDown and Out... was nominated for a Nebula Award,[39] and won the Locus Award for Best First Novel in 2004.[40] A semi-sequel short story named Truncat was published on Salon.com in August 2003.[41]\n\nHis novel Someone Comes to Town, Someone Leaves Town, published in June 2005, was chosen to launch the Sci-Fi Channel\'s book club, Sci-Fi Essentials (now defunct).\n\nDoctorow\'s other novels have been released with Creative Commons licences that allow derived works and prohibit commercial usage, and he has used the model of making digital versions available, without charge, at the same time that print versions are published.\n\nHis Sunburst Award-winning short-story collection[42] A Place So Foreign and Eight More was also published in 2004: "0wnz0red" from this collection was nominated for the 2004 Nebula Award for Best Novelette.[43]\n\nDoctorow released the bestselling novel Little Brother in 2008 with a Creative Commons Attribution-Noncommercial-ShareAlike licence.[44] It was nominated for a Hugo Award for Best Novel in 2009,[45] and won the 2009 Prometheus Award,[46] Sunburst Award,[47] and the 2009 John W. Campbell Memorial Award.[48]\n\nHis novel Makers was released in October 2009, and was serialised for free on the Tor Books website.[49]\n\nDoctorow released another young adult novel, For the Win, in May 2010.[9] The novel is available free on the author\'s website as a Creative Commons download, and is also published in traditional paper format by Tor Books. The book is about "greenfarming", and concerns massively multiplayer online role-playing games.\n\nDoctorow\'s short-story collection With a Little Help was released in printed format on 3 May 2011. It is a project to demonstrate the profitability of Doctorow\'s method of releasing his books in print and subsequently for free under Creative Commons.[50][51]\n\nIn September 2012, Doctorow released The Rapture of the Nerds, a novel written in collaboration with Charles Stross.[52]\n\nDoctorow\'s  young adult novel Pirate Cinema was released in October 2012. It won the 2013 Prometheus Award.[53]\n\nIn February 2013, Doctorow released Homeland, the sequel to his novel Little Brother.[54] It won the 2014 Prometheus Award (Doctorow\'s third novel to win this award).\n\nIn March 2019, Doctorow released Radicalized, a collection of four self-contained science-fiction novellas dealing with how life in America could be in the near future.[56] The book was selected for the 2020 edition of Canada Reads, in which it was defended by Akil Augustine.[57]\n\nAttack Surface, a standalone adult novel set in the "Little Brother" universe, was released on 13 October 2020.[58][59]\n\nHis novel called Red Team Blues, a financial thriller about cybersecurity, was released in April 2023. It features a character named Martin Hench.[60]\n\nStandalone hopepunk novel The Lost Cause, set in 2050s California about mitigating and surviving climate change impacts amidst the legacy of contemporary political divisions, was published in November 2023.[61]\n\nA second novel featuring forensic accountant Martin Hench was published in February 2024: The Bezzle is centered around the financial (mis-)management of privately owned prisons.[62]\n\nA third Martin Hench novel, Picks and Shovels, was published by Tor Books in February, 2025: the origin story of Martin Hench and the most powerful new tool for crime ever invented: the personal computer.[63]\n\nDoctorow\'s nonfiction works include his first book, The Complete Idiot\'s Guide to Publishing Science Fiction (co-written with Karl Schroeder and published in 2000),[64][65] his contributions to Boing Boing, the blog he co-edits, as well as regular columns in the magazines Popular Science and Make.[14] He is a contributing writer to Wired magazine,[14] and contributes occasionally to other magazines and newspapers such as the New York Times Sunday Magazine, The Globe and Mail, Asimov\'s Science Fiction magazine, and the Boston Globe.\n\nIn 2004, he wrote an essay on Wikipedia included in The Anthology at the End of the Universe, comparing Internet attempts at Hitchhiker\'s Guide-type resources, including a discussion of the Wikipedia article about himself.[66] Doctorow contributed the foreword to Sound Unbound: Sampling Digital Music and Culture (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky. He also was a contributing writer to the book Worldchanging: A User\'s Guide for the 21st Century.[67]\n\nHe popularised the term "metacrap" by a 2001 essay titled "Metacrap: Putting the torch to seven straw-men of the meta-utopia."[68] Some of his nonfiction published between 2001 and 2007 has been collected by Tachyon Publications as Content: Selected Essays on Technology, Creativity, Copyright, and the Future of the Future. In 2016, he wrote the article Mr. Robot Killed the Hollywood-Hacker  (published on MIT Technology Review) as a review of the TV show Mr. Robot and argued for a better portrayal and understanding of technology, computers and their risks and consequences in our modern world.[69]\n\nHis essay "You Can\'t Own Knowledge" is included in the Freesouls book project.[70]\n\nHe is the originator of Doctorow\'s Law: "Anytime someone puts a lock on something you own, against your wishes, and doesn\'t give you the key, they\'re not doing it for your benefit."[71][72][73][74][75]\n\nWriting in The Guardian in 2022, Doctorow listed the many problems confronting Facebook and suggested that its future would be increasingly fraught.[76]\n\nDoctorow believes that copyright laws should be liberalised to allow for free sharing of all digital media. He has also advocated filesharing.[77] He argues that copyright holders should have a monopoly on selling their own digital media and that copyright laws should not be operative unless someone attempts to sell a product that is under someone else\'s copyright.[78]\n\nDoctorow is an opponent of digital rights management and claims that it limits the free sharing of digital media and frequently causes problems for legitimate users (including registration problems that lock users out of their own purchases and prevent them from being able to move their media to other devices).[79]\n\nHe was a keynote speaker at the 2014 international conference CopyCamp in Warsaw, Poland[80] with the presentation "Information Doesn\'t Want to Be Free."[81]\n\nIn criticising the decay in usefulness of online platforms, Doctorow in 2022 coined the neologism enshittification,[82] (which he calls enpoopification on public airwaves[83]) which he defines as a degradation of an online environment caused by greed:\n\nHere is how platforms die: first, they are good to their users; then they abuse their users to make things better for their business customers; finally, they abuse those business customers to claw back all the value for themselves. Then, they die. I call this enshittification, and it is a seemingly inevitable consequence arising from the combination of the ease of changing how a platform allocates value, combined with the nature of a "two sided market," where a platform sits between buyers and sellers, hold each hostage to the other, raking off an ever-larger share of the value that passes between them.[84]\n\nThe word gained traction in 2023, where it was used by a variety of sources in reference to several major platforms discontinuing free features in order to further their monetization or taking other actions that were seen to degrade functionality.[85] In its annual vote, the American Dialect Society designated enshittification as 2023\'s Word of the Year.[86][87]\n\nIn November 2024, the Australian Macquarie Dictionary selected it as its word of the year, defining it as follows:[88]\n\nThe gradual deterioration of a service or product brought about by a reduction in the quality of service provided, especially of an online platform, and as a consequence of profit-seeking.\n\nIn chronological sequence, unless otherwise indicated',
        pageTitle: "Cory Doctorow",
    },
    {
        title: "Dolbear's law",
        link: "https://en.wikipedia.org/wiki/Dolbear%27s_law",
        content:
            'Dolbear\'s law states the relationship between the air temperature and the rate at which crickets chirp.[1][2] It was formulated by physicist Amos Dolbear and published in 1897 in an article called "The Cricket as a Thermometer".[3] Dolbear\'s observations on the relation between chirp rate and temperature were preceded by an 1881 report by Margarette W. Brooks,[n 1] although this paper went unnoticed until after Dolbear\'s publication.[2]\n\nDolbear did not specify the species of cricket which he observed, although subsequent researchers assumed it to be the snowy tree cricket, Oecanthus niveus.[1][2]  However, the snowy tree cricket was misidentified as O. niveus in early reports and the correct scientific name for this species is Oecanthus fultoni.[4]\n\nThe chirping of the more common field crickets is not as reliably correlated to temperature—their chirping rate varies depending on other factors such as age and mating success.\n\nDolbear expressed the relationship as the following formula which provides a way to estimate the temperature TF in degrees Fahrenheit from the number of chirps per minute N60:\n\nT\n          \n            F\n          \n        \n        =\n        50\n        +\n        \n          (\n          \n            \n              \n                \n                  N\n                  \n                    60\n                  \n                \n                −\n                40\n              \n              4\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle T_{F}=50+\\left({\\frac {N_{60}-40}{4}}\\right).}\n\nThis formula is accurate to within a degree or so when applied to the chirping of the field cricket.\n\nCounting can be sped up by simplifying the formula and counting the number of chirps produced in 15 seconds (N15):\n\nT\n          \n            F\n          \n        \n        =\n        40\n        +\n        \n          N\n          \n            15\n          \n        \n      \n    \n    {\\displaystyle \\,T_{F}=40+N_{15}}\n\nReformulated to give the temperature in degrees Celsius (°C), it is:\n\nT\n          \n            C\n          \n        \n        =\n        \n          \n            \n              \n                N\n                \n                  60\n                \n              \n              +\n              30\n            \n            7\n          \n        \n      \n    \n    {\\displaystyle T_{C}={\\frac {N_{60}+30}{7}}}\n\nA shortcut method for degrees Celsius is to count the number of chirps in 8 seconds (N8) and add 5 (this is fairly accurate between 5 and 30 °C):\n\nT\n          \n            C\n          \n        \n        =\n        5\n        +\n        \n          N\n          \n            8\n          \n        \n      \n    \n    {\\displaystyle \\,T_{C}=5+N_{8}}\n\nThe above formulae are expressed in terms of integers to make them easier to remember—they are not intended to be exact.\n\nMath textbooks will sometimes cite this as a simple example of where mathematical models break down, because at temperatures outside of the range that crickets live in, the total of chirps is zero as the crickets are dead.  You can apply algebra to the equation and see that according to the model at 1000 degrees Celsius (around 1800 degrees Fahrenheit) crickets should be chirping at 6,970 chirps per minute (around 116 chirps per second), but no known cricket can live at that temperature to chirp.\n\nThis formula was referenced in an episode (Season 3, Episode 2, "The Jiminy Conjecture") of the American TV sitcom The Big Bang Theory (although Sheldon referred to Amos Dolbear as Emile Dolbear and gave the year of publication as 1890).\nIt is also referenced in two episodes ("Highs and Lows", "Jungles") of the British comedy show QI. Richard Powers, author of the Pulitzer Prize-winning [The Overstory] (2018, W.W. Norton & Co.), has his fictional character Patricia Westerman use the formula (chapter 11. Pg 436).',
        pageTitle: "Dolbear's law",
    },
    {
        title: "Dollo's law",
        link: "https://en.wikipedia.org/wiki/Dollo%27s_law",
        content:
            "Dollo's law of irreversibility (also known as Dollo's law and Dollo's principle), proposed in 1893[1] by Belgian paleontologist Louis Dollo states that, \"an organism never returns exactly to a former state, even if it finds itself placed in conditions of existence identical to those in which it has previously lived ... it always keeps some trace of the intermediate stages through which it has passed.\"[2]\n\nThe statement is often misinterpreted as claiming that evolution is not reversible,[3] or that lost structures and organs cannot reappear in the same form by any process of devolution.[4][5] According to Richard Dawkins, the law is \"really just a statement about the statistical improbability of following exactly the same evolutionary trajectory twice (or, indeed, any particular trajectory), in either direction\".[6] Stephen Jay Gould suggested that irreversibility forecloses certain evolutionary pathways once broad forms have emerged: \"[For example], once you adopt the ordinary body plan of a reptile, hundreds of options are forever closed, and future possibilities must unfold within the limits of inherited design.\"[7]\n\nThis principle is classically applied to morphology, particularly of fossils, but may also be used to describe molecular events, such as individual mutations or gene losses.\n\nIn maximum parsimony, Dollo parsimony refers to a model whereby a characteristic is gained only one time and can never be regained if it is lost.[8] For example, the evolution and repeated loss of teeth in vertebrates could be well-modeled under Dollo parsimony, whereby teeth made from hydroxyapatite evolved only once at the origin of vertebrates, and were then lost multiple times, in birds, turtles, and seahorses, among others.[9]\n\nThis also applies to molecular characters, such as losses or inactivation of individual genes themselves.[10] The loss of gulonolactone oxidase, the final enzyme in the biosynthetic pathway of vitamin C, is responsible for the dietary requirement of vitamin C in humans, as well as many other animals.[11]\n\nA 2009 study on the evolution of protein structure proposed a new mechanism for Dollo's law. It examined a hormone receptor that had evolved from an ancestral protein that was able to bind two hormones to a new protein that was specific for a single hormone. This change was produced by two amino acid substitutions, which prevent binding of the second hormone. However, several other changes subsequently occurred, which were selectively neutral as they did not affect hormone binding. When the authors tried to revert the protein back to its ancestral state by mutating the two \"binding residues\", they found the other changes had destabilised the ancestral state of the protein. They concluded that in order for this protein to evolve in reverse and regain its ability to bind two hormones, several independent neutral mutations would have to occur purely by chance with no selection pressure. As this is extremely unlikely, it may explain why evolution tends to run in one direction.[12]\n\nAlthough the exact threshold for violations of Dollo's law is unclear, there are several case studies whose results dispute the validity of some interpretations. For example, many taxa of gastropods have reduced shells, and some have lost coiling of their shell altogether.[13] In Stephen Jay Gould's interpretation of Dollo's law, it would not be possible to regain a coiled shell after the coiling has been lost. Nevertheless, a few genera in the slipper snail family (Calyptraeidae) may have changed their developmental timing (heterochrony) and regained a coiled shell from a limpet-like shell.[13][14] Frietson Galis observed that many of these studies are based on either molecular phylogenies or morphological cladistic analyses that are tenuous and subject to change.[15]\n\nOther proposed 'exceptions' include the ocelli and wings of stick insects,[16][17] the larval stages of salamanders,[18][19] lost toes and re-evolution of oviparity in lizards,[20][21] lost lower teeth in frogs,[22] clavicles in non-avian theropod dinosaurs,[23] and neck, pectoral region, and upper limb musculature in primates, including the lineage leading to humans.[24]",
        pageTitle: "Dollo's law of irreversibility",
    },
    {
        title: "Dulong–Petit law",
        link: "https://en.wikipedia.org/wiki/Dulong%E2%80%93Petit_law",
        content:
            "The Dulong–Petit law, a thermodynamic law proposed  by French physicists Pierre Louis Dulong and Alexis Thérèse Petit, states that the classical expression for the molar specific heat capacity of certain chemical elements is constant for temperatures far from the absolute zero.\n\nIn modern terms, Dulong and Petit found that the heat capacity of a mole of many solid elements is about 3R, where R is the universal gas constant. The modern theory of the heat capacity of solids states that it is due to lattice vibrations in the solid.\n\nExperimentally Pierre Louis Dulong and Alexis Thérèse Petit had found in 1819 that the heat capacity per weight (the mass-specific heat capacity) for 13  measured elements was close to a constant value, after it had been multiplied by a number representing the presumed relative atomic weight of the element.[1] These atomic weights had shortly before been suggested by John Dalton and modified by Jacob Berzelius.\n\nDulong and Petit were unaware of the relationship with R, since this constant had not yet been defined from the later kinetic theory of gases. The value of 3R is about 25 joules per kelvin, and Dulong and Petit essentially found that this was the heat capacity of certain solid elements per mole of atoms they contained.\n\nThe Kopp's law developed in 1865 by Hermann Franz Moritz Kopp extended the Dulong–Petit law to chemical compounds from further experimental data.\n\nAmedeo Avogadro remarked in 1833 that the law did not fit the experimental data of carbon samples.[2] In 1876, Heinrich Friedrich Weber, noticed that the specific heat of diamond was sensible to temperature.[2]\n\nIn 1877, Ludwig Boltzmann showed that the constant value of Dulong–Petit law could be explained in terms of independent classical harmonic oscillators.[2][3] With the advent of quantum mechanics, this assumption was refined by Weber's student, Albert Einstein in 1907, employing quantum harmonic oscillators to explain the experimentally observed decrease of the heat capacity at low temperatures in diamond.\n\nPeter Debye followed in 1912 with a new model based on Max Planck's photon gas, where the vibrations are not to individual oscillators but as vibrational modes of the ionic lattice. Debye's model allowed to predict the behavior of the ionic heat capacity at temperature close to 0 kelvin, and as the Einstein solid, both recover the Dulong–Petit law at high temperature.\n\nThe electronic heat capacity was overestimated by the 1900 Drude-Lorentz model to be half of the value predicted by Dulong–Petit. With the development of the quantum mechanical free electron model in 1927 by Arnold Sommerfeld the electronic contribution was found to be orders of magnitude smaller. This model explained why conductors and insulators have roughly the same heat capacity at large temperatures as it depends mostly on the lattice and not on the electronic properties.\n\nAn equivalent statement of the Dulong–Petit law in modern terms is that, regardless of the nature of the substance, the specific heat capacity c of a solid element (measured in joule per kelvin per kilogram) is equal to 3R/M, where R is the gas constant (measured in joule per kelvin per mole) and M is the molar mass (measured in kilogram per mole). Thus, the heat capacity per mole of many elements is 3R.\n\nwhere K is a constant which we know today is about 3R.\n\nIn modern terms the mass m of the sample divided by molar mass M gives the number of moles n.\n\nTherefore, using uppercase C for the full heat capacity (in joule per kelvin), we have:\n\nTherefore, the heat capacity of most solid crystalline substances is 3R per mole of substance.\n\nDulong and Petit did not state their law in terms of the gas constant R (which was not then known). Instead, they measured the values of heat capacities (per weight) of substances and found them smaller for substances of greater atomic weight as inferred by Dalton and other early atomists. Dulong and Petit then found that when multiplied by these atomic weights, the value for the heat capacity per mole was nearly constant, and equal to a value which was later recognized to be 3R.\n\nIn other modern terminology, the dimensionless heat capacity[broken anchor] C/(nR) is equal to 3.\n\nThe law can also be written as a function of the total number of atoms N in the sample:\n\nDespite its simplicity, Dulong–Petit law offers a fairly good prediction for the heat capacity of many elementary solids with relatively simple crystal structure at high temperatures. This agreement is because in the classical statistical theory of Ludwig Boltzmann, the heat capacity of solids approaches a maximum of 3R per mole of atoms because full vibrational-mode degrees of freedom amount to 3 degrees of freedom per atom, each corresponding to a quadratic kinetic energy term and a quadratic potential energy term. By the equipartition theorem, the average of each quadratic term is .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠1/2⁠kBT, or ⁠1/2⁠RT per mole (see derivation below). Multiplied by 3 degrees of freedom and the two terms per degree of freedom, this amounts to 3R per mole heat capacity.\n\nThe Dulong–Petit law fails at room temperatures for light atoms bonded strongly to each other, such as in metallic beryllium and in carbon as diamond. Here, it predicts higher heat capacities than are actually found, with the difference due to higher-energy vibrational modes not being populated at room temperatures in these substances.\n\nIn the very low (cryogenic) temperature region, where the quantum mechanical nature of energy storage in all solids manifests itself with larger and larger effect, the law fails for all substances. For crystals under such conditions, the Debye model, an extension of the Einstein theory that accounts for statistical distributions in atomic vibration when there are lower amounts of energy to distribute, works well.\n\nA system of vibrations in a crystalline solid lattice can be modeled as an Einstein solid, i.e. by considering N quantum harmonic oscillator potentials along each degree of freedom. Then, the free energy of the system can be written as[4]\n\nwhere the index α sums over all the degrees of freedom. In the 1907 Einstein model (as opposed to the later Debye model) we consider only the high-energy limit:\n\nwhere g measures the total number of spatial degrees of freedom of the system.\n\nFor another more precise derivation, see Debye model.",
        pageTitle: "Dulong–Petit law",
    },
    {
        title: "Duverger's law",
        link: "https://en.wikipedia.org/wiki/Duverger%27s_law",
        content:
            "In political science, Duverger's law (/ˈduːvərʒeɪ/ DOO-vər-zhay) holds that in political systems with single-member districts and the first-past-the-post voting system, as in, for example, the United States and Britain, only two powerful political parties tend to control power.  Citizens don't  vote for small parties because they fear splitting votes away from the major party.[1][2]\n\nIn contrast, in countries with proportional representation or two-round elections, such as France, Sweden or Spain, there is no two-party duopoly on power. There is usually a significant number of political parties in parliament[3]. Citizens are encouraged to create, join and vote for new political parties if they are unhappy with current parties.[3]\n\nA two-party system is most common under plurality voting. Voters typically cast one vote per race. Maurice Duverger argued there were two main mechanisms by which plurality voting systems lead to fewer major parties: (i) small parties are disincentivized to form because they have great difficulty winning seats or representation, and (ii) voters are wary of voting for a smaller party whose policies they actually favor because they do not want to \"waste\" their votes (on a party unlikely to win a plurality) and therefore tend to gravitate to one of two major parties that is more likely to achieve a plurality, win the election, and implement policy.[4][5][6]\n\nFor legislatures where each seat represents a geographical area and the candidate with the most votes wins that seat, minor parties spread fairly evenly across many districts win less representation than geographically concentrated ones with the same overall level of public support. An example of this was the Liberal Democrats in the United Kingdom, whose proportion of seats in the legislature was, until recently, significantly less than their proportion of the national vote. The Green Party of Canada is another example; the party received about 5% of the popular vote from 2004 to 2011 but had only won one seat (out of 308) in the House of Commons in the same span of time. Another example was seen in the 1992 U.S. presidential election, when Ross Perot's candidacy received zero electoral votes despite receiving 19% of the popular vote. Gerrymandering is sometimes used to try to collect a population of like-minded voters within a geographically cohesive district so that their votes are not \"wasted\", but tends to require that minor parties have both a geographic concentration and a redistricting process that seeks to represent them. These disadvantages tend to suppress the ability of a third party to engage in the political process.[citation needed]\n\nThe second challenge to a third party is both statistical and tactical. Duverger presents the example of an election in which 100,000 moderate voters and 80,000 radical voters are to vote for candidates for a single seat or office. If two moderate parties and one radical party ran candidates, and every voter voted, the radical candidate would tend to win unless one of the moderate candidates gathered fewer than 20,000 votes. Appreciating this risk, moderate voters would be inclined to vote for the moderate candidate they deemed likely to gain more votes, with the goal of defeating the radical candidate. To win, then, either the two moderate parties must merge, or one moderate party must fail, as the voters gravitate to the two strongest parties. Duverger called this trend polarization.[7]\n\nKenneth Benoit suggested causal influence between electoral and party systems might be bidirectional or in either direction.[8] Josep Colomer agreed, arguing that changes from a plurality system to a proportional system are typically preceded by the emergence of more than two effective parties, and increases in the effective number of parties happen not in the short term, but in the mid-to-long term.[9]\n\nSome minor parties in winner-take-all systems have managed to translate their support into winning seats in government by focusing on local races, taking the place of a major party, or changing the political system.\n\nWilliam H. Riker, citing Douglas W. Rae, noted that strong regional parties can lead to more than two parties receiving seats in the national legislature, even if there are only two parties competitive in any single district.[10][11] In systems outside the United States, like Canada,[10] United Kingdom and India, multiparty parliaments exist due to the growth of minor parties finding strongholds in specific regions, potentially lessening the psychological fear of a wasted vote by voting for a minor party for a legislative seat.[12] Riker credits Canada's highly decentralized system of government as encouraging minor parties to build support by winning seats locally, which then sets the parties up to get representatives in the House of Commons of Canada.[10]\n\nThe political chaos in the United States immediately preceding the Civil War allowed the Republican Party to replace the Whig Party as the progressive half of the American political landscape.[10] Loosely united on a platform of country-wide economic reform and federally funded industrialization, the decentralized Whig leadership failed to take a decisive stance on the slavery issue, effectively splitting the party along the Mason–Dixon line. Southern rural planters, initially attracted by the prospect of federal infrastructure and schools, aligned with the pro-slavery Democrats, while urban laborers and professionals in the northern states, threatened by the sudden shift in political and economic power and losing faith in the failing Whig candidates, flocked to the increasingly vocal anti-slavery Republican Party.\n\nAbsent a major reform like switching to proportional representation, minor reforms like ranked-choice voting has the potential to allow for more choice in a winner-take-all system.[13] Under proportional representation, legislative seats are allocated according to the percentage of votes a party receives, making their success dependent on their received support. Proportional representation weakens two-party dominance, since smaller parties no longer rely on plurality they have a better chance of success.[14] In a ranked-choice voting system (RCV) voters rank candidates in order of preference rather than casting a single vote. This system is used in countries with multi-party politics like Australia, Ireland, and New Zealand. RCV systems were utilized in the US to manage or create intra-party factionalism but were repealed in most states for being unpredictable.[15] Duverger argued that \"a majority vote on two ballots is conducive to a multiparty system, inclined toward forming coalitions\": that the two-round system encourages a multiparty system, but to a lesser degree than proportional representation does.[7]\n\nWilliam Clark and Matt Golder (2006) find the effect largely holds up, noting that different methods of analyzing the data might lead to different conclusions. They emphasize other variables like the nuances of different electoral institutions and the importance that Duverger also placed on sociological factors.[16] Thomas R. Palfrey argued Duverger's law can be proven mathematically at the limit when the number of voters approaches infinity for one single-winner district and where the probability distribution of votes is known (perfect information).[17]\n\nDuverger did not regard this principle as absolute, suggesting instead that plurality would act to delay the emergence of new political forces and would accelerate the elimination of weakening ones, whereas proportional representation would have the opposite effect.[7]\n\nThe U.S. system has two major parties which have won, on average, 98% of all state and federal seats.[1] There have only been a few rare elections where a minor party was competitive with the major parties, occasionally replacing one of the major parties in the 19th century.[2][10]\n\nIn Matt Golder's 2016 review of the empirical evidence to-date, he concluded that despite some contradicting cases, the law remains a valid generalization.[18]\n\nSteven R. Reed argued in 2001 that Duverger's law could be observed in Italy, with 80% of electoral districts gradually but significantly shifting towards two major parties.[19] He finds a similar effect in Japan through a slow trial-and-error process that shifted the number of major parties towards the expected outcome.[20]\n\nEric Dickson and Kenneth Scheve argued in 2007 that Duverger's law is strongest when a society is homogenous or closely divided, but is weakened when multiple intermediate identities exist.[21] As evidence of this, Duhamel cites the case of India, where over 25 percent of voters vote for parties outside the two main alliances.[22]\n\nTwo-party politics may also emerge in systems that use a form of proportional representation, with Duverger and others arguing that Duverger's Law mostly represents a limiting factor (like a brake) on the number of major parties in other systems more than a prediction of equilibrium for governments with more proportional representation.[23][16]",
        pageTitle: "Duverger's law",
    },
    {
        title: "Edholm's law",
        link: "https://en.wikipedia.org/wiki/Edholm%27s_law",
        content:
            "Edholm's law, proposed by and named after Phil Edholm, refers to the observation that the three categories of telecommunication,[1] namely wireless (mobile), nomadic (wireless without mobility) and wired networks (fixed), are in lockstep and gradually converging.[2] Edholm's law also holds that data rates for these telecommunications categories increase on similar exponential curves, with the slower rates trailing the faster ones by a predictable time lag.[3] Edholm's law predicts that the bandwidth and data rates double every 18 months, which has proven to be true since the 1970s.[1][4] The trend is evident in the cases of Internet,[1] cellular (mobile), wireless LAN and wireless personal area networks.[4]\n\nEdholm's law was proposed by Phil Edholm of Nortel Networks. He observed that telecommunication bandwidth (including Internet access bandwidth) was doubling every 18 months, since the late 1970s through to the early 2000s. This is similar to Moore's law, which predicts an exponential rate of growth for transistor counts. He also found that there was a gradual convergence between wired (e.g. Ethernet), nomadic (e.g. modem and Wi-Fi) and wireless networks (e.g. cellular networks). The name \"Edholm's law\" was coined by his colleague, John H. Yoakum, who presented it at a 2004 Internet telephony press conference.[1]\n\nSlower communications channels like cellphones and radio modems were predicted to eclipse the capacity of early Ethernet, due to developments in the standards known as UMTS and MIMO, which boosted bandwidth by maximizing antenna usage.[1] Extrapolating forward indicates a convergence between the rates of nomadic and wireless technologies around 2030. In addition, wireless technology could end wireline communication if the cost of the latter's infrastructure remains high.[2]\n\nIn 2009, Renuka P. Jindal observed the bandwidths of online communication networks rising from bits per second to terabits per second, doubling every 18 months, as predicted by Edholm's law. Jindal identified the following three major underlying factors that have enabled the exponential growth of communication bandwidth.[5]\n\nThe bandwidths of wireless networks have been increasing at a faster pace compared to wired networks.[1] This is due to advances in MOSFET wireless technology enabling the development and growth of digital wireless networks. The wide adoption of RF CMOS (radio frequency CMOS), power MOSFET and LDMOS (lateral diffused MOS) devices led to the development and proliferation of digital wireless networks by the 1990s, with further advances in MOSFET technology leading to rapidly increasing bandwidth since the 2000s.[11][12][13] Most of the essential elements of wireless networks are built from MOSFETs, including the mobile transceivers, base station modules, routers, RF power amplifiers,[12] telecommunication circuits,[14] RF circuits, and radio transceivers,[13] in networks such as 2G, 3G,[11] 4G, and 5G.[12]\n\nIn recent years, another enabling factor in the growth of wireless communication networks has been interference alignment, which was discovered by Syed Ali Jafar at the University of California, Irvine.[15] He established it as a general principle, along with Viveck R. Cadambe, in 2008. They introduced \"a mechanism to align an arbitrarily large number of interferers, leading to the surprising conclusion that wireless networks are not essentially interference limited.\" This led to the adoption of interference alignment in the design of wireless networks.[16] According to New York University senior researcher Dr. Paul Horn, this \"revolutionized our understanding of the capacity limits of wireless networks\" and \"demonstrated the astounding result that each user in a wireless network can access half of the spectrum without interference from other users, regardless of how many users are sharing the spectrum.\"[15]",
        pageTitle: "Edholm's law",
    },
    {
        title: "Einasto's law",
        link: "https://en.wikipedia.org/wiki/Einasto_profile",
        content:
            "The Einasto profile (or Einasto model) is a mathematical function that describes how the density \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n of a spherical stellar system varies with distance \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n from its center. Jaan Einasto introduced his model at a 1963 conference in Alma-Ata, Kazakhstan.[1]\n\nThe Einasto profile possesses a power law logarithmic slope of the form:\n\n  \n    \n      \n        γ\n        (\n        r\n        )\n        ≡\n        −\n        \n          \n            \n              d\n              ⁡\n              ln\n              ⁡\n              ρ\n              (\n              r\n              )\n            \n            \n              d\n              ⁡\n              ln\n              ⁡\n              r\n            \n          \n        \n        ∝\n        \n          r\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle \\gamma (r)\\equiv -{\\frac {\\operatorname {d} \\ln \\rho (r)}{\\operatorname {d} \\ln r}}\\propto r^{\\alpha }}\n  \n\nwhich can be rearranged to give\n\n  \n    \n      \n        ρ\n        (\n        r\n        )\n        ∝\n        exp\n        ⁡\n        \n          (\n          −\n          A\n          \n            r\n            \n              α\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\rho (r)\\propto \\exp {(-Ar^{\\alpha })}.}\n  \n\nThe parameter \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n controls the degree of curvature of the profile. This can be seen by computing the slope on a log-log plot:\n\n  \n    \n      \n        \n          \n            \n              d\n              ln\n              ⁡\n              ρ\n            \n            \n              d\n              ln\n              ⁡\n              r\n            \n          \n        \n        ∝\n        −\n        \n          r\n          \n            α\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\ln \\rho }{d\\ln r}}\\propto -r^{\\alpha }.}\n\nThe larger \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n, the more rapidly the slope varies with radius (see figure). Einasto's law can be described as a generalization of a power law, \n  \n    \n      \n        ρ\n        ∝\n        \n          r\n          \n            −\n            N\n          \n        \n      \n    \n    {\\displaystyle \\rho \\propto r^{-N}}\n  \n, which has a constant slope on a log-log plot.\n\nEinasto's model has the same mathematical form as Sersic's law, which is used to describe the surface brightness (i.e. projected density) profile of galaxies, except that the Einasto model describes a spherically symmetric density distribution in 3 dimensions, whereas the Sersic law describes a circularly symmetric surface density distribution in two dimensions.\n\nEinasto's model has been used to describe many types of system, including galaxies,[2] and dark matter halos.[3][4]",
        pageTitle: "Einasto profile",
    },
    {
        title: "Emmert's law",
        link: "https://en.wikipedia.org/wiki/Emmert%27s_law",
        content:
            "Emmert's law states that objects that generate retinal images of the same size will look different in physical size (linear size) if they appear to be located at different distances. Specifically, the perceived linear size of an object increases as its perceived distance from the observer increases.  This makes intuitive sense: an object of constant size will project progressively smaller retinal images as its distance from the observer increases. Similarly, if the retinal images of two different objects at different distances are the same, the physical size of the object that is farther away must be larger than the one that is closer.\n\nEmil Emmert (1844–1911) first described the law in 1881.[1] He noted that an afterimage appeared to increase in size when projected to a greater distance. Some authors thus take Emmert's law to refer strictly to the increase in the apparent size of an after-image when the distance between observer and projection plane is increased, as it did in its original form.[2] Other authors take Emmert's law to apply to any comparative estimation of physical size in which the size of the retinal image, however it may be produced, is equated.[3]\n\nIt is unclear whether Emmert intended the increase in distance to refer to an increase in physical distance or an increase in perceived distance, but most authors assume the latter.[4] Under that interpretation, Emmert's law is a special instance of size constancy and of the size–distance invariance hypothesis, which states that the ratio of perceived linear size to perceived distance is a simple function of the visual angle.[5]\n\nThe effect of viewing distance on perceived size can be observed by first obtaining an afterimage, which can be achieved by viewing a bright light for a short time, or staring at a figure for a longer time.  It appears to grow in size when projected to a further distance. However, the increase in perceived size is much less than would be predicted by geometry, which casts some doubt on the geometrical interpretation given above.[6] Further, the change in perceived size is affected by the illusory distances in the Ames room; this also suggests that, when distance cues are reduced, there is no simple geometrical relationship between perceived afterimage size and actual viewing distance.[5]\n\nEmmert's law has been used to investigate the moon illusion (the apparent enlargement of the moon or sun near the horizon compared with higher in the sky).[7][8]  A neuroimaging study that examined brain activation when participants viewed afterimages on surfaces placed at different distances found evidence supporting Emmert's Law and thus size constancy played out in primary visual cortex (V1); i.e.  the larger the perceived size of the afterimage, the larger the retinotopic activation in V1.[9]\n\nSome have criticized the use of Emmert's law as an explanation for phenomena such as the moon illusion, because Emmert's law explains one perception in terms of another, rather than explaining any of the complex internal processes or mechanisms presumably involved in perception.[10] That is, Emmert's law is useful, but it does not explain why you perceive an object as being larger if you perceive it as being farther away.\n\nThis optics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Emmert's law",
    },
    {
        title: "Engelbart's law",
        link: "https://en.wikipedia.org/wiki/Engelbart%27s_law",
        content:
            "Engelbart's law is the observation that the intrinsic rate of human performance is exponential.[further explanation needed] The law is named after Douglas Engelbart, whose work in augmenting human performance was explicitly based on the realization that although we use technology, the ability to improve on improvements (bootstrapping, \"getting better at getting better\") resides entirely within the human sphere.\n\nEngelbart's Bootstrapping concept identifies the general, and particular, meaning of the observation with regards to rate and performance: a quantity, amount, or degree of something measured per unit of something else.[1]\nThat is, Engelbart's law is not limited to an increase in the acquisition, or use of, or quantity of knowledge, nor of the extent or depth of participation among individuals or teams, nor of the period-to-period change. The law is independent of the domain of performance and the quantity, amount, or degree on which one chooses a measure.\n\nHumans have long performed at exponential levels, and in widely varying contexts and domains.\n\nAs with other phenomena, when we notice similar results when applying a reagent or catalyst across many contexts and domains, we associate the power to produce or induce those results with the reagent, here the human animal.\n\nStephen Jay Kline presented an interesting visualization of this exponential phenomenon in his 1995 book.[2] See page 173, figure 14-1. The Growth of Human Powers Over the Past 100,000 Years Plotted as Technoextension Factors (TEFs).\nThe log-log chart (time, TEF) illustrates exponential performance extending over many domains and over hundreds of years.\n\nOn this topic Kline's work made heavy use of the work of John H. Lienhard. Kline specifically references Lienhard's The Rate of Technological Improvement before and after the 1830s.[3]\nLienhard explored this topic several times at Engines of Our Ingenuity.[4]\nSee specifically Double in a Lifetime.[5]\nOther relevant episodes include\nInfluence of War,[6] and\nInfluence of War, Updated\n.[7]\nIn these latter two references Lienhard explores, and discards,\nthe influence of an urgent necessity as a necessary driver to\nsuch performance.\n\nIn discussing the exponential nature of Moore's law,[8] Gordon Moore locates the roots of his inspiration in Engelbart's observations on the propensity of humans to envision and achieve scale, and its non-linear effects.\n\nEngelbart labeled Collective IQ as the measure of how well people can work on important problems and opportunities collectively. It is, ultimately, a measure of effectiveness.\n\nIt has long been the fashion to talk of human performance as if it were dependent on a particular socio-technology fabric. Yet Engelbart felt that what was important was not the particulars of that fabric, but its nature. He called the nature of that fabric\nThe Bootstrap Paradigm.[9][10]\n\nCentral to his realization was a Dynamic Knowledge Repository[11]\n(DKR)\ncapable of enabling the concurrent development, integration and application of knowledge (CoDIAK). Such a DKR would itself be\nsubject to the CoDIAK process.\n\nThis is a co-evolution of the human system and the tool system. To facilitate this, Engelbart observed that a particular structure of human activities is most useful and natural, the A-level ('Business as Usual'), B-level ('Improving how we do that') and C-level ('Improving how we improve') Activities.[12]\n\nIn ABC Model, and particularly Turbo Charge the C Activity and Extra Bootstrapping Engelbart addresses the necessity of the C-level activity in the shift from an incremental improvement curve to an exponential improvement curve.\n\nWhereas B-level activities achieve mildly-exponential results, Engelbart held that C-levels activities are necessary to achieve bootstrapping, improving the improvement, a direct dependence on the intrinsically exponential nature of humans.\n\nAlthough Engelbart never published a metric for measuring Bootstrap effects, the Bootstrap Alliance, in 1997, considered, the characteristics of candidate metrics.[13]\n\nAs derived from the above, candidate metrics would necessarily:\n\nIn addition to the above acronyms, the following represents the performance of ABC-level Activities by their respective letters:\n\n  \n    \n      \n        \n          \n            A\n          \n        \n        ,\n        \n          \n            B\n          \n        \n        ,\n        \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}},{\\mathcal {B}},{\\mathcal {C}}}\n  \n.\n\nAlthough this metric would reflect the augmentation of CIQ, the efficiency in the concurrent development, integration, and application of knowledge (CoDIAK) would be dependent solely on the application of B-level Activity to A-level Activity.\n(For example:\n\n  \n    \n      \n        C\n        o\n        D\n        I\n        A\n        \n          K\n          \n            t\n            →\n            t\n            +\n            1\n          \n        \n        =\n        1\n        +\n        \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle CoDIAK_{t\\to t+1}=1+{\\mathcal {B}}}\n  \n)\nThere is insufficiently-explicit accounting of improvements to CoDIAK dynamics, and no accounting of C-level Activities.\n\nAlthough powerful, Engelbart's Bootstrapping effects are also unaccounted in simple exponential power formulations.\n\nAlthough this metric signifies an improved performance rate (e.g., we might say \n  \n    \n      \n        C\n        o\n        D\n        I\n        A\n        \n          K\n          \n            t\n            →\n            t\n            +\n            1\n          \n        \n        =\n        \n          e\n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle CoDIAK_{t\\to t+1}=e^{\\mathcal {B}}}\n  \n), it too provides insuffienct accounting of other activities.\n\nComparing these two insufficient candidates illustrates the same key aspect underpinning Engelbart's law: human organization and directed activity are the essential elements to our performance.\n\nAn implication of the foregoing as insufficient Bootstrapping metrics is that simple exponential power relationships are insufficient to improve CoDIAK abilities. By definition this result precludes a simple combination of such metrics or laws, such as a factor or power relationship between Moore's law and Metcalfe's law (or variations thereon).\n\nVarious complex power functions, incorporating the effect of efforts at each of the ABC-level Activities were considered as candidates.\n\nThe differences illustrate the interplay and interdependencies of A-level, B-level and C-level Activities, and the Bootstrapping effects (improved improvement). In all three, improvement to CIQ, CoDIAK, and Bootstrapping itself depends on a recursive application of B-level Activities and C-level Activities.\n\nAlthough levels of exponential rates of performance, over many time periods and in many domains, with respect to quantity, extent, or degree, is well-documented by Lienhard and many others, modern and real-time ongoing of Bootstrapping levels of performance remain difficult to find.\n\nIn explicitly placing within the human sphere the locus of ability for improving our improvement, Engelbart's law chides us against choosing anemic measures of change in performance. Linear rates, or simple compound rates fall far short of our intrinsic abilities.\n\nIn addition to envisioning the Bootstrap Paradigm, describing the nature of a suitable socio-technical fabric, Engelbart envisioned its particular characteristics, which, when placed into use, and subjected to improvement upon improvement, would meet human requirements.\n\nIn this way, to fully use A-, B-, and C-level Activities, and achieve bootstrapping-levels of performance, we may more easily and readily redefine our measures until we have a suitable basis for such performance:[attribution needed]\n\nEngelbart, in writing and working, intended to apply this method of working to all domains of human endeavor, from the individual to the whole species, in private or public service [attribution needed].",
        pageTitle: "Engelbart's law",
    },
    {
        title: "Eroom's law",
        link: "https://en.wikipedia.org/wiki/Eroom%27s_law",
        content:
            "Eroom's law is the observation that drug discovery is becoming slower and more expensive over time, despite improvements in technology (such as high-throughput screening, biotechnology, combinatorial chemistry, and computational drug design), a trend first observed in the 1980s. The inflation-adjusted cost of developing a new drug roughly doubles every nine years.[1] In order to highlight the contrast with the exponential advancements of other forms of technology (such as transistors) over time, the name given to the observation is Moore's law spelled backwards.[2] The term was coined by Jack Scannell and colleagues in 2012 in Nature Reviews Drug Discovery.\n\nThe article that proposed the law attributes it to four main causes:[3]\n\nWhile some suspect a lack of \"low-hanging fruit\" as a significant contribution to Eroom's law, this may be less important than the four main causes, as there are still many decades' worth of new potential drug targets relative to the number of targets which already have been exploited, even if the industry exploits 4 to 5 new targets per year.[3] There is also space to explore selectively non-selective drugs (or \"dirty drugs\") that interact with several molecular targets, and which may be particularly effective as central nervous system (CNS) therapeutics, even though few of them have been introduced in the last few decades.[5]\n\nAs of 2018, academic spinouts and small biotech startups have surpassed Big Pharma with respect to the number of best-selling drugs approved, with 24/30 (80%) originating outside of Big Pharma. [6]\n\nAn alternative hypothesis is that the pharmaceutical industry has become cartelized and formed a bureaucratic oligopoly, resulting in reduced innovation and efficiency. As of 2022, approximately 20 Big Pharma companies control the majority of global branded drug sales (on the scale of ±$1 trillion annually). Critics point out that Big Pharma has reduced investment in R&D, spending double on marketing, and have focused on elevating drug prices instead of risk-taking. [7]",
        pageTitle: "Eroom's law",
    },
    {
        title: "Moore's law",
        link: "https://en.wikipedia.org/wiki/Moore%27s_law",
        content:
            "Moore's law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship. It is an experience-curve law, a type of law quantifying efficiency gains from experience in production.\n\nThe observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel and former CEO of the latter, who in 1965 noted that the number of components per integrated circuit had been doubling every year,[a] and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41%. Moore's empirical evidence did not directly imply that the historical trend would continue, nevertheless, his prediction has held since 1975 and has since become known as a law.\n\nMoore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development (R&D). Advancements in digital electronics, such as the reduction in quality-adjusted prices of microprocessors, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.\n\nIndustry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore's law. In September 2022, Nvidia CEO Jensen Huang considered Moore's law dead,[2] while Intel CEO Pat Gelsinger was of the opposite view.[3]\n\nIn 1959, Douglas Engelbart studied the projected downscaling of integrated circuit (IC) size, publishing his results in the article \"Microelectronics, and the Art of Similitude\".[4][5][6] Engelbart presented his findings at the 1960 International Solid-State Circuits Conference, where Moore was present in the audience.[7]\n\nIn 1965, Gordon Moore, who at the time was working as the director of research and development at Fairchild Semiconductor, was asked to contribute to the thirty-fifth-anniversary issue of Electronics magazine with a prediction on the future of the semiconductor components industry over the next ten years.[8] His response was a brief article entitled \"Cramming more components onto integrated circuits\".[1][9][b] Within his editorial, he speculated that by 1975 it would be possible to contain as many as 65000 components on a single quarter-square-inch (~ 1.6 cm2) semiconductor.\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.[1]\n\nMoore posited a log–linear relationship between device complexity (higher circuit density at reduced cost) and time.[12][13] In a 2015 interview, Moore noted of the 1965 article: \"... I just did a wild extrapolation saying it's going to continue to double every year for the next 10 years.\"[14] One historian of the law cites Stigler's law of eponymy, to introduce the fact that the regular doubling of components was known to many working in the field.[13]\n\nIn 1974, Robert H. Dennard at IBM recognized the rapid MOSFET scaling technology and formulated what became known as Dennard scaling, which describes that as MOS transistors get smaller, their power density stays constant such that the power use remains in proportion with area.[15][16] Evidence from the semiconductor industry shows that this inverse relationship between power density and areal density broke down in the mid-2000s.[17]\n\nAt the 1975 IEEE International Electron Devices Meeting, Moore revised his forecast rate,[18][19] predicting semiconductor complexity would continue to double annually until about 1980, after which it would decrease to a rate of doubling approximately every two years.[19][20][21] He outlined several contributing factors for this exponential behavior:[12][13]\n\nShortly after 1975, Caltech professor Carver Mead popularized the term Moore's law.[22][23] Moore's law eventually came to be widely accepted as a goal for the semiconductor industry, and it was cited by competitive semiconductor manufacturers as they strove to increase processing power. Moore viewed his eponymous law as surprising and optimistic: \"Moore's law is a violation of Murphy's law. Everything gets better and better.\"[24] The observation was even seen as a self-fulfilling prophecy.[25][26]\n\nThe doubling period is often misquoted as 18 months because of a separate prediction by Moore's colleague, Intel executive David House.[27] In 1975, House noted that Moore's revised law of doubling transistor count every 2 years in turn implied that computer chip performance would roughly double every 18 months,[28] with no increase in power consumption.[29] Mathematically, Moore's law predicted that transistor count would double every 2 years due to shrinking transistor dimensions and other improvements.[30] As a consequence of shrinking dimensions, Dennard scaling predicted that power consumption per unit area would remain constant. Combining these effects, David House deduced that computer chip performance would roughly double every 18 months. Also due to Dennard scaling, this increased performance would not be accompanied by increased power, i.e., the energy-efficiency of silicon-based computer chips roughly doubles every 18 months. Dennard scaling ended in the 2000s.[17] Koomey later showed that a similar rate of efficiency improvement predated silicon chips and Moore's law, for technologies such as vacuum tubes.\n\nMicroprocessor architects report that since around 2010, semiconductor advancement has slowed industry-wide below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, cited Moore's 1975 revision as a precedent for the current deceleration, which results from technical challenges and is \"a natural part of the history of Moore's law\".[31][32][33] The rate of improvement in physical dimensions known as Dennard scaling also ended in the mid-2000s. As a result, much of the semiconductor industry has shifted its focus to the needs of major computing applications rather than semiconductor scaling.[25][34][17] Nevertheless, as of 2019, leading semiconductor manufacturers TSMC and Samsung Electronics claimed to keep pace with Moore's law[35][36][37][38][39][40] with 10, 7, and 5 nm nodes in mass production.[35][36][41][42][43]\n\nAs the cost of computer power to the consumer falls, the cost for producers to fulfill Moore's law follows an opposite trend: R&D, manufacturing, and test costs have increased steadily with each new generation of chips. The cost of the tools, principally EUVL (Extreme ultraviolet lithography), used to manufacture chips doubles every 4 years.[44] Rising manufacturing costs are an important consideration for the sustaining of Moore's law.[45] This led to the formulation of Moore's second law, also called Rock's law (named after Arthur Rock), which is that the capital cost of a semiconductor fabrication plant also increases exponentially over time.[46][47]\n\nNumerous innovations by scientists and engineers have sustained Moore's law since the beginning of the IC era. Some of the key innovations are listed below, as examples of breakthroughs that have advanced integrated circuit and semiconductor device fabrication technology, allowing transistor counts to grow by more than seven orders of magnitude in less than five decades.\n\nComputer industry technology road maps predicted in 2001 that Moore's law would continue for several generations of semiconductor chips.[71]\n\nOne of the key technical challenges of engineering future nanoscale transistors is the design of gates. As device dimensions shrink, controlling the current flow in the thin channel becomes more difficult. Modern nanoscale transistors typically take the form of multi-gate MOSFETs, with the FinFET being the most common nanoscale transistor. The FinFET has gate dielectric on three sides of the channel. In comparison, the gate-all-around MOSFET (GAAFET) structure has even better gate control.\n\nMicroprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, announced, \"Our cadence today is closer to two and a half years than two.\"[103] Intel stated in 2015 that improvements in MOSFET devices have slowed, starting at the 22 nm feature width around 2012, and continuing at 14 nm.[104] Pat Gelsinger, Intel CEO, stated at the end of 2023 that \"we're no longer in the golden era of Moore's Law, it's much, much harder now, so we're probably doubling effectively closer to every three years now, so we've definitely seen a slowing.\"[105]\n\nThe physical limits to transistor scaling have been reached due to source-to-drain leakage, limited gate metals and limited options for channel material. Other approaches are being investigated, which do not rely on physical scaling. These include the spin state of electron spintronics, tunnel junctions, and advanced confinement of channel materials via nano-wire geometry.[106] Spin-based logic and memory options are being developed actively in labs.[107][108]\n\nThe vast majority of current transistors on ICs are composed principally of doped silicon and its alloys. As silicon is fabricated into single nanometer transistors, short-channel effects adversely changes desired material properties of silicon as a functional transistor. Below are several non-silicon substitutes in the fabrication of small nanometer transistors.\n\nOne proposed material is indium gallium arsenide, or InGaAs. Compared to their silicon and germanium counterparts, InGaAs transistors are more promising for future high-speed, low-power logic applications. Because of intrinsic characteristics of III–V compound semiconductors, quantum well and tunnel effect transistors based on InGaAs have been proposed as alternatives to more traditional MOSFET designs.\n\nBiological computing research shows that biological material has superior information density and energy efficiency compared to silicon-based computing.[116]\n\nVarious forms of graphene are being studied for graphene electronics, e.g. graphene nanoribbon transistors have shown promise since its appearance in publications in 2008. (Bulk graphene has a band gap of zero and thus cannot be used in transistors because of its constant conductivity, an inability to turn off. The zigzag edges of the nanoribbons introduce localized energy states in the conduction and valence bands and thus a bandgap that enables switching when fabricated as a transistor. As an example, a typical GNR of width of 10 nm has a desirable bandgap energy of 0.4 eV.[117][118]) More research will need to be performed, however, on sub-50 nm graphene layers, as its resistivity value increases and thus electron mobility decreases.[117]\n\nIn April 2005, Gordon Moore stated in an interview that the projection cannot be sustained indefinitely: \"It can't continue forever. The nature of exponentials is that you push them out and eventually disaster happens.\" He also noted that transistors eventually would reach the limits of miniaturization at atomic levels:\n\nIn terms of size [of transistors] you can see that we're approaching the size of atoms which is a fundamental barrier, but it'll be two or three generations before we get that far—but that's as far out as we've ever been able to see. We have another 10 to 20 years before we reach a fundamental limit. By then they'll be able to make bigger chips and have transistor budgets in the billions.[119]\n\nIn 2016 the International Technology Roadmap for Semiconductors, after using Moore's Law to drive the industry since 1998, produced its final roadmap. It no longer centered its research and development plan on Moore's law. Instead, it outlined what might be called the More than Moore strategy in which the needs of applications drive chip development, rather than a focus on semiconductor scaling. Application drivers range from smartphones to AI to data centers.[120]\n\nIEEE began a road-mapping initiative in 2016, Rebooting Computing, named the International Roadmap for Devices and Systems (IRDS).[121]\n\nSome forecasters, including Gordon Moore,[122] predict that Moore's law will end by around 2025.[123][120][124] Although Moore's Law will reach a physical limit, some forecasters are optimistic about the continuation of technological progress in a variety of other areas, including new chip architectures, quantum computing, and AI and machine learning.[125][126] Nvidia CEO Jensen Huang declared Moore's law dead in 2022;[2] several days later, Intel CEO Pat Gelsinger countered with the opposite claim.[3]\n\nDigital electronics have contributed to world economic growth in the late twentieth and early twenty-first centuries.[127] The primary driving force of economic growth is the growth of productivity,[128] which Moore's law factors into. Moore (1995) expected that \"the rate of technological progress is going to be controlled from financial realities\".[129] The reverse could and did occur around the late-1990s, however, with economists reporting that \"Productivity growth is the key economic indicator of innovation.\"[130] Moore's law describes a driving force of technological and social change, productivity, and economic growth.[131][132][128]\n\nAn acceleration in the rate of semiconductor progress contributed to a surge in U.S. productivity growth,[133][134][135] which reached 3.4% per year in 1997–2004, outpacing the 1.6% per year during both 1972–1996 and 2005–2013.[136] As economist Richard G. Anderson notes, \"Numerous studies have traced the cause of the productivity acceleration to technological innovations in the production of semiconductors that sharply reduced the prices of such components and of the products that contain them (as well as expanding the capabilities of such products).\"[137]\n\nThe primary negative implication of Moore's law is that obsolescence pushes society up against the Limits to Growth. As technologies continue to rapidly improve, they render predecessor technologies obsolete. In situations in which security and survivability of hardware or data are paramount, or in which resources are limited, rapid obsolescence often poses obstacles to smooth or continued operations.[138]\n\nSeveral measures of digital technology are improving at exponential rates related to Moore's law, including the size, cost, density, and speed of components. Moore wrote only about the density of components, \"a component being a transistor, resistor, diode or capacitor\",[129] at minimum cost.\n\nTransistors per integrated circuit – The most popular formulation is of the doubling of the number of transistors on ICs every two years. At the end of the 1970s, Moore's law became known as the limit for the number of transistors on the most complex chips. The graph at the top of this article shows this trend holds true today. As of 2022[update], the commercially available processor possessing one of the highest numbers of transistors is an AD102 graphics processor with more than 76,3 billion transistors.[139]\n\nDensity at minimum cost per transistor – This is the formulation given in Moore's 1965 paper.[1] It is not just about the density of transistors that can be achieved, but about the density of transistors at which the cost per transistor is the lowest.[140]\n\nAs more transistors are put on a chip, the cost to make each transistor decreases, but the chance that the chip will not work due to a defect increases. In 1965, Moore examined the density of transistors at which cost is minimized, and observed that, as transistors were made smaller through advances in photolithography, this number would increase at \"a rate of roughly a factor of two per year\".[1]\n\nDennard scaling – This posits that power usage would decrease in proportion to area (both voltage and current being proportional to length) of transistors. Combined with Moore's law, performance per watt would grow at roughly the same rate as transistor density, doubling every 1–2 years. According to Dennard scaling transistor dimensions would be scaled by 30% (0.7×) every technology generation, thus reducing their area by 50%. This would reduce the delay by 30% (0.7×) and therefore increase operating frequency by about 40% (1.4×). Finally, to keep electric field constant, voltage would be reduced by 30%, reducing energy by 65% and power (at 1.4× frequency) by 50%.[c] Therefore, in every technology generation transistor density would double, circuit becomes 40% faster, while power consumption (with twice the number of transistors) stays the same.[141] Dennard scaling ended in 2005–2010, due to leakage currents.[17]\n\nThe exponential processor transistor growth predicted by Moore does not always translate into exponentially greater practical CPU performance. Since around 2005–2007, Dennard scaling has ended, so even though Moore's law continued after that, it has not yielded proportional dividends in improved performance.[15][142] The primary reason cited for the breakdown is that at small sizes, current leakage poses greater challenges, and also causes the chip to heat up, which creates a threat of thermal runaway and therefore, further increases energy costs.[15][142][17]\n\nThe breakdown of Dennard scaling prompted a greater focus on multicore processors, but the gains offered by switching to more cores are lower than the gains that would be achieved had Dennard scaling continued.[143][144] In another departure from Dennard scaling, Intel microprocessors adopted a non-planar tri-gate FinFET at 22 nm in 2012 that is faster and consumes less power than a conventional planar transistor.[145] The rate of performance improvement for single-core microprocessors has slowed significantly.[146] Single-core performance was improving by 52% per year in 1986–2003 and 23% per year in 2003–2011, but slowed to just seven percent per year in 2011–2018.[146]\n\nQuality adjusted price of IT equipment – The price of information technology (IT), computers and peripheral equipment, adjusted for quality and inflation, declined 16% per year on average over the five decades from 1959 to 2009.[147][148] The pace accelerated, however, to 23% per year in 1995–1999 triggered by faster IT innovation,[130] and later, slowed to 2% per year in 2010–2013.[147][149]\n\nWhile quality-adjusted microprocessor price improvement continues,[150] the rate of improvement likewise varies, and is not linear on a log scale. Microprocessor price improvement accelerated during the late 1990s, reaching 60% per year (halving every nine months) versus the typical 30% improvement rate (halving every two years) during the years earlier and later.[151][152] Laptop microprocessors in particular improved 25–35% per year in 2004–2010, and slowed to 15–25% per year in 2010–2013.[153]\n\nThe number of transistors per chip cannot explain quality-adjusted microprocessor prices fully.[151][154][155] Moore's 1995 paper does not limit Moore's law to strict linearity or to transistor count, \"The definition of 'Moore's Law' has come to refer to almost anything related to the semiconductor industry that on a semi-log plot approximates a straight line. I hesitate to review its origins and by doing so restrict its definition.\"[129]\n\nHard disk drive areal density – A similar prediction (sometimes called Kryder's law) was made in 2005 for hard disk drive areal density.[156] The prediction was later viewed as over-optimistic. Several decades of rapid progress in areal density slowed around 2010, from 30 to 100% per year to 10–15% per year, because of noise related to smaller grain size of the disk media, thermal stability, and writability using available magnetic fields.[157][158]\n\nFiber-optic capacity – The number of bits per second that can be sent down an optical fiber increases exponentially, faster than Moore's law. Keck's law, in honor of Donald Keck.[159]\n\nNetwork capacity – According to Gerald Butters,[160][161] the former head of Lucent's Optical Networking Group at Bell Labs, there is another version, called Butters' Law of Photonics,[162] a formulation that deliberately parallels Moore's law. Butters' law says that the amount of data coming out of an optical fiber is doubling every nine months.[163] Thus, the cost of transmitting a bit over an optical network decreases by half every nine months. The availability of wavelength-division multiplexing (sometimes called WDM) increased the capacity that could be placed on a single fiber by as much as a factor of 100. Optical networking and dense wavelength-division multiplexing (DWDM) is rapidly bringing down the cost of networking, and further progress seems assured. As a result, the wholesale price of data traffic collapsed in the dot-com bubble. Nielsen's Law says that the bandwidth available to users increases by 50% annually.[164]\n\nPixels per dollar – Similarly, Barry Hendy of Kodak Australia has plotted pixels per dollar as a basic measure of value for a digital camera, demonstrating the historical linearity (on a log scale) of this market and the opportunity to predict the future trend of digital camera price, LCD and LED screens, and resolution.[165][166][167][168]\n\nThe great Moore's law compensator (TGMLC), also known as Wirth's law – generally is referred to as software bloat and is the principle that successive generations of computer software increase in size and complexity, thereby offsetting the performance gains predicted by Moore's law. In a 2008 article in InfoWorld, Randall C. Kennedy,[169] formerly of Intel, introduces this term using successive versions of Microsoft Office between the year 2000 and 2007 as his premise. Despite the gains in computational performance during this time period according to Moore's law, Office 2007 performed the same task at half the speed on a prototypical year 2007 computer as compared to Office 2000 on a year 2000 computer.\n\nLibrary expansion – was calculated in 1945 by Fremont Rider to double in capacity every 16 years, if sufficient space were made available.[170] He advocated replacing bulky, decaying printed works with miniaturized microform analog photographs, which could be duplicated on-demand for library patrons or other institutions. He did not foresee the digital technology that would follow decades later to replace analog microform with digital imaging, storage, and transmission media. Automated, potentially lossless digital technologies allowed vast increases in the rapidity of information growth in an era that now sometimes is called the Information Age.\n\nCarlson curve – is a term coined by The Economist[171] to describe the biotechnological equivalent of Moore's law, and is named after author Rob Carlson.[172] Carlson accurately predicted that the doubling time of DNA sequencing technologies (measured by cost and performance) would be at least as fast as Moore's law.[173] Carlson Curves illustrate the rapid (in some cases hyperexponential) decreases in cost, and increases in performance, of a variety of technologies, including DNA sequencing, DNA synthesis, and a range of physical and computational tools used in protein expression and in determining protein structures.\n\nEroom's law – is a pharmaceutical drug development observation that was deliberately written as Moore's Law spelled backward in order to contrast it with the exponential advancements of other forms of technology (such as transistors) over time. It states that the cost of developing a new drug roughly doubles every nine years.\n\nExperience curve effects says that each doubling of the cumulative production of virtually any product or service is accompanied by an approximate constant percentage reduction in the unit cost. The acknowledged first documented qualitative description of this dates from 1885.[174][175] A power curve was used to describe this phenomenon in a 1936 discussion of the cost of airplanes.[176]\n\nEdholm's law – Phil Edholm observed that the bandwidth of telecommunication networks (including the Internet) is doubling every 18 months.[177] The bandwidths of online communication networks has risen from bits per second to terabits per second. The rapid rise in online bandwidth is largely due to the same MOSFET scaling that enabled Moore's law, as telecommunications networks are built from MOSFETs.[178]\n\nHaitz's law predicts that the brightness of LEDs increases as their manufacturing cost goes down.\n\nSwanson's law is the observation that the price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. At present rates, costs go down 75% about every 10 years.",
        pageTitle: "Moore's law",
    },
    {
        title: "Euler's laws of motion",
        link: "https://en.wikipedia.org/wiki/Euler%27s_laws_of_motion",
        content:
            "In classical mechanics, Euler's laws of motion are equations of motion which extend Newton's laws of motion for point particle to rigid body motion.[1] They were formulated by Leonhard Euler about 50 years after Isaac Newton formulated his laws.\n\nEuler's first law states that the rate of change of linear momentum p of a rigid body is equal to the resultant of all the external forces Fext acting on the body:[2]\n\nInternal forces between the particles that make up a body do not contribute to changing the momentum of the body as there is an equal and opposite force resulting in no net effect.[3]\n\nThe linear momentum of a rigid body is the product of the mass of the body and the velocity of its center of mass vcm.[1][4][5]\n\nEuler's second law states that the rate of change of angular momentum L about a point that is fixed in an inertial reference frame (often the center of mass of the body), is equal to the sum of the external moments of force (torques) acting on that body M about that point:[1][4][5]\n\nNote that the above formula holds only if both M and L are computed with respect to a fixed inertial frame or a frame parallel to the inertial frame but fixed on the center of mass.\nFor rigid bodies translating and rotating in only two dimensions, this can be expressed as:[6]\n\nThe distribution of internal forces in a deformable body are not necessarily equal throughout, i.e. the stresses vary from one point to the next. This variation of internal forces throughout the body is governed by Newton's second law of motion of conservation of linear momentum and angular momentum, which for their simplest use are applied to a mass particle but are extended in continuum mechanics to a body of continuously distributed mass. For continuous bodies these laws are called Euler's laws of motion.[7]\n\nThe total body force applied to a continuous body with mass m, mass density ρ, and volume V, is the volume integral integrated over the volume of the body:\n\nwhere b is the force acting on the body per unit mass (dimensions of acceleration, misleadingly called the \"body force\"), and dm = ρ dV is an infinitesimal mass element of the body.\n\nBody forces and contact forces acting on the body lead to corresponding moments (torques) of those forces relative to a given point. Thus, the total applied torque M about the origin is given by\n\nwhere MB and MC respectively indicate the moments caused by the body and contact forces.\n\nThus, the sum of all applied forces and torques (with respect to the origin of the coordinate system) acting on the body can be given as the sum of a volume and surface integral:\n\nwhere t = t(n) is called the surface traction, integrated over the surface of the body, in turn n denotes a unit vector normal and directed outwards to the surface S.\n\nLet the coordinate system (x1, x2, x3) be an inertial frame of reference, r be the position vector of a point particle in the continuous body with respect to the origin of the coordinate system, and v = .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠dr/dt⁠ be the velocity vector of that point.\n\nEuler's first axiom or law (law of balance of linear momentum or balance of forces) states that in an inertial frame the time rate of change of linear momentum p of an arbitrary portion of a continuous body is equal to the total applied force F acting on that portion, and it is expressed as\n\nEuler's second axiom or law (law of balance of angular momentum or balance of torques) states that in an inertial frame the time rate of change of angular momentum L of an arbitrary portion of a continuous body is equal to the total applied torque M acting on that portion, and it is expressed as\n\nwhere \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n is the velocity, \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n the volume, and the derivatives of p and L are material derivatives.",
        pageTitle: "Euler's laws of motion",
    },
    {
        title: "Newton's laws of motion",
        link: "https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion",
        content:
            "Newton's laws of motion are three physical laws that describe the relationship between the motion of an object and the forces acting on it. These laws, which provide the basis for Newtonian mechanics, can be paraphrased as follows:\n\nThe three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), originally published in 1687.[3] Newton used them to investigate and explain the motion of many physical objects and systems.  In the time since Newton, new insights, especially around the concept of energy, built the field of classical mechanics on his foundations. Limitations to Newton's laws have also been discovered; new theories are necessary when objects move at very high speeds (special relativity), are very massive (general relativity), or are very small (quantum mechanics).\n\nNewton's laws are often stated in terms of point or particle masses, that is, bodies whose volume is negligible. This is a reasonable approximation for real bodies when the motion of internal parts can be neglected, and when the separation between bodies is much larger than the size of each. For instance, the Earth and the Sun can both be approximated as pointlike when considering the orbit of the former around the latter, but the Earth is not pointlike when considering activities on its surface.[note 1]\n\nThe mathematical description of motion, or kinematics, is based on the idea of specifying positions using numerical coordinates. Movement is represented by these numbers changing over time: a body's trajectory is represented by a function that assigns to each value of a time variable the values of all the position coordinates. The simplest case is one-dimensional, that is, when a body is constrained to move only along a straight line. Its position can then be given by a single number, indicating where it is relative to some chosen reference point. For example, a body might be free to slide along a track that runs left to right, and so its location can be specified by its distance from a convenient zero point, or origin, with negative numbers indicating positions to the left and positive numbers indicating positions to the right. If the body's location as a function of time is \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n  \n, then its average velocity over the time interval from \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n to \n  \n    \n      \n        \n          t\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle t_{1}}\n  \n is[6] \n  \n    \n      \n        \n          \n            \n              Δ\n              s\n            \n            \n              Δ\n              t\n            \n          \n        \n        =\n        \n          \n            \n              s\n              (\n              \n                t\n                \n                  1\n                \n              \n              )\n              −\n              s\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n            \n              \n                t\n                \n                  1\n                \n              \n              −\n              \n                t\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\Delta s}{\\Delta t}}={\\frac {s(t_{1})-s(t_{0})}{t_{1}-t_{0}}}.}\n  \nHere, the Greek letter \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n (delta) is used, per tradition, to mean \"change in\". A positive average velocity means that the position coordinate \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n increases over the interval in question, a negative average velocity indicates a net decrease over that interval, and an average velocity of zero means that the body ends the time interval in the same place as it began. Calculus gives the means to define an instantaneous velocity, a measure of a body's speed and direction of movement at a single moment of time, rather than over an interval. One notation for the instantaneous velocity is to replace \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n with the symbol \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n, for example,\n  \n    \n      \n        v\n        =\n        \n          \n            \n              d\n              s\n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle v={\\frac {ds}{dt}}.}\n  \nThis denotes that the instantaneous velocity is the derivative of the position with respect to time. It can roughly be thought of as the ratio between an infinitesimally small change in position \n  \n    \n      \n        d\n        s\n      \n    \n    {\\displaystyle ds}\n  \n to the infinitesimally small time interval \n  \n    \n      \n        d\n        t\n      \n    \n    {\\displaystyle dt}\n  \n over which it occurs.[7] More carefully, the velocity and all other derivatives can be defined using the concept of a limit.[6] A function \n  \n    \n      \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t)}\n  \n has a limit of \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n at a given input value \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n if the difference between \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n and \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n can be made arbitrarily small by choosing an input sufficiently close to \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n. One writes, \n  \n    \n      \n        \n          lim\n          \n            t\n            →\n            \n              t\n              \n                0\n              \n            \n          \n        \n        f\n        (\n        t\n        )\n        =\n        L\n        .\n      \n    \n    {\\displaystyle \\lim _{t\\to t_{0}}f(t)=L.}\n  \nInstantaneous velocity can be defined as the limit of the average velocity as the time interval shrinks to zero:\n  \n    \n      \n        \n          \n            \n              d\n              s\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          lim\n          \n            Δ\n            t\n            →\n            0\n          \n        \n        \n          \n            \n              s\n              (\n              t\n              +\n              Δ\n              t\n              )\n              −\n              s\n              (\n              t\n              )\n            \n            \n              Δ\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {ds}{dt}}=\\lim _{\\Delta t\\to 0}{\\frac {s(t+\\Delta t)-s(t)}{\\Delta t}}.}\n  \n Acceleration is to velocity as velocity is to position: it is the derivative of the velocity with respect to time.[note 2] Acceleration can likewise be defined as a limit:\n  \n    \n      \n        a\n        =\n        \n          \n            \n              d\n              v\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          lim\n          \n            Δ\n            t\n            →\n            0\n          \n        \n        \n          \n            \n              v\n              (\n              t\n              +\n              Δ\n              t\n              )\n              −\n              v\n              (\n              t\n              )\n            \n            \n              Δ\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a={\\frac {dv}{dt}}=\\lim _{\\Delta t\\to 0}{\\frac {v(t+\\Delta t)-v(t)}{\\Delta t}}.}\n  \nConsequently, the acceleration is the second derivative of position,[7] often written \n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              s\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}s}{dt^{2}}}}\n  \n.\n\nPosition, when thought of as a displacement from an origin point, is a vector: a quantity with both magnitude and direction.[9]: 1  Velocity and acceleration are vector quantities as well. The mathematical tools of vector algebra provide the means to describe motion in two, three or more dimensions. Vectors are often denoted with an arrow, as in  \n  \n    \n      \n        \n          \n            \n              s\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {s}}}\n  \n, or in bold typeface, such as \n  \n    \n      \n        \n          \n            s\n          \n        \n      \n    \n    {\\displaystyle {\\bf {s}}}\n  \n. Often, vectors are represented visually as arrows, with the direction of the vector being the direction of the arrow, and the magnitude of the vector indicated by the length of the arrow. Numerically, a vector can be represented as a list; for example, a body's velocity vector might be \n  \n    \n      \n        \n          v\n        \n        =\n        (\n        \n          3\n           \n          m\n          \n            /\n          \n          s\n        \n        ,\n        \n          4\n           \n          m\n          \n            /\n          \n          s\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {v} =(\\mathrm {3~m/s} ,\\mathrm {4~m/s} )}\n  \n, indicating that it is moving at 3 metres per second along the horizontal axis and 4 metres per second along the vertical axis. The same motion described in a different coordinate system will be represented by different numbers, and vector algebra can be used to translate between these alternatives.[9]: 4\n\nThe study of mechanics is complicated by the fact that household words like energy are used with a technical meaning.[10][11] Moreover, words which are synonymous in everyday speech are not so in physics: force is not the same as power or pressure, for example, and mass has a different meaning than weight.[12][13]: 150  The physics concept of force makes quantitative the everyday idea of a push or a pull. Forces in Newtonian mechanics are often due to strings and ropes, friction, muscle effort, gravity, and so forth. Like displacement, velocity, and acceleration, force is a vector quantity.\n\nNewton's first law expresses the principle of inertia: the natural behavior of a body is to move in a straight line at constant speed. A body's motion preserves the status quo, but external forces can perturb this.\n\nThe modern understanding of Newton's first law is that no inertial observer is privileged over any other. The concept of an inertial observer makes quantitative the everyday idea of feeling no effects of motion. For example, a person standing on the ground watching a train go past is an inertial observer. If the observer on the ground sees the train moving smoothly in a straight line at a constant speed, then a passenger sitting on the train will also be an inertial observer: the train passenger feels no motion. The principle expressed by Newton's first law is that there is no way to say which inertial observer is \"really\" moving and which is \"really\" standing still. One observer's state of rest is another observer's state of uniform motion in a straight line, and no experiment can deem either point of view to be correct or incorrect. There is no absolute standard of rest.[18][15]: 62–63 [19]: 7–9  Newton himself believed that absolute space and time existed, but that the only measures of space or time accessible to experiment are relative.[20]\n\nBy \"motion\", Newton meant the quantity now called momentum, which depends upon the amount of matter contained in a body, the speed at which that body is moving, and the direction in which it is moving.[21] In modern notation, the momentum of a body is the product of its mass and its velocity:\n\n  \n    \n      \n        \n          p\n        \n        =\n        m\n        \n          v\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} =m\\mathbf {v} \\,,}\n  \n\nwhere all three quantities can change over time.\nIn common cases the mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n does not change with time and the derivative acts only upon the velocity. Then force equals the product of the mass and the time derivative of the velocity, which is the acceleration:[22]\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              d\n              \n                v\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        m\n        \n          a\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {d\\mathbf {v} }{dt}}=m\\mathbf {a} \\,.}\n  \n\nAs the acceleration is the second derivative of position with respect to time, this can also be written\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \n                s\n              \n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {d^{2}\\mathbf {s} }{dt^{2}}}.}\n\nNewton's second law, in modern form, states that the time derivative of the momentum is the force:[23]: 4.1 \n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\frac {d\\mathbf {p} }{dt}}\\,.}\n  \n\nWhen applied to systems of variable mass, the equation above is only valid only for a fixed set of particles. Applying the derivative as in \n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        +\n        \n          v\n        \n        \n          \n            \n              \n                d\n              \n              m\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n         \n         \n        \n          (\n          i\n          n\n          c\n          o\n          r\n          r\n          e\n          c\n          t\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {\\mathrm {d} \\mathbf {v} }{\\mathrm {d} t}}+\\mathbf {v} {\\frac {\\mathrm {d} m}{\\mathrm {d} t}}\\ \\ \\mathrm {(incorrect)} }\n  \n\ncan lead to incorrect results.[24] For example, the momentum of a water jet system must include the momentum of the ejected water:[25]\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              e\n              x\n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        −\n        \n          \n            v\n          \n          \n            \n              e\n              j\n              e\n              c\n              t\n            \n          \n        \n        \n          \n            \n              \n                d\n              \n              m\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\mathrm {ext} }={\\mathrm {d} \\mathbf {p}  \\over \\mathrm {d} t}-\\mathbf {v} _{\\mathrm {eject} }{\\frac {\\mathrm {d} m}{\\mathrm {d} t}}.}\n\nThe forces acting on a body add as vectors, and so the total force on a body depends upon both the magnitudes and the directions of the individual forces.[23]: 58  When the net force on a body is equal to zero, then by Newton's second law, the body does not accelerate, and it is said to be in mechanical equilibrium. A state of mechanical equilibrium is stable if, when the position of the body is changed slightly, the body remains near that equilibrium. Otherwise, the equilibrium is unstable.[15]: 121 [23]: 174\n\nA common visual representation of forces acting in concert is the free body diagram, which schematically portrays a body of interest and the forces applied to it by outside influences.[26] For example, a free body diagram of a block sitting upon an inclined plane can illustrate the combination of gravitational force, \"normal\" force, friction, and string tension.[note 4]\n\nNewton's second law is sometimes presented as a definition of force, i.e., a force is that which exists when an inertial observer sees a body accelerating. This is sometimes regarded as a potential tautology — acceleration implies force, force implies acceleration. However, Newton's second law not only merely defines the force by the acceleration: forces exist as separate from the acceleration produced by the force in a particular system. The same force that is identified as producing acceleration to an object can then be applied to any other object, and the resulting accelerations (coming from that same force) will always be inversely proportional to the mass of the object.  What Newton's Second Law states is that all the effect of a force onto a system can be reduced to two pieces of information: the magnitude of the force, and it's direction, and then goes on to specify what the effect is.\n\nBeyond that, an equation detailing the force might also be specified, like Newton's law of universal gravitation. By inserting such an expression for \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n into Newton's second law, an equation with predictive power can be written.[note 5] Newton's second law has also been regarded as setting out a research program for physics, establishing that important goals of the subject are to identify the forces present in nature and to catalogue the constituents of matter.[15]: 134 [28]: 12-2\n\nHowever, forces can often be measured directly with no acceleration being involved, such as through weighing scales. By postulating a physical object that can be directly measured independently from acceleration, Newton made a objective physical statement with the second law alone, the predictions of which can be verified even if no force law is given.\n\nOverly brief paraphrases of the third law, like \"action equals reaction\" might have caused confusion among generations of students: the \"action\" and \"reaction\" apply to different bodies. For example, consider a book at rest on a table. The Earth's gravity pulls down upon the book. The \"reaction\" to that \"action\" is not the support force from the table holding up the book, but the gravitational pull of the book acting on the Earth.[note 6]\n\nNewton's third law relates to a more fundamental principle, the conservation of momentum. The latter remains true even in cases where Newton's statement does not, for instance when force fields as well as material bodies carry momentum, and when momentum is defined properly, in quantum mechanics as well.[note 7] In Newtonian mechanics, if two bodies have momenta \n  \n    \n      \n        \n          \n            p\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{1}}\n  \n and \n  \n    \n      \n        \n          \n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{2}}\n  \n respectively, then the total momentum of the pair is \n  \n    \n      \n        \n          p\n        \n        =\n        \n          \n            p\n          \n          \n            1\n          \n        \n        +\n        \n          \n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} =\\mathbf {p} _{1}+\\mathbf {p} _{2}}\n  \n, and the rate of change of \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is \n  \n    \n      \n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              d\n              \n                \n                  p\n                \n                \n                  1\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        +\n        \n          \n            \n              d\n              \n                \n                  p\n                \n                \n                  2\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\mathbf {p} }{dt}}={\\frac {d\\mathbf {p} _{1}}{dt}}+{\\frac {d\\mathbf {p} _{2}}{dt}}.}\n  \n By Newton's second law, the first term is the total force upon the first body, and the second term is the total force upon the second body. If the two bodies are isolated from outside influences, the only force upon the first body can be that from the second, and vice versa. By Newton's third law, these forces have equal magnitude but opposite direction, so they cancel when added, and \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is constant. Alternatively, if \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is known to be constant, it follows that the forces have equal magnitude and opposite direction.\n\nVarious sources have proposed elevating other ideas used in classical mechanics to the status of Newton's laws. For example, in Newtonian mechanics, the total mass of a body made by bringing together two smaller bodies is the sum of their individual masses. Frank Wilczek has suggested calling attention to this assumption by designating it \"Newton's Zeroth Law\".[37] Another candidate for a \"zeroth law\" is the fact that at any instant, a body reacts to the forces applied to it at that instant.[38] Likewise, the idea that forces add like vectors (or in other words obey the superposition principle), and the idea that forces change the energy of a body, have both been described as a \"fourth law\".[note 8]\n\nMoreover, some texts organize the basic ideas of Newtonian mechanics into different postulates, other than the three laws as commonly phrased, with the goal of being more clear about what is empirically observed and what is true by definition.[19]: 9 [27]\n\nThe study of the behavior of massive bodies using Newton's laws is known as Newtonian mechanics. Some example problems in Newtonian mechanics are particularly noteworthy for conceptual or historical reasons.\n\nIf a body falls from rest near the surface of the Earth, then in the absence of air resistance, it will accelerate at a constant rate. This is known as free fall. The speed attained during free fall is proportional to the elapsed time, and the distance traveled is proportional to the square of the elapsed time.[43] Importantly, the acceleration is the same for all bodies, independently of their mass. This follows from combining Newton's second law of motion with his law of universal gravitation. The latter states that the magnitude of the gravitational force from the Earth upon the body is\n\n  \n    \n      \n        F\n        =\n        \n          \n            \n              G\n              M\n              m\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle F={\\frac {GMm}{r^{2}}},}\n  \n\nwhere \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the mass of the falling body, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the mass of the Earth, \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n is Newton's constant, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n is the distance from the center of the Earth to the body's location, which is very nearly the radius of the Earth. Setting this equal to \n  \n    \n      \n        m\n        a\n      \n    \n    {\\displaystyle ma}\n  \n, the body's mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n cancels from both sides of the equation, leaving an acceleration that depends upon \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n can be taken to be constant. This particular value of acceleration is typically denoted \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n:\n\n  \n    \n      \n        g\n        =\n        \n          \n            \n              G\n              M\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ≈\n        \n          9.8\n           \n          m\n          \n            /\n          \n          \n            s\n            \n              2\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle g={\\frac {GM}{r^{2}}}\\approx \\mathrm {9.8~m/s^{2}} .}\n\nIf the body is not released from rest but instead launched upwards and/or horizontally with nonzero velocity, then free fall becomes projectile motion.[44] When air resistance can be neglected, projectiles follow parabola-shaped trajectories, because gravity affects the body's vertical motion and not its horizontal. At the peak of the projectile's trajectory, its vertical velocity is zero, but its acceleration is \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n downwards, as it is at all times. Setting the wrong vector equal to zero is a common confusion among physics students.[45]\n\nWhen a body is in uniform circular motion, the force on it changes the direction of its motion but not its speed. For a body moving in a circle of radius \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n at a constant speed \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n, its acceleration has a magnitude\n  \n    \n      \n        a\n        =\n        \n          \n            \n              v\n              \n                2\n              \n            \n            r\n          \n        \n      \n    \n    {\\displaystyle a={\\frac {v^{2}}{r}}}\n  \nand is directed toward the center of the circle.[note 9] The force required to sustain this acceleration, called the centripetal force, is therefore also directed toward the center of the circle and has magnitude \n  \n    \n      \n        m\n        \n          v\n          \n            2\n          \n        \n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle mv^{2}/r}\n  \n. Many orbits, such as that of the Moon around the Earth, can be approximated by uniform circular motion. In such cases, the centripetal force is gravity, and by Newton's law of universal gravitation has magnitude \n  \n    \n      \n        G\n        M\n        m\n        \n          /\n        \n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle GMm/r^{2}}\n  \n, where \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the mass of the larger body being orbited. Therefore, the mass of a body can be calculated from observations of another body orbiting around it.[47]: 130\n\nNewton's cannonball is a thought experiment that interpolates between projectile motion and uniform circular motion. A cannonball that is lobbed weakly off the edge of a tall cliff will hit the ground in the same amount of time as if it were dropped from rest, because the force of gravity only affects the cannonball's momentum in the downward direction, and its effect is not diminished by horizontal movement. If the cannonball is launched with a greater initial horizontal velocity, then it will travel farther before it hits the ground, but it will still hit the ground in the same amount of time. However, if the cannonball is launched with an even larger initial velocity, then the curvature of the Earth becomes significant: the ground itself will curve away from the falling cannonball. A very fast cannonball will fall away from the inertial straight-line trajectory at the same rate that the Earth curves away beneath it; in other words, it will be in orbit (imagining that it is not slowed by air resistance or obstacles).[48]\n\nConsider a body of mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n able to move along the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n axis, and suppose an equilibrium point exists at the position \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n. That is, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n, the net force upon the body is the zero vector, and by Newton's second law, the body will not accelerate. If the force upon the body is proportional to the displacement from the equilibrium point, and directed to the equilibrium point, then the body will perform simple harmonic motion. Writing the force as \n  \n    \n      \n        F\n        =\n        −\n        k\n        x\n      \n    \n    {\\displaystyle F=-kx}\n  \n, Newton's second law becomes\n\n  \n    \n      \n        m\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              x\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        k\n        x\n        \n        .\n      \n    \n    {\\displaystyle m{\\frac {d^{2}x}{dt^{2}}}=-kx\\,.}\n  \n\nThis differential equation has the solution\n\n  \n    \n      \n        x\n        (\n        t\n        )\n        =\n        A\n        cos\n        ⁡\n        ω\n        t\n        +\n        B\n        sin\n        ⁡\n        ω\n        t\n        \n      \n    \n    {\\displaystyle x(t)=A\\cos \\omega t+B\\sin \\omega t\\,}\n  \n\nwhere the frequency \n  \n    \n      \n        ω\n      \n    \n    {\\displaystyle \\omega }\n  \n is equal to \n  \n    \n      \n        \n          \n            k\n            \n              /\n            \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {k/m}}}\n  \n, and the constants \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n can be calculated knowing, for example, the position and velocity the body has at a given time, like \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n.\n\nOne reason that the harmonic oscillator is a conceptually important example is that it is good approximation for many systems near a stable mechanical equilibrium.[note 10] For example, a pendulum has a stable equilibrium in the vertical position: if motionless there, it will remain there, and if pushed slightly, it will swing back and forth. Neglecting air resistance and friction in the pivot, the force upon the pendulum is gravity, and Newton's second law becomes \n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        \n          \n            g\n            L\n          \n        \n        sin\n        ⁡\n        θ\n        ,\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}=-{\\frac {g}{L}}\\sin \\theta ,}\n  \nwhere \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the length of the pendulum and \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is its angle from the vertical. When the angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is small, the sine of \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is nearly equal to \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n (see small-angle approximation), and so this expression simplifies to the equation for a simple harmonic oscillator with frequency \n  \n    \n      \n        ω\n        =\n        \n          \n            g\n            \n              /\n            \n            L\n          \n        \n      \n    \n    {\\displaystyle \\omega ={\\sqrt {g/L}}}\n  \n.\n\nA harmonic oscillator can be damped, often by friction or viscous drag, in which case energy bleeds out of the oscillator and the amplitude of the oscillations decreases over time. Also, a harmonic oscillator can be driven by an applied force, which can lead to the phenomenon of resonance.[50]\n\nNewtonian physics treats matter as being neither created nor destroyed, though it may be rearranged. It can be the case that an object of interest gains or loses mass because matter is added to or removed from it. In such a situation, Newton's laws can be applied to the individual pieces of matter, keeping track of which pieces belong to the object of interest over time. For instance, if a rocket of mass \n  \n    \n      \n        M\n        (\n        t\n        )\n      \n    \n    {\\displaystyle M(t)}\n  \n, moving at velocity \n  \n    \n      \n        \n          v\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {v} (t)}\n  \n, ejects matter at a velocity \n  \n    \n      \n        \n          u\n        \n      \n    \n    {\\displaystyle \\mathbf {u} }\n  \n relative to the rocket, then[24]\n\n  \n    \n      \n        \n          F\n        \n        =\n        M\n        \n          \n            \n              d\n              \n                v\n              \n            \n            \n              d\n              t\n            \n          \n        \n        −\n        \n          u\n        \n        \n          \n            \n              d\n              M\n            \n            \n              d\n              t\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {F} =M{\\frac {d\\mathbf {v} }{dt}}-\\mathbf {u} {\\frac {dM}{dt}}\\,}\n  \n\nwhere \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n is the net external force (e.g., a planet's gravitational pull).[23]: 139\n\nThe fan and sail example is a situation studied in discussions of Newton's third law.[51] In the situation, a fan is attached to a cart or a sailboat and blows on its sail. From the third law, one would reason that the force of the air pushing in one direction would cancel out the force done by the fan on the sail, leaving the entire apparatus stationary. However, because the system is not entirely enclosed, there are conditions in which the vessel will move; for example, if the sail is built in a manner that redirects the majority of the airflow back towards the fan, the net force will result in the vessel moving forward.[34][52]\n\nThe concept of energy was developed after Newton's time, but it has become an inseparable part of what is considered \"Newtonian\" physics. Energy can broadly be classified into kinetic, due to a body's motion, and potential, due to a body's position relative to others. Thermal energy, the energy carried by heat flow, is a type of kinetic energy not associated with the macroscopic motion of objects but instead with the movements of the atoms and molecules of which they are made. According to the work-energy theorem, when a force acts upon a body while that body moves along the line of the force, the force does work upon the body, and the amount of work done is equal to the change in the body's kinetic energy.[note 11] In many cases of interest, the net work done by a force when a body moves in a closed loop — starting at a point, moving along some trajectory, and returning to the initial point — is zero. If this is the case, then the force can be written in terms of the gradient of a function called a scalar potential:[46]: 303 \n\n  \n    \n      \n        \n          F\n        \n        =\n        −\n        \n          ∇\n        \n        U\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =-\\mathbf {\\nabla } U\\,.}\n  \n\nThis is true for many forces including that of gravity, but not for friction; indeed, almost any problem in a mechanics textbook that does not involve friction can be expressed in this way.[49]: 19  The fact that the force can be written in this way can be understood from the conservation of energy. Without friction to dissipate a body's energy into heat, the body's energy will trade between potential and (non-thermal) kinetic forms while the total amount remains constant. Any gain of kinetic energy, which occurs when the net force on the body accelerates it to a higher speed, must be accompanied by a loss of potential energy. So, the net force upon the body is determined by the manner in which the potential energy decreases.\n\nA rigid body is an object whose size is too large to neglect and which maintains the same shape over time. In Newtonian mechanics, the motion of a rigid body is often understood by separating it into movement of the body's center of mass and movement around the center of mass.\n\nSignificant aspects of the motion of an extended body can be understood by imagining the mass of that body concentrated to a single point, known as the center of mass. The location of a body's center of mass depends upon how that body's material is distributed. For a collection of pointlike objects with masses \n  \n    \n      \n        \n          m\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          m\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle m_{1},\\ldots ,m_{N}}\n  \n at positions \n  \n    \n      \n        \n          \n            r\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            r\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{1},\\ldots ,\\mathbf {r} _{N}}\n  \n, the center of mass is located at \n  \n    \n      \n        \n          R\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \n              \n                m\n                \n                  i\n                \n              \n              \n                \n                  r\n                \n                \n                  i\n                \n              \n            \n            M\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {R} =\\sum _{i=1}^{N}{\\frac {m_{i}\\mathbf {r} _{i}}{M}},}\n  \n where \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the total mass of the collection. In the absence of a net external force, the center of mass moves at a constant speed in a straight line. This applies, for example, to a collision between two bodies.[55] If the total external force is not zero, then the center of mass changes velocity as though it were a point body of mass \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. This follows from the fact that the internal forces within the collection, the forces that the objects exert upon each other, occur in balanced pairs by Newton's third law. In a system of two bodies with one much more massive than the other, the center of mass will approximately coincide with the location of the more massive body.[19]: 22–24\n\nWhen Newton's laws are applied to rotating extended bodies, they lead to new quantities that are analogous to those invoked in the original laws. The analogue of mass is the moment of inertia, the counterpart of momentum is angular momentum, and the counterpart of force is torque.\n\nAngular momentum is calculated with respect to a reference point.[56] If the displacement vector from a reference point to a body is \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and the body has momentum \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n, then the body's angular momentum with respect to that point is, using the vector cross product, \n  \n    \n      \n        \n          L\n        \n        =\n        \n          r\n        \n        ×\n        \n          p\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {L} =\\mathbf {r} \\times \\mathbf {p} .}\n  \n Taking the time derivative of the angular momentum gives \n  \n    \n      \n        \n          \n            \n              d\n              \n                L\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          (\n          \n            \n              \n                d\n                \n                  r\n                \n              \n              \n                d\n                t\n              \n            \n          \n          )\n        \n        ×\n        \n          p\n        \n        +\n        \n          r\n        \n        ×\n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          v\n        \n        ×\n        m\n        \n          v\n        \n        +\n        \n          r\n        \n        ×\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\mathbf {L} }{dt}}=\\left({\\frac {d\\mathbf {r} }{dt}}\\right)\\times \\mathbf {p} +\\mathbf {r} \\times {\\frac {d\\mathbf {p} }{dt}}=\\mathbf {v} \\times m\\mathbf {v} +\\mathbf {r} \\times \\mathbf {F} .}\n  \n The first term vanishes because \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n and \n  \n    \n      \n        m\n        \n          v\n        \n      \n    \n    {\\displaystyle m\\mathbf {v} }\n  \n point in the same direction. The remaining term is the torque, \n  \n    \n      \n        \n          τ\n        \n        =\n        \n          r\n        \n        ×\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {\\tau } =\\mathbf {r} \\times \\mathbf {F} .}\n  \n When the torque is zero, the angular momentum is constant, just as when the force is zero, the momentum is constant.[19]: 14–15  The torque can vanish even when the force is non-zero, if the body is located at the reference point (\n  \n    \n      \n        \n          r\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {r} =0}\n  \n) or if the force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n and the displacement vector \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n are directed along the same line.\n\nThe angular momentum of a collection of point masses, and thus of an extended body, is found by adding the contributions from each of the points. This provides a means to characterize a body's rotation about an axis, by adding up the angular momenta of its individual pieces. The result depends on the chosen axis, the shape of the body, and the rate of rotation.[19]: 28\n\nNewton's law of universal gravitation states that any body attracts any other body along the straight line connecting them. The size of the attracting force is proportional to the product of their masses, and inversely proportional to the square of the distance between them. Finding the shape of the orbits that an inverse-square force law will produce is known as the Kepler problem. The Kepler problem can be solved in multiple ways, including by demonstrating that the Laplace–Runge–Lenz vector is constant,[57] or by applying a duality transformation to a 2-dimensional harmonic oscillator.[58] However it is solved, the result is that orbits will be conic sections, that is, ellipses (including circles), parabolas, or hyperbolas. The eccentricity of the orbit, and thus the type of conic section, is determined by the energy and the angular momentum of the orbiting body. Planets do not have sufficient energy to escape the Sun, and so their orbits are ellipses, to a good approximation; because the planets pull on one another, actual orbits are not exactly conic sections.\n\nIf a third mass is added, the Kepler problem becomes the three-body problem, which in general has no exact solution in closed form. That is, there is no way to start from the differential equations implied by Newton's laws and, after a finite sequence of standard mathematical operations, obtain equations that express the three bodies' motions over time.[59][60] Numerical methods can be applied to obtain useful, albeit approximate, results for the three-body problem.[61] The positions and velocities of the bodies can be stored in variables within a computer's memory; Newton's laws are used to calculate how the velocities will change over a short interval of time, and knowing the velocities, the changes of position over that time interval can be computed. This process is looped to calculate, approximately, the bodies' trajectories. Generally speaking, the shorter the time interval, the more accurate the approximation.[62]\n\nNewton's laws of motion allow the possibility of chaos.[63][64] That is, qualitatively speaking, physical systems obeying Newton's laws can exhibit sensitive dependence upon their initial conditions: a slight change of the position or velocity of one part of a system can lead to the whole system behaving in a radically different way within a short time. Noteworthy examples include the three-body problem, the double pendulum, dynamical billiards, and the Fermi–Pasta–Ulam–Tsingou problem.\n\nNewton's laws can be applied to fluids by considering a fluid as composed of infinitesimal pieces, each exerting forces upon neighboring pieces. The Euler momentum equation is an expression of Newton's second law adapted to fluid dynamics.[65][66] A fluid is described by a velocity field, i.e., a function \n  \n    \n      \n        \n          v\n        \n        (\n        \n          x\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {v} (\\mathbf {x} ,t)}\n  \n that assigns a velocity vector to each point in space and time. A small object being carried along by the fluid flow can change velocity for two reasons: first, because the velocity field at its position is changing over time, and second, because it moves to a new location where the velocity field has a different value. Consequently, when Newton's second law is applied to an infinitesimal portion of fluid, the acceleration \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n  \n has two terms, a combination known as a total or material derivative. The mass of an infinitesimal portion depends upon the fluid density, and there is a net force upon it if the fluid pressure varies from one side of it to another. Accordingly, \n  \n    \n      \n        \n          a\n        \n        =\n        \n          F\n        \n        \n          /\n        \n        m\n      \n    \n    {\\displaystyle \\mathbf {a} =\\mathbf {F} /m}\n  \n becomes\n\n  \n    \n      \n        \n          \n            \n              ∂\n              v\n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          ∇\n        \n        ⋅\n        \n          v\n        \n        )\n        \n          v\n        \n        =\n        −\n        \n          \n            1\n            ρ\n          \n        \n        \n          ∇\n        \n        P\n        +\n        \n          f\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\partial v}{\\partial t}}+(\\mathbf {\\nabla } \\cdot \\mathbf {v} )\\mathbf {v} =-{\\frac {1}{\\rho }}\\mathbf {\\nabla } P+\\mathbf {f} ,}\n  \n\nwhere \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is the density, \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is the pressure, and \n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle \\mathbf {f} }\n  \n stands for an external influence like a gravitational pull. Incorporating the effect of viscosity turns the Euler equation into a Navier–Stokes equation:\n\n  \n    \n      \n        \n          \n            \n              ∂\n              v\n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          ∇\n        \n        ⋅\n        \n          v\n        \n        )\n        \n          v\n        \n        =\n        −\n        \n          \n            1\n            ρ\n          \n        \n        \n          ∇\n        \n        P\n        +\n        ν\n        \n          ∇\n          \n            2\n          \n        \n        \n          v\n        \n        +\n        \n          f\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\partial v}{\\partial t}}+(\\mathbf {\\nabla } \\cdot \\mathbf {v} )\\mathbf {v} =-{\\frac {1}{\\rho }}\\mathbf {\\nabla } P+\\nu \\nabla ^{2}\\mathbf {v} +\\mathbf {f} ,}\n  \n\nwhere \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n is the kinematic viscosity.[65]\n\nIt is mathematically possible for a collection of point masses, moving in accord with Newton's laws, to launch some of themselves away so forcefully that they fly off to infinity in a finite time.[67] This unphysical behavior, known as a \"noncollision singularity\",[60] depends upon the masses being pointlike and able to approach one another arbitrarily closely, as well as the lack of a relativistic speed limit in Newtonian physics.[68]\n\nIt is not yet known whether or not the Euler and Navier–Stokes equations exhibit the analogous behavior of initially smooth solutions \"blowing up\" in finite time. The question of existence and smoothness of Navier–Stokes solutions is one of the Millennium Prize Problems.[69]\n\nClassical mechanics can be mathematically formulated in multiple different ways, other than the \"Newtonian\" description (which itself, of course, incorporates contributions from others both before and after Newton). The physical content of these different formulations is the same as the Newtonian, but they provide different insights and facilitate different types of calculations. For example, Lagrangian mechanics helps make apparent the connection between symmetries and conservation laws, and it is useful when calculating the motion of constrained bodies, like a mass restricted to move along a curving track or on the surface of a sphere.[19]: 48  Hamiltonian mechanics is convenient for statistical physics,[70][71]: 57  leads to further insight about symmetry,[19]: 251  and can be developed into sophisticated techniques for perturbation theory.[19]: 284  Due to the breadth of these topics, the discussion here will be confined to concise treatments of how they reformulate Newton's laws of motion.\n\nLagrangian mechanics differs from the Newtonian formulation by considering entire trajectories at once rather than predicting a body's motion at a single instant.[19]: 109  It is traditional in Lagrangian mechanics to denote position with \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n and velocity with \n  \n    \n      \n        \n          \n            \n              q\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {q}}}\n  \n. The simplest example is a massive point particle, the Lagrangian for which can be written as the difference between its kinetic and potential energies:\n\n  \n    \n      \n        L\n        (\n        q\n        ,\n        \n          \n            \n              q\n              ˙\n            \n          \n        \n        )\n        =\n        T\n        −\n        V\n        ,\n      \n    \n    {\\displaystyle L(q,{\\dot {q}})=T-V,}\n  \n\nwhere the kinetic energy is\n\n  \n    \n      \n        T\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle T={\\frac {1}{2}}m{\\dot {q}}^{2}}\n  \n\nand the potential energy is some function of the position, \n  \n    \n      \n        V\n        (\n        q\n        )\n      \n    \n    {\\displaystyle V(q)}\n  \n. The physical path that the particle will take between an initial point \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and a final point \n  \n    \n      \n        \n          q\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle q_{f}}\n  \n is the path for which the integral of the Lagrangian is \"stationary\". That is, the physical path has the property that small perturbations of it will, to a first approximation, not change the integral of the Lagrangian. Calculus of variations provides the mathematical tools for finding this path.[46]: 485  Applying the calculus of variations to the task of finding the path yields the Euler–Lagrange equation for the particle,\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \n          (\n          \n            \n              \n                ∂\n                L\n              \n              \n                ∂\n                \n                  \n                    \n                      q\n                      ˙\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              q\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}\\left({\\frac {\\partial L}{\\partial {\\dot {q}}}}\\right)={\\frac {\\partial L}{\\partial q}}.}\n  \n\nEvaluating the partial derivatives of the Lagrangian gives\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        (\n        m\n        \n          \n            \n              q\n              ˙\n            \n          \n        \n        )\n        =\n        −\n        \n          \n            \n              d\n              V\n            \n            \n              d\n              q\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}(m{\\dot {q}})=-{\\frac {dV}{dq}},}\n  \n\nwhich is a restatement of Newton's second law. The left-hand side is the time derivative of the momentum, and the right-hand side is the force, represented in terms of the potential energy.[9]: 737\n\nLandau and Lifshitz argue that the Lagrangian formulation makes the conceptual content of classical mechanics more clear than starting with Newton's laws.[29] Lagrangian mechanics provides a convenient framework in which to prove Noether's theorem, which relates symmetries and conservation laws.[72] The conservation of momentum can be derived by applying Noether's theorem to a Lagrangian for a multi-particle system, and so, Newton's third law is a theorem rather than an assumption.[19]: 124\n\nIn Hamiltonian mechanics, the dynamics of a system are represented by a function called the Hamiltonian, which in many cases of interest is equal to the total energy of the system.[9]: 742  The Hamiltonian is a function of the positions and the momenta of all the bodies making up the system, and it may also depend explicitly upon time. The time derivatives of the position and momentum variables are given by partial derivatives of the Hamiltonian, via Hamilton's equations.[19]: 203  The simplest example is a point mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n constrained to move in a straight line, under the effect of a potential. Writing \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n for the position coordinate and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n for the body's momentum, the Hamiltonian is\n\n  \n    \n      \n        \n          \n            H\n          \n        \n        (\n        p\n        ,\n        q\n        )\n        =\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        +\n        V\n        (\n        q\n        )\n        .\n      \n    \n    {\\displaystyle {\\mathcal {H}}(p,q)={\\frac {p^{2}}{2m}}+V(q).}\n  \n\nIn this example, Hamilton's equations are\n\n  \n    \n      \n        \n          \n            \n              d\n              q\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {dq}{dt}}={\\frac {\\partial {\\mathcal {H}}}{\\partial p}}}\n  \n\nand\n\n  \n    \n      \n        \n          \n            \n              d\n              p\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              q\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dp}{dt}}=-{\\frac {\\partial {\\mathcal {H}}}{\\partial q}}.}\n  \n\nEvaluating these partial derivatives, the former equation becomes\n\n  \n    \n      \n        \n          \n            \n              d\n              q\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            p\n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dq}{dt}}={\\frac {p}{m}},}\n  \n\nwhich reproduces the familiar statement that a body's momentum is the product of its mass and velocity. The time derivative of the momentum is\n\n  \n    \n      \n        \n          \n            \n              d\n              p\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              d\n              V\n            \n            \n              d\n              q\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dp}{dt}}=-{\\frac {dV}{dq}},}\n  \n\nwhich, upon identifying the negative derivative of the potential with the force, is just Newton's second law once again.[63][9]: 742\n\nAs in the Lagrangian formulation, in Hamiltonian mechanics the conservation of momentum can be derived using Noether's theorem, making Newton's third law an idea that is deduced rather than assumed.[19]: 251\n\nAmong the proposals to reform the standard introductory-physics curriculum is one that teaches the concept of energy before that of force, essentially \"introductory Hamiltonian mechanics\".[73][74]\n\nThe Hamilton–Jacobi equation provides yet another formulation of classical mechanics, one which makes it mathematically analogous to wave optics.[19]: 284 [75] This formulation also uses Hamiltonian functions, but in a different way than the formulation described above. The paths taken by bodies or collections of bodies are deduced from a function \n  \n    \n      \n        S\n        (\n        \n          \n            q\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            q\n          \n          \n            2\n          \n        \n        ,\n        …\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle S(\\mathbf {q} _{1},\\mathbf {q} _{2},\\ldots ,t)}\n  \n of positions \n  \n    \n      \n        \n          \n            q\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {q} _{i}}\n  \n and time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. The Hamiltonian is incorporated into the Hamilton–Jacobi equation, a differential equation for \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n. Bodies move over time in such a way that their trajectories are perpendicular to the surfaces of constant \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, analogously to how a light ray propagates in the direction perpendicular to its wavefront. This is simplest to express for the case of a single point mass, in which \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is a function \n  \n    \n      \n        S\n        (\n        \n          q\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle S(\\mathbf {q} ,t)}\n  \n, and the point mass moves in the direction along which \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n changes most steeply. In other words, the momentum of the point mass is the gradient of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n:\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          ∇\n        \n        S\n        .\n      \n    \n    {\\displaystyle \\mathbf {v} ={\\frac {1}{m}}\\mathbf {\\nabla } S.}\n  \n\nThe Hamilton–Jacobi equation for a point mass is\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        H\n        \n          (\n          \n            \n              q\n            \n            ,\n            \n              ∇\n            \n            S\n            ,\n            t\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial S}{\\partial t}}=H\\left(\\mathbf {q} ,\\mathbf {\\nabla } S,t\\right).}\n  \n\nThe relation to Newton's laws can be seen by considering a point mass moving in a time-independent potential \n  \n    \n      \n        V\n        (\n        \n          q\n        \n        )\n      \n    \n    {\\displaystyle V(\\mathbf {q} )}\n  \n, in which case the Hamilton–Jacobi equation becomes\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          \n            (\n            \n              \n                ∇\n              \n              S\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        V\n        (\n        \n          q\n        \n        )\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial S}{\\partial t}}={\\frac {1}{2m}}\\left(\\mathbf {\\nabla } S\\right)^{2}+V(\\mathbf {q} ).}\n  \n\nTaking the gradient of both sides, this becomes\n\n  \n    \n      \n        −\n        \n          ∇\n        \n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          ∇\n        \n        \n          \n            (\n            \n              \n                ∇\n              \n              S\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle -\\mathbf {\\nabla } {\\frac {\\partial S}{\\partial t}}={\\frac {1}{2m}}\\mathbf {\\nabla } \\left(\\mathbf {\\nabla } S\\right)^{2}+\\mathbf {\\nabla } V.}\n  \n\nInterchanging the order of the partial derivatives on the left-hand side, and using the power and chain rules on the first term on the right-hand side,\n\n  \n    \n      \n        −\n        \n          \n            ∂\n            \n              ∂\n              t\n            \n          \n        \n        \n          ∇\n        \n        S\n        =\n        \n          \n            1\n            m\n          \n        \n        \n          (\n          \n            \n              ∇\n            \n            S\n            ⋅\n            \n              ∇\n            \n          \n          )\n        \n        \n          ∇\n        \n        S\n        +\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial }{\\partial t}}\\mathbf {\\nabla } S={\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\mathbf {\\nabla } S+\\mathbf {\\nabla } V.}\n  \n\nGathering together the terms that depend upon the gradient of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n,\n\n  \n    \n      \n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                1\n                m\n              \n            \n            \n              (\n              \n                \n                  ∇\n                \n                S\n                ⋅\n                \n                  ∇\n                \n              \n              )\n            \n          \n          ]\n        \n        \n          ∇\n        \n        S\n        =\n        −\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle \\left[{\\frac {\\partial }{\\partial t}}+{\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\right]\\mathbf {\\nabla } S=-\\mathbf {\\nabla } V.}\n  \n\nThis is another re-expression of Newton's second law.[76] The expression in brackets is a total or material derivative as mentioned above,[77] in which the first term indicates how the function being differentiated changes over time at a fixed location, and the second term captures how a moving particle will see different values of that function as it travels from place to place:\n\n  \n    \n      \n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                1\n                m\n              \n            \n            \n              (\n              \n                \n                  ∇\n                \n                S\n                ⋅\n                \n                  ∇\n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              v\n            \n            ⋅\n            \n              ∇\n            \n          \n          ]\n        \n        =\n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\left[{\\frac {\\partial }{\\partial t}}+{\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\right]=\\left[{\\frac {\\partial }{\\partial t}}+\\mathbf {v} \\cdot \\mathbf {\\nabla } \\right]={\\frac {d}{dt}}.}\n\nIn statistical physics, the kinetic theory of gases applies Newton's laws of motion to large numbers (typically on the order of the Avogadro number) of particles. Kinetic theory can explain, for example, the pressure that a gas exerts upon the container holding it as the aggregate of many impacts of atoms, each imparting a tiny amount of momentum.[71]: 62\n\nThe Langevin equation is a special case of Newton's second law, adapted for the case of describing a small object bombarded stochastically by even smaller ones.[78]: 235  It can be written\n  \n    \n      \n        m\n        \n          a\n        \n        =\n        −\n        γ\n        \n          v\n        \n        +\n        \n          ξ\n        \n        \n      \n    \n    {\\displaystyle m\\mathbf {a} =-\\gamma \\mathbf {v} +\\mathbf {\\xi } \\,}\n  \nwhere \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is a drag coefficient and \n  \n    \n      \n        \n          ξ\n        \n      \n    \n    {\\displaystyle \\mathbf {\\xi } }\n  \n is a force that varies randomly from instant to instant, representing the net effect of collisions with the surrounding particles. This is used to model Brownian motion.[79]\n\nNewton's three laws can be applied to phenomena involving electricity and magnetism, though subtleties and caveats exist.\n\nCoulomb's law for the electric force between two stationary, electrically charged bodies has much the same mathematical form as Newton's law of universal gravitation: the force is proportional to the product of the charges, inversely proportional to the square of the distance between them, and directed along the straight line between them. The Coulomb force that a charge \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n exerts upon a charge \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n is equal in magnitude to the force that \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n exerts upon \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n, and it points in the exact opposite direction. Coulomb's law is thus consistent with Newton's third law.[80]\n\nElectromagnetism treats forces as produced by fields acting upon charges. The Lorentz force law provides an expression for the force upon a charged body that can be plugged into Newton's second law in order to calculate its acceleration.[81]: 85  According to the Lorentz force law, a charged body in an electric field experiences a force in the direction of that field, a force proportional to its charge \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n and to the strength of the electric field. In addition, a moving charged body in a magnetic field experiences a force that is also proportional to its charge, in a direction perpendicular to both the field and the body's direction of motion. Using the vector cross product,\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          E\n        \n        +\n        q\n        \n          v\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\mathbf {E} +q\\mathbf {v} \\times \\mathbf {B} .}\n\nIf the electric field vanishes (\n  \n    \n      \n        \n          E\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {E} =0}\n  \n), then the force will be perpendicular to the charge's motion, just as in the case of uniform circular motion studied above, and the charge will circle (or more generally move in a helix) around the magnetic field lines at the cyclotron frequency \n  \n    \n      \n        ω\n        =\n        q\n        B\n        \n          /\n        \n        m\n      \n    \n    {\\displaystyle \\omega =qB/m}\n  \n.[78]: 222  Mass spectrometry works by applying electric and/or magnetic fields to moving charges and measuring the resulting acceleration, which by the Lorentz force law yields the mass-to-charge ratio.[82]\n\nCollections of charged bodies do not always obey Newton's third law: there can be a change of one body's momentum without a compensatory change in the momentum of another. The discrepancy is accounted for by momentum carried by the electromagnetic field itself. The momentum per unit volume of the electromagnetic field is proportional to the Poynting vector.[83]: 184 [84]\n\nThere is subtle conceptual conflict between electromagnetism and Newton's first law: Maxwell's theory of electromagnetism predicts that electromagnetic waves will travel through empty space at a constant, definite speed. Thus, some inertial observers seemingly have a privileged status over the others, namely those who measure the speed of light and find it to be the value predicted by the Maxwell equations. In other words, light provides an absolute standard for speed, yet the principle of inertia holds that there should be no such standard. This tension is resolved in the theory of special relativity, which revises the notions of space and time in such a way that all inertial observers will agree upon the speed of light in vacuum.[note 12]\n\nIn special relativity, the rule that Wilczek called \"Newton's Zeroth Law\" breaks down: the mass of a composite object is not merely the sum of the masses of the individual pieces.[87]: 33  Newton's first law, inertial motion, remains true. A form of Newton's second law, that force is the rate of change of momentum, also holds, as does the conservation of momentum. However, the definition of momentum is modified. Among the consequences of this is the fact that the more quickly a body moves, the harder it is to accelerate, and so, no matter how much force is applied, a body cannot be accelerated to the speed of light. Depending on the problem at hand, momentum in special relativity can be represented as a three-dimensional vector, \n  \n    \n      \n        \n          p\n        \n        =\n        m\n        γ\n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {p} =m\\gamma \\mathbf {v} }\n  \n, where \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the body's rest mass and \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is the Lorentz factor, which depends upon the body's speed. Alternatively, momentum and force can be represented as four-vectors.[88]: 107\n\nNewton's third law must be modified in special relativity. The third law refers to the forces between two bodies at the same moment in time, and a key feature of special relativity is that simultaneity is relative. Events that happen at the same time relative to one observer can happen at different times relative to another. So, in a given observer's frame of reference, action and reaction may not be exactly opposite, and the total momentum of interacting bodies may not be conserved. The conservation of momentum is restored by including the momentum stored in the field that describes the bodies' interaction.[89][90]\n\nNewtonian mechanics is a good approximation to special relativity when the speeds involved are small compared to that of light.[91]: 131\n\nGeneral relativity is a theory of gravity that advances beyond that of Newton. In general relativity, the gravitational force of Newtonian mechanics is reimagined as curvature of spacetime. A curved path like an orbit, attributed to a gravitational force in Newtonian mechanics, is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: \"Spacetime tells matter how to move; matter tells spacetime how to curve.\"[92][93] Wheeler himself thought of this reciprocal relationship as a modern, generalized form of Newton's third law.[92] The relation between matter distribution and spacetime curvature is given by the Einstein field equations, which require tensor calculus to express.[87]: 43 [94]\n\nThe Newtonian theory of gravity is a good approximation to the predictions of general relativity when gravitational effects are weak and objects are moving slowly compared to the speed of light.[85]: 327 [95]\n\nQuantum mechanics is a theory of physics originally developed in order to understand microscopic phenomena: behavior at the scale of molecules, atoms or subatomic particles. Generally and loosely speaking, the smaller a system is, the more an adequate mathematical model will require understanding quantum effects. The conceptual underpinning of quantum physics is very different from that of classical physics. Instead of thinking about quantities like position, momentum, and energy as properties that an object has, one considers what result might appear when a measurement of a chosen type is performed. Quantum mechanics allows the physicist to calculate the probability that a chosen measurement will elicit a particular result.[96][97] The expectation value for a measurement is the average of the possible results it might yield, weighted by their probabilities of occurrence.[98]\n\nThe Ehrenfest theorem provides a connection between quantum expectation values and Newton's second law, a connection that is necessarily inexact, as quantum physics is fundamentally different from classical. In quantum physics, position and momentum are represented by mathematical entities known as Hermitian operators, and the Born rule is used to calculate the expectation values of a position measurement or a momentum measurement. These expectation values will generally change over time; that is, depending on the time at which (for example) a position measurement is performed, the probabilities for its different possible outcomes will vary. The Ehrenfest theorem says, roughly speaking, that the equations describing how these expectation values change over time have a form reminiscent of Newton's second law. However, the more pronounced quantum effects are in a given situation, the more difficult it is to derive meaningful conclusions from this resemblance.[note 13]\n\nThe concepts invoked in Newton's laws of motion — mass, velocity, momentum, force — have predecessors in earlier work, and the content of Newtonian physics was further developed after Newton's time. Newton combined knowledge of celestial motions with the study of events on Earth and showed that one theory of mechanics could encompass both.[note 14]\n\nAs noted by scholar I. Bernard Cohen, Newton's work was more than a mere synthesis of previous results, as he selected certain ideas and further transformed them, with each in a new form that was useful to him, while at the same time proving false of certain basic or fundamental principles of scientists such as Galileo Galilei, Johannes Kepler, René Descartes, and Nicolaus Copernicus.[103] He approached natural philosophy with mathematics in a completely novel way, in that instead of a preconceived natural philosophy, his style was to begin with a mathematical construct, and build on from there, comparing it to the real world to show that his system accurately accounted for it.[104]\n\nThe subject of physics is often traced back to Aristotle, but the history of the concepts involved is obscured by multiple factors. An exact correspondence between Aristotelian and modern concepts is not simple to establish: Aristotle did not clearly distinguish what we would call speed and force, used the same term for density and viscosity, and conceived of motion as always through a medium, rather than through space. In addition, some concepts often termed \"Aristotelian\" might better be attributed to his followers and commentators upon him.[105] These commentators found that Aristotelian physics had difficulty explaining projectile motion.[note 15] Aristotle divided motion into two types: \"natural\" and \"violent\". The \"natural\" motion of terrestrial solid matter was to fall downwards, whereas a \"violent\" motion could push a body sideways. Moreover, in Aristotelian physics, a \"violent\" motion requires an immediate cause; separated from the cause of its \"violent\" motion, a body would revert to its \"natural\" behavior. Yet, a javelin continues moving after it leaves the thrower's hand. Aristotle concluded that the air around the javelin must be imparted with the ability to move the javelin forward.\n\nJohn Philoponus, a Byzantine Greek thinker active during the sixth century, found this absurd: the same medium, air, was somehow responsible both for sustaining motion and for impeding it. If Aristotle's idea were true, Philoponus said, armies would launch weapons by blowing upon them with bellows. Philoponus argued that setting a body into motion imparted a quality, impetus, that would be contained within the body itself. As long as its impetus was sustained, the body would continue to move.[107]: 47  In the following centuries, versions of impetus theory were advanced by individuals including Nur ad-Din al-Bitruji, Avicenna, Abu'l-Barakāt al-Baghdādī, John Buridan, and Albert of Saxony. In retrospect, the idea of impetus can be seen as a forerunner of the modern concept of momentum.[note 16] The intuition that objects move according to some kind of impetus persists in many students of introductory physics.[109]\n\nThe French philosopher René Descartes introduced the concept of inertia by way of his \"laws of nature\" in The World (Traité du monde et de la lumière) written 1629–33. However, The World purported a heliocentric worldview, and in 1633 this view had given rise a great conflict between Galileo Galilei and the Roman Catholic Inquisition. Descartes knew about this controversy and did not wish to get involved. The World was not published until 1664, ten years after his death.[110]\n\nThe modern concept of inertia is credited to Galileo. Based on his experiments, Galileo concluded that the \"natural\" behavior of a moving body was to keep moving, until something else interfered with it. In Two New Sciences (1638) Galileo wrote:[111][112].mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nImagine any particle projected along a horizontal plane without friction; then we know, from what has been more fully explained in the preceding pages, that this particle will move along this same plane with a motion which is uniform and perpetual, provided the plane has no limits.\n\nGalileo recognized that in projectile motion, the Earth's gravity affects vertical but not horizontal motion.[113] However, Galileo's idea of inertia was not exactly the one that would be codified into Newton's first law. Galileo thought that a body moving a long distance inertially would follow the curve of the Earth. This idea was corrected by Isaac Beeckman, Descartes, and Pierre Gassendi, who recognized that inertial motion should be motion in a straight line.[114] Descartes published his laws of nature (laws of motion) with this correction in Principles of Philosophy (Principia Philosophiae) in 1644, with the heliocentric part toned down.[115][110]\n\nFirst Law of Nature: Each thing when left to itself continues in the same state; so any moving body goes on moving until something stops it.\n\nSecond Law of Nature: Each moving thing if left to itself moves in a straight line; so any body moving in a circle always tends to move away from the centre of the circle.\n\nAccording to American philosopher Richard J. Blackwell, Dutch scientist Christiaan Huygens had worked out his own, concise version of the law in 1656.[116] It was not published until 1703, eight years after his death, in the opening paragraph of De Motu Corporum ex Percussione.\n\nHypothesis I: Any body already in motion will continue to move perpetually with the same speed and in a straight line unless it is impeded.\n\nAccording to Huygens, this law was already known by Galileo and Descartes among others.[116]\n\nChristiaan Huygens, in his Horologium Oscillatorium (1673), put forth the hypothesis that \"By the action of gravity, whatever its sources, it happens that bodies are moved by a motion composed both of a uniform motion in one direction or another and of a motion downward due to gravity.\" Newton's second law generalized this hypothesis from gravity to all forces.[117]\n\nOne important characteristic of Newtonian physics is that forces can act at a distance without requiring physical contact.[note 17] For example, the Sun and the Earth pull on each other gravitationally, despite being separated by millions of kilometres. This contrasts with the idea, championed by Descartes among others, that the Sun's gravity held planets in orbit by swirling them in a vortex of transparent matter, aether.[124] Newton considered aetherial explanations of force but ultimately rejected them.[122] The study of magnetism by William Gilbert and others created a precedent for thinking of immaterial forces,[122] and unable to find a quantitatively satisfactory explanation of his law of gravity in terms of an aetherial model, Newton eventually declared, \"I feign no hypotheses\": whether or not a model like Descartes's vortices could be found to underlie the Principia's theories of motion and gravity, the first grounds for judging them must be the successful predictions they made.[125] And indeed, since Newton's time every attempt at such a model has failed.\n\nJohannes Kepler suggested that gravitational attractions were reciprocal — that, for example, the Moon pulls on the Earth while the Earth pulls on the Moon — but he did not argue that such pairs are equal and opposite.[126] In his Principles of Philosophy (1644), Descartes introduced the idea that during a collision between bodies, a \"quantity of motion\" remains unchanged. Descartes defined this quantity somewhat imprecisely by adding up the products of the speed and \"size\" of each body, where \"size\" for him incorporated both volume and surface area.[127] Moreover, Descartes thought of the universe as a plenum, that is, filled with matter, so all motion required a body to displace a medium as it moved.\n\nDuring the 1650s, Huygens studied collisions between hard spheres and deduced a principle that is now identified as the conservation of momentum.[128][129] Christopher Wren would later deduce the same rules for elastic collisions that Huygens had, and John Wallis would apply momentum conservation to study inelastic collisions. Newton cited the work of Huygens, Wren, and Wallis to support the validity of his third law.[130]\n\nNewton arrived at his set of three laws incrementally. In a 1684 manuscript written to Huygens, he listed four laws: the principle of inertia, the change of motion by force, a statement about relative motion that would today be called Galilean invariance, and the rule that interactions between bodies do not change the motion of their center of mass. In a later manuscript, Newton added a law of action and reaction, while saying that this law and the law regarding the center of mass implied one another. Newton probably settled on the presentation in the Principia, with three primary laws and then other statements reduced to corollaries, during 1685.[131]\n\nNewton expressed his second law by saying that the force on a body is proportional to its change of motion, or momentum. By the time he wrote the Principia, he had already developed calculus (which he called \"the science of fluxions\"), but in the Principia he made no explicit use of it, perhaps because he believed geometrical arguments in the tradition of Euclid to be more rigorous.[133]: 15 [134] Consequently, the Principia does not express acceleration as the second derivative of position, and so it does not give the second law as \n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  \n. This form of the second law was written (for the special case of constant force) at least as early as 1716, by Jakob Hermann; Leonhard Euler would employ it as a basic premise in the 1740s.[135] Euler pioneered the study of rigid bodies[136] and established the basic theory of fluid dynamics.[137] Pierre-Simon Laplace's five-volume Traité de mécanique céleste (1798–1825) forsook geometry and developed mechanics purely through algebraic expressions, while resolving questions that the Principia had left open, like a full theory of the tides.[138]\n\nThe concept of energy became a key part of Newtonian mechanics in the post-Newton period. Huygens' solution of the collision of hard spheres showed that in that case, not only is momentum conserved, but kinetic energy is as well (or, rather, a quantity that in retrospect we can identify as one-half the total kinetic energy). The question of what is conserved during all other processes, like inelastic collisions and motion slowed by friction, was not resolved until the 19th century. Debates on this topic overlapped with philosophical disputes between the metaphysical views of Newton and Leibniz, and variants of the term \"force\" were sometimes used to denote what we would call types of energy. For example, in 1742, Émilie du Châtelet wrote, \"Dead force consists of a simple tendency to motion: such is that of a spring ready to relax; living force is that which a body has when it is in actual motion.\" In modern terminology, \"dead force\" and \"living force\" correspond to potential energy and kinetic energy respectively.[139] Conservation of energy was not established as a universal principle until it was understood that the energy of mechanical work can be dissipated into heat.[140][141] With the concept of energy given a solid grounding, Newton's laws could then be derived within formulations of classical mechanics that put energy first, as in the Lagrangian and Hamiltonian formulations described above.\n\nModern presentations of Newton's laws use the mathematics of vectors, a topic that was not developed until the late 19th and early 20th centuries. Vector algebra, pioneered by Josiah Willard Gibbs and Oliver Heaviside, stemmed from and largely supplanted the earlier system of quaternions invented by William Rowan Hamilton.[142][143]",
        pageTitle: "Newton's laws of motion",
    },
    {
        title: "Faraday's law of induction",
        link: "https://en.wikipedia.org/wiki/Faraday%27s_law_of_induction",
        content:
            "Faraday's law of induction (or simply Faraday's law) is a law of electromagnetism predicting how a magnetic field will interact with an electric circuit to produce an electromotive force (emf). This phenomenon, known as electromagnetic induction, is the fundamental operating principle of transformers, inductors, and many types of electric motors, generators and solenoids.[2][3]\n\nThe Maxwell–Faraday equation (listed as one of Maxwell's equations) describes the fact that a spatially varying (and also possibly time-varying, depending on how a magnetic field varies in time) electric field always accompanies a time-varying magnetic field, while Faraday's law states that emf (electromagnetic work done on a unit charge when it has traveled one round of a conductive loop) appears on a conductive loop when the magnetic flux through the surface enclosed by the loop varies in time.\n\nOnce Faraday's law had been discovered, one aspect of it (transformer emf) was formulated as the Maxwell–Faraday equation. The equation of Faraday's law can be derived by the Maxwell–Faraday equation (describing transformer emf) and the Lorentz force (describing motional emf). The integral form of the Maxwell–Faraday equation describes only the transformer emf, while the equation of Faraday's law describes both the transformer emf and the motional emf.\n\nElectromagnetic induction was discovered independently by Michael Faraday in 1831 and Joseph Henry in 1832.[4] Faraday was the first to publish the results of his experiments.[5][6]\n\nFaraday's notebook on August 29, 1831[8] describes an experimental demonstration of electromagnetic induction (see figure)[9] that wraps two wires around opposite sides of an iron ring (like a modern toroidal transformer). His assessment of newly-discovered properties of electromagnets suggested that when current started to flow in one wire, a sort of wave would travel through the ring and cause some electrical effect on the opposite side. Indeed, a galvanometer's needle measured a transient current (which he called a \"wave of electricity\") on the right side's wire when he connected or disconnected the left side's wire to a battery.[10]: 182–183  This induction was due to the change in magnetic flux that occurred when the battery was connected and disconnected.[7] His notebook entry also noted that fewer wraps for the battery side resulted in a greater disturbance of the galvanometer's needle.[8]\n\nWithin two months, Faraday had found several other manifestations of electromagnetic induction. For example, he saw transient currents when he quickly slid a bar magnet in and out of a coil of wires, and he generated a steady (DC) current by rotating a copper disk near the bar magnet with a sliding electrical lead (\"Faraday's disk\").[10]: 191–195\n\nMichael Faraday explained electromagnetic induction using a concept he called lines of force. However, scientists at the time widely rejected his theoretical ideas, mainly because they were not formulated mathematically.[10]: 510  An exception was James Clerk Maxwell, who in 1861–62 used Faraday's ideas as the basis of his quantitative electromagnetic theory.[10]: 510 [11][12] In Maxwell's papers, the time-varying aspect of electromagnetic induction is expressed as a differential equation which Oliver Heaviside referred to as Faraday's law even though it is different from the original version of Faraday's law, and does not describe motional emf. Heaviside's version (see Maxwell–Faraday equation below) is the form recognized today in the group of equations known as Maxwell's equations.\n\nLenz's law, formulated by Emil Lenz in 1834,[13] describes \"flux through the circuit\", and gives the direction of the induced emf and current resulting from electromagnetic induction (elaborated upon in the examples below).\n\nAccording to Albert Einstein, much of the groundwork and discovery of his special relativity theory was presented by this law of induction by Faraday in 1834.[14][15]\n\nThe most widespread version of Faraday's law states:\n\nThe electromotive force around a closed path is equal to the negative of the time rate of change of the magnetic flux enclosed by the path.[16][17]\n\nFor a loop of wire in a magnetic field, the magnetic flux ΦB is defined for any surface Σ whose boundary is the given loop. Since the wire loop may be moving, we write Σ(t) for the surface. The magnetic flux is the surface integral:\n\n  \n    \n      \n        \n          Φ\n          \n            B\n          \n        \n        =\n        \n          ∬\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          B\n        \n        (\n        t\n        )\n        ⋅\n        \n          d\n        \n        \n          A\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\Phi _{B}=\\iint _{\\Sigma (t)}\\mathbf {B} (t)\\cdot \\mathrm {d} \\mathbf {A} \\,,}\n  \n\nwhere dA is an element of area vector of the moving surface Σ(t), B is the magnetic field, and B · dA is a vector dot product representing the element of flux through dA. In more visual terms, the magnetic flux through the wire loop is proportional to the number of magnetic field lines that pass through the loop.\n\nWhen the flux changes—because B changes, or because the wire loop is moved or deformed, or both—Faraday's law of induction says that the wire loop acquires an emf, defined as the energy available from a unit charge that has traveled once around the wire loop.[18]: ch17 [19][20] (Although some sources state the definition differently, this expression was chosen for compatibility with the equations of special relativity.) Equivalently, it is the voltage that would be measured by cutting the wire to create an open circuit, and attaching a voltmeter to the leads.\n\nFaraday's law states that the emf is also given by the rate of change of the magnetic flux:\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        −\n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\mathcal {E}}=-{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}},}\n  \n\nwhere \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is the electromotive force (emf) and ΦB is the magnetic flux.\n\nThe direction of the electromotive force is given by Lenz's law.\n\nThe laws of induction of electric currents in mathematical form were established by Franz Ernst Neumann in 1845.[21]\n\nFaraday's law contains the information about the relationships between both the magnitudes and the directions of its variables. However, the relationships between the directions are not explicit; they are hidden in the mathematical formula.\n\nIt is possible to find out the direction of the electromotive force (emf) directly from Faraday’s law, without invoking Lenz's law. A left hand rule helps doing that, as follows:[22][23]\n\nFor a tightly wound coil of wire, composed of N identical turns, each with the same ΦB, Faraday's law of induction states that[24][25]\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        −\n        N\n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}=-N{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}}\n  \n\nwhere N is the number of turns of wire and ΦB is the magnetic flux through a single loop.\n\nThe Maxwell–Faraday equation states that a time-varying magnetic field always accompanies a spatially varying (also possibly time-varying), non-conservative electric field, and vice versa. The Maxwell–Faraday equation is\n\n∇\n        ×\n        \n          E\n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                B\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {E} =-{\\frac {\\partial \\mathbf {B} }{\\partial t}}}\n\n(in SI units) where ∇ × is the curl operator and again E(r, t) is the electric field and B(r, t) is the magnetic field. These fields can generally be functions of position r and time t.[26]\n\nThe Maxwell–Faraday equation is one of the four Maxwell's equations, and therefore plays a fundamental role in the theory of classical electromagnetism. It can also be written in an integral form by the Kelvin–Stokes theorem,[27] thereby reproducing Faraday's law:\n\n∮\n          \n            ∂\n            Σ\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        −\n        \n          ∫\n          \n            Σ\n          \n        \n        \n          \n            \n              ∂\n              \n                B\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        ⋅\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma }\\mathbf {E} \\cdot \\mathrm {d} \\mathbf {l} =-\\int _{\\Sigma }{\\frac {\\partial \\mathbf {B} }{\\partial t}}\\cdot \\mathrm {d} \\mathbf {A} }\n\nwhere, as indicated in the figure, Σ is a surface bounded by the closed contour ∂Σ, dl is an infinitesimal vector element of the contour ∂Σ, and dA is an infinitesimal vector element of surface Σ. Its direction is orthogonal to that surface patch, the magnitude is the area of an infinitesimal patch of surface.\n\nBoth dl and dA have a sign ambiguity; to get the correct sign, the right-hand rule is used, as explained in the article Kelvin–Stokes theorem. For a planar surface Σ, a positive path element dl of curve ∂Σ is defined by the right-hand rule as one that points with the fingers of the right hand when the thumb points in the direction of the normal n to the surface Σ.\n\nThe line integral around ∂Σ is called circulation.[18]: ch3  A nonzero circulation of E is different from the behavior of the electric field generated by static charges. A charge-generated E-field can be expressed as the gradient of a scalar field that is a solution to Poisson's equation, and has a zero path integral. See gradient theorem.\n\nThe integral equation is true for any path ∂Σ through space, and any surface Σ for which that path is a boundary.\n\nIf the surface Σ is not changing in time, the equation can be rewritten:\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        −\n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          A\n        \n        .\n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma }\\mathbf {E} \\cdot \\mathrm {d} \\mathbf {l} =-{\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma }\\mathbf {B} \\cdot \\mathrm {d} \\mathbf {A} .}\n  \n\nThe surface integral at the right-hand side is the explicit expression for the magnetic flux ΦB through Σ.\n\nThe electric vector field induced by a changing magnetic flux, the solenoidal component of the overall electric field, can be approximated in the non-relativistic limit by the volume integral equation[26]: 321 \n\n  \n    \n      \n        \n          \n            E\n          \n          \n            s\n          \n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n        ≈\n        −\n        \n          \n            1\n            \n              4\n              π\n            \n          \n        \n        \n          ∭\n          \n            V\n          \n        \n         \n        \n          \n            \n              \n                (\n                \n                  \n                    \n                      ∂\n                      \n                        B\n                      \n                      (\n                      \n                        \n                          r\n                        \n                        ′\n                      \n                      ,\n                      t\n                      )\n                    \n                    \n                      ∂\n                      t\n                    \n                  \n                \n                )\n              \n              ×\n              \n                (\n                \n                  \n                    r\n                  \n                  −\n                  \n                    \n                      r\n                    \n                    ′\n                  \n                \n                )\n              \n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                \n                  r\n                \n                ′\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{s}(\\mathbf {r} ,t)\\approx -{\\frac {1}{4\\pi }}\\iiint _{V}\\ {\\frac {\\left({\\frac {\\partial \\mathbf {B} (\\mathbf {r} ',t)}{\\partial t}}\\right)\\times \\left(\\mathbf {r} -\\mathbf {r} '\\right)}{|\\mathbf {r} -\\mathbf {r} '|^{3}}}d^{3}\\mathbf {r'} }\n\nThe four Maxwell's equations (including the Maxwell–Faraday equation), along with Lorentz force law, are a sufficient foundation to derive everything in classical electromagnetism.[18][19] Therefore, it is possible to \"prove\" Faraday's law starting with these equations.[28][29]\n\nThe starting point is the time-derivative of flux through an arbitrary surface Σ (that can be moved or deformed) in space:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          B\n        \n        (\n        t\n        )\n        ⋅\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}={\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathbf {B} (t)\\cdot \\mathrm {d} \\mathbf {A} }\n\n(by definition). This total time derivative can be evaluated and simplified with the help of the Maxwell–Faraday equation and some vector identities; the details are in the box below:\n\nd\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          B\n        \n        (\n        t\n        )\n        ⋅\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}={\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathbf {B} (t)\\cdot \\mathrm {d} \\mathbf {A} }\n\nThe integral can change over time for two reasons: The integrand can change, or the integration region can change. These add linearly, therefore:\n\n  \n    \n      \n        \n          \n            \n            \n              \n                \n                  \n                    d\n                  \n                  \n                    Φ\n                    \n                      B\n                    \n                  \n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            |\n          \n          \n            t\n            =\n            \n              t\n              \n                0\n              \n            \n          \n        \n        =\n        \n          (\n          \n            \n              ∫\n              \n                Σ\n                (\n                \n                  t\n                  \n                    0\n                  \n                \n                )\n              \n            \n            \n              \n                \n                \n                  \n                    \n                      ∂\n                      \n                        B\n                      \n                    \n                    \n                      ∂\n                      t\n                    \n                  \n                \n                |\n              \n              \n                t\n                =\n                \n                  t\n                  \n                    0\n                  \n                \n              \n            \n            ⋅\n            \n              d\n            \n            \n              A\n            \n          \n          )\n        \n        +\n        \n          (\n          \n            \n              \n                \n                  d\n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            \n              ∫\n              \n                Σ\n                (\n                t\n                )\n              \n            \n            \n              B\n            \n            (\n            \n              t\n              \n                0\n              \n            \n            )\n            ⋅\n            \n              d\n            \n            \n              A\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left.{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}\\right|_{t=t_{0}}=\\left(\\int _{\\Sigma (t_{0})}\\left.{\\frac {\\partial \\mathbf {B} }{\\partial t}}\\right|_{t=t_{0}}\\cdot \\mathrm {d} \\mathbf {A} \\right)+\\left({\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathbf {B} (t_{0})\\cdot \\mathrm {d} \\mathbf {A} \\right)}\n  \n\nwhere t0 is any given fixed time. We will show that the first term on the right-hand side corresponds to transformer emf, the second to motional emf (from the magnetic Lorentz force on charge carriers due to the motion or deformation of the conducting loop in the magnetic field). The first term on the right-hand side can be rewritten using the integral form of the Maxwell–Faraday equation:\n\n  \n    \n      \n        \n          ∫\n          \n            Σ\n            (\n            \n              t\n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            \n            \n              \n                \n                  ∂\n                  \n                    B\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            |\n          \n          \n            t\n            =\n            \n              t\n              \n                0\n              \n            \n          \n        \n        ⋅\n        \n          d\n        \n        \n          A\n        \n        =\n        −\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            \n              t\n              \n                0\n              \n            \n            )\n          \n        \n        \n          E\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle \\int _{\\Sigma (t_{0})}\\left.{\\frac {\\partial \\mathbf {B} }{\\partial t}}\\right|_{t=t_{0}}\\cdot \\mathrm {d} \\mathbf {A} =-\\oint _{\\partial \\Sigma (t_{0})}\\mathbf {E} (t_{0})\\cdot \\mathrm {d} \\mathbf {l} }\n\nNext, we analyze the second term on the right-hand side:\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          B\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ⋅\n        \n          d\n        \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathbf {B} (t_{0})\\cdot \\mathrm {d} \\mathbf {A} }\n\nHere, identities of triple scalar products are used. Therefore,\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          B\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ⋅\n        \n          d\n        \n        \n          A\n        \n        =\n        −\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            \n              t\n              \n                0\n              \n            \n            )\n          \n        \n        (\n        \n          \n            v\n          \n          \n            \n              l\n            \n          \n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ×\n        \n          B\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathbf {B} (t_{0})\\cdot \\mathrm {d} \\mathbf {A} =-\\oint _{\\partial \\Sigma (t_{0})}(\\mathbf {v} _{\\mathbf {l} }(t_{0})\\times \\mathbf {B} (t_{0}))\\cdot \\mathrm {d} \\mathbf {l} }\n  \n\nwhere vl is the velocity of a part of the loop ∂Σ.\n\nPutting these together results in,\n\n  \n    \n      \n        \n          \n            \n            \n              \n                \n                  \n                    d\n                  \n                  \n                    Φ\n                    \n                      B\n                    \n                  \n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            |\n          \n          \n            t\n            =\n            \n              t\n              \n                0\n              \n            \n          \n        \n        =\n        \n          (\n          \n            −\n            \n              ∮\n              \n                ∂\n                Σ\n                (\n                \n                  t\n                  \n                    0\n                  \n                \n                )\n              \n            \n            \n              E\n            \n            (\n            \n              t\n              \n                0\n              \n            \n            )\n            ⋅\n            \n              d\n            \n            \n              l\n            \n          \n          )\n        \n        +\n        \n          (\n          \n            −\n            \n              ∮\n              \n                ∂\n                Σ\n                (\n                \n                  t\n                  \n                    0\n                  \n                \n                )\n              \n            \n            \n              \n                (\n              \n            \n            \n              \n                v\n              \n              \n                \n                  l\n                \n              \n            \n            (\n            \n              t\n              \n                0\n              \n            \n            )\n            ×\n            \n              B\n            \n            (\n            \n              t\n              \n                0\n              \n            \n            )\n            \n              \n                )\n              \n            \n            ⋅\n            \n              d\n            \n            \n              l\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left.{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}\\right|_{t=t_{0}}=\\left(-\\oint _{\\partial \\Sigma (t_{0})}\\mathbf {E} (t_{0})\\cdot \\mathrm {d} \\mathbf {l} \\right)+\\left(-\\oint _{\\partial \\Sigma (t_{0})}{\\bigl (}\\mathbf {v} _{\\mathbf {l} }(t_{0})\\times \\mathbf {B} (t_{0}){\\bigr )}\\cdot \\mathrm {d} \\mathbf {l} \\right)}\n  \n\n\n  \n    \n      \n        \n          \n            \n            \n              \n                \n                  \n                    d\n                  \n                  \n                    Φ\n                    \n                      B\n                    \n                  \n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            |\n          \n          \n            t\n            =\n            \n              t\n              \n                0\n              \n            \n          \n        \n        =\n        −\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            \n              t\n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            (\n          \n        \n        \n          E\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        +\n        \n          \n            v\n          \n          \n            \n              l\n            \n          \n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ×\n        \n          B\n        \n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        \n          \n            )\n          \n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        .\n      \n    \n    {\\displaystyle \\left.{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}\\right|_{t=t_{0}}=-\\oint _{\\partial \\Sigma (t_{0})}{\\bigl (}\\mathbf {E} (t_{0})+\\mathbf {v} _{\\mathbf {l} }(t_{0})\\times \\mathbf {B} (t_{0}){\\bigr )}\\cdot \\mathrm {d} \\mathbf {l} .}\n\nThe result is:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          ∮\n          \n            ∂\n            Σ\n          \n        \n        \n          (\n          \n            \n              E\n            \n            +\n            \n              \n                v\n              \n              \n                \n                  l\n                \n              \n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}=-\\oint _{\\partial \\Sigma }\\left(\\mathbf {E} +\\mathbf {v} _{\\mathbf {l} }\\times \\mathbf {B} \\right)\\cdot \\mathrm {d} \\mathbf {l} .}\n  \n\nwhere ∂Σ is the boundary (loop) of the surface Σ, and vl is the velocity of a part of the boundary.\n\nIn the case of a conductive loop, emf (Electromotive Force) is the electromagnetic work done on a unit charge when it has traveled around the loop once, and this work is done by the Lorentz force.  Therefore, emf is expressed as\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        ∮\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}=\\oint \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)\\cdot \\mathrm {d} \\mathbf {l} }\n  \n\nwhere \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is emf and v is the unit charge velocity.\n\nIn a macroscopic view, for charges on a segment of the loop, v consists of two components in average; one is the velocity of the charge along the segment vt, and the other is the velocity of the segment vl (the loop is deformed or moved). vt does not contribute to the work done on the charge since the direction of vt is same to the direction of \n  \n    \n      \n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {l} }\n  \n. Mathematically,\n\n  \n    \n      \n        (\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        (\n        (\n        \n          \n            v\n          \n          \n            t\n          \n        \n        +\n        \n          \n            v\n          \n          \n            l\n          \n        \n        )\n        ×\n        \n          B\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        (\n        \n          \n            v\n          \n          \n            t\n          \n        \n        ×\n        \n          B\n        \n        +\n        \n          \n            v\n          \n          \n            l\n          \n        \n        ×\n        \n          B\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        (\n        \n          \n            v\n          \n          \n            l\n          \n        \n        ×\n        \n          B\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle (\\mathbf {v} \\times \\mathbf {B} )\\cdot \\mathrm {d} \\mathbf {l} =((\\mathbf {v} _{t}+\\mathbf {v} _{l})\\times \\mathbf {B} )\\cdot \\mathrm {d} \\mathbf {l} =(\\mathbf {v} _{t}\\times \\mathbf {B} +\\mathbf {v} _{l}\\times \\mathbf {B} )\\cdot \\mathrm {d} \\mathbf {l} =(\\mathbf {v} _{l}\\times \\mathbf {B} )\\cdot \\mathrm {d} \\mathbf {l} }\n  \n\nsince \n  \n    \n      \n        (\n        \n          \n            v\n          \n          \n            t\n          \n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle (\\mathbf {v} _{t}\\times \\mathbf {B} )}\n  \n is perpendicular to \n  \n    \n      \n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {l} }\n  \n as \n  \n    \n      \n        \n          \n            v\n          \n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} _{t}}\n  \n and \n  \n    \n      \n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {l} }\n  \n are along the same direction. Now we can see that, for the conductive loop, emf is same to the time-derivative of the magnetic flux through the loop except for the sign on it. Therefore, we now reach the equation of Faraday's law (for the conductive loop) as\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}=-{\\mathcal {E}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        ∮\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\textstyle {\\mathcal {E}}=\\oint \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)\\cdot \\mathrm {d} \\mathbf {l} }\n  \n. With breaking this integral, \n  \n    \n      \n        ∮\n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\textstyle \\oint \\mathbf {E} \\cdot \\mathrm {d} \\mathbf {l} }\n  \n is for the transformer emf (due to a time-varying magnetic field) and \n  \n    \n      \n        ∮\n        \n          (\n          \n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        ∮\n        \n          (\n          \n            \n              \n                v\n              \n              \n                l\n              \n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\textstyle \\oint \\left(\\mathbf {v} \\times \\mathbf {B} \\right)\\cdot \\mathrm {d} \\mathbf {l} =\\oint \\left(\\mathbf {v} _{l}\\times \\mathbf {B} \\right)\\cdot \\mathrm {d} \\mathbf {l} }\n  \n is for the motional emf (due to the magnetic Lorentz force on charges by the motion or deformation of the loop in the magnetic field).\n\nIt is tempting to generalize Faraday's law to state: If ∂Σ is any arbitrary closed loop in space whatsoever, then the total time derivative of magnetic flux through Σ equals the emf around ∂Σ. This statement, however, is not always true and the reason is not just from the obvious reason that emf is undefined in empty space when no conductor is present. As noted in the previous section, Faraday's law is not guaranteed to work unless the velocity of the abstract curve ∂Σ matches the actual velocity of the material conducting the electricity.[31] The two examples illustrated below show that one often obtains incorrect results when the motion of ∂Σ is divorced from the motion of the material.[18]\n\nOne can analyze examples like these by taking care that the path ∂Σ moves with the same velocity as the material.[31] Alternatively, one can always correctly calculate the emf by combining Lorentz force law with the Maxwell–Faraday equation:[18]: ch17 [32]\n\nwhere \"it is very important to notice that (1) [vm] is the velocity of the conductor ... not the velocity of the path element dl and (2) in general, the partial derivative with respect to time cannot be moved outside the integral since the area is a function of time.\"[32]\n\nFaraday's law is a single equation describing two different phenomena: the motional emf generated by a magnetic force on a moving wire (see the Lorentz force), and the transformer emf generated by an electric force due to a changing magnetic field (described by the Maxwell–Faraday equation).\n\nJames Clerk Maxwell drew attention to this fact in his 1861 paper On Physical Lines of Force.[33] In the latter half of Part II of that paper, Maxwell gives a separate physical explanation for each of the two phenomena.\n\nA reference to these two aspects of electromagnetic induction is made in some modern textbooks.[34] As Richard Feynman states:\n\nSo the \"flux rule\" that the emf in a circuit is equal to the rate of change of the magnetic flux through the circuit applies whether the flux changes because the field changes or because the circuit moves (or both) ...\n\nYet in our explanation of the rule we have used two completely distinct laws for the two cases – v × B for \"circuit moves\" and ∇ × E = −∂tB for \"field changes\".\n\nWe know of no other place in physics where such a simple and accurate general principle requires for its real understanding an analysis in terms of two different phenomena.\n\nIn the general case, explanation of the motional emf appearance by action of the magnetic force on the charges in the moving wire or in the circuit changing its area is unsatisfactory. As a matter of fact, the charges in the wire or in the circuit could be completely absent, will then the electromagnetic induction effect disappear in this case? This situation is analyzed in the article, in which, when writing the integral equations of the electromagnetic field in a four-dimensional covariant form, in the Faraday’s law the total time derivative of the magnetic flux through the circuit appears instead of the partial time derivative.[36] Thus, electromagnetic induction appears either when the magnetic field changes over time or when the area of the circuit changes. From the physical point of view, it is better to speak not about the induction emf, but about the induced electric field strength \n  \n    \n      \n        \n          E\n        \n        =\n        −\n        ∇\n        \n          \n            E\n          \n        \n        −\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\textstyle \\mathbf {E} =-\\nabla {\\mathcal {E}}-{\\frac {\\partial \\mathbf {A} }{\\partial t}}}\n  \n, that occurs in the circuit when the magnetic flux changes. In this case, the contribution to \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n from the change in the magnetic field is made through the term \n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\textstyle -{\\frac {\\partial \\mathbf {A} }{\\partial t}}}\n  \n , where \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n is the vector potential. If the circuit area is changing in case of the constant magnetic field, then some part of the circuit is inevitably moving, and the electric field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n emerges in this part of the circuit in the comoving reference frame K’ as a result of the Lorentz transformation of the magnetic field \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n, present in the stationary reference frame K, which passes through the circuit. The presence of the field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n in K’ is considered as a result of the induction effect in the moving circuit, regardless of whether the charges are present in the circuit or not. In the conducting circuit, the field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n causes motion of the charges. In the reference frame K, it looks like appearance of emf of the induction \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n, the gradient of which in the form of \n  \n    \n      \n        −\n        ∇\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle -\\nabla {\\mathcal {E}}}\n  \n, taken along the circuit, seems to generate the field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n.\n\nReflection on this apparent dichotomy was one of the principal paths that led Albert Einstein to develop special relativity:\n\nIt is known that Maxwell's electrodynamics—as usually understood at the present time—when applied to moving bodies, leads to asymmetries which do not appear to be inherent in the phenomena. Take, for example, the reciprocal electrodynamic action of a magnet and a conductor.\n\nThe observable phenomenon here depends only on the relative motion of the conductor and the magnet, whereas the customary view draws a sharp distinction between the two cases in which either the one or the other of these bodies is in motion. For if the magnet is in motion and the conductor at rest, there arises in the neighbourhood of the magnet an electric field with a certain definite energy, producing a current at the places where parts of the conductor are situated.\n\nBut if the magnet is stationary and the conductor in motion, no electric field arises in the neighbourhood of the magnet. In the conductor, however, we find an electromotive force, to which in itself there is no corresponding energy, but which gives rise—assuming equality of relative motion in the two cases discussed—to electric currents of the same path and intensity as those produced by the electric forces in the former case.\n\nExamples of this sort, together with unsuccessful attempts to discover any motion of the earth relative to the \"light medium,\" suggest that the phenomena of electrodynamics as well as of mechanics possess no properties corresponding to the idea of absolute rest.",
        pageTitle: "Faraday's law of induction",
    },
    {
        title: "Faraday's law of electrolysis",
        link: "https://en.wikipedia.org/wiki/Faraday%27s_law_of_electrolysis",
        content:
            "Faraday's laws of electrolysis are quantitative relationships based on the electrochemical research published by Michael Faraday in 1833.[1][2][3]\n\nMichael Faraday reported that the mass (m) of a substance deposited or liberated at an electrode is directly proportional to the charge (Q, for which the SI unit is the ampere-second or coulomb).[3]\n\n  \n    \n      \n        m\n        ∝\n        Q\n        \n        \n        ⟹\n        \n        \n        \n          \n            m\n            Q\n          \n        \n        =\n        Z\n      \n    \n    {\\displaystyle m\\propto Q\\quad \\implies \\quad {\\frac {m}{Q}}=Z}\n\nHere, the constant of proportionality, Z, is called the electro-chemical equivalent (ECE) of the substance. Thus, the ECE can be defined as the mass of the substance deposited or liberated per unit charge.\n\nFaraday discovered that when the same amount of electric current is passed through different electrolytes connected in series, the masses of the substances deposited or liberated at the electrodes are directly proportional to their respective chemical equivalent/equivalent weight (E).[3]  This turns out to be the molar mass (M) divided by the valence (v)\n\nA monovalent ion requires one electron for discharge, a divalent ion requires two electrons for discharge and so on. Thus, if x electrons flow, \n  \n    \n      \n        \n          \n            \n              x\n              v\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {x}{v}}}\n  \n atoms are discharged.\n\nThus, the mass m discharged is\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              x\n              M\n            \n            \n              v\n              \n                N\n                \n                  \n                    A\n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              Q\n              M\n            \n            \n              e\n              \n                N\n                \n                  \n                    A\n                  \n                \n              \n              v\n            \n          \n        \n        =\n        \n          \n            \n              Q\n              M\n            \n            \n              v\n              F\n            \n          \n        \n      \n    \n    {\\displaystyle m={\\frac {xM}{vN_{\\rm {A}}}}={\\frac {QM}{eN_{\\rm {A}}v}}={\\frac {QM}{vF}}}\n  \n\nwhere\n\nwhere M is the molar mass of the substance (usually given in SI units of grams per mole) and v is the valency of the ions .\n\nFor Faraday's first law, M, F, v are constants; thus, the larger the value of Q, the larger m will be.\n\nFor Faraday's second law, Q, F, v are constants; thus, the larger the value of \n  \n    \n      \n        \n          \n            \n              M\n              v\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {M}{v}}}\n  \n (equivalent weight), the larger m will be.\n\nIn the simple case of constant-current electrolysis, Q = It, leading to\n\nFor the case of an alloy whose constituents have different valencies, we have\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              I\n              t\n            \n            \n              F\n              ×\n              \n                ∑\n                \n                  i\n                \n              \n              \n                \n                  \n                    \n                      w\n                      \n                        i\n                      \n                    \n                    \n                      v\n                      \n                        i\n                      \n                    \n                  \n                  \n                    M\n                    \n                      i\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle m={\\frac {It}{F\\times \\sum _{i}{\\frac {w_{i}v_{i}}{M_{i}}}}}}\n  \n\nwhere wi represents the mass fraction of the ith element.\n\nIn the more complicated case of a variable electric current, the total charge Q is the electric current I(τ) integrated over time τ:",
        pageTitle: "Faraday's laws of electrolysis",
    },
    {
        title: "Faxén's law",
        link: "https://en.wikipedia.org/wiki/Fax%C3%A9n%27s_law",
        content:
            "In fluid dynamics, Faxén's laws relate a sphere's velocity \n  \n    \n      \n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {U} }\n  \n and angular velocity \n  \n    \n      \n        \n          Ω\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Omega } }\n  \n to the forces, torque, stresslet and flow it experiences under low Reynolds number (creeping flow) conditions.\n\nFaxen's first law was introduced in 1922 by Swedish physicist Hilding Faxén, who at the time was active at Uppsala University, and is given by[1][2]\n\nwhere \n  \n    \n      \n        \n          b\n          \n            0\n          \n        \n        =\n        −\n        \n          \n            1\n            \n              6\n              π\n              μ\n              a\n            \n          \n        \n      \n    \n    {\\displaystyle b_{0}=-{\\frac {1}{6\\pi \\mu a}}}\n  \n is the hydrodynamic mobility.\n\nIn the case that the pressure gradient is small compared with the length scale of the sphere's diameter, and when there is no external force, the last two terms of this form may be neglected.  In this case the external fluid flow simply advects the sphere.\n\nBatchelor and Green[3] derived an equation for the stresslet, given by[1][2]\n\nNote there is no rate of strain on the sphere (no \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mathsf {E}}}}\n  \n) since the spheres are assumed to be rigid.\n\nFaxén's law is a correction to Stokes' law for the friction on spherical objects in a viscous fluid, valid where the object moves close to a wall of the container.[4]",
        pageTitle: "Faxén's law",
    },
    {
        title: "Fick's laws of diffusion",
        link: "https://en.wikipedia.org/wiki/Fick%27s_law_of_diffusion",
        content:
            "Fick's laws of diffusion describe diffusion and were first posited by Adolf Fick in 1855 on the basis of largely experimental results. They can be used to solve for the diffusion coefficient, D. Fick's first law can be used to derive his second law which in turn is identical to the diffusion equation.\n\nFick's first law: Movement of particles from high to low concentration (diffusive flux) is directly proportional to the particle's concentration gradient.[1]\n\nFick's second law: Prediction of change in concentration gradient with time due to diffusion.\n\nA diffusion process that obeys Fick's laws is called normal or Fickian diffusion; otherwise, it is called anomalous diffusion or non-Fickian diffusion.\n\nIn 1855, physiologist Adolf Fick first reported[2] his now well-known laws governing the transport of mass through diffusive means.  Fick's work was inspired by the earlier experiments of Thomas Graham, which fell short of proposing the fundamental laws for which Fick would become famous. Fick's law is analogous to the relationships discovered at the same epoch by other eminent scientists: Darcy's law (hydraulic flow), Ohm's law (charge transport), and Fourier's law (heat transport).\n\nFick's experiments (modeled on Graham's) dealt with measuring the concentrations and fluxes of salt, diffusing between two reservoirs through tubes of water. It is notable that Fick's work primarily concerned diffusion in fluids, because at the time, diffusion in solids was not considered generally possible.[3] Today, Fick's laws form the core of our understanding of diffusion in solids, liquids, and gases (in the absence of bulk fluid motion in the latter two cases).  When a diffusion process does not follow Fick's laws (which happens in cases of diffusion through porous media and diffusion of swelling penetrants, among others),[4][5] it is referred to as non-Fickian.\n\nFick's first law relates the diffusive flux to the gradient of the concentration. It postulates that the flux goes from regions of high concentration to regions of low concentration, with a magnitude that is proportional to the concentration gradient (spatial derivative), or in simplistic terms the concept that a solute will move from a region of high concentration to a region of low concentration across a concentration gradient. In one (spatial) dimension, the law can be written in various forms, where the most common form (see[6][7]) is in a molar basis:\n\nD is proportional to the squared velocity of the diffusing particles, which depends on the temperature, viscosity of the fluid and the size of the particles according to the Stokes–Einstein relation. In dilute aqueous solutions the diffusion coefficients of most ions are similar and have values that at room temperature are in the range of (0.6–2)×10−9 m2/s. For biological molecules the diffusion coefficients normally range from 10−10 to 10−11 m2/s.\n\nIn two or more dimensions we must use ∇, the del or gradient operator, which generalises the first derivative, obtaining\n\nThe driving force for the one-dimensional diffusion is the quantity −.mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠∂φ/∂x⁠, which for ideal mixtures is the concentration gradient.\n\nAnother form for the first law is to write it with the primary variable as mass fraction (yi, given for example in kg/kg), then the equation changes to\n\nThe \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is outside the gradient operator. This is because\n\nwhere ρsi is the partial density of the ith species.\n\nBeyond this, in chemical systems other than ideal solutions or mixtures, the driving force for the diffusion of each species is the gradient of chemical potential of this species. Then Fick's first law (one-dimensional case) can be written\n\nThe driving force of Fick's law can be expressed as a fugacity difference:\n\nwhere \n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f_{i}}\n  \n is the fugacity in Pa. \n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f_{i}}\n  \n is a partial pressure of component i in a vapor \n  \n    \n      \n        \n          f\n          \n            i\n          \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle f_{i}^{\\text{G}}}\n  \n or liquid \n  \n    \n      \n        \n          f\n          \n            i\n          \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle f_{i}^{\\text{L}}}\n  \n phase. At vapor liquid equilibrium the evaporation flux is zero because \n  \n    \n      \n        \n          f\n          \n            i\n          \n          \n            G\n          \n        \n        =\n        \n          f\n          \n            i\n          \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle f_{i}^{\\text{G}}=f_{i}^{\\text{L}}}\n  \n.\n\nFour versions of Fick's law for binary gas mixtures are given below. These assume: thermal diffusion is negligible; the body force per unit mass is the same on both species; and either pressure is constant or both species have the same molar mass. Under these conditions, Ref.[8] shows in detail how the diffusion equation from the kinetic theory of gases reduces to this version of Fick's law:\n\n  \n    \n      \n        \n          \n            V\n            \n              i\n            \n          \n        \n        =\n        −\n        D\n        ∇\n        ln\n        ⁡\n        \n          \n            (\n            \n              y\n              \n                i\n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {V_{i}} =-D\\nabla \\ln {\\left(y_{i}\\right)},}\n  \n\nwhere Vi is the diffusion velocity of species i. In terms of species flux this is\n\n  \n    \n      \n        \n          \n            J\n            \n              i\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ρ\n              D\n            \n            \n              M\n              \n                i\n              \n            \n          \n        \n        ∇\n        \n          y\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {J_{i}} =-{\\frac {\\rho D}{M_{i}}}\\nabla y_{i}.}\n\nIf, additionally, \n  \n    \n      \n        ∇\n        ρ\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla \\rho =0}\n  \n, this reduces to the most common form of Fick's law,\n\n  \n    \n      \n        \n          \n            J\n            \n              i\n            \n          \n        \n        =\n        −\n        D\n        ∇\n        φ\n        .\n      \n    \n    {\\displaystyle \\mathbf {J_{i}} =-D\\nabla \\varphi .}\n\nIf (instead of or in addition to \n  \n    \n      \n        ∇\n        ρ\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla \\rho =0}\n  \n) both species have the same molar mass, Fick's law becomes\n\n  \n    \n      \n        \n          \n            J\n            \n              i\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ρ\n              D\n            \n            \n              M\n              \n                i\n              \n            \n          \n        \n        ∇\n        \n          x\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {J_{i}} =-{\\frac {\\rho D}{M_{i}}}\\nabla x_{i},}\n  \n\nwhere \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n is the mole fraction of species i.\n\nFick's second law predicts how diffusion causes the concentration to change with respect to time. It is a partial differential equation which in one dimension reads\n\nIn two or more dimensions we must use the Laplacian Δ = ∇2, which generalises the second derivative, obtaining the equation\n\nFick's second law has the same mathematical form as the Heat equation and its fundamental solution is the same as the Heat kernel, except switching thermal conductivity \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n with diffusion coefficient \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n:\n\n  \n    \n      \n        φ\n        (\n        x\n        ,\n        t\n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              D\n              t\n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n                \n                  4\n                  D\n                  t\n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\varphi (x,t)={\\frac {1}{\\sqrt {4\\pi Dt}}}\\exp \\left(-{\\frac {x^{2}}{4Dt}}\\right).}\n\nFick's second law can be derived from Fick's first law and the mass conservation in absence of any chemical reactions:\n\nAssuming the diffusion coefficient D to be a constant, one can exchange the orders of the differentiation and multiply by the constant:\n\nand, thus, receive the form of the Fick's equations as was stated above.\n\nFor the case of diffusion in two or more dimensions Fick's second law becomes\n\nIf the diffusion coefficient is not a constant, but depends upon the coordinate or concentration, Fick's second law yields\n\nAn important example is the case where φ is at a steady state, i.e. the concentration does not change by time, so that the left part of the above equation is identically zero. In one dimension with constant D, the solution for the concentration will be a linear change of concentrations along x. In two or more dimensions we obtain\n\nwhich is Laplace's equation, the solutions to which are referred to by mathematicians as harmonic functions.\n\nFick's second law is a special case of the convection–diffusion equation in which there is no advective flux and no net volumetric source. It can be derived from the continuity equation:\n\nwhere j is the total flux and R is a net volumetric source for φ. The only source of flux in this situation is assumed to be diffusive flux:\n\nPlugging the definition of diffusive flux to the continuity equation and assuming there is no source (R = 0), we arrive at Fick's second law:\n\nIf flux were the result of both diffusive flux and advective flux, the convection–diffusion equation is the result.\n\nA simple case of diffusion with time t in one dimension (taken as the x-axis) from a boundary located at position x = 0, where the concentration is maintained at a value n0 is\n\nwhere erfc is the complementary error function. This is the case when corrosive gases diffuse through the oxidative layer towards the metal surface (if we assume that concentration of gases in the environment is constant and the diffusion space – that is, the corrosion product layer – is semi-infinite, starting at 0 at the surface and spreading infinitely deep in the material). If, in its turn, the diffusion space is infinite (lasting both through the layer with n(x, 0) = 0, x > 0 and that with n(x, 0) = n0, x ≤ 0), then the solution is amended only with coefficient ⁠1/2⁠ in front of n0 (as the diffusion now occurs in both directions). This case is valid when some solution with concentration n0 is put in contact with a layer of pure solvent. (Bokstein, 2005) The length 2√Dt is called the diffusion length and provides a measure of how far the concentration has propagated in the x-direction by diffusion in time t (Bird, 1976).\n\nAs a quick approximation of the error function, the first two terms of the Taylor series can be used:\n\nIf D is time-dependent, the diffusion length becomes\n\nThis idea is useful for estimating a diffusion length over a heating and cooling cycle, where D varies with temperature.\n\nAnother simple case of diffusion is the Brownian motion of one particle. The particle's Mean squared displacement from its original position is:\n\n  \n    \n      \n        \n          MSD\n        \n        ≡\n        \n          ⟨\n          \n            (\n            \n              x\n            \n            −\n            \n              \n                x\n                \n                  0\n                \n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n          ⟩\n        \n        =\n        2\n        n\n        D\n        t\n        ,\n      \n    \n    {\\displaystyle {\\text{MSD}}\\equiv \\left\\langle (\\mathbf {x} -\\mathbf {x_{0}} )^{2}\\right\\rangle =2nDt,}\n  \n\nwhere \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the dimension of the particle's Brownian motion. For example, the diffusion of a molecule across a cell membrane 8 nm thick is 1-D diffusion because of the spherical symmetry; However, the diffusion of a molecule from the membrane to the center of a eukaryotic cell is a 3-D diffusion. For a cylindrical cactus, the diffusion from photosynthetic cells on its surface to its center (the axis of its cylindrical symmetry) is a 2-D diffusion.\n\nThe square root of MSD, \n  \n    \n      \n        \n          \n            2\n            n\n            D\n            t\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {2nDt}}}\n  \n, is often used as a characterization of how far the particle has moved after time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n has elapsed. The MSD is symmetrically distributed over the 1D, 2D, and 3D space. Thus, the probability distribution of the magnitude of MSD in 1D is Gaussian and 3D is a Maxwell-Boltzmann distribution.\n\nThe Chapman–Enskog formulae for diffusion in gases include exactly the same terms. These physical models of diffusion are different from the test models ∂tφi = Σj Dij Δφj which are valid for very small deviations from the uniform equilibrium. Earlier, such terms were introduced in the Maxwell–Stefan diffusion equation.\n\nFor anisotropic multicomponent diffusion coefficients one needs a rank-four tensor, for example Dij,αβ, where i, j refer to the components and α, β = 1, 2, 3 correspond to the space coordinates.\n\nEquations based on Fick's law have been commonly used to model transport processes in foods, neurons, biopolymers, pharmaceuticals, porous soils, population dynamics, nuclear materials, plasma physics, and semiconductor doping processes. The theory of voltammetric methods is based on solutions of Fick's equation. On the other hand, in some cases a \"Fickian (another common approximation of the transport equation is that of the diffusion theory)\" description is inadequate. For example, in polymer science and food science a more general approach is required to describe transport of components in materials undergoing a glass transition. One more general framework is the Maxwell–Stefan diffusion equations[9]\nof multi-component mass transfer, from which Fick's law can be obtained as a limiting case, when the mixture is extremely dilute and every chemical species is interacting only with the bulk mixture and not with other species. To account for the presence of multiple species in a non-dilute mixture, several variations of the Maxwell–Stefan equations are used. See also non-diagonal coupled transport processes (Onsager relationship).\n\nWhen two miscible liquids are brought into contact, and diffusion takes place, the macroscopic (or average) concentration evolves following Fick's law. On a mesoscopic scale, that is, between the macroscopic scale described by Fick's law and molecular scale, where molecular random walks take place, fluctuations cannot be neglected. Such situations can be successfully modeled with Landau-Lifshitz fluctuating hydrodynamics. In this theoretical framework, diffusion is due to fluctuations whose dimensions range from the molecular scale to the macroscopic scale.[10]\n\nIn particular, fluctuating hydrodynamic equations include a Fick's flow term, with a given diffusion coefficient, along with hydrodynamics equations and stochastic terms describing fluctuations. When calculating the fluctuations with a perturbative approach, the zero order approximation is Fick's law. The first order gives the fluctuations, and it comes out that fluctuations contribute to diffusion. This represents somehow a tautology, since the phenomena described by a lower order approximation is the result of a higher approximation: this problem is solved only by renormalizing the fluctuating hydrodynamics equations.\n\nAdsorption, absorption, and collision of molecules, particles, and surfaces are important problems in many fields. These fundamental processes regulate chemical, biological, and environmental reactions. Their rate can be calculated using the diffusion constant and Fick's laws of diffusion especially when these interactions happen in diluted solutions.\n\nTypically, the diffusion constant of molecules and particles defined by Fick's equation can be calculated using the Stokes–Einstein equation. In the ultrashort time limit, in the order of the diffusion time a2/D, where a is the particle radius, the diffusion is described by the Langevin equation. At a longer time, the Langevin equation merges into the Stokes–Einstein equation. The latter is appropriate for the condition of the diluted solution, where long-range diffusion is considered. According to the fluctuation-dissipation theorem based on the Langevin equation in the long-time limit and when the particle is significantly denser than the surrounding fluid, the time-dependent diffusion constant is:[11]\n\nFor a single molecule such as organic molecules or biomolecules (e.g. proteins) in water, the exponential term is negligible due to the small product of mμ in the ultrafast picosecond region, thus irrelevant to the relatively slower adsorption of diluted solute.\n\nThe adsorption or absorption rate of a dilute solute to a surface or interface in a (gas or liquid) solution can be calculated using Fick's laws of diffusion. The accumulated number of molecules adsorbed on the surface is expressed by the Langmuir-Schaefer equation by integrating the diffusion flux equation over time as shown in the simulated molecular diffusion in the first section of this page:[13]\n\nThe equation is named after American chemists Irving Langmuir and Vincent Schaefer.\n\nBriefly as explained in,[14]\nthe concentration gradient profile near a newly created (from \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n) absorptive surface (placed at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n) in a once uniform bulk solution is solved in the above sections from Fick's equation,\n\nwhere C is the number concentration of adsorber molecules at \n  \n    \n      \n        x\n        ,\n        t\n      \n    \n    {\\displaystyle x,t}\n  \n (#/m3).\n\nThe concentration gradient at the subsurface at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n is simplified to the pre-exponential factor of the distribution\n\nAnd the rate of diffusion (flux) across area \n  \n    \n      \n        A\n        .\n      \n    \n    {\\displaystyle A.}\n  \n of the plane is\n\nThe Langmuir–Schaefer equation can be extended to the Ward–Tordai Equation to account for the \"back-diffusion\" of rejected molecules from the surface:[14]\n\nwhere \n  \n    \n      \n        \n          C\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle C_{b}}\n  \n is the bulk concentration, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the sub-surface concentration (which is a function of time depending on the reaction model of the adsorption), and \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n is a dummy variable.\n\nMonte Carlo simulations show that these two equations work to predict the adsorption rate of systems that form predictable concentration gradients near the surface but have troubles for systems without or with unpredictable concentration gradients, such as typical biosensing systems or when flow and convection are significant.[15]\n\nA brief history of diffusive adsorption is shown in the right figure.[15] A noticeable challenge of understanding the diffusive adsorption at the single-molecule level is the fractal nature of diffusion. Most computer simulations pick a time step for diffusion which ignores the fact that there are self-similar finer diffusion events (fractal) within each step. Simulating the fractal diffusion shows that a factor of two corrections should be introduced for the result of a fixed time-step adsorption simulation, bringing it to be consistent with the above two equations.[15]\n\nA more problematic result of the above equations is they predict the lower limit of adsorption under ideal situations but is very difficult to predict the actual adsorption rates. The equations are derived at the long-time-limit condition when a stable concentration gradient has been formed near the surface. But real adsorption is often done much faster than this infinite time limit i.e. the concentration gradient, decay of concentration at the sub-surface, is only partially formed before the surface has been saturated or flow is on to maintain a certain gradient, thus the adsorption rate measured is almost always faster than the equations have predicted for low or none energy barrier adsorption (unless there is a significant adsorption energy barrier that slows down the absorption significantly), for example, thousands to millions time faster in the self-assembly of monolayers at the water-air or water-substrate interfaces.[13] As such, it is necessary to calculate the evolution of the concentration gradient near the surface and find out a proper time to stop the imagined infinite evolution for practical applications. While it is hard to predict when to stop but it is reasonably easy to calculate the shortest time that matters, the critical time when the first nearest neighbor from the substrate surface feels the building-up of the concentration gradient. This yields the upper limit of the adsorption rate under an ideal situation when there are no other factors than diffusion that affect the absorber dynamics:[15]\n\nThis equation can be used to predict the initial adsorption rate of any system; It can be used to predict the steady-state adsorption rate of a typical biosensing system when the binding site is just a very small fraction of the substrate surface and a near-surface concentration gradient is never formed; It can also be used to predict the adsorption rate of molecules on the surface when there is a significant flow to push the concentration gradient very shallowly in the sub-surface.\n\nThis critical time is significantly different from the first passenger arriving time or the mean free-path time. Using the average first-passenger time and Fick's law of diffusion to estimate the average binding rate will significantly over-estimate the concentration gradient because the first passenger usually comes from many layers of neighbors away from the target, thus its arriving time is significantly longer than the nearest neighbor diffusion time. Using the mean free path time plus the Langmuir equation will cause an artificial concentration gradient between the initial location of the first passenger and the target surface because the other neighbor layers have no change yet, thus significantly lower estimate the actual binding time, i.e., the actual first passenger arriving time itself, the inverse of the above rate, is difficult to calculate. If the system can be simplified to 1D diffusion, then the average first passenger time can be calculated using the same nearest neighbor critical diffusion time for the first neighbor distance to be the MSD,[16]\n\nIn this critical time, it is unlikely the first passenger has arrived and adsorbed. But it sets the speed of the layers of neighbors to arrive. At this speed with a concentration gradient that stops around the first neighbor layer, the gradient does not project virtually in the longer time when the actual first passenger arrives. Thus, the average first passenger coming rate (unit # molecule/s) for this 3D diffusion simplified in 1D problem,\n\nwhere \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is a factor of converting the 3D diffusive adsorption problem into a 1D diffusion problem whose value depends on the system, e.g., a fraction of adsorption area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n over solute nearest neighbor sphere surface area \n  \n    \n      \n        4\n        π\n        \n          L\n          \n            2\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle 4\\pi L^{2}/4}\n  \n assuming cubic packing each unit has 8 neighbors shared with other units. This example fraction converges the result to the 3D diffusive adsorption solution shown above with a slight difference in pre-factor due to different packing assumptions and ignoring other neighbors.\n\nWhen the area of interest is the size of a molecule (specifically, a long cylindrical molecule such as DNA), the adsorption rate equation represents the collision frequency of two molecules in a diluted solution, with one molecule a specific side and the other no steric dependence, i.e., a molecule (random orientation) hit one side of the other. The diffusion constant need to be updated to the relative diffusion constant between two diffusing molecules. This estimation is especially useful in studying the interaction between a small molecule and a larger molecule such as a protein. The effective diffusion constant is dominated by the smaller one whose diffusion constant can be used instead.\n\nThe above hitting rate equation is also useful to predict the kinetics of molecular self-assembly on a surface. Molecules are randomly oriented in the bulk solution. Assuming 1/6 of the molecules has the right orientation to the surface binding sites, i.e. 1/2 of the z-direction in x, y, z three dimensions, thus the concentration of interest is just 1/6 of the bulk concentration. Put this value into the equation one should be able to calculate the theoretical adsorption kinetic curve using the Langmuir adsorption model. In a more rigid picture, 1/6 can be replaced by the steric factor of the binding geometry.\n\nThe bimolecular collision frequency related to many reactions including protein coagulation/aggregation is initially described by Smoluchowski coagulation equation proposed by Marian Smoluchowski in a seminal 1916 publication,[18] derived from Brownian motion and Fick's laws of diffusion. Under an idealized reaction condition for A + B → product in a diluted solution, Smoluchovski suggested that the molecular flux at the infinite time limit can be calculated from Fick's laws of diffusion yielding a fixed/stable concentration gradient from the target molecule, e.g. B is the target molecule holding fixed relatively, and A is the moving molecule that creates a concentration gradient near the target molecule B due to the coagulation reaction between A and B. Smoluchowski calculated the collision frequency between A and B in the solution with unit #/s/m3:\n\nThe reaction order of this bimolecular reaction is 2 which is the analogy to the result from collision theory by replacing the moving speed of the molecule with diffusive flux. In the collision theory, the traveling time between A and B is proportional to the distance which is a similar relationship for the diffusion case if the flux is fixed.\n\nHowever, under a practical condition, the concentration gradient near the target molecule is evolving over time with the molecular flux evolving as well,[15] and on average the flux is much bigger than the infinite time limit flux Smoluchowski has proposed. Before the first passenger arrival time, Fick's equation predicts a concentration gradient over time which does not build up yet in reality. Thus, this Smoluchowski frequency represents the lower limit of the real collision frequency.\n\nIn 2022, Chen calculates the upper limit of the collision frequency between A and B in a solution assuming the bulk concentration of the moving molecule is fixed after the first nearest neighbor of the target molecule.[17] Thus the concentration gradient evolution stops at the first nearest neighbor layer given a stop-time to calculate the actual flux. He named this the critical time and derived the diffusive collision frequency in unit #/s/m3:[17]\n\nThis equation assumes the upper limit of a diffusive collision frequency between A and B is when the first neighbor layer starts to feel the evolution of the concentration gradient, whose reaction order is ⁠2+1/3⁠ instead of 2. Both the Smoluchowski equation and the JChen equation satisfy dimensional checks with SI units. But the former is dependent on the radius and the latter is on the area of the collision sphere. From dimensional analysis, there will be an equation dependent on the volume of the collision sphere but eventually, all equations should converge to the same numerical rate of the collision that can be measured experimentally. The actual reaction order for a bimolecular unit reaction could be between 2 and ⁠2+1/3⁠, which makes sense because the diffusive collision time is squarely dependent on the distance between the two molecules.\n\nThese new equations also avoid the singularity on the adsorption rate at time zero for the Langmuir-Schaefer equation. The infinity rate is justifiable under ideal conditions because when you introduce target molecules magically in a solution of probe molecule or vice versa, there always be a probability of them overlapping at time zero, thus the rate of that two molecules association is infinity. It does not matter that other millions of molecules have to wait for their first mate to diffuse and arrive. The average rate is thus infinity. But statistically this argument is meaningless. The maximum rate of a molecule in a period of time larger than zero is 1, either meet or not, thus the infinite rate at time zero for that molecule pair really should just be one, making the average rate 1/millions or more and statistically negligible. This does not even count in reality no two molecules can magically meet at time zero.\n\nThe first law gives rise to the following formula:[19]\n\nFick's first law is also important in radiation transfer equations.  However, in this context, it becomes inaccurate when the diffusion constant is low and the radiation becomes limited by the speed of light rather than by the resistance of the material the radiation is flowing through.  In this situation, one can use a flux limiter.\n\nThe exchange rate of a gas across a fluid membrane can be determined by using this law together with Graham's law.\n\nUnder the condition of a diluted solution when diffusion takes control, the membrane permeability mentioned in the above section can be theoretically calculated for the solute using the equation mentioned in the last section (use with particular care because the equation is derived for dense solutes, while biological molecules are not denser than water. Also, this equation assumes ideal concentration gradient forms near the membrane and evolves):[12]\n\nThe flux is decay over the square root of time because a concentration gradient builds up near the membrane over time under ideal conditions. When there is flow and convection, the flux can be significantly different than the equation predicts and show an effective time t with a fixed value,[15] which makes the flux stable instead of decay over time. A critical time has been estimated under idealized flow conditions when there is no gradient formed.[15][17] This strategy is adopted in biology such as blood circulation.\n\nThe semiconductor is a collective term for a series of devices. It mainly includes three categories：two-terminal devices, three-terminal devices, and four-terminal devices. The combination of the semiconductors is called an integrated circuit.\n\nThe relationship between Fick's law and semiconductors: the principle of the semiconductor is transferring chemicals or dopants from a layer to a layer. Fick's law can be used to control and predict the diffusion by knowing how much the concentration of the dopants or chemicals move per meter and second through mathematics.\n\nTherefore, different types and levels of semiconductors can be fabricated.\n\nIntegrated circuit fabrication technologies, model processes like CVD, thermal oxidation, wet oxidation, doping, etc. use diffusion equations obtained from Fick's law.\n\nThe wafer is a kind of semiconductor whose silicon substrate is coated with a layer of CVD-created polymer chain and films. This film contains n-type and p-type dopants and takes responsibility for dopant conductions. The principle of CVD relies on the gas phase and gas-solid chemical reaction to create thin films.\n\nThe viscous flow regime of CVD is driven by a pressure gradient. CVD also includes a diffusion component distinct from the surface diffusion of adatoms. In CVD, reactants and products must also diffuse through a boundary layer of stagnant gas that exists next to the substrate. The total number of steps required for CVD film growth are gas phase diffusion of reactants through the boundary layer, adsorption and surface diffusion of adatoms, reactions on the substrate, and gas phase diffusion of products away through the boundary layer.\n\nThe velocity profile for gas flow is:\n\n  \n    \n      \n        δ\n        (\n        x\n        )\n        =\n        \n          (\n          \n            \n              \n                5\n                x\n              \n              \n                \n                  R\n                  e\n                \n                \n                  1\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n          )\n        \n        \n          R\n          e\n        \n        =\n        \n          \n            \n              v\n              ρ\n              L\n            \n            η\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\delta (x)=\\left({\\frac {5x}{\\mathrm {Re} ^{1/2}}}\\right)\\mathrm {Re} ={\\frac {v\\rho L}{\\eta }},}\n  \n\nwhere:\n\nIntegrated the x from 0 to L, it gives the average thickness:\n\n  \n    \n      \n        δ\n        =\n        \n          \n            \n              10\n              L\n            \n            \n              3\n              \n                \n                  R\n                  e\n                \n                \n                  1\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\delta ={\\frac {10L}{3\\mathrm {Re} ^{1/2}}}.}\n\nTo keep the reaction balanced, reactants must diffuse through the stagnant boundary layer to reach the substrate. So a thin boundary layer is desirable. According to the equations, increasing vo would result in more wasted reactants. The reactants will not reach the substrate uniformly if the flow becomes turbulent. Another option is to switch to a new carrier gas with lower viscosity or density.\n\nThe Fick's first law describes diffusion through the boundary layer. As a function of pressure (p) and temperature (T) in a gas, diffusion is determined.\n\nD\n        =\n        \n          D\n          \n            0\n          \n        \n        \n          (\n          \n            \n              \n                p\n                \n                  0\n                \n              \n              p\n            \n          \n          )\n        \n        \n          \n            (\n            \n              \n                T\n                \n                  T\n                  \n                    0\n                  \n                \n              \n            \n            )\n          \n          \n            3\n            \n              /\n            \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle D=D_{0}\\left({\\frac {p_{0}}{p}}\\right)\\left({\\frac {T}{T_{0}}}\\right)^{3/2},}\n  \n\nwhere:\n\nThe equation tells that increasing the temperature or decreasing the pressure can increase the diffusivity.\n\nFick's first law predicts the flux of the reactants to the substrate and product away from the substrate:\n\n  \n    \n      \n        J\n        =\n        −\n        \n          D\n          \n            i\n          \n        \n        \n          (\n          \n            \n              \n                d\n                \n                  c\n                  \n                    i\n                  \n                \n              \n              \n                d\n                x\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle J=-D_{i}\\left({\\frac {dc_{i}}{dx}}\\right),}\n  \n\nwhere:\n\nIn ideal gas law \n  \n    \n      \n        p\n        V\n        =\n        n\n        R\n        T\n      \n    \n    {\\displaystyle pV=nRT}\n  \n, the concentration of the gas is expressed by partial pressure.\n\nJ\n        =\n        −\n        \n          D\n          \n            i\n          \n        \n        \n          (\n          \n            \n              \n                \n                  p\n                  \n                    i\n                  \n                \n                −\n                \n                  p\n                  \n                    0\n                  \n                \n              \n              \n                δ\n                R\n                T\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle J=-D_{i}\\left({\\frac {p_{i}-p_{0}}{\\delta RT}}\\right),}\n  \n\nwhere\n\nAs a result, Fick's first law tells us we can use a partial pressure gradient to control the diffusivity and control the growth of thin films of semiconductors.\n\nIn many realistic situations, the simple Fick's law is not an adequate formulation for the semiconductor problem. It only applies to certain conditions, for example, given the semiconductor boundary conditions: constant source concentration diffusion, limited source concentration, or moving boundary diffusion (where junction depth keeps moving into the substrate).\n\nEven though Fickian diffusion has been used to model diffusion processes in semiconductor manufacturing (including CVD reactors) in early days, it often fails to validate the diffusion in advanced semiconductor nodes (< 90 nm). This mostly stems from the inability of Fickian diffusion to model diffusion processes accurately at molecular level and smaller. In advanced semiconductor manufacturing, it is important to understand the movement at atomic scales, which is failed by continuum diffusion. Today, most semiconductor manufacturers use random walk to study and model diffusion processes. This allows us to study the effects of diffusion in a discrete manner to understand the movement of individual atoms, molecules, plasma etc.\n\nIn such a process, the movements of diffusing species (atoms, molecules, plasma etc.) are treated as a discrete entity, following a random walk through the CVD reactor, boundary layer, material structures etc. Sometimes, the movements might follow a biased-random walk depending on the processing conditions. Statistical analysis is done to understand variation/stochasticity arising from the random walk of the species, which in-turn affects the overall process and electrical variations.\n\nThe formulation of Fick's first law can explain a variety of complex phenomena in the context of food and cooking: Diffusion of molecules such as ethylene promotes plant growth and ripening, salt and sugar molecules promotes meat brining and marinating, and water molecules promote dehydration. Fick's first law can also be used to predict the changing moisture profiles across a spaghetti noodle as it hydrates during cooking. These phenomena are all about the spontaneous movement of particles of solutes driven by the concentration gradient. In different situations, there is different diffusivity which is a constant.[20]\n\nBy controlling the concentration gradient, the cooking time, shape of the food, and salting can be controlled.",
        pageTitle: "Fick's laws of diffusion",
    },
    {
        title: "Finagle's law",
        link: "https://en.wikipedia.org/wiki/Finagle%27s_law",
        content:
            'Finagle\'s law of dynamic negatives (also known as Melody\'s law, Sod\'s Law or Finagle\'s corollary to Murphy\'s law) is usually rendered as "Anything that can go wrong, will—at the worst possible moment."\n\nThe term "Finagle\'s law" is often associated with John W. Campbell Jr., the influential editor of Astounding Science Fiction (later Analog).\n\nOne variant (known as O\'Toole\'s corollary of Finagle\'s law) favored among hackers is a takeoff on the second law of thermodynamics (related to the augmentation of entropy):\n\nThe perversity of the Universe tends towards a maximum.\n\nIn the Star Trek episode "Amok Time" (written by Theodore Sturgeon in 1967), Captain Kirk tells Spock, "As one of Finagle\'s laws puts it: \'Any home port the ship makes will be somebody else\'s, not mine.\'"\n\nThe term "Finagle\'s law" was popularized by science fiction author Larry Niven in several stories (for example, Protector [Ballantine Books paperback edition, 4th printing, p. 23]), depicting a frontier culture of asteroid miners; this "Belter" culture professed a religion or running joke involving the worship of the dread god Finagle and his mad prophet Murphy.[1][2]\n\n"Finagle\'s law" can also be the related belief "Inanimate objects are out to get us", also known as Resistentialism.[3][4]\nSimilar to Finagle\'s law is the verbless phrase of the German novelist Friedrich Theodor Vischer: "die Tücke des Objekts" (the perfidy of inanimate objects).\n\nA related concept, the "Finagle factor", is an ad hoc multiplicative or additive term in an equation, which can be justified only by the fact that it gives more correct results.  Also known as Finagle\'s variable constant, it is sometimes defined as the correct answer divided by your answer.\n\nOne of the first records of "Finagle factor" is probably a December 1962 article in The Michigan Technic, credited to Campbell, but bylined "I Finaglin" [5]\n\nThe term is also used in a 1960 wildlife management article.[6]\n\nArthur Bloch, in his book "Murphy\'s Law and Other Reasons Why Things Go Wrong" (1977) stated variations on this:[7]',
        pageTitle: "Finagle's law",
    },
    {
        title: "Fitts's law",
        link: "https://en.wikipedia.org/wiki/Fitts%27s_law",
        content:
            "Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in human–computer interaction and ergonomics. The law predicts that the time required to rapidly move to a target area is a function of the ratio between the distance to the target and the width of the target.[1] Fitts's law is used to model the act of pointing, either by physically touching an object with a hand or finger, or virtually, by pointing to an object on a computer monitor using a pointing device. It was initially developed by Paul Fitts.\n\nFitts's law has been shown to apply under a variety of conditions; with many different limbs (hands, feet,[2] the lower lip,[3] head-mounted sights[4]), manipulanda (input devices),[5] physical environments (including underwater[6]), and user populations (young, old,[7] special educational needs,[8] and drugged participants[9]).\n\nThe original 1954 paper by Paul Morris Fitts proposed a metric to quantify the difficulty of a target selection task.\nThe metric was based on an information analogy, where the distance to the center of the target (D) is like a signal and the tolerance or width of the target (W) is like noise.\nThe metric is Fitts's index of difficulty (ID, in bits):\n\nID\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            (\n          \n        \n        \n          \n            \n              2\n              D\n            \n            W\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{ID}}=\\log _{2}{\\Bigg (}{\\frac {2D}{W}}{\\Bigg )}}\n\nFitts also proposed an index of performance (IP, in bits per second) as a measure of human performance. The metric combines a task's index of difficulty (ID) with the movement time (MT, in seconds) in selecting the target. In Fitts's words,\n\"The average rate of information generated by a series of movements is the average information per movement divided by the time per movement.\"[1] Thus,\n\nIP\n        \n        =\n        \n          \n            (\n          \n        \n        \n          \n            ID\n            MT\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{IP}}={\\Bigg (}{\\frac {\\text{ID}}{\\text{MT}}}{\\Bigg )}}\n\nToday, IP is more commonly called throughput (TP). It is also common to include an adjustment for accuracy in the calculation.\n\nResearchers after Fitts began the practice of building linear regression equations and examining the correlation (r) for goodness of fit. The equation expresses the relationship between\nMT and the D and W task parameters:\n\nMT\n        \n        =\n        a\n        +\n        b\n        ⋅\n        \n          ID\n        \n        =\n        a\n        +\n        b\n        ⋅\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            (\n          \n        \n        \n          \n            \n              2\n              D\n            \n            W\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{MT}}=a+b\\cdot {\\text{ID}}=a+b\\cdot \\log _{2}{\\Bigg (}{\\frac {2D}{W}}{\\Bigg )}}\n\nSince shorter movement times are desirable for a given task, the value of the b parameter can be used as a metric when comparing computer pointing devices against one another. The first human–computer interface application of Fitts's law was by Card, English, and Burr,[11] who used the index of performance (IP), interpreted as 1⁄b, to compare performance of different input devices, with the mouse coming out on top compared to the joystick or directional movement keys.[11] This early work, according to Stuart Card's biography, \"was a major factor leading to the mouse's commercial introduction by Xerox\".[12]\n\nMany experiments testing Fitts's law apply the model to a dataset in which either distance or width, but not both, are varied. The model's predictive power deteriorates when both are varied over a significant range.[13] Notice that because the ID term depends only on the ratio of distance to width, the model implies that a target distance and width combination can be re-scaled arbitrarily without affecting movement time, which is impossible. Despite its flaws, this form of the model does possess remarkable predictive power across a range of computer interface modalities and motor tasks, and has provided many insights into user interface design principles.\n\nA movement during a single Fitts's law task can be split into two phases:[10]\n\nThe first phase is defined by the distance to the target. In this phase the distance can be closed quickly while still being imprecise. The second movement tries to perform a slow and controlled precise movement to actually hit the target.\nThe task duration scales linearly in regards to difficulty.[10] But as different tasks can have the same difficulty, it is derived that distance has a greater impact on the overall task completion time than target size.\n\nOften it is cited that Fitts's law can be applied to eye tracking. This seems to be at least a controversial topic as Drewes showed.[14] During fast saccadic eye movements the user is blind. During a Fitts's law task the user consciously acquires its target and can actually see it, making these two types of interaction not comparable.\n\nThe formulation of Fitts's index of difficulty most frequently used in the human–computer interaction community is called the Shannon formulation:\n\nID\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            (\n          \n        \n        \n          \n            D\n            W\n          \n        \n        +\n        1\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{ID}}=\\log _{2}{\\Bigg (}{\\frac {D}{W}}+1{\\Bigg )}}\n\nThis form was proposed by Scott MacKenzie,[15] professor at York University, and named for its resemblance to the Shannon–Hartley theorem.[16] It describes the transmission of information using bandwidth, signal strength and noise. In Fitts's law, the distance represents signal strength, while target width is noise.\n\nUsing this form of the model, the difficulty of a pointing task was equated to a quantity of information transmitted (in units of bits) by performing the task. This was justified by the assertion that pointing reduces to an information processing task. Although no formal mathematical connection was established between Fitts's law and the Shannon-Hartley theorem it was inspired by, the Shannon form of the law has been used extensively, likely due to the appeal of quantifying motor actions using information theory.[17] In 2002 the ISO 9241 was published, providing standards for human–computer interface testing, including the use of the Shannon form of Fitts's law. It has been shown that the information transmitted via serial keystrokes on a keyboard and the information implied by the ID for such a task are not consistent.[18] The Shannon-Entropy results in a different information value than Fitts's law. The authors note, though, that the error is negligible and only has to be accounted for in comparisons of devices with known entropy or measurements of human information processing capabilities.\n\nAn important improvement to Fitts's law was proposed by Crossman in 1956 (see Welford, 1968, pp. 147–148)[19] and used by Fitts\nin his 1964 paper with Peterson.[20] With the adjustment, target width (W) is replaced by an effective target width (We).\nWe is computed from the standard deviation in the selection coordinates gathered over a sequence of trials for a particular D-W condition. If the selections are logged as x coordinates along the axis of approach to the target, then\n\nW\n          \n            e\n          \n        \n        =\n        4.133\n        ×\n        S\n        \n          D\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle W_{e}=4.133\\times SD_{x}}\n\nID\n          \n          \n            e\n          \n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            (\n          \n        \n        \n          \n            D\n            \n              W\n              \n                e\n              \n            \n          \n        \n        +\n        1\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{ID}}_{e}=\\log _{2}{\\Bigg (}{\\frac {D}{W_{e}}}+1{\\Bigg )}}\n\nIP\n        \n        =\n        \n          \n            (\n          \n        \n        \n          \n            \n              I\n              \n                D\n                \n                  e\n                \n              \n            \n            \n              M\n              T\n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{IP}}={\\Bigg (}{\\frac {ID_{e}}{MT}}{\\Bigg )}}\n\nIf the selection coordinates are normally distributed, We spans 96% of the distribution. If the observed error rate was 4% in the sequence of trials, then We = W. If the error rate was greater than 4%, We > W, and if the error rate was less than 4%, We < W. By using We, a Fitts' law model more closely reflects what users actually did, rather than what they were asked to do.\n\nThe main advantage in computing IP as above is that spatial variability, or accuracy, is included in the measurement. With the adjustment for accuracy, Fitts's law more truly encompasses the speed-accuracy tradeoff. The equations above appear in ISO 9241-9 as the recommended method of computing throughput.\n\nNot long after the original model was proposed, a 2-factor variation was proposed under the intuition that target distance and width have separate effects on movement time. Welford's model, proposed in 1968, separated the influence of target distance and width into separate terms, and provided improved predictive power:[19]\n\nM\n        T\n        =\n        a\n        +\n        \n          b\n          \n            1\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        D\n        )\n        +\n        \n          b\n          \n            2\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        W\n        )\n      \n    \n    {\\displaystyle MT=a+b_{1}\\log _{2}(D)+b_{2}\\log _{2}(W)}\n\nThis model has an additional parameter, so its predictive accuracy cannot be directly compared with 1-factor forms of Fitts's law. However, a variation on Welford's model inspired by the Shannon formulation,\n\nM\n        T\n        =\n        a\n        +\n        \n          b\n          \n            1\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        D\n        +\n        W\n        )\n        +\n        \n          b\n          \n            2\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        W\n        )\n        =\n        a\n        +\n        b\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          (\n          \n            \n              \n                D\n                +\n                W\n              \n              \n                W\n                \n                  k\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle MT=a+b_{1}\\log _{2}(D+W)+b_{2}\\log _{2}(W)=a+b\\log _{2}\\left({\\frac {D+W}{W^{k}}}\\right)}\n\nThe additional parameter k allows the introduction of angles into the model. Now the users position can be accounted for. The influence of the angle can be weighted using the exponent. This addition was introduced by Kopper et al. in 2010.[21]\n\nThe formula reduces to the Shannon form when k = 1. Therefore, this model can be directly compared against the Shannon form of Fitts's law using the F-test of nested models.[22] This comparison reveals that not only does the Shannon form of Welford's model better predict movement times, but it is also more robust when control-display gain (the ratio between e.g. hand movement and cursor movement) is varied. Consequently, although the Shannon model is slightly more complex and less intuitive, it is empirically the best model to use for virtual pointing tasks.\n\nIn its original form, Fitts's law is meant to apply only to one-dimensional tasks. However, the original experiments required subjects to move a stylus (in three dimensions) between two metal plates on a table, termed the reciprocal tapping task.[1] The target width perpendicular to the direction of movement was very wide to avoid it having a significant influence on performance. A major application for Fitts's law is 2D virtual pointing tasks on computer screens, in which targets have bounded sizes in both dimensions.\n\nFitts's law has been extended to two-dimensional tasks in two different ways. For navigating e.g. hierarchical pull-down menus, the user must generate a trajectory with the pointing device that is constrained by the menu geometry; for this application the Accot-Zhai steering law was derived.\n\nFor simply pointing to targets in a two-dimensional space, the model generally holds as-is but requires adjustments to capture target geometry and quantify targeting errors in a logically consistent way.[23][24]\nMultiple Methods have been used to determine the target size :[25]\n\nWhile the W-model is sometimes considered the state-of-the-art measurement, the truly correct representation for non-circular targets is substantially more complex, as it requires computing the angle-specific convolution between the trajectory of the pointing device and the target [26]\n\nSince the a and b parameters should capture movement times over a potentially wide range of task geometries, they can serve as a performance metric for a given interface. In doing so, it is necessary to separate variation between users from variation between interfaces. \nThe a parameter is typically positive and close to zero, and sometimes ignored in characterizing average performance, as in Fitts' original experiment.[18] Multiple methods exist for identifying parameters from experimental data, and the choice of method is the subject of heated debate, since method variation can result in parameter differences that overwhelm underlying performance differences.[27][28]\n\nAn additional issue in characterizing performance is incorporating success rate: an aggressive user can achieve shorter movement times at the cost of experimental trials in which the target is missed. If the latter are not incorporated into the model, then average movement times can be artificially decreased.\n\nFitts's law deals only with targets defined in space. However, a target can be defined purely on the time axis, which is called a temporal target. A blinking target or a target moving toward a selection area are examples of temporal targets. Similar to space, the distance to the target (i.e., temporal distance Dt) and the width of the target (i.e., temporal width Wt) can be defined for temporal targets as well. The temporal distance is the amount of time a person must wait for a target to appear. The temporal width is a short duration from the moment the target appears until it disappears. For example, for a blinking target, Dt can be thought of as the period of blinking and Wt as the duration of the blinking. As with targets in space, the larger the Dt or the smaller the Wt, the more difficult it becomes to select the target.\n\nThe task of selecting the temporal target is called temporal pointing. The model for temporal pointing was first presented to the human–computer interaction field in 2016.[29] The model predicts the error rate, the human performance in temporal pointing, as a function of temporal index of difficulty (IDt):\n\nID\n          \n          \n            t\n          \n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            (\n          \n        \n        \n          \n            \n              D\n              \n                t\n              \n            \n            \n              W\n              \n                t\n              \n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{ID}}_{t}=\\log _{2}{\\Bigg (}{\\frac {D_{t}}{W_{t}}}{\\Bigg )}}\n\nMultiple design guidelines for GUIs can be derived from the implications of Fitts's law. In its basic form, Fitts's law says that targets a user has to hit should be as big as possible. This is derived from the W parameter. More specifically, the effective size of the button should be as big as possible, meaning that its form has to be optimized for the direction of the user's movement onto the target.\n\nLayouts should also cluster functions that are commonly used with each other. Optimizing for the D parameter in this way allows for smaller travel times.\n\nPlacing layout elements on the four edges of the screen allows for infinitely large targets in one dimension and therefore presents ideal scenarios. Since the pointer will always stop at the edge, the user can move the mouse with the greatest possible speed and still hit the target. The target area is effectively infinitely long along the movement axis. Therefore, this guideline is called “Rule of the infinite edges”. The use of this rule can be seen for example in MacOS, which always places the menu bar on the top left edge of the screen instead of the current program's windowframe.[30]\n\nThis effect can be exaggerated at the four corners of a screen. At these points two edges collide and form a theoretically infinitely big button. Microsoft Windows (prior to Windows 11) places its \"Start\" button in the lower left corner and Microsoft Office 2007 uses the upper left corner for its \"Office\" menu. These four spots are sometimes called \"magic corners\".[31]\nMacOS places the close button on the upper left side of the program window and the menu bar fills out the magic corner with another button.\n\nA UI that allows for pop-up menus rather than fixed drop-down menus reduces travel times for the D parameter. The user can continue interaction right from the current mouse position and doesn't have to move to a different preset area. Many operating systems use this when displaying right-click context menus. As the menu starts right on the pixel which the user clicked, this pixel is referred to as the \"magic\" or \"prime pixel\".[25]\n\nJames Boritz et al. (1991)[32] compared radial menu designs. In a radial menu all items have the same distance from the prime pixel. The research suggests that in practical implementations the direction in which a user has to move their mouse has also to be accounted for. For right-handed users, selecting the left-most menu item was significantly more difficult than the right-most one. No differences were found for transitions from upper to lower functions and vice versa.",
        pageTitle: "Fitts's law",
    },
    {
        title: "Fourier's law",
        link: "https://en.wikipedia.org/wiki/Fourier%27s_law",
        content:
            "Thermal conduction is the diffusion of thermal energy (heat) within one material or between materials in contact. The higher temperature object has molecules with more kinetic energy; collisions between molecules distributes this kinetic energy until an object has the same kinetic energy throughout. Thermal conductivity, frequently represented by k, is a property that relates the rate of heat loss per unit area of a material to its rate of change of temperature. Essentially, it is a value that accounts for any property of the material that could change the way it conducts heat.[1] Heat spontaneously flows along a temperature gradient (i.e. from a hotter body to a colder body). For example, heat is conducted from the hotplate of an electric stove to the bottom of a saucepan in contact with it. In the absence of an opposing external driving energy source, within a body or between bodies, temperature differences decay over time, and thermal equilibrium is approached, temperature becoming more uniform.\n\nEvery process involving heat transfer takes place by only three methods:\n\nA region with greater thermal energy (heat) corresponds with greater molecular agitation. Thus when a hot object touches a cooler surface, the highly agitated molecules from the hot object bump the calm molecules of the cooler surface, transferring the microscopic kinetic energy and causing the colder part or object to heat up. Mathematically, thermal conduction works just like diffusion. As temperature difference goes up, the distance traveled gets shorter or the area goes up thermal conduction increases:\n\nConduction is the main mode of heat transfer for solid materials because the strong inter-molecular forces allow the vibrations of particles to be easily transmitted, in comparison to liquids and gases. Liquids have weaker inter-molecular forces and more space between the particles, which makes the vibrations of particles harder to transmit. Gases have even more space, and therefore infrequent particle collisions. This makes liquids and gases poor conductors of heat.[1]\n\nThermal contact conductance is the study of heat conduction between solid bodies in contact. A temperature drop is often observed at the interface between the two surfaces in contact. This phenomenon is said to be a result of a thermal contact resistance existing between the contacting surfaces. Interfacial thermal resistance is a measure of an interface's resistance to thermal flow. This thermal resistance differs from contact resistance, as it exists even at atomically perfect interfaces. Understanding the thermal resistance at the interface between two materials is of primary significance in the study of its thermal properties. Interfaces often contribute significantly to the observed properties of the materials.\n\nThe inter-molecular transfer of energy could be primarily by elastic impact, as in fluids, or by free-electron diffusion, as in metals, or phonon vibration, as in insulators. In insulators, the heat flux is carried almost entirely by phonon vibrations.\n\nMetals (e.g., copper, platinum, gold, etc.) are usually good conductors of thermal energy. This is due to the way that metals bond chemically: metallic bonds (as opposed to covalent or ionic bonds) have free-moving electrons that transfer thermal energy rapidly through the metal. The electron fluid of a conductive metallic solid conducts most of the heat flux through the solid. Phonon flux is still present but carries less of the energy. Electrons also conduct electric current through conductive solids, and the thermal and electrical conductivities of most metals have about the same ratio.[clarification needed] A good electrical conductor, such as copper, also conducts heat well. Thermoelectricity is caused by the interaction of heat flux and electric current. Heat conduction within a solid is directly analogous to diffusion of particles within a fluid, in the situation where there are no fluid currents.\n\nIn gases, heat transfer occurs through collisions of gas molecules with one another. In the absence of convection, which relates to a moving fluid or gas phase, thermal conduction through a gas phase is highly dependent on the composition and pressure of this phase, and in particular, the mean free path of gas molecules relative to the size of the gas gap, as given by the Knudsen number \n  \n    \n      \n        \n          K\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle K_{n}}\n  \n.[3]\n\nTo quantify the ease with which a particular medium conducts, engineers employ the thermal conductivity, also known as the conductivity constant or conduction coefficient, k. In thermal conductivity, k is defined as \"the quantity of heat, Q, transmitted in time (t) through a thickness (L), in a direction normal to a surface of area (A), due to a temperature difference (ΔT) [...]\". Thermal conductivity is a material property that is primarily dependent on the medium's phase, temperature, density, and molecular bonding. Thermal effusivity is a quantity derived from conductivity, which is a measure of its ability to exchange thermal energy with its surroundings.\n\nSteady-state conduction is the form of conduction that happens when the temperature difference(s) driving the conduction are constant, so that (after an equilibration time), the spatial distribution of temperatures (temperature field) in the conducting object does not change any further. Thus, all partial derivatives of temperature concerning space may either be zero or have nonzero values, but all derivatives of temperature at any point concerning time are uniformly zero. In steady-state conduction, the amount of heat entering any region of an object is equal to the amount of heat coming out (if this were not so, the temperature would be rising or falling, as thermal energy was tapped or trapped in a region).\n\nFor example, a bar may be cold at one end and hot at the other, but after a state of steady-state conduction is reached, the spatial gradient of temperatures along the bar does not change any further, as time proceeds. Instead, the temperature remains constant at any given cross-section of the rod normal to the direction of heat transfer, and this temperature varies linearly in space in the case where there is no heat generation in the rod.[4]\n\nIn steady-state conduction, all the laws of direct current electrical conduction can be applied to \"heat currents\". In such cases, it is possible to take \"thermal resistances\" as the analog to electrical resistances. In such cases, temperature plays the role of voltage, and heat transferred per unit time (heat power) is the analog of electric current. Steady-state systems can be modeled by networks of such thermal resistances in series and parallel, in exact analogy to electrical networks of resistors. See purely resistive thermal circuits for an example of such a network.\n\nDuring any period in which temperatures changes in time at any place within an object, the mode of thermal energy flow is termed transient conduction. Another term is \"non-steady-state\" conduction, referring to the time-dependence of temperature fields in an object. Non-steady-state situations appear after an imposed change in temperature at a boundary of an object. They may also occur with temperature changes inside an object, as a result of a new source or sink of heat suddenly introduced within an object, causing temperatures near the source or sink to change in time.\n\nWhen a new perturbation of temperature of this type happens, temperatures within the system change in time toward a new equilibrium with the new conditions, provided that these do not change. After equilibrium, heat flow into the system once again equals the heat flow out, and temperatures at each point inside the system no longer change. Once this happens, transient conduction is ended, although steady-state conduction may continue if heat flow continues.\n\nIf changes in external temperatures or internal heat generation changes are too rapid for the equilibrium of temperatures in space to take place, then the system never reaches a state of unchanging temperature distribution in time, and the system remains in a transient state.\n\nAn example of a new source of heat \"turning on\" within an object, causing transient conduction, is an engine starting in an automobile. In this case, the transient thermal conduction phase for the entire machine is over, and the steady-state phase appears, as soon as the engine reaches steady-state operating temperature. In this state of steady-state equilibrium, temperatures vary greatly from the engine cylinders to other parts of the automobile, but at no point in space within the automobile does temperature increase or decrease. After establishing this state, the transient conduction phase of heat transfer is over.\n\nNew external conditions also cause this process: for example, the copper bar in the example steady-state conduction experiences transient conduction as soon as one end is subjected to a different temperature from the other. Over time, the field of temperatures inside the bar reaches a new steady-state, in which a constant temperature gradient along the bar is finally set up, and this gradient then stays constant in time. Typically, such a new steady-state gradient is approached exponentially with time after a new temperature-or-heat source or sink, has been introduced. When a \"transient conduction\" phase is over, heat flow may continue at high power, so long as temperatures do not change.\n\nAn example of transient conduction that does not end with steady-state conduction, but rather no conduction, occurs when a hot copper ball is dropped into oil at a low temperature. Here, the temperature field within the object begins to change as a function of time, as the heat is removed from the metal, and the interest lies in analyzing this spatial change of temperature within the object over time until all gradients disappear entirely (the ball has reached the same temperature as the oil). Mathematically, this condition is also approached exponentially; in theory, it takes infinite time, but in practice, it is over, for all intents and purposes, in a much shorter period. At the end of this process with no heat sink but the internal parts of the ball (which are finite), there is no steady-state heat conduction to reach. Such a state never occurs in this situation, but rather the end of the process is when there is no heat conduction at all.\n\nThe analysis of non-steady-state conduction systems is more complex than that of steady-state systems. If the conducting body has a simple shape, then exact analytical mathematical expressions and solutions may be possible (see heat equation for the analytical approach).[5] However, most often, because of complicated shapes with varying thermal conductivities within the shape (i.e., most complex objects, mechanisms or machines in engineering) often the application of approximate theories is required, and/or numerical analysis by computer. One popular graphical method involves the use of Heisler Charts.\n\nOccasionally, transient conduction problems may be considerably simplified if regions of the object being heated or cooled can be identified, for which thermal conductivity is very much greater than that for heat paths leading into the region. In this case, the region with high conductivity can often be treated in the lumped capacitance model, as a \"lump\" of material with a simple thermal capacitance consisting of its aggregate heat capacity. Such regions warm or cool, but show no significant temperature variation across their extent, during the process (as compared to the rest of the system). This is due to their far higher conductance. During transient conduction, therefore, the temperature across their conductive regions changes uniformly in space, and as a simple exponential in time. An example of such systems is those that follow Newton's law of cooling during transient cooling (or the reverse during heating). The equivalent thermal circuit consists of a simple capacitor in series with a resistor. In such cases, the remainder of the system with a high thermal resistance (comparatively low conductivity) plays the role of the resistor in the circuit.\n\nThe theory of relativistic heat conduction is a model that is compatible with the theory of special relativity. For most of the last century, it was recognized that the Fourier equation is in contradiction with the theory of relativity because it admits an infinite speed of propagation of heat signals. For example, according to the Fourier equation, a pulse of heat at the origin would be felt at infinity instantaneously. The speed of information propagation is faster than the speed of light in vacuum, which is physically inadmissible within the framework of relativity.\n\nSecond sound is a quantum mechanical phenomenon in which heat transfer occurs by wave-like motion, rather than by the more usual mechanism of diffusion. Heat takes the place of pressure in normal sound waves. This leads to a very high thermal conductivity. It is known as \"second sound\" because the wave motion of heat is similar to the propagation of sound in air.this is called Quantum conduction\n\nThe law of heat conduction, also known as Fourier's law (compare Fourier's heat equation), states that the rate of heat transfer through a material is proportional to the negative gradient in the temperature and to the area, at right angles to that gradient, through which the heat flows. We can state this law in two equivalent forms: the integral form, in which we look at the amount of energy flowing into or out of a body as a whole, and the differential form, in which we look at the flow rates or fluxes of energy locally.\n\nNewton's law of cooling is a discrete analogue of Fourier's law, while Ohm's law is the electrical analogue of Fourier's law and Fick's laws of diffusion is its chemical analogue.\n\nThe differential form of Fourier's law of thermal conduction shows that the local heat flux density \n  \n    \n      \n        \n          q\n        \n      \n    \n    {\\displaystyle \\mathbf {q} }\n  \n is equal to the product of thermal conductivity \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n and the negative local temperature gradient \n  \n    \n      \n        −\n        ∇\n        T\n      \n    \n    {\\displaystyle -\\nabla T}\n  \n. The heat flux density is the amount of energy that flows through a unit area per unit time.\n\n  \n    \n      \n        \n          q\n        \n        =\n        −\n        k\n        ∇\n        T\n        ,\n      \n    \n    {\\displaystyle \\mathbf {q} =-k\\nabla T,}\n  \n\nwhere (including the SI units)\n\nThe thermal conductivity \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n is often treated as a constant, though this is not always true. While the thermal conductivity of a material generally varies with temperature, the variation can be small over a significant range of temperatures for some common materials. In anisotropic materials, the thermal conductivity typically varies with orientation; in this case \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n is represented by a second-order tensor. In non-uniform materials, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n varies with spatial location.\n\nFor many simple applications, Fourier's law is used in its one-dimensional form, for example, in the x direction:\n\n  \n    \n      \n        \n          q\n          \n            x\n          \n        \n        =\n        −\n        k\n        \n          \n            \n              d\n              T\n            \n            \n              d\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle q_{x}=-k{\\frac {dT}{dx}}.}\n\nIn an isotropic medium, Fourier's law leads to the heat equation\n\n  \n    \n      \n        \n          \n            \n              ∂\n              T\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        α\n        \n          (\n          \n            \n              \n                \n                  \n                    ∂\n                    \n                      2\n                    \n                  \n                  T\n                \n                \n                  ∂\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n            \n            +\n            \n              \n                \n                  \n                    ∂\n                    \n                      2\n                    \n                  \n                  T\n                \n                \n                  ∂\n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n            \n            +\n            \n              \n                \n                  \n                    ∂\n                    \n                      2\n                    \n                  \n                  T\n                \n                \n                  ∂\n                  \n                    z\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\partial T}{\\partial t}}=\\alpha \\left({\\frac {\\partial ^{2}T}{\\partial x^{2}}}+{\\frac {\\partial ^{2}T}{\\partial y^{2}}}+{\\frac {\\partial ^{2}T}{\\partial z^{2}}}\\right)}\n  \n\nwith a fundamental solution famously known as the heat kernel.\n\nBy integrating the differential form over the material's total surface \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, we arrive at the integral form of Fourier's law:\n\nThe above differential equation, when integrated for a homogeneous material of 1-D geometry between two endpoints at constant temperature, gives the heat flow rate as\n\n  \n    \n      \n        Q\n        =\n        −\n        k\n        \n          \n            \n              A\n              Δ\n              t\n            \n            L\n          \n        \n        Δ\n        T\n        ,\n      \n    \n    {\\displaystyle Q=-k{\\frac {A\\Delta t}{L}}\\Delta T,}\n  \n\nwhere\n\nOne can define the (macroscopic) thermal resistance of the 1-D homogeneous material:\n\n  \n    \n      \n        R\n        =\n        \n          \n            1\n            k\n          \n        \n        \n          \n            L\n            A\n          \n        \n      \n    \n    {\\displaystyle R={\\frac {1}{k}}{\\frac {L}{A}}}\n\nWith a simple 1-D steady heat conduction equation which is analogous to Ohm's law for a simple electric resistance:\n\n  \n    \n      \n        Δ\n        T\n        =\n        R\n        \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta T=R\\,{\\dot {Q}}}\n\nThis law forms the basis for the derivation of the heat equation.\n\nWriting\n\n  \n    \n      \n        U\n        =\n        \n          \n            k\n            \n              Δ\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle U={\\frac {k}{\\Delta x}},}\n  \n\nwhere U is the conductance, in W/(m2 K).\n\nFourier's law can also be stated as:\n\n  \n    \n      \n        \n          \n            \n              Δ\n              Q\n            \n            \n              Δ\n              t\n            \n          \n        \n        =\n        U\n        A\n        \n        (\n        −\n        Δ\n        T\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {\\Delta Q}{\\Delta t}}=UA\\,(-\\Delta T).}\n\nThe reciprocal of conductance is resistance, \n  \n    \n      \n        \n          \n            \n          \n        \n        R\n      \n    \n    {\\displaystyle {\\big .}R}\n  \n is given by:\n\n  \n    \n      \n        R\n        =\n        \n          \n            1\n            U\n          \n        \n        =\n        \n          \n            \n              Δ\n              x\n            \n            k\n          \n        \n        =\n        \n          \n            \n              A\n              \n              (\n              −\n              Δ\n              T\n              )\n            \n            \n              \n                Δ\n                Q\n              \n              \n                Δ\n                t\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle R={\\frac {1}{U}}={\\frac {\\Delta x}{k}}={\\frac {A\\,(-\\Delta T)}{\\frac {\\Delta Q}{\\Delta t}}}.}\n\nResistance is additive when several conducting layers lie between the hot and cool regions, because A and Q are the same for all layers. In a multilayer partition, the total conductance is related to the conductance of its layers by:\n\n  \n    \n      \n        R\n        =\n        \n          R\n          \n            1\n          \n        \n        +\n        \n          R\n          \n            2\n          \n        \n        +\n        \n          R\n          \n            3\n          \n        \n        +\n        ⋯\n      \n    \n    {\\displaystyle R=R_{1}+R_{2}+R_{3}+\\cdots }\n  \n or equivalently \n  \n    \n      \n        \n          \n            1\n            U\n          \n        \n        =\n        \n          \n            1\n            \n              U\n              \n                1\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              U\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              U\n              \n                3\n              \n            \n          \n        \n        +\n        ⋯\n      \n    \n    {\\displaystyle {\\frac {1}{U}}={\\frac {1}{U_{1}}}+{\\frac {1}{U_{2}}}+{\\frac {1}{U_{3}}}+\\cdots }\n\nSo, when dealing with a multilayer partition, the following formula is usually used:\n\n  \n    \n      \n        \n          \n            \n              Δ\n              Q\n            \n            \n              Δ\n              t\n            \n          \n        \n        =\n        \n          \n            \n              A\n              \n              (\n              −\n              Δ\n              T\n              )\n            \n            \n              \n                \n                  \n                    Δ\n                    \n                      x\n                      \n                        1\n                      \n                    \n                  \n                  \n                    k\n                    \n                      1\n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    Δ\n                    \n                      x\n                      \n                        2\n                      \n                    \n                  \n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    Δ\n                    \n                      x\n                      \n                        3\n                      \n                    \n                  \n                  \n                    k\n                    \n                      3\n                    \n                  \n                \n              \n              +\n              ⋯\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\Delta Q}{\\Delta t}}={\\frac {A\\,(-\\Delta T)}{{\\frac {\\Delta x_{1}}{k_{1}}}+{\\frac {\\Delta x_{2}}{k_{2}}}+{\\frac {\\Delta x_{3}}{k_{3}}}+\\cdots }}.}\n\nFor heat conduction from one fluid to another through a barrier, it is sometimes important to consider the conductance of the thin film of fluid that remains stationary next to the barrier. This thin film of fluid is difficult to quantify because its characteristics depend upon complex conditions of turbulence and viscosity—but when dealing with thin high-conductance barriers it can sometimes be quite significant.\n\nThe previous conductance equations, written in terms of extensive properties, can be reformulated in terms of intensive properties.  Ideally, the formulae for conductance should produce a quantity with dimensions independent of distance, like Ohm's law for electrical resistance, \n  \n    \n      \n        R\n        =\n        V\n        \n          /\n        \n        I\n        \n        \n      \n    \n    {\\displaystyle R=V/I\\,\\!}\n  \n, and conductance, \n  \n    \n      \n        G\n        =\n        I\n        \n          /\n        \n        V\n        \n        \n      \n    \n    {\\displaystyle G=I/V\\,\\!}\n  \n.\n\nFrom the electrical formula: \n  \n    \n      \n        R\n        =\n        ρ\n        x\n        \n          /\n        \n        A\n      \n    \n    {\\displaystyle R=\\rho x/A}\n  \n, where ρ is resistivity, x is length, and A is cross-sectional area, we have \n  \n    \n      \n        G\n        =\n        k\n        A\n        \n          /\n        \n        x\n        \n        \n      \n    \n    {\\displaystyle G=kA/x\\,\\!}\n  \n, where G is conductance, k is conductivity, x is length, and A is cross-sectional area.\n\nFor heat,\n\n  \n    \n      \n        U\n        =\n        \n          \n            \n              k\n              A\n            \n            \n              Δ\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle U={\\frac {kA}{\\Delta x}},}\n  \n\nwhere U is the conductance.\n\nFourier's law can also be stated as:\n\n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        =\n        U\n        \n        Δ\n        T\n        ,\n      \n    \n    {\\displaystyle {\\dot {Q}}=U\\,\\Delta T,}\n  \n\nanalogous to Ohm's law, \n  \n    \n      \n        I\n        =\n        V\n        \n          /\n        \n        R\n      \n    \n    {\\displaystyle I=V/R}\n  \n or \n  \n    \n      \n        I\n        =\n        V\n        G\n        .\n      \n    \n    {\\displaystyle I=VG.}\n\nThe reciprocal of conductance is resistance, R, given by:\n\n  \n    \n      \n        R\n        =\n        \n          \n            \n              Δ\n              T\n            \n            \n              \n                Q\n                ˙\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle R={\\frac {\\Delta T}{\\dot {Q}}},}\n  \n\nanalogous to Ohm's law, \n  \n    \n      \n        R\n        =\n        V\n        \n          /\n        \n        I\n        .\n      \n    \n    {\\displaystyle R=V/I.}\n\nThe rules for combining resistances and conductances (in series and parallel) are the same for both heat flow and electric current.\n\nConduction through cylindrical shells (e.g. pipes) can be calculated from the internal radius, \n  \n    \n      \n        \n          r\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle r_{1}}\n  \n, the external radius, \n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle r_{2}}\n  \n, the length, \n  \n    \n      \n        ℓ\n      \n    \n    {\\displaystyle \\ell }\n  \n, and the temperature difference between the inner and outer wall, \n  \n    \n      \n        \n          T\n          \n            2\n          \n        \n        −\n        \n          T\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle T_{2}-T_{1}}\n  \n.\n\nThe surface area of the cylinder is \n  \n    \n      \n        \n          A\n          \n            r\n          \n        \n        =\n        2\n        π\n        r\n        ℓ\n      \n    \n    {\\displaystyle A_{r}=2\\pi r\\ell }\n\nWhen Fourier's equation is applied:\n\n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        =\n        −\n        k\n        \n          A\n          \n            r\n          \n        \n        \n          \n            \n              d\n              T\n            \n            \n              d\n              r\n            \n          \n        \n        =\n        −\n        2\n        k\n        π\n        r\n        ℓ\n        \n          \n            \n              d\n              T\n            \n            \n              d\n              r\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {Q}}=-kA_{r}{\\frac {dT}{dr}}=-2k\\pi r\\ell {\\frac {dT}{dr}}}\n  \n\nand rearranged:\n\n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        \n          ∫\n          \n            \n              r\n              \n                1\n              \n            \n          \n          \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            r\n          \n        \n        \n        d\n        r\n        =\n        −\n        2\n        k\n        π\n        ℓ\n        \n          ∫\n          \n            \n              T\n              \n                1\n              \n            \n          \n          \n            \n              T\n              \n                2\n              \n            \n          \n        \n        d\n        T\n      \n    \n    {\\displaystyle {\\dot {Q}}\\int _{r_{1}}^{r_{2}}{\\frac {1}{r}}\\,dr=-2k\\pi \\ell \\int _{T_{1}}^{T_{2}}dT}\n  \n\nthen the rate of heat transfer is:\n\n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        =\n        2\n        k\n        π\n        ℓ\n        \n          \n            \n              \n                T\n                \n                  1\n                \n              \n              −\n              \n                T\n                \n                  2\n                \n              \n            \n            \n              ln\n              ⁡\n              (\n              \n                r\n                \n                  2\n                \n              \n              \n                /\n              \n              \n                r\n                \n                  1\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {Q}}=2k\\pi \\ell {\\frac {T_{1}-T_{2}}{\\ln(r_{2}/r_{1})}}}\n  \n\nthe thermal resistance is:\n\n  \n    \n      \n        \n          R\n          \n            c\n          \n        \n        =\n        \n          \n            \n              Δ\n              T\n            \n            \n              \n                Q\n                ˙\n              \n            \n          \n        \n        =\n        \n          \n            \n              ln\n              ⁡\n              (\n              \n                r\n                \n                  2\n                \n              \n              \n                /\n              \n              \n                r\n                \n                  1\n                \n              \n              )\n            \n            \n              2\n              π\n              k\n              ℓ\n            \n          \n        \n      \n    \n    {\\displaystyle R_{c}={\\frac {\\Delta T}{\\dot {Q}}}={\\frac {\\ln(r_{2}/r_{1})}{2\\pi k\\ell }}}\n  \n\nand \n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        =\n        2\n        π\n        k\n        ℓ\n        \n          r\n          \n            m\n          \n        \n        \n          \n            \n              \n                T\n                \n                  1\n                \n              \n              −\n              \n                T\n                \n                  2\n                \n              \n            \n            \n              \n                r\n                \n                  2\n                \n              \n              −\n              \n                r\n                \n                  1\n                \n              \n            \n          \n        \n      \n    \n    {\\textstyle {\\dot {Q}}=2\\pi k\\ell r_{m}{\\frac {T_{1}-T_{2}}{r_{2}-r_{1}}}}\n  \n, where \n  \n    \n      \n        \n          r\n          \n            m\n          \n        \n        =\n        \n          \n            \n              \n                r\n                \n                  2\n                \n              \n              −\n              \n                r\n                \n                  1\n                \n              \n            \n            \n              ln\n              ⁡\n              (\n              \n                r\n                \n                  2\n                \n              \n              \n                /\n              \n              \n                r\n                \n                  1\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\textstyle r_{m}={\\frac {r_{2}-r_{1}}{\\ln(r_{2}/r_{1})}}}\n  \n. It is important to note that this is the log-mean radius.\n\nThe conduction through a spherical shell with internal radius, \n  \n    \n      \n        \n          r\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle r_{1}}\n  \n, and external radius, \n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle r_{2}}\n  \n, can be calculated in a similar manner as for a cylindrical shell.\n\nThe surface area of the sphere is: \n  \n    \n      \n        A\n        =\n        4\n        π\n        \n          r\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle A=4\\pi r^{2}.}\n\nSolving in a similar manner as for a cylindrical shell (see above) produces:\n\n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n        =\n        4\n        k\n        π\n        \n          \n            \n              \n                T\n                \n                  1\n                \n              \n              −\n              \n                T\n                \n                  2\n                \n              \n            \n            \n              1\n              \n                /\n              \n              \n                \n                  r\n                  \n                    1\n                  \n                \n              \n              −\n              1\n              \n                /\n              \n              \n                \n                  r\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        =\n        4\n        k\n        π\n        \n          \n            \n              (\n              \n                T\n                \n                  1\n                \n              \n              −\n              \n                T\n                \n                  2\n                \n              \n              )\n              \n                r\n                \n                  1\n                \n              \n              \n                r\n                \n                  2\n                \n              \n            \n            \n              \n                r\n                \n                  2\n                \n              \n              −\n              \n                r\n                \n                  1\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {Q}}=4k\\pi {\\frac {T_{1}-T_{2}}{1/{r_{1}}-1/{r_{2}}}}=4k\\pi {\\frac {(T_{1}-T_{2})r_{1}r_{2}}{r_{2}-r_{1}}}}\n\nThe heat transfer at an interface is considered a transient heat flow. To analyze this problem, the Biot number is important to understand how the system behaves. The Biot number is determined by:\n\n  \n    \n      \n        \n          Bi\n        \n        =\n        \n          \n            \n              h\n              L\n            \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\text{Bi}}={\\frac {hL}{k}}}\n  \n\nThe heat transfer coefficient \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n, is introduced in this formula, and is measured in \n  \n    \n      \n        \n          \n            J\n            \n              \n                m\n                \n                  2\n                \n              \n              s\n              K\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {\\frac {J}{m^{2}sK}} }\n  \n. If the system has a Biot number of less than 0.1, the material behaves according to Newtonian cooling, i.e. with negligible temperature gradient within the body.[6] If the Biot number is greater than 0.1, the system behaves as a series solution. however, there is a noticeable temperature gradient within the material, and a series solution is required to describe the temperature profile. The cooling equation given is:\n\n  \n    \n      \n        q\n        =\n        −\n        h\n        \n        Δ\n        T\n        ,\n      \n    \n    {\\displaystyle q=-h\\,\\Delta T,}\n  \n\nThis leads to the dimensionless form of the temperature profile as a function of time:\n\n  \n    \n      \n        \n          \n            \n              T\n              −\n              \n                T\n                \n                  f\n                \n              \n            \n            \n              \n                T\n                \n                  i\n                \n              \n              −\n              \n                T\n                \n                  f\n                \n              \n            \n          \n        \n        =\n        exp\n        ⁡\n        \n          (\n          \n            \n              \n                −\n                h\n                A\n                t\n              \n              \n                ρ\n                \n                  C\n                  \n                    p\n                  \n                \n                V\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {T-T_{f}}{T_{i}-T_{f}}}=\\exp \\left({\\frac {-hAt}{\\rho C_{p}V}}\\right).}\n  \n\nThis equation shows that the temperature decreases exponentially over time, with the rate governed by the properties of the material and the heat transfer coefficient.[7]\nThe heat transfer coefficient, h, is measured in \n  \n    \n      \n        \n          \n            W\n            \n              \n                m\n                \n                  2\n                \n              \n              K\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {\\frac {W}{m^{2}K}} }\n  \n, and represents the transfer of heat at an interface between two materials. This value is different at every interface and is an important concept in understanding heat flow at an interface.\n\nThe series solution can be analyzed with a nomogram. A nomogram has a relative temperature as the y coordinate and the Fourier number, which is calculated by\n\n  \n    \n      \n        \n          Fo\n        \n        =\n        \n          \n            \n              α\n              t\n            \n            \n              L\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\text{Fo}}={\\frac {\\alpha t}{L^{2}}}.}\n\nThe Biot number increases as the Fourier number decreases. There are five steps to determine a temperature profile in terms of time.\n\nSplat cooling is a method for quenching small droplets of molten materials by rapid contact with a cold surface. The particles undergo a characteristic cooling process, with the heat profile at \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n for initial temperature as the maximum at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n and \n  \n    \n      \n        T\n        =\n        0\n      \n    \n    {\\displaystyle T=0}\n  \n at \n  \n    \n      \n        x\n        =\n        −\n        ∞\n      \n    \n    {\\displaystyle x=-\\infty }\n  \n and \n  \n    \n      \n        x\n        =\n        ∞\n      \n    \n    {\\displaystyle x=\\infty }\n  \n, and the heat profile at \n  \n    \n      \n        t\n        =\n        ∞\n      \n    \n    {\\displaystyle t=\\infty }\n  \n for \n  \n    \n      \n        −\n        ∞\n        ≤\n        x\n        ≤\n        ∞\n      \n    \n    {\\displaystyle -\\infty \\leq x\\leq \\infty }\n  \n as the boundary conditions. Splat cooling rapidly ends in a steady state temperature, and is similar in form to the Gaussian diffusion equation. The temperature profile, with respect to the position and time of this type of cooling, varies with:\n\n  \n    \n      \n        T\n        (\n        x\n        ,\n        t\n        )\n        −\n        \n          T\n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                T\n                \n                  i\n                \n              \n              Δ\n              X\n            \n            \n              2\n              \n                \n                  π\n                  α\n                  t\n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n                \n                  4\n                  α\n                  t\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle T(x,t)-T_{i}={\\frac {T_{i}\\Delta X}{2{\\sqrt {\\pi \\alpha t}}}}\\exp \\left(-{\\frac {x^{2}}{4\\alpha t}}\\right)}\n\nSplat cooling is a fundamental concept that has been adapted for practical use in the form of thermal spraying. The thermal diffusivity coefficient, represented as \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n, can be written as \n  \n    \n      \n        α\n        =\n        \n          \n            k\n            \n              ρ\n              \n                C\n                \n                  p\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {k}{\\rho C_{p}}}}\n  \n. This varies according to the material.[8][9]\n\nMetal quenching is a transient heat transfer process in terms of the time temperature transformation (TTT). It is possible to manipulate the cooling process to adjust the phase of a suitable material. For example, appropriate quenching of steel can convert a desirable proportion of its content of austenite to martensite, creating a very hard and strong product. To achieve this, it is necessary to quench at the \"nose\" (or eutectic) of the TTT diagram. Since materials differ in their Biot numbers, the time it takes for the material to quench, or the Fourier number, varies in practice.[10] In steel, the quenching temperature range is generally from 600 °C to 200 °C. To control the quenching time and to select suitable quenching media, it is necessary to determine the Fourier number from the desired quenching time, the relative temperature drop, and the relevant Biot number. Usually, the correct figures are read from a standard nomogram.[citation needed] By calculating the heat transfer coefficient from this Biot number, one can find a liquid medium suitable for the application.[11]\n\nOne statement of the so-called zeroth law of thermodynamics is directly focused on the idea of conduction of heat. Bailyn (1994) writes that \"the zeroth law may be stated: All diathermal walls are equivalent\".[12]\n\nA diathermal wall is a physical connection between two bodies that allows the passage of heat between them. Bailyn is referring to diathermal walls that exclusively connect two bodies, especially conductive walls.\n\nThis statement of the \"zeroth law\" belongs to an idealized theoretical discourse, and actual physical walls may have peculiarities that do not conform to its generality.\n\nFor example, the material of the wall must not undergo a phase transition, such as evaporation or fusion, at the temperature at which it must conduct heat. But when only thermal equilibrium is considered and time is not urgent, so that the conductivity of the material does not matter too much, one suitable heat conductor is as good as another. Conversely, another aspect of the zeroth law is that, subject again to suitable restrictions, a given diathermal wall is indifferent to the nature of the heat bath to which it is connected. For example, the glass bulb of a thermometer acts as a diathermal wall whether exposed to a gas or a liquid, provided that they do not corrode or melt it.\n\nThese differences are among the defining characteristics of heat transfer. In a sense, they are symmetries of heat transfer.\n\nThermal conduction property of any gas under standard conditions of pressure and temperature is a fixed quantity. This property of a known reference gas or known reference gas mixtures can, therefore, be used for certain sensory applications, such as the thermal conductivity analyzer.\n\nThe working of this instrument is by principle based on the Wheatstone bridge containing four filaments whose resistances are matched. Whenever a certain gas is passed over such network of filaments, their resistance changes due to the altered thermal conductivity of the filaments and thereby changing the net voltage output from the Wheatstone Bridge. This voltage output will be correlated with the database to identify the gas sample.\n\nThe principle of thermal conductivity of gases can also be used to measure the concentration of a gas in a binary mixture of gases.\n\nWorking: if the same gas is present around all the Wheatstone bridge filaments, then the same temperature is maintained in all the filaments and hence same resistances are also maintained; resulting in a balanced Wheatstone bridge. However, If the dissimilar gas sample (or gas mixture) is passed over one set of two filaments and the reference gas on the other set of two filaments, then the Wheatstone bridge becomes unbalanced. And the resulting net voltage output of the circuit will be correlated with the database to identify the constituents of the sample gas.\n\nUsing this technique many unknown gas samples can be identified by comparing their thermal conductivity with other reference gas of known thermal conductivity. The most commonly used reference gas is nitrogen; as the thermal conductivity of most common gases (except hydrogen and helium) are similar to that of nitrogen.",
        pageTitle: "Thermal conduction",
    },
    {
        title: "Gall's law",
        link: "https://en.wikipedia.org/wiki/Gall%27s_law",
        content:
            "John Gall (September 18, 1925 – December 15, 2014) was an American author, scholar, and pediatrician.[1][2] Gall is known for his 1975 book General systemantics: an essay on how systems work, and especially how they fail..., a critique of systems theory. One of the statements from this book has become known as Gall's law.[2]\n\nGall started his studies in St. John's College in Annapolis, Maryland. He received further medical training at George Washington University Medical School in Washington, and Yale College. Eventually, in the early 1960s he took his pediatric training at the Mayo Clinic in Rochester, Minnesota.[3]\n\nIn the 1960s Gall started as a practicing pediatrician in Ann Arbor, Michigan, and became part of the faculty of the University of Michigan. In 2001 he retired after more than 40 years of private practice. In the first decades of his practice he had also \"conducted weekly seminars in Parenting Strategies for parents, prospective parents, medical students, nursing students, and other health care practitioners.\"[4] Until 2001 he held the position of clinical associate professor of pediatrics at the University of Michigan. Beginning in 1958 he was a Fellow of the American Academy of Pediatrics.\n\nAfter he retired, Gall and his wife Carol A. Gall moved to Walker, Minnesota, where he continued writing and published seven more titles.\nHe died on December 15, 2014, from natural causes.[2]\n\nGall's main research interest was the behavioral and developmental problems of children, on which subject he published several scientific papers and books. As a sideline he conducted more general research on the question of what makes systems work and fail.[5] He collected and analyzed all kinds of examples of systems-failures, and generalized problems and pitfalls into a series of \"Laws of Systems\".[6]\n\nIn 2002 Gall also published a historical novel on Hatshepsut, queen of ancient Egypt in the Eighteenth Dynasty. This interest arose from a trip he made to Egypt in 1969.\n\nIn 1975 he published his systems research under the title General systemantics, republished two years later as Systemantics: How Systems Work and Especially How They Fail by Quadrangle, The New York Times Book Company. This work has been translated into Spanish, German, Hebrew, and Japanese.\n\nIn 1986 the second edition was published with the title Systemantics: The Underground Text of Systems Lore., which was almost twice the size of the first edition.\n\nIn 2002 he published a third edition under the title The Systems Bible. This work inspired many authors in the systems movement, such as scientists Mario Bunge (1979), Paul Watzlawick (1990) and Russell L. Ackoff (1999), and systems designers Ken Orr (1981) and Grady Booch (1991).[7]\n\nGall's Law is a rule of thumb for systems design from Gall's book Systemantics: How Systems Really Work and How They Fail. It states:\n\nA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.[8]\n\nThis law is essentially an argument in favour of underspecification: it can be used to explain the success of systems like the World Wide Web and Blogosphere, which grew from simple to complex systems incrementally, and the failure of systems like CORBA, which began with complex specifications. Gall's Law has strong affinities to the practice of agile software development.[9]\n\nAlthough dubbed Gall's Law by some, the phrase is not labeled as such in the original work. The work cites Murphy's Law and the Peter Principle, and includes similar sayings.\n\nAlthough the quote may seem to validate the merits of simple systems, it is preceded by the qualifier, \"A simple system may or may not work.\" (p. 70).[full citation needed] This philosophy can also be attributed to extreme programming, which encourages doing the simplest thing first and adding features later.\n\nOne of the first systems designers to quote Gall's law was Ken Orr in 1981.[10] Notable were the quotations of Gall's Law by Grady Booch since 1991,[11] which were mentioned in multiple sources.[12][13][14]",
        pageTitle: "John Gall (author)",
    },
    {
        title: "Gause's law",
        link: "https://en.wikipedia.org/wiki/Gause%27s_law",
        content:
            "In ecology, the competitive exclusion principle,[1] sometimes referred to as Gause's law,[2] is a proposition that two species which compete for the same limited resource cannot coexist at constant population values. When one species has even the slightest advantage over another, the one with the advantage will dominate in the long term. This leads either to the extinction of the weaker competitor or to an evolutionary or behavioral shift toward a different ecological niche. The principle has been paraphrased in the maxim \"complete competitors cannot coexist\".[1]\n\nThe competitive exclusion principle is classically attributed to Georgy Gause,[3] although he actually never formulated it.[1] The principle is already present in Darwin's theory of natural selection.[2][4]\n\nThroughout its history, the status of the principle has oscillated between a priori ('two species coexisting must have different niches') and experimental truth ('we find that species coexisting do have different niches').[2]\n\nBased on field observations, Joseph Grinnell formulated the principle of competitive exclusion in 1904: \"Two species of approximately the same food habits are not likely to remain long evenly balanced in numbers in the same region. One will crowd out the other\".[5] Georgy Gause formulated the law of competitive exclusion based on laboratory competition experiments using two species of Paramecium, P. aurelia and P. caudatum. The conditions were to add fresh water every day and input a constant flow of food. Although P. caudatum initially dominated, P. aurelia recovered and subsequently drove P. caudatum extinct via exploitative resource competition. However, Gause was able to let the P. caudatum survive by differing the environmental parameters (food, water). Thus, Gause's law is valid only if the ecological factors are constant.\n\nCompetitive exclusion is predicted by mathematical and theoretical models such as the Lotka–Volterra models of competition. However, for poorly understood reasons, competitive exclusion is rarely observed in natural ecosystems, and many biological communities appear to violate Gause's law. The best-known example is the so-called \"paradox of the plankton\".[6] All plankton species live on a very limited number of resources, primarily solar energy and minerals dissolved in the water. According to the competitive exclusion principle, only a small number of plankton species should be able to coexist on these resources. Nevertheless, large numbers of plankton species coexist within small regions of open sea.\n\nSome communities that appear to uphold the competitive exclusion principle are MacArthur's warblers[7] and Darwin's finches,[8] though the latter still overlap ecologically very strongly, being only affected negatively by competition under extreme conditions.[9]\n\nA partial solution to the paradox lies in raising the dimensionality of the system. Spatial heterogeneity, trophic interactions, multiple resource competition, competition-colonization trade-offs, and lag may prevent exclusion (ignoring stochastic extinction over longer time-frames). However, such systems tend to be analytically intractable. In addition, many can, in theory, support an unlimited number of species. A new paradox is created: Most well-known models that allow for stable coexistence allow for unlimited number of species to coexist, yet, in nature, any community contains just a handful of species.\n\nRecent studies addressing some of the assumptions made for the models predicting competitive exclusion have shown these assumptions need to be reconsidered. For example, a slight modification of the assumption of how growth and body size are related leads to a different conclusion, namely that, for a given ecosystem, a certain range of species may coexist while others become outcompeted.[10][11]\n\nOne of the primary ways niche-sharing species can coexist is the competition-colonization trade-off. In other words, species that are better competitors will be specialists, whereas species that are better colonizers are more likely to be generalists. Host-parasite models are effective ways of examining this relationship, using host transfer events. There seem to be two places where the ability to colonize differs in ecologically closely related species. In feather lice, Bush and Clayton[12] provided some verification of this by showing two closely related genera of lice are nearly equal in their ability to colonize new host pigeons once transferred. Harbison[13] continued this line of thought by investigating whether the two genera differed in their ability to transfer. This research focused primarily on determining how colonization occurs and why wing lice are better colonizers than body lice. Vertical transfer is the most common occurrence, between parent and offspring, and is much-studied and well understood. Horizontal transfer is difficult to measure, but in lice seems to occur via phoresis or the \"hitchhiking\" of one species on another. Harbison found that body lice are less adept at phoresis and excel competitively, whereas wing lice excel in colonization.\n\nSupport for a model of competition-colonization trade-off is also found in small mammals related to fire disturbances. In a project focused on the long-term impacts of the 1988 Yellowstone Fires Allen et al.[14] used stable isotopes and spatial mark-recapture data to show that Southern red-backed voles (Clethrionomys gapperi)), a specialist, are excluding deer mice (Peromyscus maniculatus), a generalist, from food resources in old-growth forests. However, after wildfire disturbance deer mice are more effective colonizers, and able to take advantage of the release from competitive pressure from voles. This dynamic of establishes a pattern of ecological succession in these ecosystems, with competitive exclusion from voles shaping the amount and quality of resources deer mice can access.\n\nAn ecological community is the assembly of species which is maintained by ecological (Hutchinson, 1959;[15] Leibold, 1988[16]) and evolutionary process (Weiher and Keddy, 1995;[17] Chase et al., 2003). These two processes play an important role in shaping the existing community and will continue in the future (Tofts et al., 2000; Ackerly, 2003; Reich et al., 2003). In a local community, the potential members are filtered first by environmental factors such as temperature or availability of required resources and then secondly by its ability to co-exist with other resident species.\n\nIn an approach of understanding how two species fit together in a community or how the whole community fits together, The Origin of Species  (Darwin, 1859) proposed that under homogeneous environmental condition struggle for existence is greater between closely related species than distantly related species. He also hypothesized that the functional traits may be conserved across phylogenies. Such strong phylogenetic similarities among closely related species are known as phylogenetic effects (Derrickson et al., 1988.[18])\n\nWith field study and mathematical models, ecologists have pieced together a connection between functional traits similarity between species and its effect on species co-existence. According to competitive-relatedness hypothesis (Cahil et al., 2008[19]) or phylogenetic limiting similarity hypothesis (Violle et al., 2011[20]) interspecific competition[21] is high among the species which have similar functional traits, and which compete for similar resources and habitats. Hence, it causes reduction in the number of closely related species and even distribution of it, known as phylogenetic overdispersion (Webb et al., 2002[22]). The reverse of phylogenetic overdispersion is phylogenetic clustering in which case species with conserved functional traits are expected to co-occur due to  environmental filtering (Weiher et al., 1995; Webb, 2000). In the study performed by Webb et al., 2000, they showed that a small-plots of Borneo forest contained closely related trees together. This suggests that closely related species share features that are favored by the specific environmental factors that differ among plots causing phylogenetic clustering.\n\nFor both phylogenetic patterns (phylogenetic overdispersion and phylogenetic clustering), the baseline assumption is that phylogenetically related species are also ecologically similar (H. Burns et al., 2011[23]). There are no significant number of experiments answering to what degree the closely related species are also similar in niche. Due to that, both phylogenetic patterns are not easy to interpret. It's been shown that phylogenetic overdispersion may also result from convergence of distantly related species (Cavender-Bares et al. 2004;[24] Kraft et al. 2007[25]). In their study [citation needed], they have shown that traits are convergent rather than conserved. While, in another study [citation needed], it's been shown that phylogenetic clustering may also be due to historical or bio-geographical factors which prevents species from leaving their ancestral ranges. So, more phylogenetic experiments are required for understanding the strength of species interaction in community assembly.\n\nEvidence showing that the competitive exclusion principle operates in human groups has been reviewed and integrated into regality theory to explain warlike and peaceful societies.[26] For example, hunter-gatherer groups surrounded by other hunter-gatherer groups in the same ecological niche will fight, at least occasionally, while hunter-gatherer groups surrounded by groups with a different means of subsistence can coexist peacefully.[26]\n\nAnother recent application: in his work Historical Dynamics, Peter Turchin developed the so-called meta-ethnic frontier theory, wherein both rise and eventual fall of empires derives from geographically and or -politically colliding populations.[27] Accordingly, boundary regions, in which the competitive exclusion principle applies, are supposed to be key to human ethnogenesis. Summarizing its more wide-ranging predictions all in one:\n\nAsabiya is a concept from the writings of Ibn Khaldun which Turchin defines as “the capacity for collective action” of a society. The Metaethnic Frontier theory is meant to incorporate asabiya as a key factor in predicting the dynamics of imperial agrarian societies - how they grow, shrink, and begin. Turchin posits that multi-level selection can help us identify the dynamics of asabiya in groups. He follows by noting three ways in which the logic of multi-level selection can be relevant in understanding change in “collective solidarity”: intergroup conflict, population and resource constraints, and ethnic boundaries.\nFor small groups, intergroup conflict can increase asabiya as people need to band together to survive as a group. Conversely (again for small groups), a large population with respect to available resources can decrease asabiya as individuals compete for limited resources. For larger groups, Turchin proposes that ethnic boundries can influence how bands of small groups with moderate ethnic differences can band together against people who are even more “ethnically distanced” - more “Other”. In this process of small groups banding together against peoples more Other than themselves, they can form what Turchin calls a Metaethnic Frontier … Turchin notes that the this ethnic boundry dynamic which generates asabiya in a large group (composed of smaller groups) is weak because as the size of the group grows larger, the central regions are less exposed to intergroup conflict and asabiya decreases, leading to greater internal division. Finally, Turchin notes that all three aforementioned possiblities occur at regions which constitute imperial and metaethnic frontiers (imperial and metaethnic frontiers often coincide, he notes). It is in these regions of intense dynamics where asabiya is forged which are most prone to ethnogenesis.[28]",
        pageTitle: "Competitive exclusion principle",
    },
    {
        title: "Gauss's law",
        link: "https://en.wikipedia.org/wiki/Gauss%27s_law",
        content:
            "In physics (specifically electromagnetism), Gauss's law, also known as Gauss's flux theorem (or sometimes Gauss's theorem), is one of Maxwell's equations. It is an application of the divergence theorem, and it relates the distribution of electric charge to the resulting electric field.\n\nIn its integral form, it states that the flux of the electric field out of an arbitrary closed surface is proportional to the electric charge enclosed by the surface, irrespective of how that charge is distributed. Even though the law alone is insufficient to determine the electric field across a surface enclosing any charge distribution, this may be possible in cases where symmetry mandates uniformity of the field. Where no such symmetry exists, Gauss's law can be used in its differential form, which states that the divergence of the electric field is proportional to the local density of charge.\n\nThe law was first[1] formulated by Joseph-Louis Lagrange in 1773,[2] followed by Carl Friedrich Gauss in 1835,[3] both in the context of the attraction of ellipsoids. It is one of Maxwell's equations, which forms the basis of classical electrodynamics.[note 1] Gauss's law can be used to derive Coulomb's law,[4] and vice versa.\n\nGauss's law has a close mathematical similarity with a number of laws in other areas of physics, such as Gauss's law for magnetism and Gauss's law for gravity. In fact, any inverse-square law can be formulated in a way similar to Gauss's law: for example, Gauss's law itself is essentially equivalent to Coulomb's law, and Gauss's law for gravity is essentially equivalent to Newton's law of gravity, both of which are inverse-square laws.\n\nThe law can be expressed mathematically using vector calculus in integral form and differential form; both are equivalent since they are related by the divergence theorem, also called Gauss's theorem. Each of these forms in turn can also be expressed two ways: In terms of a relation between the electric field E and the total electric charge, or in terms of the electric displacement field D and the free electric charge.[6]\n\nGauss's law can be stated using either the electric field E or the electric displacement field D. This section shows some of the forms with E; the form with D is below, as are other forms with E.\n\nΦ\n          \n            E\n          \n        \n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi _{E}={\\frac {Q}{\\varepsilon _{0}}}}\n\nwhere ΦE is the electric flux through a closed surface S enclosing any volume V, Q is the total charge enclosed within V, and ε0 is the electric constant. The electric flux ΦE is defined as a surface integral of the electric field:\n\nwhere E is the electric field, dA is a vector representing an infinitesimal element of area of the surface,[note 2] and · represents the dot product of two vectors.\n\nIn a curved spacetime, the flux of an electromagnetic field through a closed surface is expressed as\n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the speed of light; \n  \n    \n      \n        \n          F\n          \n            κ\n            0\n          \n        \n      \n    \n    {\\displaystyle F^{\\kappa 0}}\n  \n denotes the time components of the electromagnetic tensor; \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n is the determinant of metric tensor; \n  \n    \n      \n        \n          d\n        \n        \n          S\n          \n            κ\n          \n        \n        =\n        \n          d\n        \n        \n          S\n          \n            i\n            j\n          \n        \n        =\n        \n          d\n        \n        \n          x\n          \n            i\n          \n        \n        \n          d\n        \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {d} S_{\\kappa }=\\mathrm {d} S^{ij}=\\mathrm {d} x^{i}\\mathrm {d} x^{j}}\n  \n is an orthonormal element of the two-dimensional surface surrounding the charge \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n; indices \n  \n    \n      \n        i\n        ,\n        j\n        ,\n        κ\n        =\n        1\n        ,\n        2\n        ,\n        3\n      \n    \n    {\\displaystyle i,j,\\kappa =1,2,3}\n  \n and do not match each other.[8]\n\nSince the flux is defined as an integral of the electric field, this expression of Gauss's law is called the integral form.\n\nIn problems involving conductors set at known potentials, the potential away from them is obtained by solving Laplace's equation, either analytically or numerically. The electric field is then  calculated as the potential's negative gradient. Gauss's law makes it possible to find the distribution of electric charge: The charge in any given region of the conductor can be deduced by integrating the electric field to find the flux through a small box whose sides are perpendicular to the conductor's surface and by noting that the electric field is perpendicular to the surface, and zero inside the conductor.\n\nThe reverse problem, when the electric charge distribution is known and the electric field must be computed, is much more difficult.  The total flux through a given surface gives little information about the electric field, and can go in and out of the surface in arbitrarily complicated patterns.\n\nAn exception is if there is some symmetry in the problem, which mandates that the electric field passes through the surface in a uniform way. Then, if the total flux is known, the field itself can be deduced at every point. Common examples of symmetries which lend themselves to Gauss's law include: cylindrical symmetry, planar symmetry, and spherical symmetry. See the article Gaussian surface for examples where these symmetries are exploited to compute electric fields.\n\nBy the divergence theorem, Gauss's law can alternatively be written in the differential form:\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        =\n        \n          \n            ρ\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} ={\\frac {\\rho }{\\varepsilon _{0}}}}\n\nwhere ∇ · E is the divergence of the electric field, ε0 is the vacuum permittivity and ρ is the total volume charge density (charge per unit volume).\n\nThe integral and differential forms are mathematically equivalent, by the divergence theorem. Here is the argument more specifically.\n\nfor any closed surface S containing charge Q. By the divergence theorem, this equation is equivalent to:\n\n∭\n          \n            V\n          \n        \n        ∇\n        ⋅\n        \n          E\n        \n        \n        \n          d\n        \n        V\n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\iiint _{V}\\nabla \\cdot \\mathbf {E} \\,\\mathrm {d} V={\\frac {Q}{\\varepsilon _{0}}}}\n\nfor any volume V containing charge Q. By the relation between charge and charge density, this equation is equivalent to:\n\n  \n    \n      \n        \n          ∭\n          \n            V\n          \n        \n        ∇\n        ⋅\n        \n          E\n        \n        \n        \n          d\n        \n        V\n        =\n        \n          ∭\n          \n            V\n          \n        \n        \n          \n            ρ\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        \n        \n          d\n        \n        V\n      \n    \n    {\\displaystyle \\iiint _{V}\\nabla \\cdot \\mathbf {E} \\,\\mathrm {d} V=\\iiint _{V}{\\frac {\\rho }{\\varepsilon _{0}}}\\,\\mathrm {d} V}\n  \n\nfor any volume V. In order for this equation to be simultaneously true for every possible volume V, it is necessary (and sufficient) for the integrands to be equal everywhere. Therefore, this equation is equivalent to:\n\n∇\n        ⋅\n        \n          E\n        \n        =\n        \n          \n            ρ\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} ={\\frac {\\rho }{\\varepsilon _{0}}}.}\n  \n\nThus the integral and differential forms are equivalent.\n\nThe electric charge that arises in the simplest textbook situations would be classified as \"free charge\"—for example, the charge which is transferred in static electricity, or the charge on a capacitor plate. In contrast, \"bound charge\" arises only in the context of dielectric (polarizable) materials. (All materials are polarizable to some extent.) When such materials are placed in an external electric field, the electrons remain bound to their respective atoms, but shift a microscopic distance in response to the field, so that they're more on one side of the atom than the other. All these microscopic displacements add up to give a macroscopic net charge distribution, and this constitutes the \"bound charge\".\n\nAlthough microscopically all charge is fundamentally the same, there are often practical reasons for wanting to treat bound charge differently from free charge. The result is that the more fundamental Gauss's law, in terms of E (above), is sometimes put into the equivalent form below, which is in terms of D and the free charge only.\n\nThis formulation of Gauss's law states the total charge form:\n\nΦ\n          \n            D\n          \n        \n        =\n        \n          Q\n          \n            \n              f\n              r\n              e\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi _{D}=Q_{\\mathrm {free} }}\n\nwhere ΦD is the D-field flux through a surface S which encloses a volume V, and Qfree is the free charge contained in V. The flux ΦD is defined analogously to the flux ΦE of the electric field E through S:\n\nThe differential form of Gauss's law, involving free charge only, states:\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          D\n        \n        =\n        \n          ρ\n          \n            \n              f\n              r\n              e\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {D} =\\rho _{\\mathrm {free} }}\n\nwhere ∇ · D is the divergence of the electric displacement field, and ρfree is the free electric charge density.\n\nIn this proof, we will show that the equation\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        =\n        \n          \n            \n              ρ\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} ={\\dfrac {\\rho }{\\varepsilon _{0}}}}\n  \n\nis equivalent to the equation\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          D\n        \n        =\n        \n          ρ\n          \n            \n              f\n              r\n              e\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {D} =\\rho _{\\mathrm {free} }}\n  \n\nNote that we are only dealing with the differential forms, not the integral forms, but that is sufficient since the differential and integral forms are equivalent in each case, by the divergence theorem.\n\nWe introduce the polarization density P, which has the following relation to E and D:\n\n  \n    \n      \n        \n          D\n        \n        =\n        \n          ε\n          \n            0\n          \n        \n        \n          E\n        \n        +\n        \n          P\n        \n      \n    \n    {\\displaystyle \\mathbf {D} =\\varepsilon _{0}\\mathbf {E} +\\mathbf {P} }\n  \n\nand the following relation to the bound charge:\n\n  \n    \n      \n        \n          ρ\n          \n            \n              b\n              o\n              u\n              n\n              d\n            \n          \n        \n        =\n        −\n        ∇\n        ⋅\n        \n          P\n        \n      \n    \n    {\\displaystyle \\rho _{\\mathrm {bound} }=-\\nabla \\cdot \\mathbf {P} }\n  \n\nNow, consider the three equations:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ρ\n                  \n                    \n                      b\n                      o\n                      u\n                      n\n                      d\n                    \n                  \n                \n              \n              \n                \n                =\n                ∇\n                ⋅\n                (\n                −\n                \n                  P\n                \n                )\n              \n            \n            \n              \n                \n                  ρ\n                  \n                    \n                      f\n                      r\n                      e\n                      e\n                    \n                  \n                \n              \n              \n                \n                =\n                ∇\n                ⋅\n                \n                  D\n                \n              \n            \n            \n              \n                ρ\n              \n              \n                \n                =\n                ∇\n                ⋅\n                (\n                \n                  ε\n                  \n                    0\n                  \n                \n                \n                  E\n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho _{\\mathrm {bound} }&=\\nabla \\cdot (-\\mathbf {P} )\\\\\\rho _{\\mathrm {free} }&=\\nabla \\cdot \\mathbf {D} \\\\\\rho &=\\nabla \\cdot (\\varepsilon _{0}\\mathbf {E} )\\end{aligned}}}\n  \n\nThe key insight is that the sum of the first two equations is the third equation. This completes the proof: The first equation is true by definition, and therefore the second equation is true if and only if the third equation is true. So the second and third equations are equivalent, which is what we wanted to prove.\n\nIn homogeneous, isotropic, nondispersive, linear materials, there is a simple relationship between E and D:\n\nD\n        \n        =\n        ε\n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {D} =\\varepsilon \\mathbf {E} }\n\nwhere ε is the permittivity of the material. For the case of vacuum (aka free space), ε = ε0. Under these circumstances, Gauss's law modifies to\n\nΦ\n          \n            E\n          \n        \n        =\n        \n          \n            \n              Q\n              \n                \n                  f\n                  r\n                  e\n                  e\n                \n              \n            \n            ε\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{E}={\\frac {Q_{\\mathrm {free} }}{\\varepsilon }}}\n\n∇\n        ⋅\n        \n          E\n        \n        =\n        \n          \n            \n              ρ\n              \n                \n                  f\n                  r\n                  e\n                  e\n                \n              \n            \n            ε\n          \n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} ={\\frac {\\rho _{\\mathrm {free} }}{\\varepsilon }}}\n\nStrictly speaking, Gauss's law cannot be derived from Coulomb's law alone, since Coulomb's law gives the electric field due to an individual, electrostatic point charge only. However, Gauss's law can be proven from Coulomb's law if it is assumed, in addition, that the electric field obeys the superposition principle. The superposition principle states that the resulting field is the vector sum of fields generated by each particle (or the integral, if the charges are distributed smoothly in space).\n\nCoulomb's law states that the electric field due to a stationary point charge is:\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                e\n              \n              \n                r\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {q}{4\\pi \\varepsilon _{0}}}{\\frac {\\mathbf {e} _{r}}{r^{2}}}}\n  \n\nwhere\n\nUsing the expression from Coulomb's law, we get the total field at r by using an integral to sum the field at r due to the infinitesimal charge at each other point s in space, to give\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        ∫\n        \n          \n            \n              ρ\n              (\n              \n                s\n              \n              )\n              (\n              \n                r\n              \n              −\n              \n                s\n              \n              )\n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                s\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        \n        \n          \n            d\n          \n          \n            3\n          \n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int {\\frac {\\rho (\\mathbf {s} )(\\mathbf {r} -\\mathbf {s} )}{|\\mathbf {r} -\\mathbf {s} |^{3}}}\\,\\mathrm {d} ^{3}\\mathbf {s} }\n  \n\nwhere ρ is the charge density. If we take the divergence of both sides of this equation with respect to r, and use the known theorem[9]\n\n∇\n        ⋅\n        \n          (\n          \n            \n              \n                r\n              \n              \n                \n                  |\n                \n                \n                  r\n                \n                \n                  \n                    |\n                  \n                  \n                    3\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        4\n        π\n        δ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle \\nabla \\cdot \\left({\\frac {\\mathbf {r} }{|\\mathbf {r} |^{3}}}\\right)=4\\pi \\delta (\\mathbf {r} )}\n  \n\nwhere δ(r) is the Dirac delta function, the result is\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ∫\n        ρ\n        (\n        \n          s\n        \n        )\n        \n        δ\n        (\n        \n          r\n        \n        −\n        \n          s\n        \n        )\n        \n        \n          \n            d\n          \n          \n            3\n          \n        \n        \n          s\n        \n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {1}{\\varepsilon _{0}}}\\int \\rho (\\mathbf {s} )\\,\\delta (\\mathbf {r} -\\mathbf {s} )\\,\\mathrm {d} ^{3}\\mathbf {s} }\n\nUsing the \"sifting property\" of the Dirac delta function, we arrive at\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            \n              ρ\n              (\n              \n                r\n              \n              )\n            \n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {\\rho (\\mathbf {r} )}{\\varepsilon _{0}}},}\n  \n\nwhich is the differential form of Gauss's law, as desired.\n\nSince Coulomb's law only applies to stationary charges, there is no reason to expect Gauss's law to hold for moving charges based on this derivation alone. In fact, Gauss's law does hold for moving charges, and, in this respect, Gauss's law is more general than Coulomb's law.\n\nLet \n  \n    \n      \n        Ω\n        ⊆\n        \n          R\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\Omega \\subseteq R^{3}}\n  \n be a bounded open set, and  \n  \n    \n      \n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n        \n          \n            \n              \n                r\n              \n              −\n              \n                \n                  r\n                \n                ′\n              \n            \n            \n              \n                ‖\n                \n                  \n                    r\n                  \n                  −\n                  \n                    \n                      r\n                    \n                    ′\n                  \n                \n                ‖\n              \n              \n                3\n              \n            \n          \n        \n        \n          d\n        \n        \n          \n            r\n          \n          ′\n        \n        ≡\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{0}(\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }\\rho (\\mathbf {r} '){\\frac {\\mathbf {r} -\\mathbf {r} '}{\\left\\|\\mathbf {r} -\\mathbf {r} '\\right\\|^{3}}}\\mathrm {d} \\mathbf {r} '\\equiv {\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }e(\\mathbf {r,\\mathbf {r} '} ){\\mathrm {d} \\mathbf {r} '}}\n  \n be the electric field, with \n  \n    \n      \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle \\rho (\\mathbf {r} ')}\n  \n a continuous function (density of charge).\n\nIt is true for all \n  \n    \n      \n        \n          r\n        \n        ≠\n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} \\neq \\mathbf {r'} }\n  \n that \n  \n    \n      \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        \n          e\n        \n        (\n        \n          r\n          ,\n          \n            r\n            ′\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {r} }\\cdot \\mathbf {e} (\\mathbf {r,r'} )=0}\n  \n.\n\nConsider now a compact set \n  \n    \n      \n        V\n        ⊆\n        \n          R\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle V\\subseteq R^{3}}\n  \n having a piecewise smooth boundary \n  \n    \n      \n        ∂\n        V\n      \n    \n    {\\displaystyle \\partial V}\n  \n such that \n  \n    \n      \n        Ω\n        ∩\n        V\n        =\n        ∅\n      \n    \n    {\\displaystyle \\Omega \\cap V=\\emptyset }\n  \n. It follows that \n  \n    \n      \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        ∈\n        \n          C\n          \n            1\n          \n        \n        (\n        V\n        ×\n        Ω\n        )\n      \n    \n    {\\displaystyle e(\\mathbf {r,\\mathbf {r} '} )\\in C^{1}(V\\times \\Omega )}\n  \n and so, for the divergence theorem:\n\n∮\n          \n            ∂\n            V\n          \n        \n        \n          \n            E\n          \n          \n            0\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∫\n          \n            V\n          \n        \n        \n          ∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        \n        d\n        V\n      \n    \n    {\\displaystyle \\oint _{\\partial V}\\mathbf {E} _{0}\\cdot d\\mathbf {S} =\\int _{V}\\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}\\,dV}\n\nBut because \n  \n    \n      \n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        ∈\n        \n          C\n          \n            1\n          \n        \n        (\n        V\n        ×\n        Ω\n        )\n      \n    \n    {\\displaystyle e(\\mathbf {r,\\mathbf {r} '} )\\in C^{1}(V\\times \\Omega )}\n  \n,\n\n∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          ∫\n          \n            Ω\n          \n        \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        e\n        (\n        \n          r\n          ,\n          \n            \n              r\n            \n            ′\n          \n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}(\\mathbf {r} )={\\frac {1}{4\\pi \\varepsilon _{0}}}\\int _{\\Omega }\\nabla _{\\mathbf {r} }\\cdot e(\\mathbf {r,\\mathbf {r} '} ){\\mathrm {d} \\mathbf {r} '}=0}\n  \n  for the argument above (\n  \n    \n      \n        Ω\n        ∩\n        V\n        =\n        ∅\n        \n        ⟹\n        \n        ∀\n        \n          r\n        \n        ∈\n        V\n         \n         \n        ∀\n        \n          \n            r\n            ′\n          \n        \n        ∈\n        Ω\n         \n         \n         \n        \n          r\n        \n        ≠\n        \n          \n            r\n            ′\n          \n        \n      \n    \n    {\\displaystyle \\Omega \\cap V=\\emptyset \\implies \\forall \\mathbf {r} \\in V\\ \\ \\forall \\mathbf {r'} \\in \\Omega \\ \\ \\ \\mathbf {r} \\neq \\mathbf {r'} }\n  \n and then \n  \n    \n      \n        \n          ∇\n          \n            \n              r\n            \n          \n        \n        ⋅\n        \n          e\n        \n        (\n        \n          r\n          ,\n          \n            r\n            ′\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {r} }\\cdot \\mathbf {e} (\\mathbf {r,r'} )=0}\n  \n)\n\nTherefore the flux through a closed surface generated by some charge density outside (the surface) is null.\n\nNow consider \n  \n    \n      \n        \n          \n            r\n          \n          \n            0\n          \n        \n        ∈\n        Ω\n      \n    \n    {\\displaystyle \\mathbf {r} _{0}\\in \\Omega }\n  \n, and \n  \n    \n      \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        ⊆\n        Ω\n      \n    \n    {\\displaystyle B_{R}(\\mathbf {r} _{0})\\subseteq \\Omega }\n  \n  as the sphere centered in \n  \n    \n      \n        \n          \n            r\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{0}}\n  \n having \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n as radius (it exists because \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n is an open set).\n\nLet \n  \n    \n      \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{B_{R}}}\n  \n and \n  \n    \n      \n        \n          \n            E\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} _{C}}\n  \n be the electric field created inside and outside the sphere respectively. Then,\n\nΦ\n        (\n        R\n        )\n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            0\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n        ⋅\n        d\n        \n          S\n        \n        +\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            C\n          \n        \n        ⋅\n        d\n        \n          S\n        \n        =\n        \n          ∮\n          \n            ∂\n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        \n          \n            E\n          \n          \n            \n              B\n              \n                R\n              \n            \n          \n        \n        ⋅\n        d\n        \n          S\n        \n      \n    \n    {\\displaystyle \\Phi (R)=\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{0}\\cdot d\\mathbf {S} =\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{B_{R}}\\cdot d\\mathbf {S} +\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{C}\\cdot d\\mathbf {S} =\\oint _{\\partial B_{R}(\\mathbf {r} _{0})}\\mathbf {E} _{B_{R}}\\cdot d\\mathbf {S} }\n\nThe last equality follows by observing that \n  \n    \n      \n        (\n        Ω\n        ∖\n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        )\n        ∩\n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        =\n        ∅\n      \n    \n    {\\displaystyle (\\Omega \\setminus B_{R}(\\mathbf {r} _{0}))\\cap B_{R}(\\mathbf {r} _{0})=\\emptyset }\n  \n, and the argument above.\n\nThe RHS is the electric flux generated by a charged sphere, and so:\n\nΦ\n        (\n        R\n        )\n        =\n        \n          \n            \n              Q\n              (\n              R\n              )\n            \n            \n              ε\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        \n          ∫\n          \n            \n              B\n              \n                R\n              \n            \n            (\n            \n              \n                r\n              \n              \n                0\n              \n            \n            )\n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n        \n          \n            d\n          \n          \n            \n              r\n            \n            ′\n          \n        \n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          \n            c\n          \n          ′\n        \n        )\n        \n          |\n        \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle \\Phi (R)={\\frac {Q(R)}{\\varepsilon _{0}}}={\\frac {1}{\\varepsilon _{0}}}\\int _{B_{R}(\\mathbf {r} _{0})}\\rho (\\mathbf {r} '){\\mathrm {d} \\mathbf {r} '}={\\frac {1}{\\varepsilon _{0}}}\\rho (\\mathbf {r} '_{c})|B_{R}(\\mathbf {r} _{0})|}\n  \n with \n  \n    \n      \n        \n          r\n          \n            c\n          \n          ′\n        \n        ∈\n         \n        \n          B\n          \n            R\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle r'_{c}\\in \\ B_{R}(\\mathbf {r} _{0})}\n\nWhere the last equality follows by the mean value theorem for integrals. Using the squeeze theorem and the continuity of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n, one arrives at:\n\n∇\n        \n        ⋅\n        \n          \n            E\n          \n          \n            0\n          \n        \n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n        =\n        \n          lim\n          \n            R\n            →\n            0\n          \n        \n        \n          \n            1\n            \n              \n                |\n              \n              \n                B\n                \n                  R\n                \n              \n              (\n              \n                \n                  r\n                \n                \n                  0\n                \n              \n              )\n              \n                |\n              \n            \n          \n        \n        Φ\n        (\n        R\n        )\n        =\n        \n          \n            1\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        ρ\n        (\n        \n          \n            r\n          \n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {\\nabla } \\cdot \\mathbf {E} _{0}(\\mathbf {r} _{0})=\\lim _{R\\to 0}{\\frac {1}{|B_{R}(\\mathbf {r} _{0})|}}\\Phi (R)={\\frac {1}{\\varepsilon _{0}}}\\rho (\\mathbf {r} _{0})}\n\nStrictly speaking, Coulomb's law cannot be derived from Gauss's law alone, since Gauss's law does not give any information regarding the curl of E (see Helmholtz decomposition and Faraday's law). However, Coulomb's law can be proven from Gauss's law if it is assumed, in addition, that the electric field from a point charge is spherically symmetric (this assumption, like Coulomb's law itself, is exactly true if the charge is stationary, and approximately true if the charge is in motion).\n\nTaking S in the integral form of Gauss's law to be a spherical surface of radius r, centered at the point charge Q, we have\n\n∮\n          \n            S\n          \n        \n        \n          E\n        \n        ⋅\n        d\n        \n          A\n        \n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\oint _{S}\\mathbf {E} \\cdot d\\mathbf {A} ={\\frac {Q}{\\varepsilon _{0}}}}\n\nBy the assumption of spherical symmetry, the integrand is a constant which can be taken out of the integral. The result is\n\n  \n    \n      \n        4\n        π\n        \n          r\n          \n            2\n          \n        \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            Q\n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle 4\\pi r^{2}{\\hat {\\mathbf {r} }}\\cdot \\mathbf {E} (\\mathbf {r} )={\\frac {Q}{\\varepsilon _{0}}}}\n  \n\nwhere r̂ is a unit vector pointing radially away from the charge. Again by spherical symmetry, E points in the radial direction, and so we get\n\n  \n    \n      \n        \n          E\n        \n        (\n        \n          r\n        \n        )\n        =\n        \n          \n            Q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} (\\mathbf {r} )={\\frac {Q}{4\\pi \\varepsilon _{0}}}{\\frac {\\hat {\\mathbf {r} }}{r^{2}}}}\n  \n\nwhich is essentially equivalent to Coulomb's law. Thus the inverse-square law dependence of the electric field in Coulomb's law follows from Gauss's law.",
        pageTitle: "Gauss's law",
    },
    {
        title: "Gauss's law for gravity",
        link: "https://en.wikipedia.org/wiki/Gauss%27s_law_for_gravity",
        content:
            "In physics, Gauss's law for gravity, also known as Gauss's flux theorem for gravity, is a law of physics that is equivalent to Newton's law of universal gravitation. It is named after Carl Friedrich Gauss. It states that the flux (surface integral) of the gravitational field over any closed surface is proportional to the mass enclosed.  Gauss's law for gravity is often more convenient to work from than Newton's law.[1]\n\nThe form of Gauss's law for gravity is mathematically similar to Gauss's law for electrostatics, one of Maxwell's equations. Gauss's law for gravity has the same mathematical relation to Newton's law that Gauss's law for electrostatics bears to Coulomb's law.  This is because both Newton's law and Coulomb's law describe inverse-square interaction in a 3-dimensional space.\n\nThe gravitational field g (also called gravitational acceleration) is a vector field – a vector at each point of space (and time). It is defined so that the gravitational force experienced by a particle is equal to the mass of the particle multiplied by the gravitational field at that point.\n\nGravitational flux is a surface integral of the gravitational field over a closed surface, analogous to how magnetic flux is a surface integral of the magnetic field.\n\nThe integral form of Gauss's law for gravity states:\n\n∂\n          V\n        \n      \n    \n    {\\displaystyle \\scriptstyle \\partial V}\n  \n \n  \n    \n      \n        \n          g\n        \n        ⋅\n        d\n        \n          A\n        \n        =\n        −\n        4\n        π\n        G\n        M\n      \n    \n    {\\displaystyle \\mathbf {g} \\cdot d\\mathbf {A} =-4\\pi GM}\n\nThe left-hand side of this equation is called the flux of the gravitational field. Note that according to the law it is always negative (or zero), and never positive. This can be contrasted with Gauss's law for electricity, where the flux can be either positive or negative. The difference is because charge can be either positive or negative, while mass can only be positive.\n\nThe differential form of Gauss's law for gravity states\n\n∇\n        ⋅\n        \n          g\n        \n        =\n        −\n        4\n        π\n        G\n        ρ\n        ,\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {g} =-4\\pi G\\rho ,}\n\nwhere \n  \n    \n      \n        ∇\n        ⋅\n      \n    \n    {\\displaystyle \\nabla \\cdot }\n  \n denotes divergence, G is the universal gravitational constant, and ρ is the mass density at each point.\n\nThe two forms of Gauss's law for gravity are mathematically equivalent. The divergence theorem states:\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            V\n          \n        \n        \n          g\n        \n        ⋅\n        d\n        \n          A\n        \n        =\n        \n          ∫\n          \n            V\n          \n        \n        ∇\n        ⋅\n        \n          g\n        \n        \n        d\n        V\n      \n    \n    {\\displaystyle \\oint _{\\partial V}\\mathbf {g} \\cdot d\\mathbf {A} =\\int _{V}\\nabla \\cdot \\mathbf {g} \\,dV}\n  \n\nwhere V is a closed region bounded by a simple closed oriented surface ∂V and dV is an infinitesimal piece of the volume V (see volume integral for more details). The gravitational field g must be a continuously differentiable vector field defined on a neighborhood of V.\n\nGiven also that\n\n  \n    \n      \n        M\n        =\n        \n          ∫\n          \n            V\n          \n        \n        ρ\n         \n        d\n        V\n      \n    \n    {\\displaystyle M=\\int _{V}\\rho \\ dV}\n  \n\nwe can apply the divergence theorem to the integral form of Gauss's law for gravity, which becomes:\n\n  \n    \n      \n        \n          ∫\n          \n            V\n          \n        \n        ∇\n        ⋅\n        \n          g\n        \n         \n        d\n        V\n        =\n        −\n        4\n        π\n        G\n        \n          ∫\n          \n            V\n          \n        \n        ρ\n         \n        d\n        V\n      \n    \n    {\\displaystyle \\int _{V}\\nabla \\cdot \\mathbf {g} \\ dV=-4\\pi G\\int _{V}\\rho \\ dV}\n  \n\nwhich can be rewritten:\n\n  \n    \n      \n        \n          ∫\n          \n            V\n          \n        \n        (\n        ∇\n        ⋅\n        \n          g\n        \n        )\n         \n        d\n        V\n        =\n        \n          ∫\n          \n            V\n          \n        \n        (\n        −\n        4\n        π\n        G\n        ρ\n        )\n         \n        d\n        V\n        .\n      \n    \n    {\\displaystyle \\int _{V}(\\nabla \\cdot \\mathbf {g} )\\ dV=\\int _{V}(-4\\pi G\\rho )\\ dV.}\n  \n\nThis has to hold simultaneously for every possible volume V; the only way this can happen is if the integrands are equal. Hence we arrive at\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          g\n        \n        =\n        −\n        4\n        π\n        G\n        ρ\n        ,\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {g} =-4\\pi G\\rho ,}\n  \n\nwhich is the differential form of Gauss's law for gravity.\n\nIt is possible to derive the integral form from the differential form using the reverse of this method.\n\nAlthough the two forms are equivalent, one or the other might be more convenient to use in a particular computation.\n\nGauss's law for gravity can be derived from Newton's law of universal gravitation, which states that the gravitational field due to a point mass is:\n\n  \n    \n      \n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        \n          \n            \n              G\n              M\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          \n            e\n            \n              r\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {g} (\\mathbf {r} )=-{\\frac {GM}{r^{2}}}\\mathbf {e_{r}} }\n  \n\nwhere\n\nA proof using vector calculus is shown in the box below. It is mathematically identical to the proof of Gauss's law (in electrostatics) starting from Coulomb's law.[2]\n\ng(r), the gravitational field at r, can be calculated by adding up the contribution to g(r) due to every bit of mass in the universe (see superposition principle). To do this, we integrate over every point s in space, adding up the contribution to g(r) associated with the mass (if any) at s, where this contribution is calculated by Newton's law. The result is:\n\n  \n    \n      \n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        G\n        ∫\n        ρ\n        (\n        \n          s\n        \n        )\n        \n          \n            \n              (\n              \n                r\n              \n              −\n              \n                s\n              \n              )\n            \n            \n              \n                |\n              \n              \n                r\n              \n              −\n              \n                s\n              \n              \n                \n                  |\n                \n                \n                  3\n                \n              \n            \n          \n        \n        \n          d\n          \n            3\n          \n        \n        \n          s\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {g} (\\mathbf {r} )=-G\\int \\rho (\\mathbf {s} ){\\frac {(\\mathbf {r} -\\mathbf {s} )}{|\\mathbf {r} -\\mathbf {s} |^{3}}}d^{3}\\mathbf {s} .}\n  \n\n(d3s stands for dsxdsydsz, each of which is integrated from −∞ to +∞.) If we take the divergence of both sides of this equation with respect to r, and use the known theorem[2]\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          (\n          \n            \n              \n                r\n              \n              \n                \n                  |\n                \n                \n                  r\n                \n                \n                  \n                    |\n                  \n                  \n                    3\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        4\n        π\n        δ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle \\nabla \\cdot \\left({\\frac {\\mathbf {r} }{|\\mathbf {r} |^{3}}}\\right)=4\\pi \\delta (\\mathbf {r} )}\n  \n\nwhere δ(r) is the Dirac delta function, the result is\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        4\n        π\n        G\n        ∫\n        ρ\n        (\n        \n          s\n        \n        )\n         \n        δ\n        (\n        \n          r\n        \n        −\n        \n          s\n        \n        )\n         \n        \n          d\n          \n            3\n          \n        \n        \n          s\n        \n        .\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {g} (\\mathbf {r} )=-4\\pi G\\int \\rho (\\mathbf {s} )\\ \\delta (\\mathbf {r} -\\mathbf {s} )\\ d^{3}\\mathbf {s} .}\n  \n\nUsing the \"sifting property\" of the Dirac delta function, we arrive at\n\n  \n    \n      \n        ∇\n        ⋅\n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        4\n        π\n        G\n        ρ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {g} (\\mathbf {r} )=-4\\pi G\\rho (\\mathbf {r} )}\n  \n\nwhich is the differential form of Gauss's law for gravity, as desired.\n\nIt is impossible to mathematically prove Newton's law from Gauss's law alone, because Gauss's law specifies the divergence of g but does not contain any information regarding the curl of g (see Helmholtz decomposition). In addition to Gauss's law, the assumption is used that g is irrotational (has zero curl), as gravity is a conservative force:\n\nEven these are not enough: Boundary conditions on g are also necessary to prove Newton's law, such as the assumption that the field is zero infinitely far from a mass.\n\nThe proof of Newton's law from these assumptions is as follows:\n\nStart with the integral form of Gauss's law:\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            V\n          \n        \n        \n          g\n        \n        ⋅\n        d\n        \n          A\n        \n        =\n        −\n        4\n        π\n        G\n        M\n        .\n      \n    \n    {\\displaystyle \\oint _{\\partial V}\\mathbf {g} \\cdot d\\mathbf {A} =-4\\pi GM.}\n  \n\nApply this law to the situation where the volume V is a sphere of radius r centered on a point-mass M. It's reasonable to expect the gravitational field from a point mass to be spherically symmetric. (We omit the proof for simplicity.) By making this assumption, g takes the following form:\n\n  \n    \n      \n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        g\n        (\n        r\n        )\n        \n        \n          \n            e\n            \n              r\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {g} (\\mathbf {r} )=-g(r)\\,\\mathbf {e_{r}} }\n  \n\n(i.e., the direction of g is antiparallel to the direction of r, and the magnitude of g depends only on the magnitude, not direction, of r). Plugging this in, and using the fact that ∂V is a spherical surface with constant r and area \n  \n    \n      \n        4\n        π\n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 4\\pi r^{2}}\n  \n,\n\nSince the gravitational field has zero curl (equivalently, gravity is a conservative force) as mentioned above, it can be written as the gradient of a scalar potential, called the gravitational potential:\n\n  \n    \n      \n        \n          g\n        \n        =\n        −\n        ∇\n        ϕ\n        .\n      \n    \n    {\\displaystyle \\mathbf {g} =-\\nabla \\phi .}\n  \n\nThen the differential form of Gauss's law for gravity becomes Poisson's equation:\n\n  \n    \n      \n        \n          ∇\n          \n            2\n          \n        \n        ϕ\n        =\n        4\n        π\n        G\n        ρ\n        .\n      \n    \n    {\\displaystyle \\nabla ^{2}\\phi =4\\pi G\\rho .}\n  \n\nThis provides an alternate means of calculating the gravitational potential and gravitational field. Although computing g via Poisson's equation is mathematically equivalent to computing g directly from Gauss's law, one or the other approach may be an easier computation in a given situation.\n\nIn radially symmetric systems, the gravitational potential is a function of only one variable (namely, \n  \n    \n      \n        r\n        =\n        \n          |\n        \n        \n          r\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle r=|\\mathbf {r} |}\n  \n), and Poisson's equation becomes (see Del in cylindrical and spherical coordinates):\n\n  \n    \n      \n        \n          \n            1\n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          \n            ∂\n            \n              ∂\n              r\n            \n          \n        \n        \n          (\n          \n            \n              r\n              \n                2\n              \n            \n            \n            \n              \n                \n                  ∂\n                  ϕ\n                \n                \n                  ∂\n                  r\n                \n              \n            \n          \n          )\n        \n        =\n        4\n        π\n        G\n        ρ\n        (\n        r\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{r^{2}}}{\\frac {\\partial }{\\partial r}}\\left(r^{2}\\,{\\frac {\\partial \\phi }{\\partial r}}\\right)=4\\pi G\\rho (r)}\n  \n\nwhile the gravitational field is:\n\n  \n    \n      \n        \n          g\n        \n        (\n        \n          r\n        \n        )\n        =\n        −\n        \n          \n            e\n            \n              r\n            \n          \n        \n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              r\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {g} (\\mathbf {r} )=-\\mathbf {e_{r}} {\\frac {\\partial \\phi }{\\partial r}}.}\n\nWhen solving the equation it should be taken into account that in the case of finite densities ∂ϕ/∂r has to be continuous at boundaries (discontinuities of the density), and zero for r = 0.\n\nGauss's law can be used to easily derive the gravitational field in certain cases where a direct application of Newton's law would be more difficult (but not impossible). See the article Gaussian surface for more details on how these derivations are done. Three such applications are as follows:\n\nWe can conclude (by using a \"Gaussian pillbox\") that for an infinite, flat plate (Bouguer plate) of any finite thickness, the gravitational field outside the plate is perpendicular to the plate, towards it, with magnitude 2πG times the mass per unit area, independent of the distance to the plate[3] (see also gravity anomalies).\n\nMore generally, for a mass distribution with the density depending on one Cartesian coordinate z only, gravity for any z is 2πG times the difference in mass per unit area on either side of this z value.\n\nIn particular, a parallel combination of two parallel infinite plates of equal mass per unit area produces no gravitational field between them.\n\nIn the case of an infinite uniform (in z) cylindrically symmetric mass distribution we can conclude (by using a cylindrical Gaussian surface) that the field strength at a distance r from the center is inward with a magnitude of 2G/r times the total mass per unit length at a smaller distance (from the axis), regardless of any masses at a larger distance.\n\nFor example, inside an infinite uniform hollow cylinder, the field is zero.\n\nIn the case of a spherically symmetric mass distribution we can conclude (by using a spherical Gaussian surface) that the field strength at a distance r from the center is inward with a magnitude of G/r2 times only the total mass within a smaller distance than r. All the mass at a greater distance than r from the center has no resultant effect.\n\nFor example, a hollow sphere does not produce any net gravity inside. The gravitational field inside is the same as if the hollow sphere were not there (i.e. the resultant field is that of all masses not including the sphere, which can be inside and outside the sphere).\n\nAlthough this follows in one or two lines of algebra from Gauss's law for gravity, it took Isaac Newton several pages of cumbersome calculus to derive it directly using his law of gravity; see the article shell theorem for this direct derivation.\n\nThe Lagrangian density for Newtonian gravity is\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        ,\n        t\n        )\n        =\n        −\n        ρ\n        (\n        \n          x\n        \n        ,\n        t\n        )\n        ϕ\n        (\n        \n          x\n        \n        ,\n        t\n        )\n        −\n        \n          \n            1\n            \n              8\n              π\n              G\n            \n          \n        \n        (\n        ∇\n        ϕ\n        (\n        \n          x\n        \n        ,\n        t\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,t)=-\\rho (\\mathbf {x} ,t)\\phi (\\mathbf {x} ,t)-{1 \\over 8\\pi G}(\\nabla \\phi (\\mathbf {x} ,t))^{2}}\n  \n\nApplying Hamilton's principle to this Lagrangian, the result is Gauss's law for gravity:\n\n  \n    \n      \n        4\n        π\n        G\n        ρ\n        (\n        \n          x\n        \n        ,\n        t\n        )\n        =\n        \n          ∇\n          \n            2\n          \n        \n        ϕ\n        (\n        \n          x\n        \n        ,\n        t\n        )\n        .\n      \n    \n    {\\displaystyle 4\\pi G\\rho (\\mathbf {x} ,t)=\\nabla ^{2}\\phi (\\mathbf {x} ,t).}\n  \n\nSee Lagrangian (field theory) for details.",
        pageTitle: "Gauss's law for gravity",
    },
    {
        title: "Gauss's law for magnetism",
        link: "https://en.wikipedia.org/wiki/Gauss%27s_law_for_magnetism",
        content:
            "In physics, Gauss's law for magnetism is one of the four Maxwell's equations that underlie classical electrodynamics. It states that the magnetic field B has divergence equal to zero,[1] in other words, that it is a solenoidal vector field. It is equivalent to the statement that magnetic monopoles do not exist.[2] Rather than \"magnetic charges\", the basic entity for magnetism is the magnetic dipole. (If monopoles were ever found, the law would have to be modified, as elaborated below.)\n\nGauss's law for magnetism can be written in two forms, a differential form and an integral form. These forms are equivalent due to the divergence theorem.\n\nThe name \"Gauss's law for magnetism\"[1] is not universally used. The law is also called \"Absence of free magnetic poles\".[2]  It is also referred to as the \"transversality requirement\"[3] because for plane waves it requires that the polarization be transverse to the direction of propagation.\n\nThe differential form for Gauss's law for magnetism is:\n\n∇\n        ⋅\n        \n          B\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\nabla \\cdot \\mathbf {B} =0}\n\nwhere ∇ · denotes divergence, and B is the magnetic field.\n\nThe integral form of Gauss's law for magnetism states:\n\nΦ\n          \n            B\n          \n        \n        =\n      \n    \n    {\\displaystyle \\Phi _{B}=}\n  \n \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\scriptstyle S}\n  \n \n  \n    \n      \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {B} \\cdot \\mathrm {d} \\mathbf {S} =0}\n\nwhere S is any closed surface (see image right), \n  \n    \n      \n        \n          Φ\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{B}}\n  \n is the magnetic flux through S, and dS is a vector, whose magnitude is the area of an infinitesimal piece of the surface S, and whose direction is the outward-pointing surface normal (see surface integral for more details).\n\nGauss's law for magnetism thus states that the net magnetic flux through a closed surface equals zero.\n\nThe integral and differential forms of Gauss's law for magnetism are mathematically equivalent, due to the divergence theorem. That said, one or the other might be more convenient to use in a particular computation.\n\nThe law in this form states that for each volume element in space, there are exactly the same number of \"magnetic field lines\" entering and exiting the volume. No total \"magnetic charge\" can build up in any point in space. For example, the south pole of the magnet is exactly as strong as the north pole, and free-floating south poles without accompanying north poles (magnetic monopoles) are not allowed. In contrast, this is not true for other fields such as electric fields or gravitational fields, where total electric charge or mass can build up in a volume of space.\n\nDue to the Helmholtz decomposition theorem, Gauss's law for magnetism is equivalent to the following statement:[4][5]\n\nThe vector field A is called the magnetic vector potential.\n\nNote that there is more than one possible A which satisfies this equation for a given B field. In fact, there are infinitely many: any field of the form ∇ϕ can be added onto A to get an alternative choice for A, by the identity (see Vector calculus identities):\n\n  \n    \n      \n        ∇\n        ×\n        \n          A\n        \n        =\n        ∇\n        ×\n        (\n        \n          A\n        \n        +\n        ∇\n        ϕ\n        )\n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {A} =\\nabla \\times (\\mathbf {A} +\\nabla \\phi )}\n  \n\nsince the curl of a gradient is the zero vector field:\n\n  \n    \n      \n        ∇\n        ×\n        ∇\n        ϕ\n        =\n        \n          0\n        \n      \n    \n    {\\displaystyle \\nabla \\times \\nabla \\phi ={\\boldsymbol {0}}}\n\nThe magnetic field B can be depicted via field lines (also called flux lines) – that is, a set of curves whose direction corresponds to the direction of B, and whose areal density is proportional to the magnitude of B. Gauss's law for magnetism is equivalent to the statement that the field lines have neither a beginning nor an end: Each one either forms a closed loop, winds around forever without ever quite joining back up to itself exactly, or extends to infinity.\n\nIf magnetic monopoles were to be discovered, then Gauss's law for magnetism would state the divergence of B would be proportional to the magnetic charge density ρm, analogous to Gauss's law for electric field. For zero net magnetic charge density (ρm = 0), the original form of Gauss's magnetism law is the result.\n\nThe modified formula for use with the SI is not standard and depends on the choice of defining equation for the magnetic charge and current; in one variation, magnetic charge has units of webers, in another it has units of ampere-meters.\n\nSo far, examples of magnetic monopoles are disputed in extensive search,[9] although certain papers report examples matching that behavior.\n[10]\n\nThis idea of the nonexistence of the  magnetic monopoles originated in 1269 by Petrus Peregrinus de Maricourt. His work heavily influenced William Gilbert, whose 1600 work De Magnete spread the idea further. In the early 1800s Michael Faraday reintroduced this law, and it subsequently made its way into James Clerk Maxwell's electromagnetic field equations.\n\nIn numerical computation, the numerical solution may not satisfy Gauss's law for magnetism due to the discretization errors of the numerical methods. However, in many cases, e.g., for magnetohydrodynamics, it is important to preserve Gauss's law for magnetism precisely (up to the machine precision). Violation of Gauss's law for magnetism on the discrete level will introduce a strong non-physical force. In view of energy conservation, violation of this condition leads to a non-conservative energy integral, and the error is proportional to the divergence of the magnetic field.[11]\n\nThere are various ways to preserve Gauss's law for magnetism in numerical methods, including the divergence-cleaning techniques,[12] the constrained transport method,[13] potential-based formulations[14] and de Rham complex based finite element methods[15][16] where stable and structure-preserving algorithms are constructed on unstructured meshes with finite element differential forms.",
        pageTitle: "Gauss's law for magnetism",
    },
    {
        title: "Gay-Lussac's law",
        link: "https://en.wikipedia.org/wiki/Gay-Lussac%27s_law",
        content:
            "Gay-Lussac's law usually refers to Joseph-Louis Gay-Lussac's law of combining volumes of gases, discovered in 1808 and published in 1809.[1]  However, it sometimes refers to the proportionality of the volume of a gas to its absolute temperature at constant pressure.  The latter law was published by Gay-Lussac in 1802,[2] but in the article in which he described his work, he cited earlier unpublished work from the 1780s by Jacques Charles. Consequently, the volume-temperature proportionality is usually known as Charles's Law.\n\nThe law of combining volumes states that when gases chemically react together, they do so in amounts by volume which bear small whole-number ratios (the volumes calculated at the same temperature and pressure).\n\nThe ratio between the volumes of the reactant gases and the gaseous products can be expressed in simple whole numbers.\n\nFor example, Gay-Lussac found that two volumes of hydrogen react with one volume of oxygen to form two volumes of gaseous water. Expressed concretely, 100 mL of hydrogen combine with 50 mL of oxygen to give 100 mL of water vapor: Hydrogen(100 mL) + Oxygen(50 mL) = Water(100 mL). Thus, the volumes of hydrogen and oxygen which combine (i.e., 100mL and 50mL) bear a simple ratio of 2:1, as also is the case for the ratio of product water vapor to reactant oxygen.\n\nBased on Gay-Lussac's results, Amedeo Avogadro hypothesized in 1811 that, at the same temperature and pressure, equal volumes of gases (of whatever kind) contain equal numbers of molecules (Avogadro's law). He pointed out that if this hypothesis is true, then the previously stated result\n\nThe law of combining volumes of gases was announced publicly by Joseph Louis Gay-Lussac on the last day of 1808, and published in 1809.[3][4] Since there was no direct evidence for Avogadro's molecular theory, very few chemists adopted Avogadro's hypothesis as generally valid until the Italian chemist Stanislao Cannizzaro argued convincingly for it during the First International Chemical Congress in 1860.[5]\n\nIn the 17th century Guillaume Amontons discovered a regular relationship between the pressure and temperature of a gas at constant volume. Some introductory physics textbooks still define the pressure-temperature relationship as Gay-Lussac's law.[6][7][8] Gay-Lussac primarily investigated the relationship between volume and temperature and published it in 1802, but his work did cover some comparison between pressure and temperature.[9] Given the relative technology available to both men, Amontons could only work with air as a gas, whereas Gay-Lussac was able to experiment with multiple types of common gases, such as oxygen, nitrogen, and hydrogen.[10]\n\nRegarding the volume-temperature relationship, Gay-Lussac attributed his findings to Jacques Charles because he used much of Charles's unpublished data from 1787 – hence, the law became known as Charles's law or the Law of Charles and Gay-Lussac.[11]\n\nAmontons's, Charles', and Boyle's law form the combined gas law. These three gas laws in combination with Avogadro's law can be generalized by the ideal gas law.\n\nGay-Lussac used the formula acquired from ΔV/V = αΔT to define the rate of expansion α for gases. For air, he found a relative expansion ΔV/V = 37.50% and obtained a value of α = 37.50%/100 °C = 1/266.66 °C which indicated that the value of absolute zero was approximately 266.66 °C below 0 °C.[12] The value of the rate of expansion α is approximately the same for all gases and this is also sometimes referred to as Gay-Lussac's Law. See the introduction to this article, and Charles's Law.",
        pageTitle: "Gay-Lussac's law",
    },
    {
        title: "Gérson's law",
        link: "https://en.wikipedia.org/wiki/G%C3%A9rson%27s_law",
        content:
            'In Brazilian media culture, Gérson\'s law is a principle in which a certain person or company gains advantages indiscriminately, without caring about ethical or moral issues.\n\nGérson\'s Law has come to express highly characteristic and unflattering traits of the national media character, which is interpreted as the character of the population, associated with the spread of corruption and disregard for social rules to obtain advantages.\n\nThe expression emerged in the mid-1980s when journalist Mauricio Dias interviewed the professor and psychoanalyst from Pernambuco, Jurandir Freire Costa, for the magazine IstoÉ, on the occasion of his article "Narcissism in Dark Times". It was during this interview that Dias coined the term "Gérson\'s Law" to refer to the desire that a large portion of Brazilians have to take advantage of everything. Later, in 1992, in the 18th edition of the magazine Teoria e Debate, the term "Gerson\'s Law" was mentioned again by Maria Rita Kehl in an interview with the same Jurandir Freire da Costa.[1]\n\nWhen Maurício Dias coined the expression "Gérson\'s Law", he alluded to a television advertisement from 1976 created by the agency Caio Domingues & Associados, which had been hired by the cigarette manufacturer J. Reynolds, owner of the Vila Rica brand, to promote the product. The video featured midfielder Gérson, a player for the Brazilian national football team, as the protagonist.[2][3]\n\nThe video begins with the statement that Gérson was the "brain behind the world champion team of the 1970 World Cup." The narration is done by the interviewer, who appears in a suit, tie, and microphone in hand. The scene takes place on a sofa in a living room.\n\nThe interviewer asks why Gérson chose Vila Rica cigarettes. As he begins his response, Gérson takes out a pack of Vila Rica and offers a cigarette to the interviewer. While the interviewer smokes his Vila Rica cigarette, Gérson explains the reasons that made him prefer that brand.\n\nWhy pay more if Vila gives me everything I want in a good cigarette? I like to take advantage of everything, right? Take advantage too, have Vila Rica!\n\nLater, Gérson said he regretted associating his image with the advertisement since any unethical behavior came to be associated with his name in expressions such as the Gérson Syndrome or Gérson\'s Law.\n\nThe director of the Caio Domingues & Associados commercial, José Monserrat Filho, trying to disclaim responsibility, maintains that the public misinterpreted their video: "There was a misinterpretation, people started to understand it as being sly. In the second advertisement, we said: taking advantage is not about stepping over others, it\'s about getting ahead, but that phrase didn\'t stick, the popular wisdom uses what suits it."\n\nIn the 1980s, Brazilian mass media began reporting on corruption in Brazilian politics, and the population started using the term "Gérson\'s Law".[4]',
        pageTitle: "Gérson's law",
    },
    {
        title: "Gibrat's law",
        link: "https://en.wikipedia.org/wiki/Gibrat%27s_law",
        content:
            "Gibrat's law, sometimes called Gibrat's rule of proportionate growth or the law of proportionate effect,[1] is a rule defined by Robert Gibrat (1904–1980) in 1931 stating that the proportional rate of growth of a firm is independent of its absolute size.[2][3] The law of proportionate growth gives rise to a firm size distribution that is log-normal.[4]\n\nGibrat's law is also applied to cities size and growth rate,[5] where proportionate growth process may give rise to a distribution of city sizes that is log-normal, as predicted by Gibrat's law. While the city size distribution is often associated with Zipf's law, this holds only in the upper tail. When considering the entire size distribution, not just the largest cities, then the city size distribution is log-normal.[6] The log-normality of the distribution reconciles Gibrat's law also for cities: The law of proportionate effect will therefore imply that the logarithms of the variable will be distributed following the log-normal distribution.[2] In isolation, the upper tail (less than 1,000 out of 24,000 cities) fits both the log-normal and the Pareto distribution: the uniformly most powerful unbiased test comparing the lognormal to the power law shows that the largest 1000 cities are distinctly in the power law regime.[7]\n\nHowever, it has been argued that it is problematic to define cities through their fairly arbitrary legal boundaries (the places method treats Cambridge and Boston, Massachusetts, as two separate units). A clustering method to construct cities from the bottom up by clustering populated areas obtained from high-resolution data finds a power-law distribution of city size consistent with Zipf's law in almost the entire range of sizes.[8] Note that populated areas are still aggregated rather than individual based. A new method based on individual street nodes for the clustering process leads to the concept of natural cities. It has been found that natural cities exhibit a striking Zipf's law [9] Furthermore, the clustering method allows for a direct assessment of Gibrat's law. It is found that the growth of agglomerations is not consistent with Gibrat's law: the mean and standard deviation of the growth rates of cities follows a power-law with the city size.[10]\n\nIn general, processes characterized by Gibrat's law converge to a limiting distribution, often proposed to be the log-normal, or a power law, depending on more specific assumptions about the stochastic growth process. However, the tail of the lognormal may fall off too quickly, and its PDF is not monotonic, but rather has a Y-intercept of zero probability at the origin. The typical power law is the Pareto I, which has a tail that cannot model fall-off in the tail at large outcomes size, and which does not extend downwards to zero, but rather must be truncated at some positive minimum value. More recently, the Weibull distribution has been derived as the limiting distribution for Gibrat processes, by recognizing that (a) the increments of the growth process are not independent, but rather correlated, in magnitude, and (b) the increment magnitudes typically have monotonic PDFs.[11] The Weibull PDF can appear essentially log-log linear over orders of magnitude ranging from zero, while eventually falling off at unreasonably large outcome sizes.\n\nIn the study of the firms (business), the scholars do not agree that the foundation and the outcome of Gibrat's law are empirically correct.[citation needed][12]",
        pageTitle: "Gibrat's law",
    },
    {
        title: "Gibson's law",
        link: "https://en.wikipedia.org/wiki/Gibson%27s_law",
        content:
            'In public relations,[1]\nand in the practice of law, Gibson\'s law holds that "For every PhD there is an equal and opposite PhD."[2]\nThe term specifically refers to the conflict between testimony of expert witnesses called by opposing parties in a trial under an adversarial system of justice.[3]\nIt is also applied to conflicting scientific opinion injected into policy decisions by interested parties creating a controversy to promote their interests.[4]\n\nAn early mention of "Gibson\'s Rule" comes from Ivan Preston[5] who wrote "The problem of having opposing sides present opposite pictures of the consumer and of the research process was nicely distilled in a statement made by Fletcher Waller while representing General Mills in the children’s rulemaking. Waller quoted what he called Gibson’s Rule,\nwhich is that for every Ph.D. there is an equal and opposite Ph.D."',
        pageTitle: "Gibson's law",
    },
    {
        title: "laws of thermodynamics",
        link: "https://en.wikipedia.org/wiki/Laws_of_thermodynamics",
        content:
            "The laws of thermodynamics are a set of scientific laws which define a group of physical quantities, such as temperature, energy, and entropy, that characterize thermodynamic systems in thermodynamic equilibrium. The laws also use various parameters for thermodynamic processes, such as thermodynamic work and heat, and establish relationships between them. They state empirical facts that form a basis of precluding the possibility of certain phenomena, such as perpetual motion. In addition to their use in thermodynamics, they are important fundamental laws of physics in general and are applicable in other natural sciences.\n\nTraditionally, thermodynamics has recognized three fundamental laws, simply named by an ordinal identification, the first law, the second law, and the third law.[1][2][3] A more fundamental statement was later labelled as the zeroth law after the first three laws had been established.\n\nThe zeroth law of thermodynamics defines thermal equilibrium and forms a basis for the definition of temperature: if two systems are each in thermal equilibrium with a third system, then they are in thermal equilibrium with each other.\n\nThe first law of thermodynamics states that, when energy passes into or out of a system (as work, heat, or matter), the system's internal energy changes in accordance with the law of conservation of energy. This also results in the observation that, in an externally isolated system, even with internal changes, the sum of all forms of energy must remain constant, as energy cannot be created or destroyed.\n\nThe second law of thermodynamics states that in a natural thermodynamic process, the sum of the entropies of the interacting thermodynamic systems never decreases. A common corollary of the statement is that heat does not spontaneously pass from a colder body to a warmer body.\n\nThe third law of thermodynamics states that a system's entropy approaches a constant value as the temperature approaches absolute zero. With the exception of non-crystalline solids (glasses), the entropy of a system at absolute zero is typically close to zero.[2]\n\nThe first and second laws prohibit two kinds of perpetual motion machines, respectively: the perpetual motion machine of the first kind which produces work with no energy input, and the perpetual motion machine of the second kind which spontaneously converts thermal energy into mechanical work.\n\nThe history of thermodynamics is fundamentally interwoven with the history of physics and the history of chemistry, and ultimately dates back to theories of heat in antiquity. The laws of thermodynamics are the result of progress made in this field over the nineteenth and early twentieth centuries. The first established thermodynamic principle, which eventually became the second law of thermodynamics, was formulated by Sadi Carnot in 1824 in his book Reflections on the Motive Power of Fire. By 1860, as formalized in the works of scientists such as Rudolf Clausius and William Thomson, what are now known as the first and second laws were established. Later, Nernst's theorem (or Nernst's postulate), which is now known as the third law, was formulated by Walther Nernst over the period 1906–1912. While the numbering of the laws is universal today, various textbooks throughout the 20th century have numbered the laws differently.  In some fields, the second law was considered to deal with the efficiency of heat engines only, whereas what was called the third law dealt with entropy increases. Gradually, this resolved itself and a zeroth law was later added to allow for a self-consistent definition of temperature. Additional laws have been suggested, but have not achieved the generality of the four accepted laws, and are generally not discussed in standard textbooks.\n\nThe zeroth law of thermodynamics provides for the foundation of temperature as an empirical parameter in thermodynamic systems and establishes the transitive relation between the temperatures of multiple bodies in thermal equilibrium. The law may be stated in the following form:\n\nIf two systems are both in thermal equilibrium with a third system, then they are in thermal equilibrium with each other.[4]\n\nThough this version of the law is one of the most commonly stated versions, it is only one of a diversity of statements that are labeled as \"the zeroth law\". Some statements go further, so as to supply the important physical fact that temperature is one-dimensional and that one can conceptually arrange bodies in a real number sequence from colder to hotter.[5][6][7]\n\nThese concepts of temperature and of thermal equilibrium are fundamental to thermodynamics and were clearly stated in the nineteenth century. The name 'zeroth law' was invented by Ralph H. Fowler in the 1930s, long after the first, second, and third laws were widely recognized. The law allows the definition of temperature in a non-circular way without reference to entropy, its conjugate variable. Such a temperature definition is said to be 'empirical'.[8][9][10][11][12][13]\n\nThe first law of thermodynamics is a version of the law of conservation of energy, adapted for thermodynamic processes. In general, the conservation law states that the total energy of an isolated system is constant; energy can be transformed from one form to another, but can be neither created nor destroyed.\n\nIn a closed system (i.e. there is no transfer of matter into or out of the system), the first law states that the change in internal energy of the system (ΔUsystem) is equal to the difference between the heat supplied to the system (Q) and the work (W) done by the system on its surroundings. (Note, an alternate sign convention, not used in this article, is to define W as the work done on the system by its surroundings): \n  \n    \n      \n        Δ\n        \n          U\n          \n            \n              s\n              y\n              s\n              t\n              e\n              m\n            \n          \n        \n        =\n        Q\n        −\n        W\n        .\n      \n    \n    {\\displaystyle \\Delta U_{\\rm {system}}=Q-W.}\n\nFor processes that include the transfer of matter, a further statement is needed.\n\nWhen two initially isolated systems are combined into a new system, then the total internal energy of the new system, Usystem, will be equal to the sum of the internal energies of the two initial systems, U1 and U2: \n  \n    \n      \n        \n          U\n          \n            \n              s\n              y\n              s\n              t\n              e\n              m\n            \n          \n        \n        =\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{\\rm {system}}=U_{1}+U_{2}.}\n\nCombining these principles leads to one traditional statement of the first law of thermodynamics: it is not possible to construct a machine which will perpetually output work without an equal amount of energy input to that machine. Or more briefly, a perpetual motion machine of the first kind is impossible.\n\nThe second law of thermodynamics indicates the irreversibility of natural processes, and in many cases, the tendency of natural processes to lead towards spatial homogeneity of matter and energy, especially of temperature. It can be formulated in a variety of interesting and important ways. One of the simplest is the Clausius statement, that heat does not spontaneously pass from a colder to a hotter body.\n\nIt implies the existence of a quantity called the entropy of a thermodynamic system. In terms of this quantity it implies that\n\nWhen two initially isolated systems in separate but nearby regions of space, each in thermodynamic equilibrium with itself but not necessarily with each other, are then allowed to interact, they will eventually reach a mutual thermodynamic equilibrium. The sum of the entropies of the initially isolated systems is less than or equal to the total entropy of the final combination. Equality occurs just when the two original systems have all their respective intensive variables (temperature, pressure) equal; then the final system also has the same values.\n\nThe second law is applicable to a wide variety of processes, both reversible and irreversible. According to the second law, in a reversible heat transfer, an element of heat transferred, \n  \n    \n      \n        δ\n        Q\n      \n    \n    {\\displaystyle \\delta Q}\n  \n, is the product of the temperature (\n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n), both of the system and of the sources or destination of the heat, with the increment (\n  \n    \n      \n        d\n        S\n      \n    \n    {\\displaystyle dS}\n  \n) of the system's conjugate variable, its entropy (\n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n):[1]\n\nWhile reversible processes are a useful and convenient theoretical limiting case, all natural processes are irreversible. A prime example of this irreversibility is the transfer of heat by conduction or radiation. It was known long before the discovery of the notion of entropy that when two bodies, initially of different temperatures, come into direct thermal connection, then heat immediately and spontaneously flows from the hotter body to the colder one.\n\nEntropy may also be viewed as a physical measure concerning the microscopic details of the motion and configuration of a system, when only the macroscopic states are known. Such details are often referred to as disorder on a microscopic or molecular scale, and less often as dispersal of energy. For two given macroscopically specified states of a system, there is a mathematically defined quantity called the 'difference of information entropy between them'. This defines how much additional microscopic physical information is needed to specify one of the macroscopically specified states, given the macroscopic specification of the other – often a conveniently chosen reference state which may be presupposed to exist rather than explicitly stated. A final condition of a natural process always contains microscopically specifiable effects which are not fully and exactly predictable from the macroscopic specification of the initial condition of the process. This is why entropy increases in natural processes – the increase tells how much extra microscopic information is needed to distinguish the initial macroscopically specified state from the final macroscopically specified state.[14] Equivalently, in a thermodynamic process, energy spreads.\n\nThe third law of thermodynamics can be stated as:[2]\n\nA system's entropy approaches a constant value as its temperature approaches absolute zero.\n\nAt absolute zero temperature, the system is in the state with the minimum thermal energy, the ground state. The constant value (not necessarily zero) of entropy at this point is called the residual entropy of the system. With the exception of non-crystalline solids (e.g. glass) the residual entropy of a system is typically close to zero.[2] However, it reaches zero only when the system has a unique ground state (i.e., the state with the minimum thermal energy has only one configuration, or microstate). Microstates are used here to describe the probability of a system being in a specific state, as each microstate is assumed to have the same probability of occurring, so macroscopic states with fewer microstates are less probable. In general, entropy is related to the number of possible microstates according to the Boltzmann principle\n\nwhere S is the entropy of the system, kB is the Boltzmann constant, and Ω the number of microstates. At absolute zero there is only 1 microstate possible (Ω = 1 as all the atoms are identical for a pure substance, and as a result all orders are identical as there is only one combination) and \n  \n    \n      \n        ln\n        ⁡\n        (\n        1\n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\ln(1)=0}\n  \n.\n\nThe Onsager reciprocal relations have been considered the fourth law of thermodynamics.[15][16][17] They describe the relation between thermodynamic flows and forces in non-equilibrium thermodynamics, under the assumption that thermodynamic variables can be defined locally in a condition of local equilibrium. These relations are derived from statistical mechanics under the principle of microscopic reversibility (in the absence of external magnetic fields). Given a set of extensive parameters  Xi  (energy, mass, entropy, number of particles and so on) and thermodynamic forces  Fi  (related to their related intrinsic parameters, such as temperature and pressure), the Onsager theorem states that[16]\n\nwhere i, k = 1,2,3,... index every parameter and its related force, and",
        pageTitle: "Laws of thermodynamics",
    },
    {
        title: "Godwin's law",
        link: "https://en.wikipedia.org/wiki/Godwin%27s_law",
        content:
            'Godwin\'s law (or Godwin\'s rule), short for Godwin\'s law of Nazi analogies,[1] is an Internet adage asserting: "As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches one."[2]\n\nPromulgated by the American attorney and author Mike Godwin in 1990,[1] Godwin\'s law originally referred specifically to Usenet newsgroup discussions.[3] He stated that he introduced Godwin\'s law in 1990 as an experiment in memetics,[1] specifically to address the ubiquity of such comparisons which he believes regrettably trivialize the Holocaust.[4][5] Later, it was applied to any threaded online discussion, such as Internet forums, chat rooms, and social-media comment threads, as well as to speeches, articles, and other rhetoric[6][7] where reductio ad Hitlerum occurs.\n\nIn 2012, Godwin\'s law became an entry in the third edition of the Oxford English Dictionary.[8]\n\nGodwin\'s law can be applied mistakenly or abused as a distraction, a diversion, or even censorship, when miscasting an opponent\'s argument as hyperbole even when the comparison made by the argument is appropriate.[9] Godwin has criticized the over-application of the adage, claiming that it does not articulate a fallacy, but rather is intended to reduce the frequency of inappropriate and hyperbolic comparisons:[10]\n\nAlthough deliberately framed as if it were a law of nature or of mathematics, its purpose has always been rhetorical and pedagogical: I wanted folks who glibly compared someone else to Hitler to think a bit harder about the Holocaust.\n\nIn 2021, Harvard researchers published an article showing that the Nazi-comparison phenomenon does not occur with statistically meaningful frequency in Reddit discussions.[11][12]\n\nGodwin\'s law has many corollaries, some considered more canonical (by being adopted by Godwin himself)[2] than others. For example, many newsgroups and other Internet discussion forums have a tradition that, when a Nazi or Hitler comparison is made, the thread is finished and whoever made the comparison loses whatever debate is in progress.[13] This idea is itself sometimes mistakenly referred to as Godwin\'s law.[14]\n\nGodwin rejects the idea that whoever invokes Godwin\'s law has lost the argument, and suggests that, applied appropriately, the rule "should function less as a conversation ender and more as a conversation starter."[15] In an interview with Time Magazine, Godwin said that making comparisons to Hitler would actually be appropriate under the right circumstances:[16]\n\nI urge people to develop enough perspective to do it thoughtfully. If you think the comparison is valid, and you\'ve given it some thought, do it. All I ask you to do is think about the human beings capable of acting very badly. We have to keep the magnitude of those events in mind, and not be glib. Our society needs to be more humane, more civilized and to grow up.\n\nIn August 2017, while commenting on the Unite the Right rally in Charlottesville, Virginia, Godwin himself endorsed and encouraged social-media users to compare its "alt-right" participants to Nazis.[17][18]\n\nGodwin has denied the need to update or amend the rule. In June 2018, he wrote, in an opinion piece for the Los Angeles Times: "It still serves us as a tool to recognize specious comparisons to Nazism – but also, by contrast, to recognize comparisons that aren\'t."[15]\nAdditionally, when a potential subject of Godwin\'s law seems "intent on making the Hitler comparison",[19] the comparison with fascism may be appropriate rather than devaluing the argument; a "MAGA" corollary to the Law recognizes the pernicious embrace of Nazi-inspired tropes and phrases by the "alt-right".\n\nIn 2023, Godwin published an opinion in The Washington Post stating "Yes, it\'s okay to compare Trump to Hitler. Don\'t let me stop you."[20] In the article, Godwin says "But when people draw parallels between Donald Trump’s 2024 candidacy and Hitler’s progression from fringe figure to Great Dictator, we aren’t joking. Those of us who hope to preserve our democratic institutions need to underscore the resemblance before we enter the twilight of American democracy."[21]',
        pageTitle: "Godwin's law",
    },
    {
        title: "Gompertz–Makeham law of mortality",
        link: "https://en.wikipedia.org/wiki/Gompertz%E2%80%93Makeham_law_of_mortality",
        content:
            'The Gompertz–Makeham law states that the human death rate is the sum of an age-dependent component (the Gompertz function, named after Benjamin Gompertz),[1] which increases exponentially with age,[2] and an age-independent component (the Makeham term, named after William Makeham).[3] In a protected environment where external causes of death are rare (laboratory conditions, low mortality countries, etc.), the age-independent mortality component is often negligible. In this case the formula simplifies to a Gompertz law of mortality. In 1825, Benjamin Gompertz proposed an exponential increase in death rates with age.\n\nThe Gompertz–Makeham law of mortality describes the age dynamics of human mortality rather accurately in the age window from about 30 to 80 years of age. At more advanced ages, some studies have found that death rates increase more slowly – a phenomenon known as the late-life mortality deceleration[2] – but more recent studies disagree.[4]\n\nThe decline in the human mortality rate before the 1950s was mostly due to a decrease in the age-independent (Makeham) mortality component, while the age-dependent (Gompertz) mortality component was surprisingly stable.[2][5]  Since the 1950s, a new mortality trend has started in the form of an unexpected decline in mortality rates at advanced ages and "rectangularization" of the survival curve.[6][7]\n\nThe hazard function for the Gompertz-Makeham distribution is most often characterised as \n  \n    \n      \n        h\n        (\n        x\n        )\n        =\n        α\n        \n          e\n          \n            β\n            x\n          \n        \n        +\n        λ\n      \n    \n    {\\displaystyle h(x)=\\alpha e^{\\beta x}+\\lambda }\n  \n. The empirical magnitude of the beta-parameter is about .085, implying a doubling of mortality every .69/.085 = 8 years (Denmark, 2006).\n\nThe quantile function can be expressed in a closed-form expression using the Lambert W function:[8]\n\nThe Gompertz law is the same as a Fisher–Tippett distribution for the negative of age, restricted to negative values for the random variable (positive values for age).',
        pageTitle: "Gompertz–Makeham law of mortality",
    },
    {
        title: "Goodhart's law",
        link: "https://en.wikipedia.org/wiki/Goodhart%27s_law",
        content:
            "Goodhart's law is an adage often stated as, \"When a measure becomes a target, it ceases to be a good measure\".[1] It is named after British economist Charles Goodhart, who is credited with expressing the core idea of the adage in a 1975 article on monetary policy in the United Kingdom:[2]\n\nAny observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.[3]\n\nIt was used to criticize the British Thatcher government for trying to conduct monetary policy on the basis of targets for broad and narrow money,[4] but the law reflects a much more general phenomenon.[5]\n\nNumerous concepts are related to this idea, at least one of which predates Goodhart's statement.[6] Notably, Campbell's law likely has precedence, as Jeff Rodamar has argued, since various formulations date to 1969.[7] Other academics had similar insights at the time. Jerome Ravetz's 1971 book Scientific Knowledge and Its Social Problems[8] also predates Goodhart, though it does not formulate the same law. He discusses how systems in general can be gamed, focuses on cases where the goals of a task are complex, sophisticated, or subtle. In such cases, the persons possessing the skills to execute the tasks properly seek their own goals to the detriment of the assigned tasks. When the goals are instantiated as metrics, this could be seen as equivalent to Goodhart and Campbell's claim.\n\nShortly after Goodhart's publication, others suggested closely related ideas, including the Lucas critique (1976). As applied in economics, the law is also implicit in the idea of rational expectations, a theory in economics that states that those who are aware of a system of rewards and punishments will optimize their actions within that system to achieve their desired results. For example, if an employee is rewarded by the number of cars sold each month, they will try to sell more cars, even at a loss.\n\nWhile it originated in the context of market responses, the law has profound implications for the selection of high-level targets in organizations.[3] Jon Danielsson states the law as\n\nAny statistical relationship will break down when used for policy purposes.\n\nAnd suggested a corollary for use in financial risk modelling:\n\nA risk model breaks down when used for regulatory purposes.[9]\n\nMario Biagioli related the concept to consequences of using citation impact measures to estimate the importance of scientific publications:[10][11]\n\nAll metrics of scientific evaluation are bound to be abused. Goodhart's law [...] states that when a feature of the economy is picked as an indicator of the economy, then it inexorably ceases to function as that indicator because people start to game it.\n\nLater writers generalized Goodhart's point about monetary policy into a more general adage about measures and targets in accounting and evaluation systems. In a book chapter published in 1996, Keith Hoskin wrote:\n\n'Goodhart's Law' – That every measure which becomes a target becomes a bad measure – is inexorably, if ruefully, becoming recognized as one of the overriding laws of our times. Ruefully, for this law of the unintended consequence seems so inescapable. But it does so, I suggest, because it is the inevitable corollary of that invention of modernity: accountability.[12][full citation needed]\n\nIn a 1997 paper responding to the work of Hoskin and others on financial accounting and grades in education, anthropologist Marilyn Strathern expressed Goodhart's Law as \"When a measure becomes a target, it ceases to be a good measure\", and linked the sentiment to the history of accounting stretching back into Britain in the 1800s:\n\nWhen a measure becomes a target, it ceases to be a good measure. The more a 2.1 examination performance becomes an expectation, the poorer it becomes as a discriminator of individual performances. Hoskin describes this as 'Goodhart's law', after the latter's observation on instruments for monetary control which led to other devices for monetary flexibility having to be invented. However, targets that seem measurable become enticing tools for improvement. The linking of improvement to commensurable increase produced practices of wide application. It was that conflation of 'is' and 'ought', alongside the techniques of quantifiable written assessments, which led in Hoskin's view to the modernist invention of accountability. This was articulated in Britain for the first time around 1800 as 'the awful idea of accountability' (Ref. 3, p. 268).[1]",
        pageTitle: "Goodhart's law",
    },
    {
        title: "Gossen's laws",
        link: "https://en.wikipedia.org/wiki/Gossen%27s_laws",
        content:
            "Gossen's laws, named for Hermann Heinrich Gossen (1810–1858), are three laws of economics:\n\nThis economics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Gossen's laws",
    },
    {
        title: "Graham's law",
        link: "https://en.wikipedia.org/wiki/Graham%27s_law",
        content:
            "Graham's law of effusion (also called Graham's law of diffusion) was formulated by Scottish physical chemist Thomas Graham in 1848.[1] Graham found experimentally that the rate of effusion of a gas is inversely proportional to the square root of the molar mass of its particles.[1] This formula is stated as:\n\nGraham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight. Thus, if the molecular weight of one gas is four times that of another, it would diffuse through a porous plug or escape through a small pinhole in a vessel at half the rate of the other (heavier gases diffuse more slowly). A complete theoretical explanation of Graham's law was provided years later by the kinetic theory of gases. Graham's law provides a basis for separating isotopes by diffusion—a method that came to play a crucial role in the development of the atomic bomb.[2]\n\nGraham's law is most accurate for molecular effusion which involves the movement of one gas at a time through a hole. It is only approximate for diffusion of one gas in another or in air, as these processes involve the movement of more than one gas.[2]\n\nIn the same conditions of temperature and pressure, the molar mass is proportional to the mass density. Therefore, the rates of diffusion of different gases are inversely proportional to the square roots of their mass densities:\n\nFirst Example: Let gas 1 be H2 and gas 2 be O2. (This example is solving for the ratio between the rates of the two gases)\n\nTherefore, hydrogen molecules effuse four times faster than those of oxygen.[1]\n\nGraham's law can also be used to find the approximate molecular weight of a gas if one gas is a known species, and if there is a specific ratio between the rates of two gases (such as in the previous example).  The equation can be solved for the unknown molecular weight.\n\nGraham's law was the basis for separating uranium-235 from uranium-238 found in natural uraninite (uranium ore) during the Manhattan Project to build the first atomic bomb. The United States government built a gaseous diffusion plant at the Clinton Engineer Works in Oak Ridge, Tennessee, at the cost of $479 million (equivalent to $6.44 billion in 2023). In this plant, uranium from uranium ore was first converted to uranium hexafluoride and then forced repeatedly to diffuse through porous barriers, each time becoming a little more enriched in the slightly lighter uranium-235 isotope.[2]\n\nSecond Example: An unknown gas diffuses 0.25 times as fast as He. What is the molar mass of the unknown gas?\n\nUsing the formula of gaseous diffusion, we can set up this equation.\n\nWhich is the same as the following because the problem states that the rate of diffusion of the unknown gas relative to the helium gas is 0.25.\n\nGraham's research on the diffusion of gases was triggered by his reading about the observations of German chemist Johann Döbereiner that hydrogen gas diffused out of a small crack in a glass bottle faster than the surrounding air diffused in to replace it. Graham measured the rate of diffusion of gases through plaster plugs, through very fine tubes, and through small orifices. In this way he slowed down the process so that it could be studied quantitatively. He first stated in 1831 that the rate of effusion of a gas is inversely proportional to the square root of its density, and later in 1848 showed that this rate is inversely proportional to the square root of the molar mass.[1] Graham went on to study the diffusion of substances in solution and in the process made the discovery that some apparent solutions actually are suspensions of particles too large to pass through a parchment filter. He termed these materials colloids, a term that has come to denote an important class of finely divided materials.[3]\n\nAround the time Graham did his work, the concept of molecular weight was being established largely through the measurements of gases. Daniel Bernoulli suggested in 1738 in his book Hydrodynamica that heat increases in proportion to the velocity, and thus kinetic energy, of gas particles. Italian physicist Amedeo Avogadro also suggested in 1811 that equal volumes of different gases contain equal numbers of molecules. Thus, the relative molecular weights of two gases are equal to the ratio of weights of equal volumes of the gases. Avogadro's insight together with other studies of gas behaviour provided a basis for later theoretical work by Scottish physicist James Clerk Maxwell to explain the properties of gases as collections of small particles moving through largely empty space.[4]\n\nPerhaps the greatest success of the kinetic theory of gases, as it came to be called, was the discovery that for gases, the temperature as measured on the Kelvin (absolute) temperature scale is directly proportional to the average kinetic energy of the gas molecules. Graham's law for diffusion could thus be understood as a consequence of the molecular kinetic energies being equal at the same temperature.[5]\n\nThe rationale of the above can be summed up as follows:\n\nKinetic energy of each type of particle (in this example, Hydrogen and Oxygen, as above) within the system is equal, as defined by thermodynamic temperature:\n\nErgo, when constraining the system to the passage of particles through an area, Graham's law appears as written at the start of this article.",
        pageTitle: "Graham's law",
    },
    {
        title: "Grassmann's law",
        link: "https://en.wikipedia.org/wiki/Grassmann%27s_law",
        content:
            "Grassmann's law, named after its discoverer Hermann Grassmann, is a dissimilatory phonological process in Ancient Greek and Sanskrit which states that if an aspirated consonant is followed by another aspirated consonant in the next syllable, the first one loses the aspiration. The descriptive version was given for Sanskrit by Pāṇini.\n\nHere are some examples in Greek of the effects of Grassmann's law:\n\nIn reduplication, which forms the perfect tense in both Greek and Sanskrit, if the initial consonant is aspirated, the prepended consonant is unaspirated by Grassmann's law. For instance /pʰy-ɔː/ φύω 'I grow' : /pe-pʰyː-ka/ πέφυκα 'I have grown'.\n\nThe fact that deaspiration in Greek took place after the change of Proto-Indo-European *bʰ, *dʰ, *gʰ to /pʰ, tʰ, kʰ/ (PIE *bʰn̥ǵʰús > παχύς (pakhús) not bakhús but Sanskrit बहु (bahú)) and the fact that all other Indo-European languages do not apply Grassmann's law both suggest that it was developed separately in Greek and Sanskrit (although quite possibly by areal influence spread across a then-contiguous Graeco-Aryan–speaking area) and so it was not inherited from Proto-Indo-European.[1]\n\nAlso, Grassmann's law in Greek also affects the aspirate /h-/ < *s- developed specifically in Greek but not in Sanskrit or most other Indo-European. (For example, *ségʰō > *hekʰō > ἔχω /ékʰɔː/ \"I have\", with dissimilation of *h...kʰ, but the future tense *ségʰ-sō > ἕξω /hék-sɔː/ \"I will have\" was unaffected, as aspiration was lost before /s/.) The evidence from other languages is not strictly negative: many branches, including Sanskrit's closest relative, Iranian, merge the Proto-Indo-European voiced aspirated and unaspirated stops and so it is not possible to tell if Grassmann's law ever operated in them.\n\nAccording to Filip De Decker,[2] Grassmann's law had not operated in Mycenaean Greek yet, and it is almost certain that it occurred later than 1200 BC; it might even postdate the Homeric Greek period.\n\nIn Koine Greek, in cases other than reduplication, alternations involving labials and velars have been completely levelled, and Grassmann's law remains in effect only for the alternation between /t/ and /tʰ/, as in the last two examples above. (It makes no difference whether the /tʰ/ in question continues Proto-Indo-European *dʰ or *ɡʷʰ.)\n\nThus, alongside the pair ταχύς /takʰýs/ 'fast' : θάσσων /tʰássɔːn/ 'faster', displaying Grassmann's law, Greek has the pair παχύς /pakʰýs/ 'thick' : πάσσων /pássɔːn/ 'thicker' from the Proto-Indo-European etymon *bʰn̻ɡʰ- (established by cognate forms like Sanskrit बहु /bahú-/ 'abundant' since *bʰ is the only point of intersection between Greek /p/ and Sanskrit /b/) in which the /p/ in the comparative is a result of levelling. Similarly, πεύθομαι /peútʰomai/ ~ πυνθάνομαι /pyntʰánomai/ 'come to know' from PIE *bʰeudʰ- has the future πεύσομαι /peúsomai/. However, only /tʰ/ dissimilates before aspirated affixes like the aorist passive in /-tʰɛː/ and the imperative in /-tʰi/; /pʰ/ and /kʰ/ do not, as in φάθι /pʰátʰi/ 'speak!'.\n\nCases like /tʰrík-s/ ~ /tríkʰ-es/ and /tʰáp-sai/ ~ /tapʰ-êːn/ illustrate the phenomenon of diaspirate roots for which two different analyses have been given.\n\nIn one account, the underlying diaspirate theory, the underlying roots are taken to be /tʰrikʰ/ and /tʰapʰ/.  When an /s/, a word edge, or various other sounds immediately follow, the second aspiration is lost, and the first aspirate therefore survives (/tʰrík-s/, /tʰáp-sai/). If a vowel follows the second aspirate, the second aspirate survives unaltered, and the first aspiration is thus lost by Grassmann's law (/tríkʰ-es/, /tápʰ-os/).\n\nA different analytical approach was taken by the Indian grammarians. They took the roots to be underlying /trikʰ/ and /tapʰ/. The roots persist unaltered in /tríkʰ-es/ and /tapʰ-êːn/. If an /s/ follows, it triggers an aspiration throwback and the aspiration migrates leftward, docking onto the initial consonant (/tʰrík-s/, /tʰáp-sai/).\n\nIn his initial formulation of the law, Grassmann briefly referred to aspiration throwback to explain the seemingly aberrant forms. However, the consensus among contemporary historical linguists is that the former explanation (underlying representation) is the correct one, as aspiration throwback would require multiple root shapes for the same basic root in different languages whenever an aspirate follows in the next syllable (*d for Sanskrit, *t for Greek, *dʰ for Proto-Germanic and Proto-Italic which have no dissimilation), but the underlying diaspirate allows for a single root shape, with *dʰ for all languages.\n\nIn the later course of Sanskrit, under the influence of the grammarians, aspiration throwback was applied to original mono-aspirate roots by analogy. Thus, from the verb root गाह /ɡaːh-/ ('to plunge'), the desiderative stem जिघाख /dʑi-ɡʱaːkʰa-/ is formed by analogy with the forms बुभुत्सा /bu-bʱutsaː-/ (a desiderative form) and भुत /bʱut-/ (a nominal form, both from the root बुध /budʱ-/ 'to be awake', originally Proto-Indo-European *bʰudʰ-).\n\nThe linguist Ivan Sag has pointed out an advantage of the ancient Indian theory: it explains why there are no patterns like hypothetical */trík-s/ ~ */tríkʰ-es/, which are not ruled out by the underlying diaspirate theory. However, aspiration fails to account for reduplication patterns in roots with initial aspirates, such as Greek /tí-tʰɛːmi/ 'I put', with an unaspirated reduplicated consonant. Aspiration throwback thus needs to be enhanced with a stipulation that aspirates reduplicate as their unaspirated counterparts. From a diachronic standpoint, the absence of these patterns in Greek is explained by the Proto-Indo-European constraint against roots of the form *T...Dʰ-.\n\nProcesses similar to Grassmann's law continue to work in Middle Indo-Aryan, although it tends to be inconsistent regarding direction, for example Sanskrit स्कन्ध (skandha) → khandha → Assamese কান্ধ (kandh), but भ्रष्ट (bhraṣṭa) → bhaṭṭha → ভাটা (bhata).[3]\n\nA process similar to Grassmann's law is also known to occur in Ofo, an extinct and underdocumented Siouan language. The law is found in compounds such as the following:\n\nA similar phenomenon occurs in Meitei (a Tibeto-Burman language) in which an aspirated consonant is deaspirated if preceded by an aspirated consonant (including /h/, /s/) in the previous syllable. The deaspirated consonants are then voiced between sonorants.\n\nHadza, spoken in Northern Tanzania, exhibits Grassmann's law in its lexicon, but most obviously in reduplication:\n\nA similar effect takes place in Koti and other Makhuwa languages, where it was dubbed Katupha's law in Schadeberg (1999).  If two aspirated consonants are brought together in one stem, the first loses its aspiration. The effect is particularly clear in reduplicated words: kopikophi 'eyelash'; piriphiri 'pepper' (cf. Swahili 'piripiri'); okukuttha 'to wipe'.  This is slightly different from in Greek and Sanskrit, in that the two syllables need not be adjacent.\n\nThe four Salishan languages Salish–Spokane–Kalispel, Okanagan, Shuswap and Tillamook exhibit a similar process affecting ejective rather than aspirated consonants, which has been called \"Grassmann's law for Salish\", for example Shuswap underlying /x-tʼək-tʼəkʔ-éχn/ 'crutches' → surface /xtəktʼəkʔéχn/.[4]",
        pageTitle: "Grassmann's law",
    },
    {
        title: "Grassmann's law (optics)",
        link: "https://en.wikipedia.org/wiki/Grassmann%27s_law_(optics)",
        content:
            "Grassmann's laws describe empirical results about how the perception of mixtures of colored lights (i.e., lights that co-stimulate the same area on the retina) composed of different spectral power distributions can be algebraically related to one another in a color matching context. Discovered by Hermann Grassmann[1] these \"laws\" are actually principles used to predict color match responses to a good approximation under photopic and mesopic vision.  A number of studies have examined how and why they provide poor predictions under specific conditions.[2][3]\n\nThe four laws are described in modern texts[5] with varying degrees of algebraic notation and are summarized as follows (the precise numbering and corollary definitions can vary across sources[6]):\n\nThese laws entail an algebraic representation of colored light.[7] Assuming beam 1 and 2 each have a color, and the observer chooses \n  \n    \n      \n        (\n        \n          R\n          \n            1\n          \n        \n        ,\n        \n          G\n          \n            1\n          \n        \n        ,\n        \n          B\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (R_{1},G_{1},B_{1})}\n  \n as the strengths of the primaries that match beam 1 and \n  \n    \n      \n        (\n        \n          R\n          \n            2\n          \n        \n        ,\n        \n          G\n          \n            2\n          \n        \n        ,\n        \n          B\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (R_{2},G_{2},B_{2})}\n  \n as the strengths of the primaries that match beam 2, then if the two beams were combined, the matching values will be the sums of the components. Precisely, they will be \n  \n    \n      \n        (\n        R\n        ,\n        G\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle (R,G,B)}\n  \n, where:\n\nGrassmann's laws can be expressed in general form by stating that for a given color with a spectral power distribution \n  \n    \n      \n        I\n        (\n        λ\n        )\n      \n    \n    {\\displaystyle I(\\lambda )}\n  \n the RGB coordinates are given by:\n\nObserve that these are linear in \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n; the functions \n  \n    \n      \n        \n          \n            \n              r\n              ¯\n            \n          \n        \n        (\n        λ\n        )\n        ,\n        \n          \n            \n              g\n              ¯\n            \n          \n        \n        (\n        λ\n        )\n        ,\n        \n          \n            \n              b\n              ¯\n            \n          \n        \n        (\n        λ\n        )\n      \n    \n    {\\displaystyle {\\bar {r}}(\\lambda ),{\\bar {g}}(\\lambda ),{\\bar {b}}(\\lambda )}\n  \n are the color matching functions with respect to the chosen primaries.",
        pageTitle: "Grassmann's laws (color science)",
    },
    {
        title: "Gresham's law",
        link: "https://en.wikipedia.org/wiki/Gresham%27s_law",
        content:
            'In economics, Gresham\'s law is a monetary principle stating that "bad money drives out good". For example, if there are two forms of commodity money in circulation, which are accepted by law as having similar face value, the more valuable commodity will gradually disappear from circulation.[1][2]\n\nThe law was named in 1857 by economist Henry Dunning Macleod after Sir Thomas Gresham (1519–1579), an English financier during the Tudor dynasty.[3] Gresham had urged Queen Elizabeth to restore confidence in then-debased English currency.\n\nThe concept was thoroughly defined in Renaissance Europe by Nicolaus Copernicus and known centuries earlier in classical Antiquity, the Near East, and China.\n\nUnder Gresham\'s law, "good money" is money that shows little difference between its nominal value (the face value of the coin) and its commodity value (the value of the metal of which it is made, often precious metals, such as gold or silver).[4]\n\nThe price spread between face value and commodity value when it is minted is called seigniorage. As some coins do not circulate, remaining in the possession of coin collectors, this can increase demand for coinage.\n\nOn the other hand, "bad money" is money that has a commodity value considerably lower than its face value and is in circulation along with good money, where both forms are required to be accepted at equal value as legal tender.\n\nIn Gresham\'s day, bad money included any coin that had been debased. Debasement was often done by the issuing body, where less than the officially specified amount of precious metal was contained in an issue of coinage, usually by alloying it with a base metal. The public could also debase coins, usually by clipping or scraping off small portions of the precious metal, also known as "stemming" (reeded edges on coins were intended to make clipping evident). Other examples of bad money include counterfeit coins made from base metal. Today virtually all circulating coins are made from base metals, often the cheapest available, durable base metal; collectively these monies are known as fiat money. While virtually all contemporary coinage is composed solely of base metals, there have been periods during the 21st century in which the market value of some base metals, like copper, have been high enough that at least one common coin (the U.S. nickel) still maintained "good money" status.[4]\n\nIn the case of clipped, scraped, or counterfeit coins, the commodity value was reduced by fraud.  The face value remains at the previous higher level. On the other hand, with a coinage debased by a government issuer, the commodity value of the coinage was often reduced quite openly, while the face value of the debased coins was held at the higher level by legal tender laws.\n\nThe old saying, "a bad penny always turns up" is a colloquial recognition of Gresham\'s law.\n\nThe law states that any circulating currency consisting of both "good" and "bad" money (both forms required to be accepted at equal value under legal tender law) quickly becomes dominated by the "bad" money. This is because people spending money will hand over the "bad" coins rather than the "good" ones, keeping the "good" ones for themselves. Legal tender laws act as a form of price control. In such a case, the intrinsically less valuable money is preferred in exchange, because people prefer to save the intrinsically more valuable money.\n\nImagine that a customer with several silver sixpence coins purchases an item which costs five pence. Some of the customer\'s coins are more debased, while others are less so – but legally, they are all mandated to be of equal value. The customer would prefer to retain the better coins, and so offers the shopkeeper the most debased one. In turn, the shopkeeper must give one penny in change, and has every reason to give the most debased penny. Thus, the coins that circulate in the transaction will tend to be of the most debased sort available to the parties.\n\nIf "good" coins have a face value below that of their metallic content, individuals may be motivated to melt them down and sell the metal for its higher intrinsic value, even if such destruction is illegal. The 1965 United States half-dollar coins contained 40% silver; in previous years these coins were 90% silver (.900, or one nine fine). With the release of the 1965 half-dollar, which was legally required to be accepted at the same value as the earlier 90% halves, the older 90% silver coinage quickly disappeared from circulation, while the newer debased coins remained in use.[citation needed] As the value of the dollar (Federal Reserve notes) continued to decline, resulting in the value of the silver content exceeding the face value of the coins, many of the older half dollars were melted down[citation needed] or removed from circulation and into private collections and hoards. Beginning in 1971, the U.S. government abandoned including any silver in half dollars.  The metal value of the 40% silver coins began to exceed their face value, which resulted in a repeat of the previous event.  The 40% silver coins also began to vanish from circulation and into coin hoards.\n\nA similar situation occurred in 2007 in the United States with the rising price of copper, zinc, and nickel, which led the U.S. government to ban the melting or mass exportation of one-cent and five-cent coins.[5]\n\nIn addition to being melted down for its bullion value, money that is considered to be "good" tends to leave an economy through international trade. International traders are not bound by legal tender laws as citizens of the issuing country are, so they will offer higher value for good coins than bad ones. The good coins may leave their country of origin to become part of international trade, escaping that country\'s legal tender laws and leaving the "bad" money behind. This occurred in Britain during the period of adoption of the gold standard: In 1717 Isaac Newton, then Master of the Mint, declared the gold guinea to be worth 21 silver shillings. This overvalued the gold guinea in Britain, making it "bad", and encouraged people to send "good" silver shillings abroad, where it could buy more gold than at home. This gold was then minted as currency, which bought silver shillings, which were sent abroad for gold, and so on. For a century hardly any silver coins were minted in Britain, and Britain moved onto a de facto gold standard.[6]\n\nAustrian economist Hans-Hermann Hoppe said that "so-called Gresham\'s law" only applies under certain conditions, largely a result of governmental interventionist policies. In his 2021 book, Economy, Society, and History Hoppe states:\n\nYou might have heard about the so-called Gresham\'s law, which states that bad money drives out good money, but this law only holds if there are price controls in effect, only if the exchange ratios of different monies are fixed and no longer reflect market forces. Is it the case that bad money drives out good money under normal circumstances without any interference? No, for money holds to exactly the same law that holds for every other good. Good goods drive out bad goods. Good money drives out bad money, so this bezant was for something like 800 years considered to be the best money available and was preferred by merchants from India to Rome to the Baltic Sea.[7]\n\nGresham was not the first to state the law which took his name. The phenomenon had been noted by Aristophanes in his play The Frogs, which dates from around the end of the 5th century BC. The referenced passage from The Frogs is as follows (usually dated at 405 BC):[8]\n\nIt has often struck our notice that the course our city runs\nIs the same towards men and money. She has true and worthy sons:\nShe has good and ancient silver, she has good and recent gold.\nThese are coins untouched with alloys; everywhere their fame is told;\nNot all Hellas holds their equal, not all Barbary far and near.\nGold or silver, each well minted, tested each and ringing clear.\nYet, we never use them! Others always pass from hand to hand.\nSorry brass just struck last week and branded with a wretched brand.\nSo with men we know for upright, blameless lives and noble names.\nTrained in music and palaestra, freemen\'s choirs and freemen\'s games,\nThese we spurn for men of brass...\n\nAccording to Ben Tamari, the currency devaluation phenomenon was already recognized in ancient sources.[9] He brings some examples which include the Machpela Cave transaction[10] and the building of the Temple[11] from the Bible and the Mishna in tractate Bava Metzia (Bava Metzia 4:1) from the Talmud.[9]\n\nIn China, Yuan dynasty economic authors Yeh Shih and Yuan Hsieh (c. 1223) were aware of the same phenomenon.[12]\n\nIbn Taimiyyah (1263–1328) described the phenomenon as follows:\n\nIf the ruler cancels the use of a certain coin and mints another kind of money for the people, he will spoil the riches (amwal) which they possess, by decreasing their value as the old coins will now become merely a commodity. He will do injustice to them by depriving them of the higher values originally owned by them. Moreover, if the intrinsic values of coins are different it will become a source of profit for the wicked to collect the small (bad) coins and exchange them (for good money) and then they will take them to another country and shift the small (bad) money of that country (to this country). So (the value of) people\'s goods will be damaged.\n\nNotably this passage mentions only the flight of good money abroad and says nothing of its disappearance due to hoarding or melting.[13] Palestinian economist Adel Zagha also attributes a similar concept to medieval Islamic thinker Al-Maqrizi, who offered, claims Zagha, a close approximation to what would become known as Gresham\'s law centuries later.[14]\n\nIn the 14th century it was noted by Nicole Oresme c. 1350,[15][full citation needed] in his treatise On the Origin, Nature, Law, and Alterations of Money,[16] and by jurist and historian Al-Maqrizi (1364–1442) in the Mamluk Empire.[17]\n\nJohannes de Strigys, an agent of Ludovico III Gonzaga, Marquis of Mantua in Venice, wrote in a June 1472 report che la cativa cazarà via la bona ("that the bad money will chase out the good").[18]\n\nIn the year that Gresham was born, 1519, it was described by Nicolaus Copernicus in a treatise called Monetae cudendae ratio: "bad (debased) coinage drives good (un-debased) coinage out of circulation". Copernicus was aware of the practice of exchanging bad coins for good ones and melting down the latter or sending them abroad, and he seems to have drawn up some notes on this subject while he was at Olsztyn in 1519. He made them the basis of a report which he presented to the Prussian Diet held in 1522, attending the session with his friend Tiedemann Giese to represent his chapter. Copernicus\'s Monetae cudendae ratio was an enlarged, Latin version of that report, setting forth a general theory of money for the 1528 diet. He also formulated a version of the quantity theory of money.[19] For this reason, it is occasionally known as the Gresham–Copernicus law.[20]\n\nSir Thomas Gresham, a 16th century financial agent of the English Crown in the city of Antwerp, was one in a long series of proponents of the law, which he did to explain to Queen Elizabeth I what was happening to the English shilling. Her father, Henry VIII, had replaced 40% of the silver in the coin with base metals, to increase the government\'s income without raising taxes. Astute English merchants and ordinary subjects saved the good shillings from pure silver and circulated the bad ones. Hence, the bad money would be used whenever possible, and the good coinage would be saved and disappear from circulation.\n\nAccording to the economist George Selgin in his paper "Gresham\'s Law":\n\nAs for Gresham himself, he observed "that good and bad coin cannot circulate together" in a letter written to Queen Elizabeth on the occasion of her accession in 1558. The statement was part of Gresham\'s explanation for the "unexampled state of badness" that England\'s coinage had been left in following the "Great Debasements" of Henry VIII and Edward VI, which reduced the metallic value of English silver coins to a small fraction of what it had been at the time of Henry VII. Owing to these debasements, Gresham observed to the Queen, that "all your fine gold was convayed out of this your realm".[21]\n\nGresham made his observations of good and bad money while in the service of Queen Elizabeth, with respect only to the observed poor quality of British coinage. Earlier monarchs, Henry VIII and Edward VI, had forced the people to accept debased coinage by means of legal tender laws. Gresham also made his comparison of good and bad money where the precious metal in the money was the same metal, but of different weight. He did not compare silver to gold, or gold to paper.\n\nIn his "Gresham\'s Law" article, Selgin also offers the following comments regarding the origin of the name:\n\nThe expression "Gresham\'s Law" dates back only to 1858, when British economist Henry Dunning Macleod (1858, pp. 476–8) decided to name the tendency for bad money to drive good money out of circulation after Sir Thomas Gresham (1519–1579). However, references to such a tendency, sometimes accompanied by discussion of conditions promoting it, occur in various medieval writings, most notably Nicholas Oresme\'s (c. 1357) Treatise on money. The concept can be traced to ancient works, including Aristophanes\' The Frogs, where the prevalence of bad politicians is attributed to forces similar to those favoring bad money over good.[21]\n\nThe experiences of dollarization in countries with weak economies and currencies (such as Israel in the 1980s, Eastern Europe and countries in the period immediately after the collapse of the Soviet bloc, or Ecuador throughout the late 20th and early 21st century) may be seen as Gresham\'s law operating in its reverse form (Guidotti & Rodriguez, 1992) because in general, the dollar has not been legal tender in such situations, and in some cases, its use has been illegal.[22]\n\nAdam Fergusson and Costantino Bresciani-Turroni (in his book Le vicende del marco tedesco, published in 1931) pointed out that, during the great inflation in the Weimar Republic in 1923,[dubious – discuss] as the official money became so worthless that virtually nobody would take it, people simply stopped accepting the currency in exchange for goods. That was particularly serious because farmers began to hoard food. Accordingly, any currency backed by any sort of value became a circulating medium of exchange.[23] In 2009, hyperinflation in Zimbabwe began to show similar characteristics.\n\nThose examples show that in the absence of effective legal tender laws, Gresham\'s law works in reverse. If given the choice of what money to accept, people will accept the money they believe to be of highest long-term value, and not accept what they believe to be of low long-term value. If not given the choice and required to accept all money, good and bad, they will tend to keep the money of greater perceived value in their own possession and pass the bad money to others.\n\nIn short, in the absence of legal tender laws, the seller will not accept anything but money of certain value (good money), but the existence of legal tender laws will cause the buyer to offer only money with the lowest commodity value (bad money), as the creditor must accept such money at face value.[24][unreliable source?]\n\nNobel Prize winner Robert Mundell believes that Gresham\'s law could be more accurately rendered, taking care of the reverse, if it were expressed as: "Bad money drives out good if they exchange for the same price."[25]\n\nThe reverse of Gresham\'s law, that good money drives out bad money whenever the bad money becomes nearly worthless, has been named "Thiers\' law" by economist Peter Bernholz in honor of French politician and historian Adolphe Thiers.[26] "Thiers\' Law will only operate later [in the inflation] when the increase of the new flexible exchange rate and of the rate of inflation lower the real demand for the inflating money."[27]\n\nThe principles of Gresham\'s law can sometimes be applied to different fields of study. Gresham\'s law may be generally applied to any circumstance in which the true value of something is markedly different from the value people are required to accept, due to factors such as lack of information or governmental decree.\n\nVice President Spiro Agnew used Gresham\'s law in describing American news media, stating that "Bad news drives out good news", although his argument was closer to that of a race to the bottom for higher ratings rather than over- and under-valuing certain kinds of news.[28]\n\nGregory Bateson postulated an analogue to Gresham\'s law operating in cultural evolution, in which "the oversimplified ideas will always displace the sophisticated and the vulgar and hateful will always displace the beautiful. And yet the beautiful persists."[29]\n\nCory Doctorow wrote that a similar effect to Gresham\'s law occurred in carbon offset trading. The alleged information asymmetry is that people find it difficult to distinguish just how effective credits purchased are, but can easily tell the price. As a result, cheap credits that are ineffective can displace expensive but worthwhile carbon credits.[30] The example given was The Nature Conservancy offering cheap, yet "meaningless", carbon credits by purchasing cheap land unlikely to be logged anyway, rather than expensive and valuable land at risk of logging.[31]\n\nA corollary, Hughes\' law, exists in moral philosophy, stating that, "The evil acts of bad men elicit from better men acts which, under better circumstances, would also be called evil."[32]\n\nIn the market for used cars, lemon automobiles (analogous to bad currency) will drive out the good cars.[33] The problem is one of asymmetry of information. Sellers have a strong financial incentive to pass all used cars off as good cars, especially lemons. This makes it difficult to buy a good car at a fair price, as the buyer risks overpaying for a lemon. The result is that buyers will only pay the fair price of a lemon, so at least they reduce the risk of overpaying. High-quality cars tend to be pushed out of the market, because there is no good way to establish that they really are worth more. Certified pre-owned programs are an attempt to mitigate this problem by providing a warranty and other guarantees of quality. The Market for Lemons is a work that examines this problem in more detail.',
        pageTitle: "Gresham's law",
    },
    {
        title: "Grimm's law",
        link: "https://en.wikipedia.org/wiki/Grimm%27s_law",
        content:
            "Grimm's law, also known as the First Germanic Sound Shift, is a set of sound laws describing the Proto-Indo-European (PIE) stop consonants as they developed in Proto-Germanic in the first millennium BC, first discovered by Rasmus Rask but systematically put forward by Jacob Grimm.[1] It establishes a set of regular correspondences between early Germanic stops and fricatives and stop consonants of certain other Indo-European languages.\n\nGrimm's law was the first discovered systematic sound change, creating historical phonology as a historical linguistics discipline. Friedrich von Schlegel first noted the correspondence between Latin p and Germanic f in 1806. In 1818, Rasmus Rask extended the correspondences to other Indo-European languages, such as Sanskrit and Greek, and to the full range of consonants involved. In 1822, Jacob Grimm put forth the rule in his book Deutsche Grammatik and extended it to include standard German. He noticed that many words had consonants different from what his law predicted. These exceptions defied linguists for several decades, until Danish linguist Karl Verner explained them in Verner's law.\n\nGrimm's law consists of three parts, forming consecutive phases in the sense of a chain shift.[2] The phases are usually constructed as follows:\n\nThis chain shift (in the order 3, 2, 1) can be abstractly represented as:\n\nHere each sound moves one position to the right to take on its new sound value. Within Proto-Germanic, the sounds denoted by ⟨b⟩, ⟨d⟩, ⟨g⟩ and ⟨gw⟩ were stops in some environments and fricatives in others, so bʰ → b indicates bʰ → b/β, and likewise for the others. The voiceless fricatives are customarily spelled ⟨f⟩, ⟨þ⟩, ⟨h⟩ and ⟨hw⟩ in the context of Germanic.\n\nThe exact details of the shift are unknown, and it may have progressed in a variety of ways before arriving at the final situation. The three stages listed above show the progression of a \"pull chain\", in which each change leaves a \"gap\" in the phonological system that \"pulls\" other phonemes into it to fill the gap. Alternatively, the shift may have occurred as a “push chain”, where the sounds changed in reverse order, with each change \"pushing\" the next forward to avoid merging the phonemes.\n\nThe steps could also have occurred somewhat differently. Another possible sequence of events could have been:\n\nThis sequence would lead to the same result. This variety of Grimm's law is often suggested in the context of Proto-Indo-European glottalic theory, which is followed by a minority of linguists. This theoretical framework assumes that PIE \"voiced stops\" were actually voiceless to begin with, so that the second phase did not actually exist as such, or was not actually devoicing but was losing some other articulatory feature like glottalization or ejectiveness. This alternative sequence also accounts for Verner's law phonetics (see below), which are easier to explain within the glottalic theory framework when Grimm's law is formulated in this manner. Additionally, aspirated stops are known to have changed to fricatives when transiting between Proto-Indo-European and Proto-Italic, so representing a plausible potential change from Proto-Indo-European to Proto-Germanic.\n\nOnce the sounds described by Grimm's law had changed, only one type of voiced consonant was left, with no distinction between voiced stops and voiced fricatives. They eventually became stops at the start of a word (for the most part), as well as after a nasal consonant, but fricatives elsewhere. Whether they were plosives or fricatives at first is therefore not clear. The voiced aspirated stops may have first become voiced fricatives, before becoming stops under certain conditions. But they may also have become stops at first, then become fricatives in most positions later.\n\nAround the same time as the Grimm's law sounds shifted, another change occurred known as Verner's law. Verner's law caused the voiceless fricatives that resulted from the Grimm's law changes to become voiced under certain conditions, creating apparent exceptions to the rule. For example:\n\nHere, the same sound *t appears as *þ /θ/ in one word (following Grimm's law), but as *d /ð/ in another (apparently violating Grimm's law). See the Verner's law article for a more detailed explanation of this discrepancy.\n\nThe early Germanic *gw that had arisen from Proto-Indo-European *gʷʰ (and from *kʷ through Verner's law) further changed with various sorts:\n\nPerhaps the usual reflex was *b (as suggested by the connection of bid < *bidjaną and Old Irish guidid), but *w appears in certain cases (possibly through dissimilation when another labial consonant followed?) like warm and wife (provided that the proposed explanations are correct). Proto-Germanic *hw voiced by Verner's law fell together with this sound and developed identically, compare the words for 'she-wolf': from Middle High German wülbe[citation needed] and Old Norse ylgr, one can reconstruct Proto-Germanic nominative singular *wulbī, genitive singular *wulgijōz, from earlier *wulgwī, *wulgwijōz.[3][failed verification]\n\nFurther changes following Grimm's law, as well as sound changes in other Indo-European languages, can occasionally obscure the law's effects. The most illustrative examples are used here.\n\nThis process appears strikingly regular.  Each phase involves one single change which applies equally to the labials (p, b, bʰ, f) and their equivalent dentals (t, d, dʰ, þ), velars (k, g, gʰ, h) and rounded velars (kʷ, gʷ, gʷʰ, hʷ).  The first phase left the phoneme repertoire of the language without voiceless stops, the second phase filled this gap, but created a new one, and so on until the chain had run its course.\n\nWhen two obstruents occurred in a pair, the first was changed according to Grimm's law, if possible, while the second was not. If either of the two was voiceless, the whole cluster was devoiced, and the first obstruent also lost its labialisation, if it was present.\n\nMost examples of this occurred with obstruents preceded by *s (resulting in *sp, *st, *sk, *skʷ), or obstruents followed by *t (giving *ft, *ss, *ht, *ht) or *s (giving *fs, *ss, *hs, *hs). The latter change was frequent in suffixes, and became a phonotactic restriction known as the Germanic spirant law. This rule remained productive throughout the Proto-Germanic period. The cluster *tt became *ss (as in many Indo-European daughter languages), but this was often restored analogically to *st later on.\n\nThe Germanic \"sound laws\", combined with regular changes reconstructed for other Indo-European languages, allow one to define the expected sound correspondences between different branches of the family. For example, Germanic (word-initial) *b- corresponds regularly to Latin *f-, Greek pʰ-, Sanskrit bʰ-, Slavic, Baltic or Celtic b-, etc., while Germanic *f- corresponds to Latin, Greek, Sanskrit, Slavic and Baltic p- and to zero (no initial consonant) in Celtic. The former set goes back to PIE *bʰ- (faithfully reflected in Sanskrit and modified in various ways elsewhere), and the latter set to PIE *p- (shifted in Germanic, lost in Celtic, but preserved in the other groups mentioned here).\n\nOne of the more conspicuous present surface correspondences is the English digraph wh and the corresponding Latin and Romance digraph qu, notably found in interrogative words (wh-words) such as the five Ws. These both come from kʷ. The present pronunciations have further changed, like many English varieties reducing the wh-cluster, though the spellings reflect the history more; see English interrogative words: Etymology for details.",
        pageTitle: "Grimm's law",
    },
    {
        title: "Grosch's law",
        link: "https://en.wikipedia.org/wiki/Grosch%27s_law",
        content:
            "Grosch's law is an assertion made by Herb Grosch in 1953:[1]\n\nI believe that there is a fundamental rule, which I modestly call Grosch's law, giving added economy only as the square root of the increase in speed — that is, to do a calculation ten times as cheaply you must do it hundred times as fast.\n\nComputer performance increases as the square of the cost. If computer A costs twice as much as computer B, you should expect computer A to be four times as fast as computer B.[2]\n\nTwo years before Grosch's statement, Seymour Cray was quoted in Business Week (August 1963) expressing this very same thought:\n\nComputers should obey a square law — when the price doubles, you should get at least four times as much speed.[3]\n\nThe law can also be interpreted as meaning that computers present economies of scale: the more costly is the computer, the price–performance ratio linearly becomes better. This implies that low-cost computers cannot compete in the market.\n\nAn analysis of rental cost/performance data for computers between 1951 and 1963 by Kenneth E. Knight found that Grosch's law held for commercial and scientific operations[4] (a modern analysis of the same data found that Grosch's law only applied to commercial operations[5]).  In a separate study, Knight found that Grosch's law did not apply to computers between 1963-1967[6] (also confirmed by the aforementioned modern analysis[5]).",
        pageTitle: "Grosch's law",
    },
    {
        title: "Grotthuss–Draper law",
        link: "https://en.wikipedia.org/wiki/Grotthuss%E2%80%93Draper_law",
        content:
            "Photoelectrochemical processes are processes in photoelectrochemistry; they usually involve transforming light into other forms of energy.[1] These processes apply to photochemistry, optically pumped lasers, sensitized solar cells, luminescence, and photochromism.\n\nElectron excitation is the movement of an electron to a higher energy state. This can either be done by photoexcitation (PE), where the original electron absorbs the photon and gains all the photon's energy or by electrical excitation (EE), where the original electron absorbs the energy of another, energetic electron. Within a semiconductor crystal lattice, thermal excitation is a process where lattice vibrations provide enough energy to move electrons to a higher energy band. When an excited electron falls back to a lower energy state again, it is called electron relaxation. This can be done by radiation of a photon or giving the energy to a third spectator particle as well.[2]\n\nIn physics there is a specific technical definition for energy level which is often associated with an atom being excited to an excited state. The excited state, in general, is in relation to the ground state, where the excited state is at a higher energy level than the ground state.\n\nPhotoexcitation is the mechanism of electron excitation by photon absorption, when the energy of the photon is too low to cause photoionization. The absorption of the photon takes place in accordance with Planck's quantum theory.\n\nPhotoexcitation plays role in photoisomerization. Photoexcitation is exploited in dye-sensitized solar cells, photochemistry, luminescence, optically pumped lasers, and in some photochromic applications.\n\nIn chemistry, photoisomerization is molecular behavior in which structural change between isomers is caused by photoexcitation. Both reversible and irreversible photoisomerization reactions exist. However, the word \"photoisomerization\" usually indicates a reversible process. Photoisomerizable molecules are already put to practical use, for instance, in pigments for rewritable CDs, DVDs, and 3D optical data storage solutions. In addition, recent interest in photoisomerizable molecules has been aimed at molecular devices, such as molecular switches,[3] molecular motors,[4] and molecular electronics.\n\nPhotoisomerization behavior can be roughly categorized into several classes. Two major classes are trans-cis (or 'E-'Z) conversion, and open-closed ring transition. Examples of the former include stilbene and azobenzene. This type of compounds has a double bond, and rotation or inversion around the double bond affords isomerization between the two states. Examples of the latter include fulgide and diarylethene. This type of compounds undergoes bond cleavage and bond creation upon irradiation with particular wavelengths of light. Still another class is the di-π-methane rearrangement.\n\nPhotoionization is the physical process in which an incident photon ejects one or more electrons from an atom, ion or molecule. This is essentially the same process that occurs with the photoelectric effect with metals. In the case of a gas or single atoms, the term photoionization is more common.[5]\n\nThe ejected electrons, known as photoelectrons, carry information about their pre-ionized states. For example, a single electron can have a kinetic energy equal to the energy of the incident photon minus the electron binding energy of the state it left. Photons with energies less than the electron binding energy may be absorbed or scattered but will not photoionize the atom or ion.[5]\n\nFor example, to ionize hydrogen, photons need an energy greater than 13.6 electronvolts (the Rydberg energy), which corresponds to a wavelength of 91.2 nm.[6] For photons with greater energy than this, the energy of the emitted photoelectron is given by:\n\nwhere h is the Planck constant and ν is the frequency of the photon.\n\nNot every photon which encounters an atom or ion will photoionize it. The probability of photoionization is related to the photoionization cross-section, which depends on the energy of the photon and the target being considered. For photon energies below the ionization threshold, the photoionization cross-section is near zero. But with the development of pulsed lasers it has become possible to create extremely intense, coherent light where multi-photon ionization may occur. At even higher intensities (around 1015 - 1016 W/cm2 of infrared or visible light), non-perturbative phenomena such as barrier suppression ionization[7] and rescattering ionization[8] are observed.\n\nSeveral photons of energy below the ionization threshold may actually combine their energies to ionize an atom. This probability decreases rapidly with the number of photons required, but the development of very intense, pulsed lasers still makes it possible. In the perturbative regime (below about 1014 W/cm2 at optical frequencies), the probability of absorbing N photons depends on the laser-light intensity I as IN .[9]\n\nAbove threshold ionization (ATI) [10] is an extension of multi-photon ionization where even more photons are absorbed than actually would be necessary to ionize the atom. The excess energy gives the released electron higher kinetic energy than the usual case of just-above threshold ionization. More precisely, the system will have multiple peaks in its photoelectron spectrum which are separated by the photon energies, this indicates that the emitted electron has more kinetic energy than in the normal (lowest possible number of photons) ionization case. The electrons released from the target will have approximately an integer number of photon-energies more kinetic energy. In intensity regions between 1014 W/cm2 and 1018 W/cm2, each of MPI, ATI, and barrier suppression ionization can occur simultaneously, each contributing to the overall ionization of the atoms involved.[11]\n\nIn semiconductor physics the Photo-Dember effect (named after its discoverer H. Dember) consists in the formation of a charge dipole in the vicinity of a semiconductor surface after ultra-fast photo-generation of charge carriers. The dipole forms owing to the difference of mobilities (or diffusion constants) for holes and electrons which combined with the break of symmetry provided by the surface lead to an effective charge separation in the direction perpendicular to the surface.[12]\n\nThe Grotthuss–Draper law (also called the principle of photochemical activation) states that only that light which is absorbed by a system can bring about a photochemical change. Materials such as dyes and phosphors must be able to absorb \"light\" at optical frequencies. This law provides a basis for fluorescence and phosphorescence. The law was first proposed in 1817 by Theodor Grotthuss and in 1842, independently, by John William Draper.[5]\n\nThis is considered to be one of the two basic laws of photochemistry. The second law is the Stark–Einstein law, which says that primary chemical or physical reactions occur with each photon absorbed.[5]\n\nThe Stark–Einstein law is named after German-born physicists Johannes Stark and Albert Einstein, who independently formulated the law between 1908 and 1913. It is also known as the photochemical equivalence law or photoequivalence law. In essence it says that every photon that is absorbed will cause a (primary) chemical or physical reaction.[13]\n\nThe photon is a quantum of radiation, or one unit of radiation. Therefore, this is a single unit of EM radiation that is equal to the Planck constant (h) times the frequency of light. This quantity is symbolized by γ, hν, or ħω.\n\nThe photochemical equivalence law is also restated as follows: for every mole of a substance that reacts, an equivalent mole of quanta of light are absorbed. The formula is:[13]\n\nThe photochemical equivalence law applies to the part of a light-induced reaction that is referred to as the primary process (i.e. absorption or fluorescence).[13]\n\nIn most photochemical reactions the primary process is usually followed by so-called secondary photochemical processes that are normal interactions between reactants not requiring absorption of light. As a result, such reactions do not appear to obey the one quantum–one molecule reactant relationship.[13]\n\nThe law is further restricted to conventional photochemical processes using light sources with moderate intensities; high-intensity light sources such as those used in flash photolysis and in laser experiments are known to cause so-called biphotonic processes; i.e., the absorption by a molecule of a substance of two photons of light.[13]\n\nIn physics, absorption of electromagnetic radiation is the way by which the energy of a photon is taken up by matter, typically the electrons of an atom. Thus, the electromagnetic energy is transformed to other forms of energy, for example, to heat. The absorption of light during wave propagation is often called attenuation. Usually, the absorption of waves does not depend on their intensity (linear absorption), although in certain conditions (usually, in optics), the medium changes its transparency dependently on the intensity of waves going through, and the Saturable absorption (or nonlinear absorption) occurs.\n\nPhotosensitization is a process of transferring the energy of absorbed light. After absorption, the energy is transferred to the (chosen) reactants. This is part of the work of photochemistry in general. In particular this process is commonly employed where reactions require light sources of certain wavelengths that are not readily available.[14]\n\nFor example, mercury absorbs radiation at 1849 and 2537 angstroms, and the source is often high-intensity mercury lamps. It is a commonly used sensitizer. When mercury vapor is mixed with ethylene, and the compound is irradiated with a mercury lamp, this results in the photodecomposition of ethylene to acetylene. This occurs on absorption of light to yield excited state mercury atoms, which are able to transfer this energy to the ethylene molecules, and are in turn deactivated to their initial energy state.[14]\n\nCadmium; some of the noble gases, for example xenon; zinc; benzophenone; and a large number of organic dyes, are also used as sensitizers.[14]\n\nPhotosensitisers are a key component of photodynamic therapy used to treat cancers.\n\nA sensitizer in chemiluminescence is a chemical compound, capable of light emission after it has received energy from a molecule, which became excited previously in the chemical reaction. A good example is this:\n\nWhen an alkaline solution of sodium hypochlorite and a concentrated solution of hydrogen peroxide are mixed, a reaction occurs:\n\nO2*is excited oxygen – meaning, one or more electrons in the O2 molecule have been promoted to higher-energy molecular orbitals. Hence, oxygen produced by this chemical reaction somehow 'absorbed' the energy released by the reaction and became excited. This energy state is unstable, therefore it will return to the ground state by lowering its energy. It can do that in more than one way:\n\nThe intensity, duration and color of emitted light depend on quantum and kinetical factors. However, excited molecules are frequently less capable of light emission in terms of brightness and duration when compared to sensitizers. This is because sensitizers can store energy (that is, be excited) for longer periods of time than other excited molecules. The energy is stored through means of quantum vibration, so sensitizers are usually compounds which either include systems of aromatic rings or many conjugated double and triple bonds in their structure. Hence, if an excited molecule transfers its energy to a sensitizer thus exciting it, longer and easier to quantify light emission is often observed.\n\nThe color (that is, the wavelength), brightness and duration of emission depend upon the sensitizer used. Usually, for a certain chemical reaction, many different sensitizers can be used.\n\nFluorescence spectroscopy aka fluorometry or spectrofluorometry, is a type of electromagnetic spectroscopy which analyzes fluorescence from a sample. It involves using a beam of light, usually ultraviolet light, that excites the electrons in molecules of certain compounds and causes them to emit light of a lower energy, typically, but not necessarily, visible light. A complementary technique is absorption spectroscopy.[15][16]\n\nDevices that measure fluorescence are called fluorometers or fluorimeters.\n\nAbsorption spectroscopy refers to spectroscopic techniques that measure the absorption of radiation, as a function of frequency or wavelength, due to its interaction with a sample. The sample absorbs energy, i.e., photons, from the radiating field. The intensity of the absorption varies as a function of frequency, and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum.[15][16]",
        pageTitle: "Photoelectrochemical process",
    },
    {
        title: "Gustafson's law",
        link: "https://en.wikipedia.org/wiki/Gustafson%27s_law",
        content:
            "In computer architecture, Gustafson's law (or Gustafson–Barsis's law[1]) gives the speedup in the execution time of a task that theoretically gains from parallel computing, using a hypothetical run of the task on a single-core machine as the baseline. To put it another way, it is the theoretical \"slowdown\" of an already parallelized task if running on a serial machine. It is named after computer scientist John L. Gustafson and his colleague Edwin H. Barsis, and was presented in the article Reevaluating Amdahl's Law in 1988.[2]\n\nGustafson estimated the speedup \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n of a program gained by using parallel computing as follows:\n\nS\n              \n              \n                \n                =\n                s\n                +\n                p\n                ×\n                N\n              \n            \n            \n              \n              \n                \n                =\n                s\n                +\n                (\n                1\n                −\n                s\n                )\n                ×\n                N\n              \n            \n            \n              \n              \n                \n                =\n                N\n                +\n                (\n                1\n                −\n                N\n                )\n                ×\n                s\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}S&=s+p\\times N\\\\&=s+(1-s)\\times N\\\\&=N+(1-N)\\times s\\end{aligned}}}\n\nAlternatively, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n can be expressed using \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n:\n\nS\n              \n              \n                \n                =\n                (\n                1\n                −\n                p\n                )\n                +\n                p\n                ×\n                N\n              \n            \n            \n              \n              \n                \n                =\n                1\n                +\n                (\n                N\n                −\n                1\n                )\n                ×\n                p\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}S&=(1-p)+p\\times N\\\\&=1+(N-1)\\times p\\end{aligned}}}\n\nGustafson's law addresses the shortcomings of Amdahl's law, which is based on the assumption of a fixed problem size, that is of an execution workload that does not change with respect to the improvement of the resources. Gustafson's law instead proposes that programmers tend to increase the size of problems to fully exploit the computing power that becomes available as the resources improve.[2]\n\nGustafson and his colleagues further observed from their workloads that time for the serial part typically does not grow as the problem and the system scale,[2] that is, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is fixed. This gives a linear model between the processor count \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and the speedup \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n with slope \n  \n    \n      \n        1\n        −\n        s\n      \n    \n    {\\displaystyle 1-s}\n  \n, as shown in the figure above (which uses different notations: \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n for \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n for \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n). Also, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n scales linearly with \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n rather than exponentially in the Amdahl's Law.[2] With these observations, Gustafson \"expect[ed] to extend [their] success [on parallel computing] to a broader range of applications and even larger values for \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n\".[2]\n\nThe impact of Gustafson's law was to shift[citation needed] research goals to select or reformulate problems so that solving a larger problem in the same amount of time would be possible. In a way the law redefines efficiency, due to the possibility that limitations imposed by the sequential part of a program may be countered by increasing the total amount of computation.\n\nThe execution time of a program running on a parallel system can be split into two parts:\n\nExample. — A computer program that processes files from disk. A part of that program may scan the directory of the disk and create a list of files internally in memory. After that, another part of the program passes each file to a separate thread for processing. The part that scans the directory and creates the file list cannot be sped up on a parallel computer, but the part that processes the files can.\n\nWithout loss of generality, let the total execution time on the parallel system be \n  \n    \n      \n        T\n        =\n        1\n      \n    \n    {\\displaystyle T=1}\n  \n. Denote the serial time as \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and the parallel time as  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, where \n  \n    \n      \n        s\n        +\n        p\n        =\n        1\n      \n    \n    {\\displaystyle s+p=1}\n  \n. Denote the number of processors as \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n.\n\nHypothetically, when running the program on a serial system (only one processor), the serial part still takes \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, while the parallel part now takes \n  \n    \n      \n        N\n        p\n      \n    \n    {\\displaystyle Np}\n  \n. The execution time on the serial system is:\n\nT\n          ′\n        \n        =\n        s\n        +\n        N\n        p\n      \n    \n    {\\displaystyle T'=s+Np}\n  \n\nUsing \n  \n    \n      \n        \n          T\n          ′\n        \n      \n    \n    {\\displaystyle T'}\n  \n as the baseline, the speedup for the parallel system is:\n\nS\n        =\n        \n          \n            \n              T\n              ′\n            \n            T\n          \n        \n        =\n        \n          \n            \n              s\n              +\n              N\n              p\n            \n            \n              s\n              +\n              p\n            \n          \n        \n        =\n        \n          \n            \n              s\n              +\n              N\n              p\n            \n            1\n          \n        \n        =\n        s\n        +\n        N\n        p\n      \n    \n    {\\displaystyle S={\\frac {T'}{T}}={\\frac {s+Np}{s+p}}={\\frac {s+Np}{1}}=s+Np}\n\nBy substituting \n  \n    \n      \n        p\n        =\n        1\n        −\n        s\n      \n    \n    {\\displaystyle p=1-s}\n  \n or \n  \n    \n      \n        s\n        =\n        1\n        −\n        p\n      \n    \n    {\\displaystyle s=1-p}\n  \n, several forms in the previous section can be derived.\n\nAmdahl's law presupposes that the computing requirements will stay the same, given increased processing power. In other words, an analysis of the same data will take less time given more computing power.\n\nGustafson, on the other hand, argues that more computing power will cause the data to be more carefully and fully analyzed: pixel by pixel or unit by unit, rather than on a larger scale. Where it would not have been possible or practical to simulate the impact of nuclear detonation on every building, car, and their contents (including furniture, structure strength, etc.) because such a calculation would have taken more time than was available to provide an answer, the increase in computing power will prompt researchers to add more data to more fully simulate more variables, giving a more accurate result.\n\nAmdahl's Law reveals a limitation in, for example, the ability of multiple cores to reduce the time it takes for a computer to boot to its operating system and be ready for use. Assuming the boot process was mostly parallel, quadrupling computing power on a system that took one minute to load might reduce the boot time to just over fifteen seconds. But greater and greater parallelization would eventually fail to make bootup go any faster, if any part of the boot process were inherently sequential.\n\nGustafson's law argues that a fourfold increase in computing power would instead lead to a similar increase in expectations of what the system will be capable of. If the one-minute load time is acceptable to most users, then that is a starting point from which to increase the features and functions of the system. The time taken to boot to the operating system will be the same, i.e. one minute, but the new system would include more graphical or user-friendly features.\n\nSome problems do not have fundamentally larger datasets. As an example, processing one data point per world citizen gets larger at only a few percent per year. The principal point of Gustafson's law is that such problems are not likely to be the most fruitful applications of parallelism.\n\nAlgorithms with nonlinear runtimes may find it hard to take advantage of parallelism \"exposed\" by Gustafson's law. Snyder[3] points out an \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{3})}\n  \n algorithm means that double the concurrency gives only about a 26% increase in problem size. Thus, while it may be possible to occupy vast concurrency, doing so may bring little advantage over the original, less concurrent solution—however in practice there have still been considerable improvements.\n\nHill and Marty[4] emphasize also that methods of speeding sequential execution are still needed, even for multicore machines. They point out that locally inefficient methods can be globally efficient when they reduce the sequential phase. Furthermore, Woo and Lee[5] studied the implication of energy and power on future many-core processors based on Amdahl's law, showing that an asymmetric many-core processor can achieve the best possible energy efficiency by activating an optimal number of cores given the amount of parallelism is known prior to execution.\n\nAl-hayanni, Rafiev et al have developed novel speedup and energy consumption models based on a general representation of core heterogeneity, referred to as the normal form heterogeneity, that support a wide range of heterogeneous many-core architectures. These modelling methods aim to predict system power efficiency and performance ranges, and facilitates research and development at the hardware and system software levels.[6][7]",
        pageTitle: "Gustafson's law",
    },
    {
        title: "Hack's law",
        link: "https://en.wikipedia.org/wiki/Hack%27s_law",
        content:
            "Hack's law is an empirical relationship between the length of streams and the area of their basins. If L is the length of the longest stream in a basin, and A is the area of the basin, then Hack's law may be written as\n\nfor some constant C where the exponent h is slightly less than 0.6 in most basins. h varies slightly from region to region and slightly decreases for larger basins (>8,000 mi2, or 20,720 km2). In addition to the catchment-scales, Hack's law was observed on unchanneled small-scale surfaces when the morphology measured at high resolutions (Cheraghi et al., 2018).\n\nThe law is named after American geomorphologist John Tilton Hack.\n\nThis geomorphology article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Hack's law",
    },
    {
        title: "Hagen–Poiseuille law",
        link: "https://en.wikipedia.org/wiki/Hagen%E2%80%93Poiseuille_law",
        content:
            "In non ideal fluid dynamics, the Hagen–Poiseuille equation, also known as the Hagen–Poiseuille law, Poiseuille law or Poiseuille equation, is a physical law that gives the pressure drop in an incompressible and Newtonian fluid in laminar flow flowing through a long cylindrical pipe of constant cross section. \nIt can be successfully applied to air flow in lung alveoli, or the flow through a drinking straw or through a hypodermic needle. It was experimentally derived independently by Jean Léonard Marie Poiseuille in 1838[1] and Gotthilf Heinrich Ludwig Hagen,[2] and published by Hagen in 1839[1] and then by Poiseuille in 1840–41 and 1846.[1] The theoretical justification of the Poiseuille law was given by George Stokes in 1845.[3]\n\nThe assumptions of the equation are that the fluid is incompressible and Newtonian; the flow is laminar through a pipe of constant circular cross-section that is substantially longer than its diameter; and there is no acceleration of fluid in the pipe. For velocities and pipe diameters above a threshold, actual fluid flow is not laminar but turbulent, leading to larger pressure drops than calculated by the Hagen–Poiseuille equation.\n\nPoiseuille's equation describes the pressure drop due to the viscosity of the fluid; other types of pressure drops may still occur in a fluid (see a demonstration here).[4] For example, the pressure needed to drive a viscous fluid up against gravity would contain both that as needed in Poiseuille's law plus that as needed in Bernoulli's equation, such that any point in the flow would have a pressure greater than zero (otherwise no flow would happen).\n\nAnother example is when blood flows into a narrower constriction, its speed will be greater than in a larger diameter (due to continuity of volumetric flow rate), and its pressure will be lower than in a larger diameter[4] (due to Bernoulli's equation). However, the viscosity of blood will cause additional pressure drop along the direction of flow, which is proportional to length traveled[4] (as per Poiseuille's law). Both effects contribute to the actual pressure drop.\n\nThe equation does not hold close to the pipe entrance.[8]: 3\n\nThe equation fails in the limit of low viscosity, wide and/or short pipe. Low viscosity or a wide pipe may result in turbulent flow, making it necessary to use more complex models, such as the Darcy–Weisbach equation. The ratio of length to radius of a pipe should be greater than 1/48 of the Reynolds number for the Hagen–Poiseuille law to be valid.[9] If the pipe is too short, the Hagen–Poiseuille equation may result in unphysically high flow rates; the flow is bounded by Bernoulli's principle, under less restrictive conditions, by\n\nbecause it is impossible to have negative (absolute) pressure (not to be confused with gauge pressure) in an incompressible flow.\n\nNormally, Hagen–Poiseuille flow implies not just the relation for the pressure drop, above, but also the full solution for the laminar flow profile, which is parabolic. However, the result for the pressure drop can be extended to turbulent flow by inferring an effective turbulent viscosity in the case of turbulent flow, even though the flow profile in turbulent flow is strictly speaking not actually parabolic. In both cases, laminar or turbulent, the pressure drop is related to the stress at the wall, which determines the so-called friction factor. The wall stress can be determined phenomenologically by the Darcy–Weisbach equation in the field of hydraulics, given a relationship for the friction factor in terms of the Reynolds number. In the case of laminar flow, for a circular cross section:\n\nwhere Re is the Reynolds number, ρ is the fluid density, and v is the mean flow velocity, which is half the maximal flow velocity in the case of laminar flow. It proves more useful to define the Reynolds number in terms of the mean flow velocity because this quantity remains well defined even in the case of turbulent flow, whereas the maximal flow velocity may not be, or in any case, it may be difficult to infer. In this form the law approximates the Darcy friction factor, the energy (head) loss factor, friction loss factor or Darcy (friction) factor Λ in the laminar flow at very low velocities in cylindrical tube. The theoretical derivation of a slightly different form of the law was made independently by Wiedman in 1856 and Neumann and E. Hagenbach in 1858 (1859, 1860). Hagenbach was the first who called this law Poiseuille's law.\n\nThe law is also very important in hemorheology and hemodynamics, both fields of physiology.[10]\n\nPoiseuille's law was later in 1891 extended to turbulent flow by L. R. Wilberforce, based on Hagenbach's work.\n\nThe Hagen–Poiseuille equation can be derived from the Navier–Stokes equations. The laminar flow through a pipe of uniform (circular) cross-section is known as Hagen–Poiseuille flow. The equations governing the Hagen–Poiseuille flow can be derived directly from the Navier–Stokes momentum equations in 3D cylindrical coordinates (r,θ,x) by making the following set of assumptions:\n\nThen the angular equation in the momentum equations and the continuity equation are identically satisfied. The radial momentum equation reduces to ⁠∂p/∂r⁠ = 0, i.e., the pressure p is a function of the axial coordinate x only. For brevity, use u instead of \n  \n    \n      \n        \n          u\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle u_{x}}\n  \n. The axial momentum equation reduces to\n\nwhere μ is the dynamic viscosity of the fluid. In the above equation, the left-hand side is only a function of r and the right-hand side term is only a function of x, implying that both terms must be the same constant. Evaluating this constant is straightforward. If we take the length of the pipe to be L and denote the pressure difference between the two ends of the pipe by Δp (high pressure minus low pressure), then the constant is simply\n\nSince u needs to be finite at r = 0, c1 = 0. The no slip boundary condition at the pipe wall requires that u = 0 at r = R (radius of the pipe), which yields c2 = ⁠GR2/4μ⁠. Thus we have finally the following parabolic velocity profile:\n\nThe maximum velocity occurs at the pipe centerline (r = 0), umax = ⁠GR2/4μ⁠. The average velocity can be obtained by integrating over the pipe cross section,\n\nThe easily measurable quantity in experiments is the volumetric flow rate Q = πR2 uavg. Rearrangement of this gives the Hagen–Poiseuille equation\n\nAlthough more lengthy than directly using the Navier–Stokes equations, an alternative method of deriving the Hagen–Poiseuille equation is as follows.\n\nTo figure out the motion of the liquid, all forces acting on each lamina must be known:\n\nWhen two layers of liquid in contact with each other move at different speeds, there will be a shear force between them. This force is proportional to the area of contact A, the velocity gradient perpendicular to the direction of flow ⁠Δvx/Δy⁠, and a proportionality constant (viscosity) and is given by\n\nThe negative sign is in there because we are concerned with the faster moving liquid (top in figure), which is being slowed by the slower liquid (bottom in figure). By Newton's third law of motion, the force on the slower liquid is equal and opposite (no negative sign) to the force on the faster liquid. This equation assumes that the area of contact is so large that we can ignore any effects from the edges and that the fluids behave as Newtonian fluids.\n\nAssume that we are figuring out the force on the lamina with radius r. From the equation above, we need to know the area of contact and the velocity gradient. Think of the lamina as a ring of radius r, thickness dr, and length Δx. The area of contact between the lamina and the faster one is simply the surface area of the cylinder: A = 2πr Δx. We don't know the exact form for the velocity of the liquid within the tube yet, but we do know (from our assumption above) that it is dependent on the radius. Therefore, the velocity gradient is the change of the velocity with respect to the change in the radius at the intersection of these two laminae. That intersection is at a radius of r. So, considering that this force will be positive with respect to the movement of the liquid (but the derivative of the velocity is negative), the final form of the equation becomes\n\nwhere the vertical bar and subscript r following the derivative indicates that it should be taken at a radius of r.\n\nNext let's find the force of drag from the slower lamina. We need to calculate the same values that we did for the force from the faster lamina. In this case, the area of contact is at r + dr instead of r. Also, we need to remember that this force opposes the direction of movement of the liquid and will therefore be negative (and that the derivative of the velocity is negative).\n\nTo find the solution for the flow of a laminar layer through a tube, we need to make one last assumption. There is no acceleration of liquid in the pipe, and by Newton's first law, there is no net force. If there is no net force then we can add all of the forces together to get zero\n\nFirst, to get everything happening at the same point, use the first two terms of a Taylor series expansion of the velocity gradient:\n\nThe expression is valid for all laminae. Grouping like terms and dropping the vertical bar since all derivatives are assumed to be at radius r,\n\nFinally, put this expression in the form of a differential equation, dropping the term quadratic in dr.\n\nThe above equation is the same as the one obtained from the Navier–Stokes equations and the derivation from here on follows as before.\n\nWhen a constant pressure gradient G = −⁠dp/dx⁠ is applied between two ends of a long pipe, the flow will not immediately obtain Poiseuille profile, rather it develops through time and reaches the Poiseuille profile at steady state. The Navier–Stokes equations reduce to\n\nwhere J0(⁠λnr/R⁠) is the Bessel function of the first kind of order zero and λn are the positive roots of this function and J1(λn) is the Bessel function of the first kind of order one. As t → ∞, Poiseuille solution is recovered.[11]\n\nIf R1 is the inner cylinder radii and R2 is the outer cylinder radii, with constant applied pressure gradient between the two ends G = −⁠dp/dx⁠, the velocity distribution and the volume flux through the annular pipe are\n\nWhen R2 = R, R1 = 0, the original problem is recovered.[12]\n\nFlow through pipes with an oscillating pressure gradient finds applications in blood flow through large arteries.[13][14][15][16] The imposed pressure gradient is given by\n\nwhere G, α and β are constants and ω is the frequency. The velocity field is given by\n\nwhere ber and bei are the Kelvin functions and k2 = ⁠ρω/μ⁠.\n\nPlane Poiseuille flow is flow created between two infinitely long parallel plates, separated by a distance h with a constant pressure gradient G = −⁠dp/dx⁠ is applied in the direction of flow. The flow is essentially unidirectional because of infinite length. The Navier–Stokes equations reduce to\n\nTherefore, the velocity distribution and the volume flow rate per unit length are\n\nJoseph Boussinesq derived the velocity profile and volume flow rate in 1868 for rectangular channel and tubes of equilateral triangular cross-section and for elliptical cross-section.[17] Joseph Proudman derived the same for isosceles triangles in 1914.[18] Let G = −⁠dp/dx⁠ be the constant pressure gradient acting in direction parallel to the motion.\n\nThe velocity and the volume flow rate in a rectangular channel of height 0 ≤ y ≤ h and width 0 ≤ z ≤ l are\n\nThe velocity and the volume flow rate of tube with equilateral triangular cross-section of side length ⁠2h/√3⁠ are\n\nThe velocity and the volume flow rate in the right-angled isosceles triangle y = π, y ± z = 0 are\n\nThe velocity distribution for tubes of elliptical cross-section with semiaxes a and b is[11]\n\nHere, when a = b, Poiseuille flow for circular pipe is recovered and when a → ∞, plane Poiseuille flow is recovered. More explicit solutions with cross-sections such as snail-shaped sections, sections having the shape of a notch circle following a semicircle, annular sections between homofocal ellipses, annular sections between non-concentric circles are also available, as reviewed by Ratip Berker [tr; de].[19][20]\n\nThe flow through arbitrary cross-section u(y,z) satisfies the condition that u = 0 on the walls. The governing equation reduces to[21]\n\nthen it is easy to see that the problem reduces to that integrating a Laplace equation\n\nFor a compressible fluid in a tube the volumetric flow rate Q(x) and the axial velocity are not constant along the tube; but the mass flow rate is constant along the tube length. The volumetric flow rate is usually expressed at the outlet pressure. As fluid is compressed or expanded, work is done and the fluid is heated or cooled. This means that the flow rate depends on the heat transfer to and from the fluid. For an ideal gas in the isothermal case, where the temperature of the fluid is permitted to equilibrate with its surroundings, an approximate relation for the pressure drop can be derived.[22] Using ideal gas equation of state for constant temperature process (i.e., \n  \n    \n      \n        p\n        \n          /\n        \n        ρ\n      \n    \n    {\\displaystyle p/\\rho }\n  \n is constant) and the conservation of mass flow rate (i.e., \n  \n    \n      \n        \n          \n            \n              m\n              ˙\n            \n          \n        \n        =\n        ρ\n        Q\n      \n    \n    {\\displaystyle {\\dot {m}}=\\rho Q}\n  \n is constant), the relation Qp = Q1p1 = Q2p2 can be obtained. Over a short section of the pipe, the gas flowing through the pipe can be assumed to be incompressible so that Poiseuille law can be used locally,\n\nHere we assumed the local pressure gradient is not too great to have any compressibility effects. Though locally we ignored the effects of pressure variation due to density variation, over long distances these effects are taken into account. Since μ is independent of pressure, the above equation can be integrated over the length L to give\n\nHence the volumetric flow rate at the pipe outlet is given by\n\nThis equation can be seen as Poiseuille's law with an extra correction factor ⁠p1 + p2/2p2⁠ expressing the average pressure relative to the outlet pressure.\n\nElectricity was originally understood to be a kind of fluid. This hydraulic analogy is still conceptually useful for understanding circuits. This analogy is also used to study the frequency response of fluid-mechanical networks using circuit tools, in which case the fluid network is termed a hydraulic circuit. Poiseuille's law corresponds to Ohm's law for electrical circuits, V = IR. Since the net force acting on the fluid is equal to ΔF = SΔp, where S = πr2, i.e. ΔF = πr2 ΔP, then from Poiseuille's law, it follows that\n\nFor electrical circuits, let n be the concentration of free charged particles (in m−3) and let q* be the charge of each particle (in coulombs). (For electrons, q* = e = 1.6×10−19 C.) Then nQ is the number of particles in the volume Q, and nQq* is their total charge. This is the charge that flows through the cross section per unit time, i.e. the current I. Therefore, I = nQq*. Consequently, Q = ⁠I/nq*⁠, and\n\nBut ΔF = Eq, where q is the total charge in the volume of the tube. The volume of the tube is equal to πr2L, so the number of charged particles in this volume is equal to nπr2L, and their total charge is q = nπr2 Lq*. Since the voltage V = EL, it follows then\n\nThis is exactly Ohm's law, where the resistance R = ⁠V/I⁠ is described by the formula\n\nIt follows that the resistance R is proportional to the length L of the resistor, which is true. However, it also follows that the resistance R is inversely proportional to the fourth power of the radius r, i.e. the resistance R is inversely proportional to the second power of the cross section area S = πr2 of the resistor, which is different from the electrical formula. The electrical relation for the resistance is\n\nwhere ρ is the resistivity; i.e. the resistance R is inversely proportional to the cross section area S of the resistor.[23] The reason why Poiseuille's law leads to a different formula for the resistance R is the difference between the fluid flow and the electric current. Electron gas is inviscid, so its velocity does not depend on the distance to the walls of the conductor. The resistance is due to the interaction between the flowing electrons and the atoms of the conductor. Therefore, Poiseuille's law and the hydraulic analogy are useful only within certain limits when applied to electricity. Both Ohm's law and Poiseuille's law illustrate transport phenomena.\n\nThe Hagen–Poiseuille equation is useful in determining the vascular resistance and hence flow rate of intravenous (IV) fluids that may be achieved using various sizes of peripheral and central cannulas. The equation states that flow rate is proportional to the radius to the fourth power, meaning that a small increase in the internal diameter of the cannula yields a significant increase in flow rate of IV fluids. The radius of IV cannulas is typically measured in \"gauge\", which is inversely proportional to the radius. Peripheral IV cannulas are typically available as (from large to small) 14G, 16G, 18G, 20G, 22G, 26G. As an example, assuming cannula lengths are equal, the flow of a 14G cannula is 1.73 times that of a 16G cannula, and 4.16 times that of a 20G cannula. It also states that flow is inversely proportional to length, meaning that longer lines have lower flow rates. This is important to remember as in an emergency, many clinicians favor shorter, larger catheters compared to longer, narrower catheters. While of less clinical importance, an increased change in pressure (∆p) — such as by pressurizing the bag of fluid, squeezing the bag, or hanging the bag higher (relative to the level of the cannula) — can be used to speed up flow rate. It is also useful to understand that viscous fluids will flow slower (e.g. in blood transfusion).",
        pageTitle: "Hagen–Poiseuille equation",
    },
    {
        title: "Haitz's law",
        link: "https://en.wikipedia.org/wiki/Haitz%27s_law",
        content:
            "Haitz's law is an observation and forecast about the steady improvement, over many years, of light-emitting diodes (LEDs).\n\nIt claims that every decade, the cost per lumen (unit of useful light emitted) falls by a factor of 10, and the amount of light generated per LED package increases by a factor of 20, for a given wavelength (color) of light. It is considered the LED counterpart to Moore's law, which states that the number of transistors in a given integrated circuit doubles every 18 to 24 months.[1] Both laws rely on the process optimization of the production of semiconductor devices.\n\nHaitz's law is named after Roland Haitz (1935–2015),[2] a scientist at Agilent Technologies among others. It was first presented to the larger public at Strategies in Light 2000, the first of a series of annual conferences organized by Strategies Unlimited.[3]\nBesides the forecast of exponential development of cost per lumen and amount of light per package, the publication also forecast that the luminous efficacy of LED-based lighting could reach 200 lm/W (lumen per watt) in 2020, crossing 100 lm/W in 2010. This would be the case if enough industrial and government resources were spent for research on LED-lighting. More than 50% of the electricity consumption for lighting (20% of the totally consumed electrical energy) would be saved reaching 200 lm/W. This prospect and other stepping-stone applications of LEDs (e.g. mobile phone flash and LCD-backlighting) led to a massive investment in LED-research so that the LED efficacy did indeed cross 100 lm/W in 2010. If this trend continues, LEDs will become the most efficient light source by 2020.\n\nThe theoretical maximum for truncated blackbody white light source (at 5800K colour temperature with wavelengths restricted to the visible band of between 400nm and 700nm) is 251 lm/W.[4] However, some \"white\" LEDs have achieved efficacies of over 300 lm/W.[5][6]\n\nIn 2010, Cree Inc., developed and marketed the XM-L LED that claimed 1000 lumens at 100 lm/W efficacy and 160 lm/W at 350 mA and 150 lm/W at 700 mA.[7] They also claimed to have broken the 200 lm/W barrier in R&D with a prototype producing 208 lm at 350 mA.[8] In May 2011, Cree announced another prototype with 231 lm/W efficacy at 350 mA.[9] In March 2014, Cree announced another prototype with a record-breaking 303 lm/W efficacy at 350 mA.[5]\n\nIn 2017, Philips Lighting started offering consumer LED lights with 200 lm/W efficacy in Dubai[10] using LED filament technology, three years before what Haitz's law predicted.",
        pageTitle: "Haitz's law",
    },
    {
        title: "Finagle's law",
        link: "https://en.wikipedia.org/wiki/Finagle%27s_law",
        content:
            'Finagle\'s law of dynamic negatives (also known as Melody\'s law, Sod\'s Law or Finagle\'s corollary to Murphy\'s law) is usually rendered as "Anything that can go wrong, will—at the worst possible moment."\n\nThe term "Finagle\'s law" is often associated with John W. Campbell Jr., the influential editor of Astounding Science Fiction (later Analog).\n\nOne variant (known as O\'Toole\'s corollary of Finagle\'s law) favored among hackers is a takeoff on the second law of thermodynamics (related to the augmentation of entropy):\n\nThe perversity of the Universe tends towards a maximum.\n\nIn the Star Trek episode "Amok Time" (written by Theodore Sturgeon in 1967), Captain Kirk tells Spock, "As one of Finagle\'s laws puts it: \'Any home port the ship makes will be somebody else\'s, not mine.\'"\n\nThe term "Finagle\'s law" was popularized by science fiction author Larry Niven in several stories (for example, Protector [Ballantine Books paperback edition, 4th printing, p. 23]), depicting a frontier culture of asteroid miners; this "Belter" culture professed a religion or running joke involving the worship of the dread god Finagle and his mad prophet Murphy.[1][2]\n\n"Finagle\'s law" can also be the related belief "Inanimate objects are out to get us", also known as Resistentialism.[3][4]\nSimilar to Finagle\'s law is the verbless phrase of the German novelist Friedrich Theodor Vischer: "die Tücke des Objekts" (the perfidy of inanimate objects).\n\nA related concept, the "Finagle factor", is an ad hoc multiplicative or additive term in an equation, which can be justified only by the fact that it gives more correct results.  Also known as Finagle\'s variable constant, it is sometimes defined as the correct answer divided by your answer.\n\nOne of the first records of "Finagle factor" is probably a December 1962 article in The Michigan Technic, credited to Campbell, but bylined "I Finaglin" [5]\n\nThe term is also used in a 1960 wildlife management article.[6]\n\nArthur Bloch, in his book "Murphy\'s Law and Other Reasons Why Things Go Wrong" (1977) stated variations on this:[7]',
        pageTitle: "Finagle's law",
    },
    {
        title: "Hartley's law",
        link: "https://en.wikipedia.org/wiki/Hartley%27s_law",
        content:
            "In information theory, the Shannon–Hartley theorem tells the maximum rate at which information can be transmitted over a communications channel of a specified bandwidth in the presence of noise. It is an application of the noisy-channel coding theorem to the archetypal case of a continuous-time analog communications channel subject to Gaussian noise. The theorem establishes Shannon's channel capacity for such a communication link, a bound on the maximum amount of error-free information per time unit that can be transmitted with a specified bandwidth in the presence of the noise interference, assuming that the signal power is bounded, and that the Gaussian noise process is characterized by a known power or power spectral density. The law is named after Claude Shannon and Ralph Hartley.\n\nThe Shannon–Hartley theorem states the channel capacity \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, meaning the theoretical tightest upper bound on the information rate of data that can be communicated at an arbitrarily low error rate using an average received signal power \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n through an analog communication channel subject to additive white Gaussian noise (AWGN) of power \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n:\n\nDuring the late 1920s, Harry Nyquist and Ralph Hartley developed a handful of fundamental ideas related to the transmission of information, particularly in the context of the telegraph as a communications system. At the time, these concepts were powerful breakthroughs individually, but they were not part of a comprehensive theory. In the 1940s, Claude Shannon developed the concept of channel capacity, based in part on the ideas of Nyquist and Hartley, and then formulated a complete theory of information and its transmission.\n\nIn 1927, Nyquist determined that the number of independent pulses that could be put through a telegraph channel per unit time is limited to twice the bandwidth of the channel. In symbolic notation,\n\nwhere \n  \n    \n      \n        \n          f\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle f_{p}}\n  \n is the pulse frequency (in pulses per second) and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the bandwidth (in hertz). The quantity \n  \n    \n      \n        2\n        B\n      \n    \n    {\\displaystyle 2B}\n  \n later came to be called the Nyquist rate, and transmitting at the limiting pulse rate of \n  \n    \n      \n        2\n        B\n      \n    \n    {\\displaystyle 2B}\n  \n pulses per second as signalling at the Nyquist rate. Nyquist published his results in 1928 as part of his paper \"Certain topics in Telegraph Transmission Theory\".[1]\n\nDuring 1928, Hartley formulated a way to quantify information and its line rate (also known as data signalling rate R bits per second).[2] This method, later known as Hartley's law, became an important precursor for Shannon's more sophisticated notion of channel capacity.\n\nHartley argued that the maximum number of distinguishable pulse levels that can be transmitted and received reliably over a communications channel is limited by the dynamic range of the signal amplitude and the precision with which the receiver can distinguish amplitude levels. Specifically, if the amplitude of the transmitted signal is restricted to the range of [−A ... +A] volts, and the precision of the receiver is ±ΔV volts, then the maximum number of distinct pulses M is given by\n\nBy taking information per pulse in bit/pulse to be the base-2-logarithm of the number of distinct messages M that could be sent, Hartley[3] constructed a measure of the line rate R as:\n\nwhere \n  \n    \n      \n        \n          f\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle f_{p}}\n  \n is the pulse rate, also known as the symbol rate, in symbols/second or baud.\n\nHartley then combined the above quantification with Nyquist's observation that the number of independent pulses that could be put through a channel of bandwidth \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n hertz was \n  \n    \n      \n        2\n        B\n      \n    \n    {\\displaystyle 2B}\n  \n pulses per second, to arrive at his quantitative measure for achievable line rate.\n\nHartley's law is sometimes quoted as just a proportionality between the analog bandwidth, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, in Hertz and what today is called the digital bandwidth, \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n, in bit/s.[4]\nOther times it is quoted in this more quantitative form, as an achievable line rate of \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n bits per second:[5]\n\nHartley did not work out exactly how the number M should depend on the noise statistics of the channel, or how the communication could be made reliable even when individual symbol pulses could not be reliably distinguished to M levels; with Gaussian noise statistics, system designers had to choose a very conservative value of \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n to achieve a low error rate.\n\nThe concept of an error-free capacity awaited Claude Shannon, who built on Hartley's observations about a logarithmic measure of information and Nyquist's observations about the effect of bandwidth limitations.\n\nHartley's rate result can be viewed as the capacity of an errorless M-ary channel of \n  \n    \n      \n        2\n        B\n      \n    \n    {\\displaystyle 2B}\n  \n symbols per second. Some authors refer to it as a capacity. But such an errorless channel is an idealization, and if M is chosen small enough to make the noisy channel nearly errorless, the result is necessarily less than the Shannon capacity of the noisy channel of bandwidth \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, which is the Hartley–Shannon result that followed later.\n\nClaude Shannon's development of information theory during World War II provided the next big step in understanding how much information could be reliably communicated through noisy channels. Building on Hartley's foundation, Shannon's noisy channel coding theorem (1948) describes the maximum possible efficiency of error-correcting methods versus levels of noise interference and data corruption.[6][7] The proof of the theorem shows that a randomly constructed error-correcting code is essentially as good as the best possible code; the theorem is proved through the statistics of such random codes.\n\nShannon's theorem shows how to compute a channel capacity from a statistical description of a channel, and establishes that given a noisy channel with capacity \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n and information transmitted at a line rate \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n, then if\n\nthere exists a coding technique which allows the probability of error at the receiver to be made arbitrarily small. This means that theoretically, it is possible to transmit information nearly without error up to nearly a limit of \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n bits per second.\n\nthe probability of error at the receiver increases without bound as the rate is increased, so no useful information can be transmitted beyond the channel capacity. The theorem does not address the rare situation in which rate and capacity are equal.\n\nThe Shannon–Hartley theorem establishes what that channel capacity is for a finite-bandwidth continuous-time channel subject to Gaussian noise. It connects Hartley's result with Shannon's channel capacity theorem in a form that is equivalent to specifying the M in Hartley's line rate formula in terms of a signal-to-noise ratio, but achieving reliability through error-correction coding rather than through reliably distinguishable pulse levels.\n\nIf there were such a thing as a noise-free analog channel, one could transmit unlimited amounts of error-free data over it per unit of time (Note that an infinite-bandwidth analog channel could not transmit unlimited amounts of error-free data absent infinite signal power). Real channels, however, are subject to limitations imposed by both finite bandwidth and nonzero noise.\n\nBandwidth and noise affect the rate at which information can be transmitted over an analog channel. Bandwidth limitations alone do not impose a cap on the maximum information rate because it is still possible for the signal to take on an indefinitely large number of different voltage levels on each symbol pulse, with each slightly different level being assigned a different meaning or bit sequence. Taking into account both noise and bandwidth limitations, however, there is a limit to the amount of information that can be transferred by a signal of a bounded power, even when sophisticated multi-level encoding techniques are used.\n\nIn the channel considered by the Shannon–Hartley theorem, noise and signal are combined by addition. That is, the receiver measures a signal that is equal to the sum of the signal encoding the desired information and a continuous random variable that represents the noise. This addition creates uncertainty as to the original signal's value. If the receiver has some information about the random process that generates the noise, one can in principle recover the information in the original signal by considering all possible states of the noise process. In the case of the Shannon–Hartley theorem, the noise is assumed to be generated by a Gaussian process with a known variance. Since the variance of a Gaussian process is equivalent to its power, it is conventional to call this variance the noise power.\n\nSuch a channel is called the Additive White Gaussian Noise channel, because Gaussian noise is added to the signal; \"white\" means equal amounts of noise at all frequencies within the channel bandwidth. Such noise can arise both from random sources of energy and also from coding and measurement error at the sender and receiver respectively. Since sums of independent Gaussian random variables are themselves Gaussian random variables, this conveniently simplifies analysis, if one assumes that such error sources are also Gaussian and independent.\n\nComparing the channel capacity to the information rate from Hartley's law, we can find the effective number of distinguishable levels M:[8]\n\nThe square root effectively converts the power ratio back to a voltage ratio, so the number of levels is approximately proportional to the ratio of signal RMS amplitude to noise standard deviation.\n\nThis similarity in form between Shannon's capacity and Hartley's law should not be interpreted to mean that \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n pulse levels can be literally sent without any confusion. More levels are needed to allow for redundant coding and error correction, but the net data rate that can be approached with coding is equivalent to using that \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n in Hartley's law.\n\nIn the simple version above, the signal and noise are fully uncorrelated, in which case \n  \n    \n      \n        S\n        +\n        N\n      \n    \n    {\\displaystyle S+N}\n  \n is the total power of the received signal and noise together. A generalization of the above equation for the case where the additive noise is not white (or that the ⁠\n  \n    \n      \n        S\n        \n          /\n        \n        N\n      \n    \n    {\\displaystyle S/N}\n  \n⁠ is not constant with frequency over the bandwidth) is obtained by treating the channel as many narrow, independent Gaussian channels in parallel:\n\nNote: the theorem only applies to Gaussian stationary process noise. This formula's way of introducing frequency-dependent noise cannot describe all continuous-time noise processes. For example, consider a noise process consisting of adding a random wave whose amplitude is 1 or −1 at any point in time, and a channel that adds such a wave to the source signal. Such a wave's frequency components are highly dependent. Though such a noise may have a high power, it is fairly easy to transmit a continuous signal with much less power than one would need if the underlying noise was a sum of independent noises in each frequency band.\n\nFor large or small and constant signal-to-noise ratios, the capacity formula can be approximated:\n\nWhen the SNR is large (S/N ≫ 1), the logarithm is approximated by\n\nin which case the capacity is logarithmic in power and approximately linear in bandwidth (not quite linear, since N increases with bandwidth, imparting a logarithmic effect). This is called the bandwidth-limited regime.\n\nSimilarly, when the SNR is small (if ⁠\n  \n    \n      \n        S\n        \n          /\n        \n        N\n        ≪\n        1\n      \n    \n    {\\displaystyle S/N\\ll 1}\n  \n⁠), applying the approximation to the logarithm:\n\nthen the capacity is linear in power. This is called the power-limited regime.\n\nIn this low-SNR approximation, capacity is independent of bandwidth if the noise is white, of spectral density \n  \n    \n      \n        \n          N\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle N_{0}}\n  \n watts per hertz, in which case the total noise power is \n  \n    \n      \n        N\n        =\n        B\n        ⋅\n        \n          N\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle N=B\\cdot N_{0}}\n  \n.",
        pageTitle: "Shannon–Hartley theorem",
    },
    {
        title: "Hauser's law",
        link: "https://en.wikipedia.org/wiki/Hauser%27s_law",
        content:
            'Hauser\'s law is the empirical observation that, in the United States, federal tax revenues since World War II have always been approximately equal to 19.5% of GDP, regardless of wide fluctuations in the marginal tax rate.[1] Historically, since the end of World War II, federal tax receipts as a percentage of gross domestic product averaged 17.9%, with a range from 14.4% to 20.9% between 1946 and 2007.[2]\n\nThe proposition was first put forward in 1993 by William Kurt Hauser, a San Francisco investment analyst, who wrote,\n\nNo matter what the tax rates have been, in postwar America tax revenues have remained at about 19.5% of GDP.[3]\n\nHauser cited Arthur Laffer\'s concept of the Laffer curve in his original article. While the two concepts are similar, Hauser\'s law was put forward as an empirical observation whereas the Laffer curve is a theoretical argument.[3]\n\nIn a May 20, 2008 editorial by David Ranson, the Wall St. Journal published a graph showing that even though the top marginal tax rate of federal income tax had varied between a low of 28% to a high of 91%, between 1950 and 2007, federal tax revenues had remained close to 19.5% of GDP.[3] The editorial went on to say,\n\nThe economics of taxation will be moribund until economists accept and explain Hauser\'s law. For progress to be made, they will have to face up to it, reconcile it with other facts, and incorporate it within the body of accepted knowledge.[3]\n\nFrom fiscal year 1946 to fiscal year 2007, federal tax receipts as a percentage of gross domestic product averaged 17.9%, with a range from 14.4% to 20.9%.[2] 2009 tax collections, at 15% of GDP, were the lowest level of the past 50 years and 4.5 percentage points lower than Hauser\'s law suggests.[4] The Heritage Foundation has stated that the recent world economic recession pushed receipts to a level significantly below the historical average.[5]\n\nDaniel J. Mitchell has argued that Hauser\'s Law has been observed due to the fact that the U.S. does not have a national sales tax and instead collects taxes in a federalist system, in contrast to many other Western nations. He also stated that the U.S. has an inherently more progressive system as well. Thus, he concluded that the Law represents a socio-political policy trend rather than a true economic law and that the trend could change rapidly if value-added taxes are imposed at the federal level.[6]\n\nEconomist Mike Kimel has stated that Hauser\'s Law is misleading as it sweeps large differences under the table. He wrote that tax revenue is higher in the years following a tax increase and lower in the years following a tax cut. He defined the time periods 1951–1953, 1967–1968, and 1991–2001 as "tax hike eras", and 1953–1967, 1969–1991, 2001–2010 as "tax cut eras", and points out that tax revenues increase in "tax hike eras" and that tax cuts lead to lower revenues.[7] It is misleading to refer to 1969–1984 as part of a "tax cut era", however, as the tax cuts of those times were compensating for bracket creep, as the era combined both high inflation with tax brackets not yet indexed for inflation. The tax cuts of that period merely kept taxes in line with inflation, and should not be conflated with later tax cuts which took place on top of a tax code already indexed for inflation.\n\nZubin Jelveh criticized the Wall Street Journal editorial for failing to adequately separate social insurance taxes from other types of tax revenues (such as income tax and corporate tax revenue). Because social insurance taxes go directly into the Social Security trust fund, revenue that is not earmarked for pension checks has actually declined as a percentage of GDP over the last 50 years. Jelveh points out that the main reason for this decline is a dramatic decline in corporate tax revenues, from more than 5% of GDP to less than 2%. Jelveh uses these facts to critique editorialist David Ranson\'s use of Hauser\'s Law to argue that raising tax rates on the rich will be ineffective at raising revenue.[8]\n\nJournalist Jonathan Chait has written that "swings are fairly dramatic" through U.S. history for tax receipts as a percent of GDP. He stated that the George H. W. Bush and Bill Clinton administrations received "massive" extra revenues as the result of tax increases while the George W. Bush administration tax cuts lead to a "massive" drop in revenues. He labeled the idea of static, flat revenues as a "scam".[9]',
        pageTitle: "Hauser's law",
    },
    {
        title: "Heaps' law",
        link: "https://en.wikipedia.org/wiki/Heaps%27_law",
        content:
            "In linguistics, Heaps' law (also called Herdan's law) is an empirical law which describes the number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation). It can be formulated as\n\nwhere VR is the number of distinct words in an instance text of size n. K and β are free parameters determined empirically. With English text corpora, typically K is between 10 and 100, and β is between 0.4 and 0.6.\n\nThe law is frequently attributed to Harold Stanley Heaps, but was originally discovered by Gustav Herdan (1960).[1] Under mild assumptions, the Herdan–Heaps law is asymptotically equivalent to Zipf's law concerning the frequencies of individual words within a text.[2] This is a consequence of the fact that the type-token relation (in general) of a homogenous text can be derived from the distribution of its types.[3]\n\nEmpirically, Heaps' law is preserved even when the document is randomly shuffled,[4] meaning that it does not depend on the ordering of words, but only the frequency of words.[5] This is used as evidence for deriving Heaps' law from Zipf's law.[4]\n\nHeaps' law means that as more instance text is gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn.\n\nDeviations from Heaps' law, as typically observed in English text corpora, have been identified in corpora generated with large language models.[6]\n\nHeaps' law also applies to situations in which the \"vocabulary\" is just some set of distinct types which are attributes of some collection of objects. For example, the objects could be people, and the types could be country of origin of the person. If persons are selected randomly (that is, we are not selecting based on country of origin), then Heaps' law says we will quickly have representatives from most countries (in proportion to their population) but it will become increasingly difficult to cover the entire set of countries by continuing this method of sampling.\nHeaps' law has been observed also in single-cell transcriptomes[7] considering genes as the distinct objects in the \"vocabulary\".\n\nThis computational linguistics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Heaps' law",
    },
    {
        title: "Hebb's law",
        link: "https://en.wikipedia.org/wiki/Donald_O._Hebb",
        content:
            "Donald Olding Hebb FRS[1] (July 22, 1904 – August 20, 1985) was a Canadian psychologist who was influential in the area of neuropsychology, where he sought to understand how the function of neurons contributed to psychological processes such as learning. He is best known for his theory of Hebbian learning, which he introduced in his classic 1949 work The Organization of Behavior.[3] He has been described as the father of neuropsychology and neural networks.[4] A Review of General Psychology survey, published in 2002, ranked Hebb as the 19th most cited psychologist of the 20th century.[5] His views on learning described behavior and thought in terms of brain function, explaining cognitive processes in terms of connections between neuron assemblies.\n\nDonald Hebb was born in Chester, Nova Scotia, the oldest of four children of Arthur M. and M. Clara (Olding) Hebb, and lived there until the age of 16, when his parents moved to Dartmouth, Nova Scotia.\n\nHebb's parents were both medical doctors. Donald's mother was heavily influenced by the ideas of Maria Montessori, and she home-schooled him until the age of 8. He performed so well in elementary school that he was promoted to the 7th grade at 10 years of age but, as a result of failing and then repeating the 11th grade in Chester, he graduated from the 12th grade at 16 years of age from Halifax County Academy. (Many or most of the single class of grade 9, 10 and 11 students at the Chester school failed the provincial examinations. Those in 9th and 10th grades were permitted to advance despite their failure but there was no 12th grade in Chester.) He entered Dalhousie University aiming to become a novelist. He graduated with a Bachelor of Arts degree in 1925. Afterward, he became a teacher, teaching at his old school in Chester. Later, he worked on a farm in Alberta and then traveled around, working as a laborer in Quebec.\n\nIn 1928, he became a graduate student at McGill University. But, at the same time, he was appointed headmaster of Verdun High School in the suburbs of Montreal.[6] He worked with two colleagues from the university, Kellogg and Clarke, to improve the situation. He took a more innovative approach to education—for example, assigning more interesting schoolwork and sending anyone misbehaving outside (making schoolwork a privilege). He completed his master's degree in psychology at McGill in 1932 under the direction of the eminent psychologist Boris Babkin. Hebb's master's thesis, entitled Conditioned and Unconditioned Reflexes and Inhibition, tried to show that skeletal reflexes were due to cellular learning.[7]\n\nBy the beginning of 1934, Hebb's life was in a slump. His wife had died, following a car accident, on his twenty-ninth birthday (July 22, 1933). His work at the Montreal school was going badly. In his words, it was \"defeated by the rigidity of the curriculum in Quebec's protestant schools.\" The focus of study at McGill was more in the direction of education and intelligence, and Hebb was now more interested in physiological psychology and was critical of the methodology of the experiments there.\n\nHe decided to leave Montreal and wrote to Robert Yerkes at Yale, where he was offered a position to study for a PhD. Babkin, however, convinced Hebb to study instead with Karl Lashley at the University of Chicago.\n\nIn July 1934, Hebb was accepted to study under Karl Lashley at the University of Chicago. His thesis was titled \"The problem of spatial orientation and place learning\". Hebb, along with two other students, followed Lashley to Harvard University in September 1935. Here, he had to change his thesis. At Harvard, he did his thesis research on the effects of early visual deprivation upon size and brightness perception in a rat. That is, he raised rats in the dark and some in the light and compared their brains. In 1936, he received his PhD from Harvard.[8] The following year he worked as a research assistant to Lashley and as a teaching assistant in introductory psychology for Edwin G. Boring at Radcliffe College. His Harvard thesis was soon published, and he finished the thesis he started at University of Chicago.\n\nIn 1937, Hebb married his second wife, Elizabeth Nichols Donovan. That same year, on a tip from his sister Catherine (herself a PhD student with Babkin at McGill University), he applied to work with Wilder Penfield at the Montreal Neurological Institute. Here he researched the effect of brain surgery and injury on human brain function. He saw that the brain of a child could regain partial or full function when a portion of it is removed but that similar damage in an adult could be far more damaging, even catastrophic. From this, he deduced the prominent role that external stimulation played in the thought processes of adults. In fact, the lack of this stimulation, he showed, caused diminished function and sometimes hallucinations.\n\nHe also became critical of the Stanford-Binet and Wechsler intelligence tests for use with brain surgery patients. These tests were designed to measure overall intelligence, whereas Hebb believed tests should be designed to measure more specific effects that surgery could have had on the patient. Together with N.W. Morton, he created the Adult Comprehension Test and the Picture Anomaly Test.\n\nPutting the Picture Anomaly Test to use, he provided the first indication that the right temporal lobe was involved in visual recognition. He also showed that removal of large parts of the frontal lobe had little effect on intelligence. In fact, in one adult patient, who had a large portion of his frontal lobes removed in order to treat his epilepsy, he noted \"a striking post-operative improvement in personality and intellectual capacity.\" From these sorts of results, he started to believe that the frontal lobes were instrumental in learning only early in life.\n\nIn 1939, he was appointed to a teaching position at Queen's University. In order to test his theory of the changing role of the frontal lobes with age, he designed a variable path maze for rats with Kenneth Williams called the Hebb-Williams maze, a method for testing animal intelligence later used in countless studies. He used the maze to test the intelligence of rats blinded at different developmental stages, showing that \"there is a lasting effect of infant experience on the problem-solving ability of the adult rat.\" This became one of the main principles of developmental psychology, later helping those arguing the importance of the proposed Head Start programs for preschool children in economically poor neighborhoods.\n\nIn 1942, he moved to Orange Park, Florida to once again work with Karl Lashley who had replaced Yerkes as the Director of the Yerkes Laboratories of Primate Biology at the Yerkes National Primate Research Center. Here, studying primate behavior, Hebb developed emotional tests for chimpanzees. The experiments were somewhat unsuccessful, however because chimpanzees turned out to be hard to teach. During the course of the work there, Hebb wrote The Organization of Behavior: A Neuropsychological Theory,[3] his groundbreaking book that set forth the theory that the only way to explain behavior was in terms of brain function.\n\nAfterward, he returned to McGill University to become a professor of psychology in 1947 and was made chairman of the department in 1948. Here he once again worked with Penfield, but this time through his students, which included Mortimer Mishkin, Haldor Enger Rosvold, and Brenda Milner, all of whom extended his earlier work with Penfield on the human brain.\n\nHis wife Elizabeth died in 1962. In 1966, Hebb married his third wife, Margaret Doreen Wright (née Williamson), a widow.\n\nHebb remained at McGill until retirement in 1972. He remained at McGill after retirement for a few years, in the Department of Psychology as an emeritus professor, conducting a seminar course required of all department graduate students.\n\nIn 1977 Hebb retired to his birthplace in Nova Scotia, where he completed his last book, Essay on Mind. He was appointed an honorary professor of psychology at his alma mater, Dalhousie, and regularly participated in colloquia there until his death at 81, in 1985.[9] He was survived by two daughters (both by his second marriage), Mary Ellen Hebb and Jane Hebb Paul.\n\nHebb was a member of both the Canadian Psychological Association (CPA) and the American Psychological Association (APA). He was elected President of the CPA in 1953 and of the APA in 1960. He won the APA Distinguished Scientific Contribution Award in 1961.\n\nHe was elected a Fellow of the Royal Society of Canada and a Fellow of the Royal Society of London in March 1966.[1][10][11]\n\nHe received an honorary doctorate from 15 universities, including in 1961 from University of Chicago,[12] in 1965 from Dalhousie University[13] and in 1975 from Concordia University.[14]\n\nThe Donald O. Hebb Award, named in his honor, is awarded by the Canadian Psychological Association to distinguished Canadian psychologists. The award is presented yearly to a person who has made a significant contribution to promoting the discipline of psychology as a science by conducting research, by teaching and leadership, or as a spokesperson. The inaugural award was presented to Hebb in 1980.[15]\n\nIn 2011 he was posthumously inducted into the Halifax, Nova Scotia, Discovery Centre's Hall of Fame.[16] At a 2011 meeting of the executive council of the Committee for Skeptical Inquiry (CSI), Hebb was selected for inclusion in CSI's Pantheon of Skeptics, an award given to deceased fellows of CSI.[17]\n\nHis archives, including records relating to research and teaching activities, are held by the McGill University Archives, McGill University, in Montreal.[8]\n\nThe Organization of Behavior is considered Hebb's most significant contribution to the field of neuroscience. A combination of his years of work in brain surgery mixed with his study of human behavior, it finally brought together the two realms of human perception that for a long time could not be connected properly, that is, it connected the biological function of the brain as an organ together with the higher function of the mind.[3]\n\nIn 1929, Hans Berger discovered that the brain exhibits continuous electrical activity and cast doubt on the Pavlovian model of perception and response because, now, there appeared to be something going on in the brain even without much stimulus.\n\nAt the same time, there were many mysteries. For example, if there was a method for the brain to recognize a circle, how does it recognize circles of various sizes or imperfect roundness? To accommodate every single possible circle that could exist, the brain would need a far greater capacity than it has.\n\nAnother theory, the Gestalt theory, stated that signals to the brain established a sort of field. The form of this field depended only on the pattern of the inputs, but it still could not explain how this field was understood by the mind.\n\nThe behaviorist theories at the time did well at explaining how the processing of patterns happened. However, they could not account for how these patterns made it into the mind.\n\nHebb combined up-to-date data about behavior and the brain into a single theory. And, while the understanding of the anatomy of the brain did not advance much since the development of the older theories on the operation of the brain, he was still able to piece together a theory that got a lot of the important functions of the brain right.\n\nHebb's theory became known as Hebbian theory and the models which follow this theory are said to exhibit \"Hebbian learning.\" He proposed a neurophysiological account of learning and memory based in a simple principle:[18]\n\nWhen an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.[3]\n\nThis is often paraphrased as \"Neurons that fire together wire together.\"[19] It is commonly referred to as Hebb's Law.\n\nThe combination of neurons which could be grouped together as one processing unit, Hebb referred to as \"cell-assemblies\". And their combination of connections made up the ever-changing algorithm which dictated the brain's response to stimuli.\n\nNot only did Hebb's model for the working of the mind influence how psychologists understood the processing of stimuli within the mind but also it opened up the way for the creation of computational machines that mimicked the biological processes of a living nervous system. And while the dominant form of synaptic transmission in the nervous system was later found to be chemical, modern artificial neural networks are still based on the transmission of signals via electrical impulses that Hebbian theory was first designed around.\n\nHebb was instrumental in defining psychology as a biological science by identifying thought as the integrated activity of the brain.[20] His views on learning described behavior and thought in terms of brain function, explaining cognitive processes in terms of connections between neuron assemblies. These ideas played a large part in his views on education and learning.\n\nHebb viewed motivation and learning as related properties. He believed that everything in the brain was interrelated and worked together. His theory was that everything we experience in our environment fires a set of neurons called a cell assembly. This cell assembly is the brain's thoughts or ideas. These cell assemblies then work together to form phase sequences, which are streams of thoughts.[21] Once these cell assemblies and phase sequences are formed, they can be activated by stimulation from the environment. Therefore, the more stimulating and rich the environment, the more the cell assemblies grow and learn. This theory played into his beliefs in education. Hebb believed that the environment was very important to learning in children. Children learn by building up these cell assemblies and phase sequences. An enriched environment with varied opportunities for sensory and motor experiences contribute to children developing the cell assemblies and phase sequences necessary for continued learning in adulthood. To attempt to prove this, Hebb and his daughters raised pet rats at home. By raising them in an enriched environment, the rats showed improved maze learning in adulthood.[22]  This research into environmental enrichment contributed to the development of the Head Start Program used today.\n\nHead Start is a program for preschool children in low-income families. The aim of the program is to prepare children for success in school through an early learning program providing cognitively stimulating educational activities. According to the findings in a study on Head Start participation and school readiness, full-time Head Start participation was associated with higher academic skills in children of less-educated parents.[23]\n\nAnother long-term study by Hart and Risley tracked 42 children and their families over two years. The study focused on early language acquisition and the role of the home and family in the growth of word learning and language development. The results of their study showed that two of the most important aspects in language acquisition are the economic advantages of the children's homes and the frequency of language experiences. The study demonstrated that children of lower socioeconomic status homes, with fewer economic resources, learn fewer words and acquire vocabulary more slowly than children of professional parents with a higher socioeconomic status with access to more varied and enriched vocabulary experiences.[24]\n\nHebb believed that providing an enriched environment for childhood learning would benefit adult learning as well, since a second type of learning occurs as adults. This second type of learning is a more rapid and insightful learning because the cell assemblies and phase sequences have already been created and now can be rearranged in any number of ways.[25] The Hebbian theory of learning implies that every experience a person encounters becomes set into the network of brain cells. Then, each time a certain action or thought is repeated, the connection between neurons is strengthened, changing the brain and strengthening the learning. An individual is, in essence, training their brain. The more challenging new experiences a person has and practices, the more new connections are created in their brain.\n\nThroughout his life Hebb enjoyed teaching and was very successful as a teacher. Both in his early years as a teacher and a headmaster in a Montreal school and in his later years at McGill University, he proved to be a very effective educator and a great influence on the scientific thinking of his students.\n\nAs a professor at McGill, he believed that one could not teach motivation, but rather create the conditions necessary for students under which to do their study and research. One could train them to write, help them choose a problem to study, and even help keep them from being distracted, but the motivation and passion for research and study had to come from the students themselves. He believed that students should be evaluated on their ability to think and create rather than their ability to memorize and reprocess older ideas.\n\nHebb believed in a very objective study of the human mind, more as a study of a biological science.  This attitude toward psychology and the way it is taught made McGill University a prominent center of psychological study.\n\nHebb also came up with the A/S ratio, a value that measures the brain complexity of an organism.\n\nHebb's name has often been invoked in discussions of the involvement of psychological researchers in interrogation techniques, including the use of sensory deprivation, because of his research into this field. Speaking at a Harvard symposium on sensory deprivation in June 1958, Hebb is quoted as remarking:\n\nThe work that we have done at McGill University began, actually, with the problem of brainwashing. We were not permitted to say so in the first publishing.... The chief impetus, of course, was the dismay at the kind of \"confessions\" being produced at the Russian Communist trials. \"Brainwashing\" was a term that came a little later, applied to Chinese procedures. We did not know what the Russian procedures were, but it seemed that they were producing some peculiar changes of attitude. How?\nOne possible factor was perceptual isolation and we concentrated on that.[26]\n\nRecent research has argued that Hebb's sensory deprivation research was funded by and coordinated with the CIA (with the CIA intending to use the research to develop new interrogation and torture techniques).[27] Some of this research was done in secret, and the results were initially shared only with United States authorities. Some of this research involved volunteers who spent hours in sensory deprivation conditions that some argue should be considered torture,[27][28] although the subjects in his studies were university student volunteers,[29] not patients, and were free to quit the experiment at any time.",
        pageTitle: "Donald O. Hebb",
    },
    {
        title: "Henry's law",
        link: "https://en.wikipedia.org/wiki/Henry%27s_law",
        content:
            "In physical chemistry, Henry's law is a gas law that states that the amount of dissolved gas in a liquid is directly proportional at equilibrium to its partial pressure above the liquid. The proportionality factor is called Henry's law constant. It was formulated by the English chemist William Henry, who studied the topic in the early 19th century.\nIn simple words, we can say that the  partial pressure of a gas in vapour phase is directly proportional to the mole fraction of a gas in solution.\n\nAn example where Henry's law is at play is the depth-dependent dissolution of oxygen and nitrogen in the blood of underwater divers that changes during decompression, going to decompression sickness. An everyday example is carbonated soft drinks, which contain dissolved carbon dioxide. Before opening, the gas above the drink in its container is almost pure carbon dioxide, at a pressure higher than atmospheric pressure. After the bottle is opened, this gas escapes, moving the partial pressure of carbon dioxide above the liquid to be much lower, resulting in degassing as the dissolved carbon dioxide comes out of the solution.\n\nIn his 1803 publication about the quantity of gases absorbed by water,[1] William Henry described the results of his experiments:\n\n… water takes up, of gas condensed by one, two, or more additional atmospheres, a quantity which, ordinarily compressed, would be equal to twice, thrice, &c. the volume absorbed under the common pressure of the atmosphere.\n\nCharles Coulston Gillispie states that John Dalton \"supposed that the separation of gas particles one from another in the vapor phase bears the ratio of a small whole number to their interatomic distance in solution. Henry's law follows as a consequence if this ratio is a constant for each gas at a given temperature.\"[2]\n\nUnder high pressure, solubility of CO2 increases. On opening a container of a carbonated beverage under pressure, pressure decreases to atmospheric, so that solubility decreases and the carbon dioxide forms bubbles that are released from the liquid.\n\nIt is often noted that beer served by gravity (that is, directly from a tap in the cask) is less heavily carbonated than the same beer served via a hand-pump (or beer-engine). This is because beer is pressurised on its way to the point of service by the action of the beer engine, causing carbon dioxide to dissolve in the beer. This then comes out of solution once the beer has left the pump, causing a higher level of perceptible 'condition' in the beer.\n\nConcentration of O2 in the blood and tissues is so low that they feel weak and are unable to think properly, a condition called hypoxia.\n\nIn underwater diving, gas is breathed at the ambient pressure which increases with depth due to the hydrostatic pressure. Solubility of gases increases with greater depth (greater pressure) according to Henry's law, so the body tissues take on more gas over time in greater depths of water. When ascending the diver is decompressed and the solubility of the gases dissolved in the tissues decreases accordingly. If the supersaturation is too great, bubbles may form and grow, and the presence of these bubbles can cause blockages in capillaries, or distortion in the more solid tissues which can cause damage known as decompression sickness. To avoid this injury the diver must ascend slowly enough that the excess dissolved gas is carried away by the blood and released into the lung gas.\n\nThere are many ways to define the proportionality constant of Henry's law, which can be subdivided into two fundamental types: One possibility is to put the aqueous phase into the numerator and the gaseous phase into the denominator (\"aq/gas\"). This results in the Henry's law solubility constant \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}}\n  \n. Its value increases with increased solubility. Alternatively, numerator and denominator can be switched (\"gas/aq\"), which results in the Henry's law volatility constant \n  \n    \n      \n        \n          H\n          \n            \n              v\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v}}}\n  \n. The value of \n  \n    \n      \n        \n          H\n          \n            \n              v\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v}}}\n  \n decreases with increased solubility. IUPAC describes several variants of both fundamental types.[3] This results from the multiplicity of quantities that can be chosen to describe the composition of the two phases. Typical choices for the aqueous phase are molar concentration (\n  \n    \n      \n        \n          c\n          \n            \n              a\n            \n          \n        \n      \n    \n    {\\displaystyle c_{\\rm {a}}}\n  \n), molality (\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n), and molar mixing ratio (\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n). For the gas phase, molar concentration (\n  \n    \n      \n        \n          c\n          \n            \n              g\n            \n          \n        \n      \n    \n    {\\displaystyle c_{\\rm {g}}}\n  \n) and partial pressure (\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n) are often used. It is not possible to use the gas-phase mixing ratio (\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n) because at a given gas-phase mixing ratio, the aqueous-phase concentration \n  \n    \n      \n        \n          c\n          \n            \n              a\n            \n          \n        \n      \n    \n    {\\displaystyle c_{\\rm {a}}}\n  \n depends on the total pressure and thus the ratio \n  \n    \n      \n        y\n        \n          /\n        \n        \n          c\n          \n            \n              a\n            \n          \n        \n      \n    \n    {\\displaystyle y/c_{\\rm {a}}}\n  \n is not a constant.[4] To specify the exact variant of the Henry's law constant, two superscripts are used. They refer to the numerator and the denominator of the definition. For example, \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            c\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{cp}}\n  \n refers to the Henry solubility defined as \n  \n    \n      \n        c\n        \n          /\n        \n        p\n      \n    \n    {\\displaystyle c/p}\n  \n.\n\nAtmospheric chemists often define the Henry solubility as\n\nHere \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n is the concentration of a species in the aqueous phase, and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is the partial pressure of that species in the gas phase under equilibrium conditions.\n\nThe SI unit for \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            c\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{cp}}\n  \n is mol/(m3·Pa); however, often the unit M/atm is used, since \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n is usually expressed in M (1 M = 1 mol/dm3) and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n in atm (1 atm = 101325 Pa).\n\nThe Henry solubility can also be expressed as the dimensionless ratio between the aqueous-phase concentration \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n of a species and its gas-phase concentration \n  \n    \n      \n        \n          c\n          \n            g\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{g}}}\n  \n:\n\nwhere \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n is the gas constant, and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the temperature.\n\nSometimes, this dimensionless constant is called the water–air partitioning coefficient \n  \n    \n      \n        \n          K\n          \n            WA\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{WA}}}\n  \n.[5] It is closely related to the various, slightly different definitions of the Ostwald coefficient \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, as discussed by Battino (1984).[6]\n\nHere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is the molar mixing ratio in the aqueous phase. For a dilute aqueous solution the conversion between \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n and \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n is:\n\nwhere \n  \n    \n      \n        \n          ϱ\n          \n            \n              \n                H\n                \n                  2\n                \n              \n              O\n            \n          \n        \n      \n    \n    {\\displaystyle \\varrho _{\\mathrm {H_{2}O} }}\n  \n is the density of water and \n  \n    \n      \n        \n          M\n          \n            \n              \n                H\n                \n                  2\n                \n              \n              O\n            \n          \n        \n      \n    \n    {\\displaystyle M_{\\mathrm {H_{2}O} }}\n  \n is the molar mass of water. Thus\n\nThe SI unit for \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            x\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{xp}}\n  \n is Pa−1, although atm−1 is still frequently used.\n\nIt can be advantageous to describe the aqueous phase in terms of molality instead of concentration. The molality of a solution does not change with \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n, since it refers to the mass of the solvent. In contrast, the concentration \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n does change with \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n, since the density of a solution and thus its volume are temperature-dependent. Defining the aqueous-phase composition via molality has the advantage that any temperature dependence of the Henry's law constant is a true solubility phenomenon and not introduced indirectly via a density change of the solution. Using molality, the Henry solubility can be defined as\n\nHere \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is used as the symbol for molality (instead of \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n) to avoid confusion with the symbol \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n for mass. The SI unit for \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            b\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{bp}}\n  \n is mol/(kg·Pa). There is no simple way to calculate \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            c\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{cp}}\n  \n from \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            b\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{bp}}\n  \n, since the conversion between concentration \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n and molality \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n involves all solutes of a solution. For a solution with a total of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n solutes with indices \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ldots ,n}\n  \n, the conversion is:\n\nwhere \n  \n    \n      \n        ϱ\n      \n    \n    {\\displaystyle \\varrho }\n  \n is the density of the solution, and \n  \n    \n      \n        \n          M\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle M_{i}}\n  \n are the molar masses. Here \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is identical to one of the \n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle b_{i}}\n  \n in the denominator. If there is only one solute, the equation simplifies to\n\nHenry's law is only valid for dilute solutions where \n  \n    \n      \n        b\n        M\n        ≪\n        1\n      \n    \n    {\\displaystyle bM\\ll 1}\n  \n and \n  \n    \n      \n        ϱ\n        ≈\n        \n          ϱ\n          \n            \n              \n                H\n                \n                  2\n                \n              \n              O\n            \n          \n        \n      \n    \n    {\\displaystyle \\varrho \\approx \\varrho _{\\mathrm {H_{2}O} }}\n  \n. In this case the conversion reduces further to\n\nAccording to Sazonov and Shaw,[7] the dimensionless Bunsen coefficient \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is defined as \"the volume of saturating gas, V1, reduced to T° = 273.15 K, p° = 1 bar, which is absorbed by unit volume V2* of pure solvent at the temperature of measurement and partial pressure of 1 bar.\" If the gas is ideal, the pressure cancels out, and the conversion to \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            c\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{cp}}\n  \n is simply\n\nwith \n  \n    \n      \n        \n          T\n          \n            STP\n          \n        \n      \n    \n    {\\displaystyle T^{\\text{STP}}}\n  \n = 273.15 K. Note, that according to this definition, the conversion factor is not temperature-dependent. Independent of the temperature that the Bunsen coefficient refers to, 273.15 K is always used for the conversion. The Bunsen coefficient, which is named after Robert Bunsen, has been used mainly in the older literature, and IUPAC considers it to be obsolete.[3]\n\nAccording to Sazonov and Shaw,[7] the Kuenen coefficient \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is defined as \"the volume of saturating gas V(g), reduced to T° = 273.15 K, p° = bar, which is dissolved by unit mass of pure solvent at the temperature of measurement and partial pressure 1 bar.\" If the gas is ideal, the relation to \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            c\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{cp}}\n  \n is\n\nwhere \n  \n    \n      \n        ϱ\n      \n    \n    {\\displaystyle \\varrho }\n  \n is the density of the solvent, and \n  \n    \n      \n        \n          T\n          \n            STP\n          \n        \n      \n    \n    {\\displaystyle T^{\\text{STP}}}\n  \n = 273.15 K. The SI unit for \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is m3/kg.[7] The Kuenen coefficient, which is named after Johannes Kuenen, has been used mainly in the older literature, and IUPAC considers it to be obsolete.[3]\n\nA common way to define a Henry volatility is dividing the partial pressure by the aqueous-phase concentration:\n\nThe SI unit for \n  \n    \n      \n        \n          H\n          \n            \n              v\n            \n          \n          \n            p\n            c\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v}}^{pc}}\n  \n is Pa·m3/mol.\n\nThe SI unit for \n  \n    \n      \n        \n          H\n          \n            \n              v\n            \n          \n          \n            p\n            x\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v}}^{px}}\n  \n is Pa. However, atm is still frequently used.\n\nThe Henry volatility can also be expressed as the dimensionless ratio between the gas-phase concentration \n  \n    \n      \n        \n          c\n          \n            g\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{g}}}\n  \n of a species and its aqueous-phase concentration \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{\\text{a}}}\n  \n:\n\nIn chemical engineering and environmental chemistry, this dimensionless constant is often called the air–water partitioning coefficient \n  \n    \n      \n        \n          K\n          \n            AW\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{AW}}}\n  \n.[8][9]\n\nA large compilation of Henry's law constants has been published by Sander (2023).[10] A few selected values are shown in the table below:\n\nWhen the temperature of a system changes, the Henry constant also changes. The temperature dependence of equilibrium constants can generally be described with the Van 't Hoff equation, which also applies to Henry's law constants:\n\nwhere \n  \n    \n      \n        \n          Δ\n          \n            sol\n          \n        \n        H\n      \n    \n    {\\displaystyle \\Delta _{\\text{sol}}H}\n  \n is the enthalpy of dissolution. Note that the letter \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n in the symbol \n  \n    \n      \n        \n          Δ\n          \n            sol\n          \n        \n        H\n      \n    \n    {\\displaystyle \\Delta _{\\text{sol}}H}\n  \n refers to enthalpy and is not related to the letter \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n for Henry's law constants. This applies to the Henry's solubility ratio, \n  \n    \n      \n        \n          H\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle H_{s}}\n  \n; for Henry's volatility ratio,\n  \n    \n      \n        \n          H\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle H_{v}}\n  \n, the sign of the right-hand side must be reversed.\n\nIntegrating the above equation and creating an expression based on \n  \n    \n      \n        \n          H\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle H^{\\circ }}\n  \n at the reference temperature \n  \n    \n      \n        \n          T\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle T^{\\circ }}\n  \n = 298.15 K yields:\n\nThe van 't Hoff equation in this form is only valid for a limited temperature range in which \n  \n    \n      \n        \n          Δ\n          \n            sol\n          \n        \n        H\n      \n    \n    {\\displaystyle \\Delta _{\\text{sol}}H}\n  \n does not change much with temperature (around 20K of variations).\n\nThe following table lists some temperature dependencies:\n\nSolubility of permanent gases usually decreases with increasing temperature at around room temperature. However, for aqueous solutions, the Henry's law solubility constant for many species goes through a minimum. For most permanent gases, the minimum is below 120 °C. Often, the smaller the gas molecule (and the lower the gas solubility in water), the lower the temperature of the maximum of the Henry's law constant. Thus, the maximum is at about 30 °C for helium, 92 to 93 °C for argon, nitrogen and oxygen, and 114 °C for xenon.[12]\n\nThe Henry's law constants mentioned so far do not consider any chemical equilibria in the aqueous phase. This type is called the intrinsic, or physical, Henry's law constant. For example, the intrinsic Henry's law solubility constant of formaldehyde can be defined as\n\nIn aqueous solution, formaldehyde is almost completely hydrated:\n\nThe total concentration of dissolved formaldehyde is\n\nTaking this equilibrium into account, an effective Henry's law constant \n  \n    \n      \n        \n          H\n          \n            \n              s\n              ,\n              e\n              f\n              f\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s,eff}}}\n  \n can be defined as\n\nFor acids and bases, the effective Henry's law constant is not a useful quantity because it depends on the pH of the solution.[10] In order to obtain a pH-independent constant, the product of the intrinsic Henry's law constant \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            \n              cp\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{{\\ce {cp}}}}\n  \n and the acidity constant \n  \n    \n      \n        \n          K\n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle K_{{\\ce {A}}}}\n  \n is often used for strong acids like hydrochloric acid (HCl):\n\nAlthough \n  \n    \n      \n        \n          H\n          ′\n        \n      \n    \n    {\\displaystyle H'}\n  \n is usually also called a Henry's law constant, it is a different quantity and it has different units than \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            \n              cp\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{{\\ce {cp}}}}\n  \n.\n\nValues of Henry's law constants for aqueous solutions depend on the composition of the solution, i.e., on its ionic strength and on dissolved organics. In general, the solubility of a gas decreases with increasing salinity (\"salting out\"). However, a \"salting in\" effect has also been observed, for example for the effective Henry's law constant of glyoxal. The effect can be described with the Sechenov equation, named after the Russian physiologist Ivan Sechenov (sometimes the German transliteration \"Setschenow\" of the Cyrillic name Се́ченов is used). There are many alternative ways to define the Sechenov equation, depending on how the aqueous-phase composition is described (based on concentration, molality, or molar fraction) and which variant of the Henry's law constant is used. Describing the solution in terms of molality is preferred because molality is invariant to temperature and to the addition of dry salt to the solution. Thus, the Sechenov equation can be written as\n\nwhere \n  \n    \n      \n        \n          H\n          \n            \n              s\n              ,\n              0\n            \n          \n          \n            b\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s,0}}^{bp}}\n  \n is the Henry's law constant in pure water, \n  \n    \n      \n        \n          H\n          \n            \n              s\n            \n          \n          \n            b\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s}}^{bp}}\n  \n is the Henry's law constant in the salt solution, \n  \n    \n      \n        \n          k\n          \n            \n              s\n            \n          \n        \n      \n    \n    {\\displaystyle k_{\\rm {s}}}\n  \n is the molality-based Sechenov constant, and \n  \n    \n      \n        b\n        (\n        \n          salt\n        \n        )\n      \n    \n    {\\displaystyle b({\\text{salt}})}\n  \n is the molality of the salt.\n\nHenry's law has been shown to apply to a wide range of solutes in the limit of infinite dilution (x → 0), including non-volatile substances such as sucrose. In these cases, it is necessary to state the law in terms of chemical potentials. For a solute in an ideal dilute solution, the chemical potential depends only on the concentration. For non-ideal solutions, the activity coefficients of the components must be taken into account:\n\nwhere \n  \n    \n      \n        \n          γ\n          \n            c\n          \n        \n        =\n        \n          \n            \n              H\n              \n                \n                  v\n                \n              \n            \n            \n              p\n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma _{c}={\\frac {H_{\\rm {v}}}{p^{*}}}}\n  \n for a volatile solute; c° = 1 mol/L.\n\nFor non-ideal solutions, the infinite dilution activity coefficient γc depends on the concentration and must be determined at the concentration of interest. The activity coefficient can also be obtained for non-volatile solutes, where the vapor pressure of the pure substance is negligible, by using the Gibbs-Duhem relation:\n\nBy measuring the change in vapor pressure (and hence chemical potential) of the solvent, the chemical potential of the solute can be deduced.\n\nThe standard state for a dilute solution is also defined in terms of infinite-dilution behavior. Although the standard concentration c° is taken to be 1 mol/L by convention, the standard state is a hypothetical solution of 1 mol/L in which the solute has its limiting infinite-dilution properties. This has the effect that all non-ideal behavior is described by the activity coefficient: the activity coefficient at 1 mol/L is not necessarily unity (and is frequently quite different from unity).\n\nAll the relations above can also be expressed in terms of molalities b rather than concentrations, e.g.:\n\nwhere \n  \n    \n      \n        \n          γ\n          \n            b\n          \n        \n        =\n        \n          \n            \n              H\n              \n                \n                  v\n                \n              \n              \n                p\n                b\n              \n            \n            \n              p\n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma _{b}={\\frac {H_{\\rm {v}}^{pb}}{p^{*}}}}\n  \n for a volatile solute; b° = 1 mol/kg.\n\nThe standard chemical potential μm°, the activity coefficient γm and the Henry's law constant Hvpb all have different numerical values when molalities are used in place of concentrations.\n\nHenry's law solubility constant \n  \n    \n      \n        \n          H\n          \n            \n              s\n              ,\n              2\n              ,\n              M\n            \n          \n          \n            x\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s,2,M}}^{xp}}\n  \n for a gas 2 in a mixture M of two solvents 1 and 3 depends on the individual constants for each solvent, \n  \n    \n      \n        \n          H\n          \n            \n              s\n              ,\n              2\n              ,\n              1\n            \n          \n          \n            x\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s,2,1}}^{xp}}\n  \n and \n  \n    \n      \n        \n          H\n          \n            \n              s\n              ,\n              2\n              ,\n              3\n            \n          \n          \n            x\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {s,2,3}}^{xp}}\n  \n according [13] to:\n\nWhere \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n, \n  \n    \n      \n        \n          x\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle x_{3}}\n  \n are the molar ratios of each solvent in the mixture and a13 is the interaction parameter of the solvents from Wohl expansion of the excess chemical potential of the ternary mixtures.\n\nA similar relationship can be found for the volatility constant \n  \n    \n      \n        \n          H\n          \n            \n              v\n              ,\n              2\n              ,\n              M\n            \n          \n          \n            p\n            x\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v,2,M}}^{px}}\n  \n, by remembering that \n  \n    \n      \n        \n          H\n          \n            \n              v\n            \n          \n          \n            p\n            x\n          \n        \n        =\n        1\n        \n          /\n        \n        \n          H\n          \n            \n              s\n            \n          \n          \n            x\n            p\n          \n        \n      \n    \n    {\\displaystyle H_{\\rm {v}}^{px}=1/H_{\\rm {s}}^{xp}}\n  \n and that, both being positive real numbers, \n  \n    \n      \n        ln\n        ⁡\n        \n          H\n          \n            \n              s\n            \n          \n          \n            x\n            p\n          \n        \n        =\n        −\n        ln\n        ⁡\n        (\n        1\n        \n          /\n        \n        \n          H\n          \n            \n              s\n            \n          \n          \n            x\n            p\n          \n        \n        )\n        =\n        −\n        ln\n        ⁡\n        \n          H\n          \n            \n              v\n            \n          \n          \n            p\n            x\n          \n        \n      \n    \n    {\\displaystyle \\ln H_{\\rm {s}}^{xp}=-\\ln(1/H_{\\rm {s}}^{xp})=-\\ln H_{\\rm {v}}^{px}}\n  \n, thus:\n\nFor a water-ethanol mixture, the interaction parameter a13 has values around \n  \n    \n      \n        0.1\n        ±\n        0.05\n      \n    \n    {\\displaystyle 0.1\\pm 0.05}\n  \n for ethanol concentrations (volume/volume) between 5% and 25%.[14]\n\nIn geochemistry, a version of Henry's law applies to the solubility of a noble gas in contact with silicate melt. One equation used is\n\nHenry's law is a limiting law that only applies for \"sufficiently dilute\" solutions, while Raoult's law is generally valid when the liquid phase is almost pure or for mixtures of similar substances.[15] The range of concentrations in which Henry's law applies becomes narrower the more the system diverges from ideal behavior. Roughly speaking, that is the more chemically \"different\" the solute is from the solvent.\n\nFor a dilute solution, the concentration of the solute is approximately proportional to its mole fraction x, and Henry's law can be written as\n\nwhere p* is the vapor pressure of the pure component.\n\nAt first sight, Raoult's law appears to be a special case of Henry's law, where Hvpx = p*. This is true for pairs of closely related substances, such as benzene and toluene, which obey Raoult's law over the entire composition range: such mixtures are called ideal mixtures.\n\nThe general case is that both laws are limit laws, and they apply at opposite ends of the composition range. The vapor pressure of the component in large excess, such as the solvent for a dilute solution, is proportional to its mole fraction, and the constant of proportionality is the vapor pressure of the pure substance (Raoult's law). The vapor pressure of the solute is also proportional to the solute's mole fraction, but the constant of proportionality is different and must be determined experimentally (Henry's law). In mathematical terms:\n\nRaoult's law can also be related to non-gas solutes.",
        pageTitle: "Henry's law",
    },
    {
        title: "Hess's law",
        link: "https://en.wikipedia.org/wiki/Hess%27s_law",
        content:
            "Hess's law of constant heat summation, also known simply as Hess's law, is a relationship in physical chemistry and thermodynamics[1] named after Germain Hess, a Swiss-born Russian chemist and physician who published it in 1840. The law states that the total enthalpy change during the complete course of a chemical reaction is independent of the sequence of steps taken.[2][3]\n\nHess's law is now understood as an expression of the fact that the enthalpy of a chemical process is independent of the path taken from the initial to the final state (i.e. enthalpy is a state function). According to the first law of thermodynamics, the enthalpy change in a system due to a reaction at constant pressure is equal to the heat absorbed (or the negative of the heat released), which can be determined by calorimetry for many reactions. The values are usually stated for reactions with the same initial and final temperatures and pressures (while conditions are allowed to vary during the course of the reactions). Hess's law can be used to determine the overall energy required for a chemical reaction that can be divided into synthetic steps that are individually easier to characterize. This affords the compilation of standard enthalpies of formation, which may be used to predict the enthalpy change in complex synthesis.\n\nHess's law states that the change of enthalpy in a chemical reaction is the same regardless of whether the reaction takes place in one step or several steps, provided the initial and final states of the reactants and products are the same. Enthalpy is an extensive property, meaning that its value is proportional to the system size.[4] Because of this, the enthalpy change is proportional to the number of moles participating in a given reaction.\n\nIn other words, if a chemical change takes place by several different routes, the overall enthalpy change is the same, regardless of the route by which the chemical change occurs (provided the initial and final condition are the same). If this were not true, then one could violate the first law of thermodynamics.\n\nHess's law allows the enthalpy change (ΔH) for a reaction to be calculated even when it cannot be measured directly. This is accomplished by performing basic algebraic operations based on the chemical equations of reactions using previously determined values for the enthalpies of formation.\n\nCombination of chemical equations leads to a net or overall equation. If the enthalpy changes are known for all the equations in the sequence, their sum will be the enthalpy change for the net equation. If the net enthalpy change is negative (\n  \n    \n      \n        Δ\n        \n          H\n          \n            net\n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle \\Delta H_{\\text{net}}<0}\n  \n), the reaction is exothermic and is more likely to be spontaneous; positive ΔH values correspond to endothermic reactions. (Entropy also plays an important role in determining spontaneity, as some reactions with a positive enthalpy change are nevertheless spontaneous due to an entropy increase in the reaction system.)\n\nHess's law states that enthalpy changes are additive. Thus the value of the standard enthalpy of reaction can be calculated from standard enthalpies of formation of products and reactants as follows:\n\nHere, the first sum is over all products and the second over all reactants, \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle a_{i}}\n  \n and \n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle b_{i}}\n  \n are the stoichiometric coefficients of products and reactants respectively, \n  \n    \n      \n        \n          Δ\n          \n            f\n          \n        \n        \n          H\n          \n            p\n            r\n            o\n            d\n            u\n            c\n            t\n            s\n          \n          \n            ⊖\n          \n        \n      \n    \n    {\\displaystyle \\Delta _{\\text{f}}H_{products}^{\\ominus }}\n  \n and\n  \n    \n      \n        \n          Δ\n          \n            f\n          \n        \n        \n          H\n          \n            r\n            e\n            a\n            c\n            t\n            a\n            n\n            t\n            s\n          \n          \n            ⊖\n          \n        \n      \n    \n    {\\displaystyle \\Delta _{\\text{f}}H_{reactants}^{\\ominus }}\n  \n are the standard enthalpies of formation of products and reactants respectively, and the o superscript indicates standard state values. This may be considered as the sum of two (real or fictitious) reactions:\n\nReaction (a) is the sum of reactions (b) and (c), for which the total ΔH = −393.5 kJ/mol, which is equal to ΔH in (a).\n\nThe concepts of Hess's law can be expanded to include changes in entropy and in Gibbs free energy, since these are also state functions. The Bordwell thermodynamic cycle is an example of such an extension that takes advantage of easily measured equilibria and redox potentials to determine experimentally inaccessible Gibbs free energy values. Combining ΔGo values from Bordwell thermodynamic cycles and ΔHo values found with Hess's law can be helpful in determining entropy values that have not been measured directly and therefore need to be calculated through alternative paths.\n\nFor entropy, the situation is a little different. Because entropy can be measured as an absolute value, not relative to those of the elements in their reference states (as with ΔHo and ΔGo), there is no need to use the entropy of formation; one simply uses the absolute entropies for products and reactants:\n\nHess's law is useful in the determination of enthalpies of the following:[2]",
        pageTitle: "Hess's law",
    },
    {
        title: "Hick's law",
        link: "https://en.wikipedia.org/wiki/Hick%27s_law",
        content:
            "Hick's law, or the Hick–Hyman law, named after British and American psychologists William Edmund Hick and Ray Hyman, describes the time it takes for a person to make a decision as a result of the possible choices: increasing the number of choices will increase the decision time logarithmically. The Hick–Hyman law assesses cognitive information capacity in choice reaction experiments. The amount of time taken to process a certain amount of bits in the Hick–Hyman law is known as the \"rate of gain of information\". The plain language implication of the finding is that increasing the number of choices does not directly increase the time to choose. In other words, twice as many choices does not result in twice as long to choose. Also, because the relationship is logarithmic, the increase in time it takes to choose becomes less and less as the number of choices increases.\n\nIn 1868, Franciscus Donders reported the relationship between having multiple stimuli and choice reaction time. In 1885, J. Merkel discovered that the response time is longer when a stimulus belongs to a larger set of stimuli. Psychologists began to see similarities between this phenomenon and information theory.[who?]\n\nHick first began experimenting with this theory in 1951.[1] In his first experiment, there were 10 lamps arranged circularly around the subject. There were 10 Morse keys for each of his fingers that corresponded to these lamps. A running pre-punched tape roll activated a random lamp every 5 seconds; 4 electric pens recorded this lamp activation on moving paper in 4-bit binary. When the subject tapped the corresponding key, the 4 pens recorded the response, using the same system. Although Hicks notes his experimental design using a 4-bit binary recording process was capable of showing up to 15 positions and \"all clear\", in his experiment he required the device to give an accurate record of reaction time between 10 options after a stimulus for the experiment.\n\nHick performed a second experiment using the same task, while keeping the number of alternatives at 10. The participant performed the task the first two times with the instruction to perform the task as accurately as possible. For the last task, the participant was asked to perform the task as quickly as possible.\n\nWhile Hick was stating that the relationship between reaction time and the number of choices was logarithmic, Hyman wanted to better understand the relationship between the reaction time and the mean number of choices. In Hyman’s experiment, he had eight different lights arranged in a 6x6 matrix.[2] Each of these different lights was given a name, so the participant was timed in the time it took to say the name of the light after it was lit. Further experiments changed the number of each different type of light. Hyman was responsible for determining a linear relation between reaction time and the information transmitted.\n\nGiven n equally probable choices, the average reaction time T required to choose among the choices is approximately:\n\nwhere b is a constant that can be determined empirically by fitting a line to measured data. The logarithm expresses depth of \"choice tree\" hierarchy – log2 indicates binary search was performed. Addition of 1 to n takes into account the \"uncertainty about whether to respond or not, as well as about which response to make.\"[3]\n\nIn the case of choices with unequal probabilities, the law can be generalized as:\n\nwhere H is strongly related to the information-theoretic entropy of the decision, defined as\n\nwhere pi refers to the probability of the ith alternative yielding the information-theoretic entropy.\n\nHick's law is similar in form to Fitts's law. Hick's law has a logarithmic form because people subdivide the total collection of choices into categories, eliminating about half of the remaining choices at each step, rather than considering each and every choice one-by-one, which would require linear time.\n\nE. Roth (1964) demonstrated a correlation between IQ and information processing speed, which is the reciprocal of the slope of the function:[4]\n\nwhere n is the number of choices. The time it takes to come to a decision is:\n\nproportional to :\n  \n    \n      \n        \n          \n            \n              \n                log\n                \n                  2\n                \n              \n              ⁡\n              (\n              n\n              )\n            \n            Processing Speed\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\log _{2}(n)}{\\text{Processing Speed}}}}\n\nThe stimulus–response compatibility is known to also affect the choice reaction time for the Hick–Hyman law. This means that the response should be similar to the stimulus itself (such as turning a steering wheel to turn the wheels of the car). The action the user performs is similar to the response the driver receives from the car.\n\nStudies suggest that the search for a word within a randomly ordered list—in which the reaction time increases linearly according to the number of items—does not allow for the generalization of the scientific law, considering that, in other conditions, the reaction time may not be linearly associated to the logarithm of the number of elements or even show other variations of the basic plane.\n\nExceptions to Hick's law have been identified in studies of verbal response to familiar stimuli, where there is no relationship or only a subtle increase in the reaction time associated with an increased number of elements,[5] and saccade responses, where it was shown that there is either no relationship,[6] or a decrease in the saccadic time with the increase of the number of elements, thus an antagonistic effect to that postulated by Hick's law.[7]\n\nThe generalization of Hick's law was also tested in studies on the predictability of transitions associated with the reaction time of elements that appeared in a structured sequence.[8][9] This process was first described as being in accordance to Hick's law,[10] but more recently it was shown that the relationship between predictability and reaction time is sigmoid, not linear associated with different modes of action.[11]\n\nHick's law is sometimes cited to justify menu design decisions. For example, to find a given word (e.g. the name of a command) in a randomly ordered word list (e.g. a menu), scanning of each word in the list is required, consuming linear time, so Hick's law does not apply. However, if the list is alphabetical and the user knows the name of the command, he or she may be able to use a subdividing strategy that works in logarithmic time.[12]",
        pageTitle: "Hick's law",
    },
    {
        title: "Hofstadter's law",
        link: "https://en.wikipedia.org/wiki/Hofstadter%27s_law",
        content:
            "Hofstadter's law is a self-referential adage, coined by Douglas Hofstadter in his book Gödel, Escher, Bach: An Eternal Golden Braid (1979) to describe the widely experienced difficulty of accurately estimating the time it will take to complete tasks of substantial complexity:[1][2]\n\nHofstadter's Law: It always takes longer than you expect, even when you take into account Hofstadter's Law.[2]\n\nThe law is often cited by programmers in discussions of techniques to improve productivity, such as The Mythical Man-Month or extreme programming.[3]\n\nIn 1979, Hofstadter introduced the law in connection with a discussion of chess-playing computers, which at the time were continually being beaten by top-level human players, despite outpacing humans in depth of analysis.  Hofstadter wrote:\n\nIn the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away...  This is just one more piece of evidence for the rather recursive Hofstadter's Law.[4][5][6][7]\n\nIn 1997, the chess computer Deep Blue became the first to beat a human champion by defeating Garry Kasparov.[8]",
        pageTitle: "Hofstadter's law",
    },
    {
        title: "Hooke's law",
        link: "https://en.wikipedia.org/wiki/Hooke%27s_law",
        content:
            "In physics, Hooke's law is an empirical law which states that the force (F) needed to extend or compress a spring by some distance (x) scales linearly with respect to that distance—that is, Fs = kx, where k is a constant factor characteristic of the spring (i.e., its stiffness), and x is small compared to the total possible deformation of the spring. The law is named after 17th-century British physicist Robert Hooke. He first stated the law in 1676 as a Latin anagram.[1][2] He published the solution of his anagram in 1678[3] as: ut tensio, sic vis (\"as the extension, so the force\" or \"the extension is proportional to the force\"). Hooke states in the 1678 work that he was aware of the law since 1660.\n\nHooke's equation holds (to some extent) in many other situations where an elastic body is deformed, such as wind blowing on a tall building, and a musician plucking a string of a guitar. An elastic body or material for which this equation can be assumed is said to be linear-elastic or Hookean.\n\nHooke's law is only a first-order linear approximation to the real response of springs and other elastic bodies to applied forces. It must eventually fail once the forces exceed some limit, since no material can be compressed beyond a certain minimum size, or stretched beyond a maximum size, without some permanent deformation or change of state. Many materials will noticeably deviate from Hooke's law well before those elastic limits are reached.\n\nOn the other hand, Hooke's law is an accurate approximation for most solid bodies, as long as the forces and deformations are small enough. For this reason, Hooke's law is extensively used in all branches of science and engineering, and is the foundation of many disciplines such as seismology, molecular mechanics and acoustics. It is also the fundamental principle behind the spring scale, the manometer, the galvanometer, and the balance wheel of the mechanical clock.\n\nThe modern theory of elasticity generalizes Hooke's law to say that the strain (deformation) of an elastic object or material is proportional to the stress applied to it. However, since general stresses and strains may have multiple independent components, the \"proportionality factor\" may no longer be just a single real number, but rather a linear map (a tensor) that can be represented by a matrix of real numbers.\n\nIn this general form, Hooke's law makes it possible to deduce the relation between strain and stress for complex objects in terms of intrinsic properties of the materials they are made of. For example, one can deduce that a homogeneous rod with uniform cross section will behave like a simple spring when stretched, with a stiffness k directly proportional to its cross-section area and inversely proportional to its length.\n\nConsider a simple helical spring that has one end attached to some fixed object, while the free end is being pulled by a force whose magnitude is Fs. Suppose that the spring has reached a state of equilibrium, where its length is not changing anymore. Let x be the amount by which the free end of the spring was displaced from its \"relaxed\" position (when it is not being stretched). Hooke's law states that \n  \n    \n      \n        \n          F\n          \n            s\n          \n        \n        =\n        k\n        x\n      \n    \n    {\\displaystyle F_{s}=kx}\n  \n or, equivalently, \n  \n    \n      \n        x\n        =\n        \n          \n            \n              F\n              \n                s\n              \n            \n            k\n          \n        \n      \n    \n    {\\displaystyle x={\\frac {F_{s}}{k}}}\n  \n\nwhere k is a positive real number, characteristic of the spring. A spring with spaces between the coils can be compressed, and the same formula holds for compression, with Fs and x both negative in that case.[4]\n\nAccording to this formula, the graph of the applied force Fs as a function of the displacement x will be a straight line passing through the origin, whose slope is k.\n\nHooke's law for a spring is also stated under the convention that Fs is the restoring force exerted by the spring on whatever is pulling its free end. In that case, the equation becomes \n  \n    \n      \n        \n          F\n          \n            s\n          \n        \n        =\n        −\n        k\n        x\n      \n    \n    {\\displaystyle F_{s}=-kx}\n  \n since the direction of the restoring force is opposite to that of the displacement.\n\nThe torsional analog of Hooke's law applies to torsional springs. It states that the torque (τ) required to rotate an object is directly proportional to the angular displacement (θ) from the equilibrium position. It describes the relationship between the torque applied to an object and the resulting angular deformation due to torsion. Mathematically, it can be expressed as:\n\nJust as in the linear case, this law shows that the torque is proportional to the angular displacement, and the negative sign indicates that the torque acts in a direction opposite to the angular displacement, providing a restoring force to bring the system back to equilibrium.\n\nHooke's spring law usually applies to any elastic object, of arbitrary complexity, as long as both the deformation and the stress can be expressed by a single number that can be both positive and negative.\n\nFor example, when a block of rubber attached to two parallel plates is deformed by shearing, rather than stretching or compression, the shearing force Fs and the sideways displacement of the plates x obey Hooke's law (for small enough deformations).\n\nHooke's law also applies when a straight steel bar or concrete beam (like the one used in buildings), supported at both ends, is bent by a weight F placed at some intermediate point. The displacement x in this case is the deviation of the beam, measured in the transversal direction, relative to its unloaded shape.\n\nIn the case of a helical spring that is stretched or compressed along its axis, the applied (or restoring) force and the resulting elongation or compression have the same direction (which is the direction of said axis). Therefore, if Fs and x are defined as vectors, Hooke's equation still holds and says that the force vector is the elongation vector multiplied by a fixed scalar.\n\nSome elastic bodies will deform in one direction when subjected to a force with a different direction. One example is a horizontal wood beam with non-square rectangular cross section that is bent by a transverse load that is neither vertical nor horizontal. In such cases, the magnitude of the displacement x will be proportional to the magnitude of the force Fs, as long as the direction of the latter remains the same (and its value is not too large); so the scalar version of Hooke's law Fs = −kx will hold. However, the force and displacement vectors will not be scalar multiples of each other, since they have different directions. Moreover, the ratio k between their magnitudes will depend on the direction of the vector Fs.\n\nYet, in such cases there is often a fixed linear relation between the force and deformation vectors, as long as they are small enough. Namely, there is a function κ from vectors to vectors, such that F = κ(X), and κ(αX1 + βX2) = ακ(X1) + βκ(X2) for any real numbers α, β and any displacement vectors X1, X2. Such a function is called a (second-order) tensor.\n\nWith respect to an arbitrary Cartesian coordinate system, the force and displacement vectors can be represented by 3 × 1 matrices of real numbers. Then the tensor κ connecting them can be represented by a 3 × 3 matrix κ of real coefficients, that, when multiplied by the displacement vector, gives the force vector:\n\nF\n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    F\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    F\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    F\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    κ\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    κ\n                    \n                      21\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    κ\n                    \n                      31\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      32\n                    \n                  \n                \n                \n                  \n                    κ\n                    \n                      33\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    X\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    X\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    X\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          κ\n        \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {F} \\,=\\,{\\begin{bmatrix}F_{1}\\\\F_{2}\\\\F_{3}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}\\kappa _{11}&\\kappa _{12}&\\kappa _{13}\\\\\\kappa _{21}&\\kappa _{22}&\\kappa _{23}\\\\\\kappa _{31}&\\kappa _{32}&\\kappa _{33}\\end{bmatrix}}{\\begin{bmatrix}X_{1}\\\\X_{2}\\\\X_{3}\\end{bmatrix}}\\,=\\,{\\boldsymbol {\\kappa }}\\mathbf {X} }\n\nThat is, \n  \n    \n      \n        \n          F\n          \n            i\n          \n        \n        =\n        \n          κ\n          \n            i\n            1\n          \n        \n        \n          X\n          \n            1\n          \n        \n        +\n        \n          κ\n          \n            i\n            2\n          \n        \n        \n          X\n          \n            2\n          \n        \n        +\n        \n          κ\n          \n            i\n            3\n          \n        \n        \n          X\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle F_{i}=\\kappa _{i1}X_{1}+\\kappa _{i2}X_{2}+\\kappa _{i3}X_{3}}\n  \n for i = 1, 2, 3. Therefore, Hooke's law F = κX can be said to hold also when X and F are vectors with variable directions, except that the stiffness of the object is a tensor κ, rather than a single real number k.\n\nThe stresses and strains of the material inside a continuous elastic material (such as a block of rubber, the wall of a boiler, or a steel bar) are connected by a linear relationship that is mathematically similar to Hooke's spring law, and is often referred to by that name.\n\nHowever, the strain state in a solid medium around some point cannot be described by a single vector. The same parcel of material, no matter how small, can be compressed, stretched, and sheared at the same time, along different directions. Likewise, the stresses in that parcel can be at once pushing, pulling, and shearing.\n\nIn order to capture this complexity, the relevant state of the medium around a point must be represented by two-second-order tensors, the strain tensor ε (in lieu of the displacement X) and the stress tensor σ (replacing the restoring force F). The analogue of Hooke's spring law for continuous media is then \n  \n    \n      \n        \n          σ\n        \n        =\n        \n          c\n        \n        \n          ε\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}=\\mathbf {c} {\\boldsymbol {\\varepsilon }},}\n  \n where c is a fourth-order tensor (that is, a linear map between second-order tensors) usually called the stiffness tensor or elasticity tensor. One may also write it as \n  \n    \n      \n        \n          ε\n        \n        =\n        \n          s\n        \n        \n          σ\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}=\\mathbf {s} {\\boldsymbol {\\sigma }},}\n  \n where the tensor s, called the compliance tensor, represents the inverse of said linear map.\n\nIn a Cartesian coordinate system, the stress and strain tensors can be represented by 3 × 3 matrices\n\nε\n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      21\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      31\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      32\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ;\n        \n        \n          σ\n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      21\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      31\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      32\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}\\,=\\,{\\begin{bmatrix}\\varepsilon _{11}&\\varepsilon _{12}&\\varepsilon _{13}\\\\\\varepsilon _{21}&\\varepsilon _{22}&\\varepsilon _{23}\\\\\\varepsilon _{31}&\\varepsilon _{32}&\\varepsilon _{33}\\end{bmatrix}}\\,;\\qquad {\\boldsymbol {\\sigma }}\\,=\\,{\\begin{bmatrix}\\sigma _{11}&\\sigma _{12}&\\sigma _{13}\\\\\\sigma _{21}&\\sigma _{22}&\\sigma _{23}\\\\\\sigma _{31}&\\sigma _{32}&\\sigma _{33}\\end{bmatrix}}}\n\nBeing a linear mapping between the nine numbers σij and the nine numbers εkl, the stiffness tensor c is represented by a matrix of 3 × 3 × 3 × 3 = 81 real numbers cijkl. Hooke's law then says that\n\n  \n    \n      \n        \n          σ\n          \n            i\n            j\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            3\n          \n        \n        \n          ∑\n          \n            l\n            =\n            1\n          \n          \n            3\n          \n        \n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n        \n          ε\n          \n            k\n            l\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{ij}=\\sum _{k=1}^{3}\\sum _{l=1}^{3}c_{ijkl}\\varepsilon _{kl}}\n  \n\nwhere i,j = 1,2,3.\n\nAll three tensors generally vary from point to point inside the medium, and may vary with time as well. The strain tensor ε merely specifies the displacement of the medium particles in the neighborhood of the point, while the stress tensor σ specifies the forces that neighboring parcels of the medium are exerting on each other. Therefore, they are independent of the composition and physical state of the material. The stiffness tensor c, on the other hand, is a property of the material, and often depends on physical state variables such as temperature, pressure, and microstructure.\n\nDue to the inherent symmetries of σ, ε, and c, only 21 elastic coefficients of the latter are independent.[6]  This number can be further reduced by the symmetry of the material: 9 for an orthorhombic crystal, 5 for an hexagonal structure, and 3 for a cubic symmetry.[7] For isotropic media (which have the same physical properties in any direction), c can be reduced to only two independent numbers, the bulk modulus K and the shear modulus G, that quantify the material's resistance to changes in volume and to shearing deformations, respectively.\n\nSince Hooke's law is a simple proportionality between two quantities, its formulas and consequences are mathematically similar to those of many other physical laws, such as those describing the motion of fluids, or the polarization of a dielectric by an electric field.\n\nIn particular, the tensor equation σ = cε relating elastic stresses to strains is entirely similar to the equation τ = με̇ relating the viscous stress tensor τ and the strain rate tensor ε̇ in flows of viscous fluids; although the former pertains to static stresses (related to amount of deformation) while the latter pertains to dynamical stresses (related to the rate of deformation).\n\nIn SI units, displacements are measured in meters (m), and forces in newtons (N or kg·m/s2). Therefore, the spring constant k, and each element of the tensor κ, is measured in newtons per meter (N/m), or kilograms per second squared (kg/s2).\n\nFor continuous media, each element of the stress tensor σ is a force divided by an area; it is therefore measured in units of pressure, namely pascals (Pa, or N/m2, or kg/(m·s2). The elements of the strain tensor ε are dimensionless (displacements divided by distances). Therefore, the entries of cijkl are also expressed in units of pressure.\n\nObjects that quickly regain their original shape after being deformed by a force, with the molecules or atoms of their material returning to the initial state of stable equilibrium, often obey Hooke's law.\n\nHooke's law only holds for some materials under certain loading conditions. Steel exhibits linear-elastic behavior in most engineering applications; Hooke's law is valid for it throughout its elastic range (i.e., for stresses below the yield strength). For some other materials, such as aluminium, Hooke's law is only valid for a portion of the elastic range. For these materials a proportional limit stress is defined, below which the errors associated with the linear approximation are negligible.\n\nRubber is generally regarded as a \"non-Hookean\" material because its elasticity is stress dependent and sensitive to temperature and loading rate.\n\nGeneralizations of Hooke's law for the case of large deformations is provided by models of neo-Hookean solids and Mooney–Rivlin solids.\n\nA rod of any elastic material may be viewed as a linear spring. The rod has length L and cross-sectional area A. Its tensile stress σ is linearly proportional to its fractional extension or strain ε by the modulus of elasticity E:\n\n  \n    \n      \n        σ\n        =\n        E\n        ε\n        .\n      \n    \n    {\\displaystyle \\sigma =E\\varepsilon .}\n\nThe modulus of elasticity may often be considered constant. In turn,\n\n  \n    \n      \n        ε\n        =\n        \n          \n            \n              Δ\n              L\n            \n            L\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon ={\\frac {\\Delta L}{L}}}\n  \n\n(that is, the fractional change in length), and since\n\n  \n    \n      \n        σ\n        =\n        \n          \n            F\n            A\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma ={\\frac {F}{A}}\\,,}\n  \n\nit follows that:\n\nε\n        =\n        \n          \n            σ\n            E\n          \n        \n        =\n        \n          \n            F\n            \n              A\n              E\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\varepsilon ={\\frac {\\sigma }{E}}={\\frac {F}{AE}}\\,.}\n\nΔ\n        L\n        =\n        ε\n        L\n        =\n        \n          \n            \n              F\n              L\n            \n            \n              A\n              E\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\Delta L=\\varepsilon L={\\frac {FL}{AE}}\\,.}\n\nThe potential energy Uel(x) stored in a spring is given by \n  \n    \n      \n        \n          U\n          \n            \n              e\n              l\n            \n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        k\n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{\\mathrm {el} }(x)={\\tfrac {1}{2}}kx^{2}}\n  \n which comes from adding up the energy it takes to incrementally compress the spring. That is, the integral of force over displacement. Since the external force has the same general direction as the displacement, the potential energy of a spring is always non-negative. Substituting \n  \n    \n      \n        x\n        =\n        F\n        \n          /\n        \n        k\n      \n    \n    {\\displaystyle x=F/k}\n  \n gives \n  \n    \n      \n        \n          U\n          \n            \n              e\n              l\n            \n          \n        \n        (\n        F\n        )\n        =\n        \n          \n            \n              F\n              \n                2\n              \n            \n            \n              2\n              k\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle U_{\\mathrm {el} }(F)={\\frac {F^{2}}{2k}}.}\n\nThis potential Uel can be visualized as a parabola on the Ux-plane such that Uel(x) = .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠1/2⁠kx2. As the spring is stretched in the positive x-direction, the potential energy increases parabolically (the same thing happens as the spring is compressed). Since the change in potential energy changes at a constant rate:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \n                U\n                \n                  \n                    e\n                    l\n                  \n                \n              \n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        k\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d^{2}U_{\\mathrm {el} }}{dx^{2}}}=k\\,.}\n  \n\nNote that the change in the change in U is constant even when the displacement and acceleration are zero.\n\nRelaxed force constants (the inverse of generalized compliance constants) are uniquely defined for molecular systems, in contradistinction to the usual \"rigid\" force constants, and thus their use allows meaningful correlations to be made between force fields calculated for reactants, transition states, and products of a chemical reaction. Just as the potential energy can be written as a quadratic form in the internal coordinates, so it can also be written in terms of generalized forces. The resulting coefficients are termed compliance constants. A direct method exists for calculating the compliance constant for any internal coordinate of a molecule, without the need to do the normal mode analysis.[8] The suitability of relaxed force constants (inverse compliance constants) as covalent bond strength descriptors was demonstrated as early as 1980. Recently, the suitability as non-covalent bond strength descriptors was demonstrated too.[9]\n\nA mass m attached to the end of a spring is a classic example of a harmonic oscillator. By pulling slightly on the mass and then releasing it, the system will be set in sinusoidal oscillating motion about the equilibrium position. To the extent that the spring obeys Hooke's law, and that one can neglect friction and the mass of the spring, the amplitude of the oscillation will remain constant; and its frequency f will be independent of its amplitude, determined only by the mass and the stiffness of the spring:\n\n  \n    \n      \n        f\n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n          \n            \n              k\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle f={\\frac {1}{2\\pi }}{\\sqrt {\\frac {k}{m}}}}\n  \n\nThis phenomenon made possible the construction of accurate mechanical clocks and watches that could be carried on ships and people's pockets.\n\nIf the mass m were attached to a spring with force constant k and rotating in free space, the spring tension (Ft) would supply the required centripetal force (Fc):\n\nF\n          \n            \n              t\n            \n          \n        \n        =\n        k\n        x\n        \n        ;\n        \n        \n          F\n          \n            \n              c\n            \n          \n        \n        =\n        m\n        \n          ω\n          \n            2\n          \n        \n        r\n      \n    \n    {\\displaystyle F_{\\mathrm {t} }=kx\\,;\\qquad F_{\\mathrm {c} }=m\\omega ^{2}r}\n  \n\nSince Ft = Fc and x = r, then:\n\n  \n    \n      \n        k\n        =\n        m\n        \n          ω\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle k=m\\omega ^{2}}\n  \n\nGiven that ω = 2πf, this leads to the same frequency equation as above:\n\n  \n    \n      \n        f\n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n          \n            \n              k\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle f={\\frac {1}{2\\pi }}{\\sqrt {\\frac {k}{m}}}}\n\nIsotropic materials are characterized by properties which are independent of direction in space. Physical equations involving isotropic materials must therefore be independent of the coordinate system chosen to represent them. The strain tensor is a symmetric tensor. Since the trace of any tensor is independent of any coordinate system, the most complete coordinate-free decomposition of a symmetric tensor is to represent it as the sum of a constant tensor and a traceless symmetric tensor.[10] Thus in index notation:\n\nε\n          \n            i\n            j\n          \n        \n        =\n        \n          (\n          \n            \n              \n                \n                  1\n                  3\n                \n              \n            \n            \n              ε\n              \n                k\n                k\n              \n            \n            \n              δ\n              \n                i\n                j\n              \n            \n          \n          )\n        \n        +\n        \n          (\n          \n            \n              ε\n              \n                i\n                j\n              \n            \n            −\n            \n              \n                \n                  1\n                  3\n                \n              \n            \n            \n              ε\n              \n                k\n                k\n              \n            \n            \n              δ\n              \n                i\n                j\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\varepsilon _{ij}=\\left({\\tfrac {1}{3}}\\varepsilon _{kk}\\delta _{ij}\\right)+\\left(\\varepsilon _{ij}-{\\tfrac {1}{3}}\\varepsilon _{kk}\\delta _{ij}\\right)}\n  \n\nwhere δij is the Kronecker delta. In direct tensor notation:\n\n  \n    \n      \n        \n          ε\n        \n        =\n        vol\n        ⁡\n        (\n        \n          ε\n        \n        )\n        +\n        dev\n        ⁡\n        (\n        \n          ε\n        \n        )\n        \n        ;\n        \n        vol\n        ⁡\n        (\n        \n          ε\n        \n        )\n        =\n        \n          \n            \n              1\n              3\n            \n          \n        \n        tr\n        ⁡\n        (\n        \n          ε\n        \n        )\n         \n        \n          I\n        \n        \n        ;\n        \n        dev\n        ⁡\n        (\n        \n          ε\n        \n        )\n        =\n        \n          ε\n        \n        −\n        vol\n        ⁡\n        (\n        \n          ε\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}=\\operatorname {vol} ({\\boldsymbol {\\varepsilon }})+\\operatorname {dev} ({\\boldsymbol {\\varepsilon }})\\,;\\qquad \\operatorname {vol} ({\\boldsymbol {\\varepsilon }})={\\tfrac {1}{3}}\\operatorname {tr} ({\\boldsymbol {\\varepsilon }})~\\mathbf {I} \\,;\\qquad \\operatorname {dev} ({\\boldsymbol {\\varepsilon }})={\\boldsymbol {\\varepsilon }}-\\operatorname {vol} ({\\boldsymbol {\\varepsilon }})}\n\nThe first term on the right is the constant tensor, also known as the volumetric strain tensor, and the second term is the traceless symmetric tensor, also known as the deviatoric strain tensor or shear tensor.\n\nThe most general form of Hooke's law for isotropic materials may now be written as a linear combination of these two tensors:\n\nσ\n          \n            i\n            j\n          \n        \n        =\n        3\n        K\n        \n          (\n          \n            \n              \n                \n                  1\n                  3\n                \n              \n            \n            \n              ε\n              \n                k\n                k\n              \n            \n            \n              δ\n              \n                i\n                j\n              \n            \n          \n          )\n        \n        +\n        2\n        G\n        \n          (\n          \n            \n              ε\n              \n                i\n                j\n              \n            \n            −\n            \n              \n                \n                  1\n                  3\n                \n              \n            \n            \n              ε\n              \n                k\n                k\n              \n            \n            \n              δ\n              \n                i\n                j\n              \n            \n          \n          )\n        \n        \n        ;\n        \n        \n          σ\n        \n        =\n        3\n        K\n        vol\n        ⁡\n        (\n        \n          ε\n        \n        )\n        +\n        2\n        G\n        dev\n        ⁡\n        (\n        \n          ε\n        \n        )\n      \n    \n    {\\displaystyle \\sigma _{ij}=3K\\left({\\tfrac {1}{3}}\\varepsilon _{kk}\\delta _{ij}\\right)+2G\\left(\\varepsilon _{ij}-{\\tfrac {1}{3}}\\varepsilon _{kk}\\delta _{ij}\\right)\\,;\\qquad {\\boldsymbol {\\sigma }}=3K\\operatorname {vol} ({\\boldsymbol {\\varepsilon }})+2G\\operatorname {dev} ({\\boldsymbol {\\varepsilon }})}\n  \n\nwhere K is the bulk modulus and G is the shear modulus.\n\nUsing the relationships between the elastic moduli, these equations may also be expressed in various other ways. A common form of Hooke's law for isotropic materials, expressed in direct tensor notation, is\n[11]\n\nσ\n        \n        =\n        λ\n        tr\n        ⁡\n        (\n        \n          ε\n        \n        )\n        \n          I\n        \n        +\n        2\n        μ\n        \n          ε\n        \n        =\n        \n          \n            c\n          \n        \n        :\n        \n          ε\n        \n        \n        ;\n        \n        \n          \n            c\n          \n        \n        =\n        λ\n        \n          I\n        \n        ⊗\n        \n          I\n        \n        +\n        2\n        μ\n        \n          \n            I\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}=\\lambda \\operatorname {tr} ({\\boldsymbol {\\varepsilon }})\\mathbf {I} +2\\mu {\\boldsymbol {\\varepsilon }}={\\mathsf {c}}:{\\boldsymbol {\\varepsilon }}\\,;\\qquad {\\mathsf {c}}=\\lambda \\mathbf {I} \\otimes \\mathbf {I} +2\\mu {\\mathsf {I}}}\n  \n\nwhere λ = K − ⁠2/3⁠G = c1111 − 2c1212 and μ = G = c1212 are the Lamé constants, I is the second-rank identity tensor, and I is the symmetric part of the fourth-rank identity tensor. In index notation:\n\n  \n    \n      \n        \n          σ\n          \n            i\n            j\n          \n        \n        =\n        λ\n        \n          ε\n          \n            k\n            k\n          \n        \n         \n        \n          δ\n          \n            i\n            j\n          \n        \n        +\n        2\n        μ\n        \n          ε\n          \n            i\n            j\n          \n        \n        =\n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n        \n          ε\n          \n            k\n            l\n          \n        \n        \n        ;\n        \n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n        =\n        λ\n        \n          δ\n          \n            i\n            j\n          \n        \n        \n          δ\n          \n            k\n            l\n          \n        \n        +\n        μ\n        \n          (\n          \n            \n              δ\n              \n                i\n                k\n              \n            \n            \n              δ\n              \n                j\n                l\n              \n            \n            +\n            \n              δ\n              \n                i\n                l\n              \n            \n            \n              δ\n              \n                j\n                k\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\sigma _{ij}=\\lambda \\varepsilon _{kk}~\\delta _{ij}+2\\mu \\varepsilon _{ij}=c_{ijkl}\\varepsilon _{kl}\\,;\\qquad c_{ijkl}=\\lambda \\delta _{ij}\\delta _{kl}+\\mu \\left(\\delta _{ik}\\delta _{jl}+\\delta _{il}\\delta _{jk}\\right)}\n\nε\n        \n        =\n        \n          \n            1\n            \n              2\n              μ\n            \n          \n        \n        \n          σ\n        \n        −\n        \n          \n            λ\n            \n              2\n              μ\n              (\n              3\n              λ\n              +\n              2\n              μ\n              )\n            \n          \n        \n        tr\n        ⁡\n        (\n        \n          σ\n        \n        )\n        \n          I\n        \n        =\n        \n          \n            1\n            \n              2\n              G\n            \n          \n        \n        \n          σ\n        \n        +\n        \n          (\n          \n            \n              \n                1\n                \n                  9\n                  K\n                \n              \n            \n            −\n            \n              \n                1\n                \n                  6\n                  G\n                \n              \n            \n          \n          )\n        \n        tr\n        ⁡\n        (\n        \n          σ\n        \n        )\n        \n          I\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}={\\frac {1}{2\\mu }}{\\boldsymbol {\\sigma }}-{\\frac {\\lambda }{2\\mu (3\\lambda +2\\mu )}}\\operatorname {tr} ({\\boldsymbol {\\sigma }})\\mathbf {I} ={\\frac {1}{2G}}{\\boldsymbol {\\sigma }}+\\left({\\frac {1}{9K}}-{\\frac {1}{6G}}\\right)\\operatorname {tr} ({\\boldsymbol {\\sigma }})\\mathbf {I} }\n\nTherefore, the compliance tensor in the relation ε = s : σ is\n\ns\n          \n        \n        =\n        −\n        \n          \n            λ\n            \n              2\n              μ\n              (\n              3\n              λ\n              +\n              2\n              μ\n              )\n            \n          \n        \n        \n          I\n        \n        ⊗\n        \n          I\n        \n        +\n        \n          \n            1\n            \n              2\n              μ\n            \n          \n        \n        \n          \n            I\n          \n        \n        =\n        \n          (\n          \n            \n              \n                1\n                \n                  9\n                  K\n                \n              \n            \n            −\n            \n              \n                1\n                \n                  6\n                  G\n                \n              \n            \n          \n          )\n        \n        \n          I\n        \n        ⊗\n        \n          I\n        \n        +\n        \n          \n            1\n            \n              2\n              G\n            \n          \n        \n        \n          \n            I\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {s}}=-{\\frac {\\lambda }{2\\mu (3\\lambda +2\\mu )}}\\mathbf {I} \\otimes \\mathbf {I} +{\\frac {1}{2\\mu }}{\\mathsf {I}}=\\left({\\frac {1}{9K}}-{\\frac {1}{6G}}\\right)\\mathbf {I} \\otimes \\mathbf {I} +{\\frac {1}{2G}}{\\mathsf {I}}}\n\nIn terms of Young's modulus and Poisson's ratio, Hooke's law for isotropic materials can then be expressed as\n\nε\n          \n            i\n            j\n          \n        \n        =\n        \n          \n            1\n            E\n          \n        \n        \n          \n            (\n          \n        \n        \n          σ\n          \n            i\n            j\n          \n        \n        −\n        ν\n        (\n        \n          σ\n          \n            k\n            k\n          \n        \n        \n          δ\n          \n            i\n            j\n          \n        \n        −\n        \n          σ\n          \n            i\n            j\n          \n        \n        )\n        \n          \n            )\n          \n        \n        \n        ;\n        \n        \n          ε\n        \n        =\n        \n          \n            1\n            E\n          \n        \n        \n          \n            (\n          \n        \n        \n          σ\n        \n        −\n        ν\n        (\n        tr\n        ⁡\n        (\n        \n          σ\n        \n        )\n        \n          I\n        \n        −\n        \n          σ\n        \n        )\n        \n          \n            )\n          \n        \n        =\n        \n          \n            \n              1\n              +\n              ν\n            \n            E\n          \n        \n        \n          σ\n        \n        −\n        \n          \n            ν\n            E\n          \n        \n        tr\n        ⁡\n        (\n        \n          σ\n        \n        )\n        \n          I\n        \n      \n    \n    {\\displaystyle \\varepsilon _{ij}={\\frac {1}{E}}{\\big (}\\sigma _{ij}-\\nu (\\sigma _{kk}\\delta _{ij}-\\sigma _{ij}){\\big )}\\,;\\qquad {\\boldsymbol {\\varepsilon }}={\\frac {1}{E}}{\\big (}{\\boldsymbol {\\sigma }}-\\nu (\\operatorname {tr} ({\\boldsymbol {\\sigma }})\\mathbf {I} -{\\boldsymbol {\\sigma }}){\\big )}={\\frac {1+\\nu }{E}}{\\boldsymbol {\\sigma }}-{\\frac {\\nu }{E}}\\operatorname {tr} ({\\boldsymbol {\\sigma }})\\mathbf {I} }\n\nThis is the form in which the strain is expressed in terms of the stress tensor in engineering. The expression in expanded form is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    11\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    11\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    22\n                  \n                \n                +\n                \n                  σ\n                  \n                    33\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n              \n            \n            \n              \n                \n                  ε\n                  \n                    22\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    22\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    11\n                  \n                \n                +\n                \n                  σ\n                  \n                    33\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n              \n            \n            \n              \n                \n                  ε\n                  \n                    33\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    33\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    11\n                  \n                \n                +\n                \n                  σ\n                  \n                    22\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n              \n            \n            \n              \n                \n                  ε\n                  \n                    12\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                      G\n                    \n                  \n                \n                \n                  σ\n                  \n                    12\n                  \n                \n                \n                ;\n                \n                \n                  ε\n                  \n                    13\n                  \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                      G\n                    \n                  \n                \n                \n                  σ\n                  \n                    13\n                  \n                \n                \n                ;\n                \n                \n                  ε\n                  \n                    23\n                  \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                      G\n                    \n                  \n                \n                \n                  σ\n                  \n                    23\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{11}&={\\frac {1}{E}}{\\big (}\\sigma _{11}-\\nu (\\sigma _{22}+\\sigma _{33}){\\big )}\\\\\\varepsilon _{22}&={\\frac {1}{E}}{\\big (}\\sigma _{22}-\\nu (\\sigma _{11}+\\sigma _{33}){\\big )}\\\\\\varepsilon _{33}&={\\frac {1}{E}}{\\big (}\\sigma _{33}-\\nu (\\sigma _{11}+\\sigma _{22}){\\big )}\\\\\\varepsilon _{12}&={\\frac {1}{2G}}\\sigma _{12}\\,;\\qquad \\varepsilon _{13}={\\frac {1}{2G}}\\sigma _{13}\\,;\\qquad \\varepsilon _{23}={\\frac {1}{2G}}\\sigma _{23}\\end{aligned}}}\n  \n\nwhere E is Young's modulus and ν is Poisson's ratio. (See 3-D elasticity).\n\nThe three-dimensional form of Hooke's law can be derived using Poisson's ratio and the one-dimensional form of Hooke's law as follows.\nConsider the strain and stress relation as a superposition of two effects: stretching in direction of the load (1) and shrinking (caused by the load) in perpendicular directions (2 and 3),\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                  ′\n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  σ\n                  \n                    1\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    2\n                  \n                  ′\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    1\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    3\n                  \n                  ′\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    1\n                  \n                \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}'&={\\frac {1}{E}}\\sigma _{1}\\,,\\\\\\varepsilon _{2}'&=-{\\frac {\\nu }{E}}\\sigma _{1}\\,,\\\\\\varepsilon _{3}'&=-{\\frac {\\nu }{E}}\\sigma _{1}\\,,\\end{aligned}}}\n  \n\nwhere ν is Poisson's ratio and E is Young's modulus.\n\nWe get similar equations to the loads in directions 2 and 3,\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                  ″\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    2\n                  \n                  ″\n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    3\n                  \n                  ″\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}''&=-{\\frac {\\nu }{E}}\\sigma _{2}\\,,\\\\\\varepsilon _{2}''&={\\frac {1}{E}}\\sigma _{2}\\,,\\\\\\varepsilon _{3}''&=-{\\frac {\\nu }{E}}\\sigma _{2}\\,,\\end{aligned}}}\n  \n\nand\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                  ‴\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    3\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    2\n                  \n                  ‴\n                \n              \n              \n                \n                =\n                −\n                \n                  \n                    ν\n                    E\n                  \n                \n                \n                  σ\n                  \n                    3\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    3\n                  \n                  ‴\n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  σ\n                  \n                    3\n                  \n                \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}'''&=-{\\frac {\\nu }{E}}\\sigma _{3}\\,,\\\\\\varepsilon _{2}'''&=-{\\frac {\\nu }{E}}\\sigma _{3}\\,,\\\\\\varepsilon _{3}'''&={\\frac {1}{E}}\\sigma _{3}\\,.\\end{aligned}}}\n\nSumming the three cases together (εi = εi′ + εi″ + εi‴) we get\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    1\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    2\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    3\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  σ\n                  \n                    3\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}&={\\frac {1}{E}}{\\big (}\\sigma _{1}-\\nu (\\sigma _{2}+\\sigma _{3}){\\big )}\\,,\\\\\\varepsilon _{2}&={\\frac {1}{E}}{\\big (}\\sigma _{2}-\\nu (\\sigma _{1}+\\sigma _{3}){\\big )}\\,,\\\\\\varepsilon _{3}&={\\frac {1}{E}}{\\big (}\\sigma _{3}-\\nu (\\sigma _{1}+\\sigma _{2}){\\big )}\\,,\\end{aligned}}}\n  \n\nor by adding and subtracting one νσ\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                (\n                1\n                +\n                ν\n                )\n                \n                  σ\n                  \n                    1\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                (\n                1\n                +\n                ν\n                )\n                \n                  σ\n                  \n                    2\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  ε\n                  \n                    3\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                (\n                1\n                +\n                ν\n                )\n                \n                  σ\n                  \n                    3\n                  \n                \n                −\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}&={\\frac {1}{E}}{\\big (}(1+\\nu )\\sigma _{1}-\\nu (\\sigma _{1}+\\sigma _{2}+\\sigma _{3}){\\big )}\\,,\\\\\\varepsilon _{2}&={\\frac {1}{E}}{\\big (}(1+\\nu )\\sigma _{2}-\\nu (\\sigma _{1}+\\sigma _{2}+\\sigma _{3}){\\big )}\\,,\\\\\\varepsilon _{3}&={\\frac {1}{E}}{\\big (}(1+\\nu )\\sigma _{3}-\\nu (\\sigma _{1}+\\sigma _{2}+\\sigma _{3}){\\big )}\\,,\\end{aligned}}}\n  \n\nand further we get by solving σ1\n\n  \n    \n      \n        \n          σ\n          \n            1\n          \n        \n        =\n        \n          \n            E\n            \n              1\n              +\n              ν\n            \n          \n        \n        \n          ε\n          \n            1\n          \n        \n        +\n        \n          \n            ν\n            \n              1\n              +\n              ν\n            \n          \n        \n        (\n        \n          σ\n          \n            1\n          \n        \n        +\n        \n          σ\n          \n            2\n          \n        \n        +\n        \n          σ\n          \n            3\n          \n        \n        )\n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{1}={\\frac {E}{1+\\nu }}\\varepsilon _{1}+{\\frac {\\nu }{1+\\nu }}(\\sigma _{1}+\\sigma _{2}+\\sigma _{3})\\,.}\n\nCalculating the sum\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n                +\n                \n                  ε\n                  \n                    3\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    E\n                  \n                \n                \n                  \n                    (\n                  \n                \n                (\n                1\n                +\n                ν\n                )\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                −\n                3\n                ν\n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                =\n                \n                  \n                    \n                      1\n                      −\n                      2\n                      ν\n                    \n                    E\n                  \n                \n                (\n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  σ\n                  \n                    1\n                  \n                \n                +\n                \n                  σ\n                  \n                    2\n                  \n                \n                +\n                \n                  σ\n                  \n                    3\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    E\n                    \n                      1\n                      −\n                      2\n                      ν\n                    \n                  \n                \n                (\n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n                +\n                \n                  ε\n                  \n                    3\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\varepsilon _{1}+\\varepsilon _{2}+\\varepsilon _{3}&={\\frac {1}{E}}{\\big (}(1+\\nu )(\\sigma _{1}+\\sigma _{2}+\\sigma _{3})-3\\nu (\\sigma _{1}+\\sigma _{2}+\\sigma _{3}){\\big )}={\\frac {1-2\\nu }{E}}(\\sigma _{1}+\\sigma _{2}+\\sigma _{3})\\\\\\sigma _{1}+\\sigma _{2}+\\sigma _{3}&={\\frac {E}{1-2\\nu }}(\\varepsilon _{1}+\\varepsilon _{2}+\\varepsilon _{3})\\end{aligned}}}\n  \n\nand substituting it to the equation solved for σ1 gives\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  σ\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    E\n                    \n                      1\n                      +\n                      ν\n                    \n                  \n                \n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                \n                  \n                    \n                      E\n                      ν\n                    \n                    \n                      (\n                      1\n                      +\n                      ν\n                      )\n                      (\n                      1\n                      −\n                      2\n                      ν\n                      )\n                    \n                  \n                \n                (\n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n                +\n                \n                  ε\n                  \n                    3\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                2\n                μ\n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                λ\n                (\n                \n                  ε\n                  \n                    1\n                  \n                \n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n                +\n                \n                  ε\n                  \n                    3\n                  \n                \n                )\n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sigma _{1}&={\\frac {E}{1+\\nu }}\\varepsilon _{1}+{\\frac {E\\nu }{(1+\\nu )(1-2\\nu )}}(\\varepsilon _{1}+\\varepsilon _{2}+\\varepsilon _{3})\\\\&=2\\mu \\varepsilon _{1}+\\lambda (\\varepsilon _{1}+\\varepsilon _{2}+\\varepsilon _{3})\\,,\\end{aligned}}}\n  \n\nwhere μ and λ are the Lamé parameters.\n\nSimilar treatment of directions 2 and 3 gives the Hooke's law in three dimensions.\n\nIn matrix form, Hooke's law for isotropic materials can be written as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  \n                    γ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    γ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    γ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            1\n            E\n          \n        \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  −\n                  ν\n                \n                \n                  −\n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  ν\n                \n                \n                  1\n                \n                \n                  −\n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  ν\n                \n                \n                  −\n                  ν\n                \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                  +\n                  2\n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                  +\n                  2\n                  ν\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                  +\n                  2\n                  ν\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\\\varepsilon _{33}\\\\2\\varepsilon _{23}\\\\2\\varepsilon _{13}\\\\2\\varepsilon _{12}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\\\varepsilon _{33}\\\\\\gamma _{23}\\\\\\gamma _{13}\\\\\\gamma _{12}\\end{bmatrix}}\\,=\\,{\\frac {1}{E}}{\\begin{bmatrix}1&-\\nu &-\\nu &0&0&0\\\\-\\nu &1&-\\nu &0&0&0\\\\-\\nu &-\\nu &1&0&0&0\\\\0&0&0&2+2\\nu &0&0\\\\0&0&0&0&2+2\\nu &0\\\\0&0&0&0&0&2+2\\nu \\end{bmatrix}}{\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{33}\\\\\\sigma _{23}\\\\\\sigma _{13}\\\\\\sigma _{12}\\end{bmatrix}}}\n  \n\nwhere γij = 2εij is the engineering shear strain. The inverse relation may be written as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            E\n            \n              (\n              1\n              +\n              ν\n              )\n              (\n              1\n              −\n              2\n              ν\n              )\n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  1\n                  −\n                  ν\n                \n                \n                  ν\n                \n                \n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  ν\n                \n                \n                  1\n                  −\n                  ν\n                \n                \n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  ν\n                \n                \n                  ν\n                \n                \n                  1\n                  −\n                  ν\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        1\n                        −\n                        2\n                        ν\n                      \n                      2\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        1\n                        −\n                        2\n                        ν\n                      \n                      2\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        1\n                        −\n                        2\n                        ν\n                      \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{33}\\\\\\sigma _{23}\\\\\\sigma _{13}\\\\\\sigma _{12}\\end{bmatrix}}\\,=\\,{\\frac {E}{(1+\\nu )(1-2\\nu )}}{\\begin{bmatrix}1-\\nu &\\nu &\\nu &0&0&0\\\\\\nu &1-\\nu &\\nu &0&0&0\\\\\\nu &\\nu &1-\\nu &0&0&0\\\\0&0&0&{\\frac {1-2\\nu }{2}}&0&0\\\\0&0&0&0&{\\frac {1-2\\nu }{2}}&0\\\\0&0&0&0&0&{\\frac {1-2\\nu }{2}}\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\\\varepsilon _{33}\\\\2\\varepsilon _{23}\\\\2\\varepsilon _{13}\\\\2\\varepsilon _{12}\\end{bmatrix}}}\n  \n\nwhich can be simplified thanks to the Lamé constants:\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  2\n                  μ\n                  +\n                  λ\n                \n                \n                  λ\n                \n                \n                  λ\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  λ\n                \n                \n                  2\n                  μ\n                  +\n                  λ\n                \n                \n                  λ\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  λ\n                \n                \n                  λ\n                \n                \n                  2\n                  μ\n                  +\n                  λ\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  μ\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  μ\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  μ\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{33}\\\\\\sigma _{23}\\\\\\sigma _{13}\\\\\\sigma _{12}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}2\\mu +\\lambda &\\lambda &\\lambda &0&0&0\\\\\\lambda &2\\mu +\\lambda &\\lambda &0&0&0\\\\\\lambda &\\lambda &2\\mu +\\lambda &0&0&0\\\\0&0&0&\\mu &0&0\\\\0&0&0&0&\\mu &0\\\\0&0&0&0&0&\\mu \\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\\\varepsilon _{33}\\\\2\\varepsilon _{23}\\\\2\\varepsilon _{13}\\\\2\\varepsilon _{12}\\end{bmatrix}}}\n  \n\nIn vector notation this becomes\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        2\n        μ\n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        +\n        λ\n        \n          I\n        \n        \n          (\n          \n            \n              ε\n              \n                11\n              \n            \n            +\n            \n              ε\n              \n                22\n              \n            \n            +\n            \n              ε\n              \n                33\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}&\\sigma _{12}&\\sigma _{13}\\\\\\sigma _{12}&\\sigma _{22}&\\sigma _{23}\\\\\\sigma _{13}&\\sigma _{23}&\\sigma _{33}\\end{bmatrix}}\\,=\\,2\\mu {\\begin{bmatrix}\\varepsilon _{11}&\\varepsilon _{12}&\\varepsilon _{13}\\\\\\varepsilon _{12}&\\varepsilon _{22}&\\varepsilon _{23}\\\\\\varepsilon _{13}&\\varepsilon _{23}&\\varepsilon _{33}\\end{bmatrix}}+\\lambda \\mathbf {I} \\left(\\varepsilon _{11}+\\varepsilon _{22}+\\varepsilon _{33}\\right)}\n  \n\nwhere I is the identity tensor.\n\nUnder plane stress conditions, σ31 = σ13 = σ32 = σ23 = σ33 = 0. In that case Hooke's law takes the form\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            E\n            \n              1\n              −\n              \n                ν\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  ν\n                \n                \n                  0\n                \n              \n              \n                \n                  ν\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        1\n                        −\n                        ν\n                      \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{12}\\end{bmatrix}}\\,=\\,{\\frac {E}{1-\\nu ^{2}}}{\\begin{bmatrix}1&\\nu &0\\\\\\nu &1&0\\\\0&0&{\\frac {1-\\nu }{2}}\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\2\\varepsilon _{12}\\end{bmatrix}}}\n\nIn vector notation this becomes\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            E\n            \n              1\n              −\n              \n                ν\n                \n                  2\n                \n              \n            \n          \n        \n        \n          (\n          \n            (\n            1\n            −\n            ν\n            )\n            \n              \n                [\n                \n                  \n                    \n                      \n                        ε\n                        \n                          11\n                        \n                      \n                    \n                    \n                      \n                        ε\n                        \n                          12\n                        \n                      \n                    \n                  \n                  \n                    \n                      \n                        ε\n                        \n                          12\n                        \n                      \n                    \n                    \n                      \n                        ε\n                        \n                          22\n                        \n                      \n                    \n                  \n                \n                ]\n              \n            \n            +\n            ν\n            \n              I\n            \n            \n              (\n              \n                \n                  ε\n                  \n                    11\n                  \n                \n                +\n                \n                  ε\n                  \n                    22\n                  \n                \n              \n              )\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}&\\sigma _{12}\\\\\\sigma _{12}&\\sigma _{22}\\end{bmatrix}}\\,=\\,{\\frac {E}{1-\\nu ^{2}}}\\left((1-\\nu ){\\begin{bmatrix}\\varepsilon _{11}&\\varepsilon _{12}\\\\\\varepsilon _{12}&\\varepsilon _{22}\\end{bmatrix}}+\\nu \\mathbf {I} \\left(\\varepsilon _{11}+\\varepsilon _{22}\\right)\\right)}\n\nThe inverse relation is usually written in the reduced form\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            1\n            E\n          \n        \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  −\n                  ν\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  ν\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                  +\n                  2\n                  ν\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\2\\varepsilon _{12}\\end{bmatrix}}\\,=\\,{\\frac {1}{E}}{\\begin{bmatrix}1&-\\nu &0\\\\-\\nu &1&0\\\\0&0&2+2\\nu \\end{bmatrix}}{\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{12}\\end{bmatrix}}}\n\nUnder plane strain conditions, ε31 = ε13 = ε32 = ε23 = ε33 = 0. In this case Hooke's law takes the form\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            E\n            \n              (\n              1\n              +\n              ν\n              )\n              (\n              1\n              −\n              2\n              ν\n              )\n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  1\n                  −\n                  ν\n                \n                \n                  ν\n                \n                \n                  0\n                \n              \n              \n                \n                  ν\n                \n                \n                  1\n                  −\n                  ν\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        1\n                        −\n                        2\n                        ν\n                      \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{12}\\end{bmatrix}}\\,=\\,{\\frac {E}{(1+\\nu )(1-2\\nu )}}{\\begin{bmatrix}1-\\nu &\\nu &0\\\\\\nu &1-\\nu &0\\\\0&0&{\\frac {1-2\\nu }{2}}\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\2\\varepsilon _{12}\\end{bmatrix}}}\n\nThe symmetry of the Cauchy stress tensor (σij = σji) and the generalized Hooke's laws (σij = cijklεkl) implies that cijkl = cjikl. Similarly, the symmetry of the infinitesimal strain tensor implies that cijkl = cijlk. These symmetries are called the minor symmetries of the stiffness tensor c. This reduces the number of elastic constants from 81 to 36.\n\nIf in addition, since the displacement gradient and the Cauchy stress are work conjugate, the stress–strain relation can be derived from a strain energy density functional (U), then\n\n  \n    \n      \n        \n          σ\n          \n            i\n            j\n          \n        \n        =\n        \n          \n            \n              ∂\n              U\n            \n            \n              ∂\n              \n                ε\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        \n        \n        ⟹\n        \n        \n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n        =\n        \n          \n            \n              \n                ∂\n                \n                  2\n                \n              \n              U\n            \n            \n              ∂\n              \n                ε\n                \n                  i\n                  j\n                \n              \n              ∂\n              \n                ε\n                \n                  k\n                  l\n                \n              \n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{ij}={\\frac {\\partial U}{\\partial \\varepsilon _{ij}}}\\quad \\implies \\quad c_{ijkl}={\\frac {\\partial ^{2}U}{\\partial \\varepsilon _{ij}\\partial \\varepsilon _{kl}}}\\,.}\n  \n\nThe arbitrariness of the order of differentiation implies that cijkl = cklij. These are called the major symmetries of the stiffness tensor. This reduces the number of elastic constants from 36 to 21. The major and minor symmetries indicate that the stiffness tensor has only 21 independent components.\n\nIt is often useful to express the anisotropic form of Hooke's law in matrix notation, also called Voigt notation. To do this we take advantage of the symmetry of the stress and strain tensors and express them as six-dimensional vectors in an orthonormal coordinate system (e1,e2,e3) as\n\n  \n    \n      \n        [\n        \n          σ\n        \n        ]\n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ≡\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ;\n        \n        [\n        \n          ε\n        \n        ]\n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      11\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      22\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      33\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      23\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      12\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ≡\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [{\\boldsymbol {\\sigma }}]\\,=\\,{\\begin{bmatrix}\\sigma _{11}\\\\\\sigma _{22}\\\\\\sigma _{33}\\\\\\sigma _{23}\\\\\\sigma _{13}\\\\\\sigma _{12}\\end{bmatrix}}\\,\\equiv \\,{\\begin{bmatrix}\\sigma _{1}\\\\\\sigma _{2}\\\\\\sigma _{3}\\\\\\sigma _{4}\\\\\\sigma _{5}\\\\\\sigma _{6}\\end{bmatrix}}\\,;\\qquad [{\\boldsymbol {\\varepsilon }}]\\,=\\,{\\begin{bmatrix}\\varepsilon _{11}\\\\\\varepsilon _{22}\\\\\\varepsilon _{33}\\\\2\\varepsilon _{23}\\\\2\\varepsilon _{13}\\\\2\\varepsilon _{12}\\end{bmatrix}}\\,\\equiv \\,{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\end{bmatrix}}}\n  \n\nThen the stiffness tensor (c) can be expressed as\n\n  \n    \n      \n        [\n        \n          \n            c\n          \n        \n        ]\n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    c\n                    \n                      1111\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1122\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1133\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1123\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1131\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1112\n                    \n                  \n                \n              \n              \n                \n                  \n                    c\n                    \n                      2211\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2222\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2233\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2223\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2231\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2212\n                    \n                  \n                \n              \n              \n                \n                  \n                    c\n                    \n                      3311\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3322\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3333\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3323\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3331\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3312\n                    \n                  \n                \n              \n              \n                \n                  \n                    c\n                    \n                      2311\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2322\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2333\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2323\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2331\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2312\n                    \n                  \n                \n              \n              \n                \n                  \n                    c\n                    \n                      3111\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3122\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3133\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3123\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3131\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3112\n                    \n                  \n                \n              \n              \n                \n                  \n                    c\n                    \n                      1211\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1222\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1233\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1223\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1231\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1212\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ≡\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    C\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      14\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      15\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      16\n                    \n                  \n                \n              \n              \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      24\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      25\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      26\n                    \n                  \n                \n              \n              \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      33\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      34\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      35\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      36\n                    \n                  \n                \n              \n              \n                \n                  \n                    C\n                    \n                      14\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      24\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      34\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      44\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      45\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      46\n                    \n                  \n                \n              \n              \n                \n                  \n                    C\n                    \n                      15\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      25\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      35\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      45\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      55\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      56\n                    \n                  \n                \n              \n              \n                \n                  \n                    C\n                    \n                      16\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      26\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      36\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      46\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      56\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      66\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [{\\mathsf {c}}]\\,=\\,{\\begin{bmatrix}c_{1111}&c_{1122}&c_{1133}&c_{1123}&c_{1131}&c_{1112}\\\\c_{2211}&c_{2222}&c_{2233}&c_{2223}&c_{2231}&c_{2212}\\\\c_{3311}&c_{3322}&c_{3333}&c_{3323}&c_{3331}&c_{3312}\\\\c_{2311}&c_{2322}&c_{2333}&c_{2323}&c_{2331}&c_{2312}\\\\c_{3111}&c_{3122}&c_{3133}&c_{3123}&c_{3131}&c_{3112}\\\\c_{1211}&c_{1222}&c_{1233}&c_{1223}&c_{1231}&c_{1212}\\end{bmatrix}}\\,\\equiv \\,{\\begin{bmatrix}C_{11}&C_{12}&C_{13}&C_{14}&C_{15}&C_{16}\\\\C_{12}&C_{22}&C_{23}&C_{24}&C_{25}&C_{26}\\\\C_{13}&C_{23}&C_{33}&C_{34}&C_{35}&C_{36}\\\\C_{14}&C_{24}&C_{34}&C_{44}&C_{45}&C_{46}\\\\C_{15}&C_{25}&C_{35}&C_{45}&C_{55}&C_{56}\\\\C_{16}&C_{26}&C_{36}&C_{46}&C_{56}&C_{66}\\end{bmatrix}}}\n\n[\n        \n          σ\n        \n        ]\n        =\n        [\n        \n          \n            C\n          \n        \n        ]\n        [\n        \n          ε\n        \n        ]\n        \n        \n          or\n        \n        \n        \n          σ\n          \n            i\n          \n        \n        =\n        \n          C\n          \n            i\n            j\n          \n        \n        \n          ε\n          \n            j\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle [{\\boldsymbol {\\sigma }}]=[{\\mathsf {C}}][{\\boldsymbol {\\varepsilon }}]\\qquad {\\text{or}}\\qquad \\sigma _{i}=C_{ij}\\varepsilon _{j}\\,.}\n  \n\nSimilarly the compliance tensor (s) can be written as\n\n  \n    \n      \n        [\n        \n          \n            s\n          \n        \n        ]\n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    s\n                    \n                      1111\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      1122\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      1133\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      1123\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      1131\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      1112\n                    \n                  \n                \n              \n              \n                \n                  \n                    s\n                    \n                      2211\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      2222\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      2233\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      2223\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      2231\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      2212\n                    \n                  \n                \n              \n              \n                \n                  \n                    s\n                    \n                      3311\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      3322\n                    \n                  \n                \n                \n                  \n                    s\n                    \n                      3333\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      3323\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      3331\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      3312\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    s\n                    \n                      2311\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      2322\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      2333\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      2323\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      2331\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      2312\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    s\n                    \n                      3111\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      3122\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      3133\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      3123\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      3131\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      3112\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    s\n                    \n                      1211\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      1222\n                    \n                  \n                \n                \n                  2\n                  \n                    s\n                    \n                      1233\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      1223\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      1231\n                    \n                  \n                \n                \n                  4\n                  \n                    s\n                    \n                      1212\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        ≡\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    S\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      14\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      15\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      16\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      24\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      25\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      26\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      33\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      34\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      35\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      36\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      14\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      24\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      34\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      44\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      45\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      46\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      15\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      25\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      35\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      45\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      55\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      56\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      16\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      26\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      36\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      46\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      56\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      66\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [{\\mathsf {s}}]\\,=\\,{\\begin{bmatrix}s_{1111}&s_{1122}&s_{1133}&2s_{1123}&2s_{1131}&2s_{1112}\\\\s_{2211}&s_{2222}&s_{2233}&2s_{2223}&2s_{2231}&2s_{2212}\\\\s_{3311}&s_{3322}&s_{3333}&2s_{3323}&2s_{3331}&2s_{3312}\\\\2s_{2311}&2s_{2322}&2s_{2333}&4s_{2323}&4s_{2331}&4s_{2312}\\\\2s_{3111}&2s_{3122}&2s_{3133}&4s_{3123}&4s_{3131}&4s_{3112}\\\\2s_{1211}&2s_{1222}&2s_{1233}&4s_{1223}&4s_{1231}&4s_{1212}\\end{bmatrix}}\\,\\equiv \\,{\\begin{bmatrix}S_{11}&S_{12}&S_{13}&S_{14}&S_{15}&S_{16}\\\\S_{12}&S_{22}&S_{23}&S_{24}&S_{25}&S_{26}\\\\S_{13}&S_{23}&S_{33}&S_{34}&S_{35}&S_{36}\\\\S_{14}&S_{24}&S_{34}&S_{44}&S_{45}&S_{46}\\\\S_{15}&S_{25}&S_{35}&S_{45}&S_{55}&S_{56}\\\\S_{16}&S_{26}&S_{36}&S_{46}&S_{56}&S_{66}\\end{bmatrix}}}\n\nIf a linear elastic material is rotated from a reference configuration to another, then the material is symmetric with respect to the rotation if the components of the stiffness tensor in the rotated configuration are related to the components in the reference configuration by the relation[13]\n\nc\n          \n            p\n            q\n            r\n            s\n          \n        \n        =\n        \n          l\n          \n            p\n            i\n          \n        \n        \n          l\n          \n            q\n            j\n          \n        \n        \n          l\n          \n            r\n            k\n          \n        \n        \n          l\n          \n            s\n            l\n          \n        \n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n      \n    \n    {\\displaystyle c_{pqrs}=l_{pi}l_{qj}l_{rk}l_{sl}c_{ijkl}}\n  \n\nwhere lab are the components of an orthogonal rotation matrix [L]. The same relation also holds for inversions.\n\nIn matrix notation, if the transformed basis (rotated or inverted) is related to the reference basis by\n\n[\n        \n          \n            e\n          \n          \n            i\n          \n          ′\n        \n        ]\n        =\n        [\n        L\n        ]\n        [\n        \n          \n            e\n          \n          \n            i\n          \n        \n        ]\n      \n    \n    {\\displaystyle [\\mathbf {e} _{i}']=[L][\\mathbf {e} _{i}]}\n\nC\n          \n            i\n            j\n          \n        \n        \n          ε\n          \n            i\n          \n        \n        \n          ε\n          \n            j\n          \n        \n        =\n        \n          C\n          \n            i\n            j\n          \n          ′\n        \n        \n          ε\n          \n            i\n          \n          ′\n        \n        \n          ε\n          \n            j\n          \n          ′\n        \n        \n        .\n      \n    \n    {\\displaystyle C_{ij}\\varepsilon _{i}\\varepsilon _{j}=C_{ij}'\\varepsilon '_{i}\\varepsilon '_{j}\\,.}\n  \n\nIn addition, if the material is symmetric with respect to the transformation [L] then\n\n  \n    \n      \n        \n          C\n          \n            i\n            j\n          \n        \n        =\n        \n          C\n          \n            i\n            j\n          \n          ′\n        \n        \n        \n        ⟹\n        \n        \n        \n          C\n          \n            i\n            j\n          \n        \n        (\n        \n          ε\n          \n            i\n          \n        \n        \n          ε\n          \n            j\n          \n        \n        −\n        \n          ε\n          \n            i\n          \n          ′\n        \n        \n          ε\n          \n            j\n          \n          ′\n        \n        )\n        =\n        0\n        \n        .\n      \n    \n    {\\displaystyle C_{ij}=C'_{ij}\\quad \\implies \\quad C_{ij}(\\varepsilon _{i}\\varepsilon _{j}-\\varepsilon '_{i}\\varepsilon '_{j})=0\\,.}\n\nOrthotropic materials have three orthogonal planes of symmetry. If the basis vectors (e1,e2,e3) are normals to the planes of symmetry then the coordinate transformation relations imply that\n\n[\n            \n              \n                \n                  \n                    σ\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    C\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      22\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      23\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      23\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      33\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    C\n                    \n                      44\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    C\n                    \n                      55\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    C\n                    \n                      66\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{1}\\\\\\sigma _{2}\\\\\\sigma _{3}\\\\\\sigma _{4}\\\\\\sigma _{5}\\\\\\sigma _{6}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}C_{11}&C_{12}&C_{13}&0&0&0\\\\C_{12}&C_{22}&C_{23}&0&0&0\\\\C_{13}&C_{23}&C_{33}&0&0&0\\\\0&0&0&C_{44}&0&0\\\\0&0&0&0&C_{55}&0\\\\0&0&0&0&0&C_{66}\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\end{bmatrix}}}\n  \n\nThe inverse of this relation is commonly written as[14][page needed]\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      z\n                      z\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      y\n                      z\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      z\n                      x\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          y\n                          x\n                        \n                      \n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          z\n                          x\n                        \n                      \n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          x\n                          y\n                        \n                      \n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          z\n                          y\n                        \n                      \n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          x\n                          z\n                        \n                      \n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          y\n                          z\n                        \n                      \n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          y\n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          z\n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          x\n                          y\n                        \n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      z\n                      z\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      z\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      z\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\varepsilon _{xx}\\\\\\varepsilon _{yy}\\\\\\varepsilon _{zz}\\\\2\\varepsilon _{yz}\\\\2\\varepsilon _{zx}\\\\2\\varepsilon _{xy}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}{\\frac {1}{E_{x}}}&-{\\frac {\\nu _{yx}}{E_{y}}}&-{\\frac {\\nu _{zx}}{E_{z}}}&0&0&0\\\\-{\\frac {\\nu _{xy}}{E_{x}}}&{\\frac {1}{E_{y}}}&-{\\frac {\\nu _{zy}}{E_{z}}}&0&0&0\\\\-{\\frac {\\nu _{xz}}{E_{x}}}&-{\\frac {\\nu _{yz}}{E_{y}}}&{\\frac {1}{E_{z}}}&0&0&0\\\\0&0&0&{\\frac {1}{G_{yz}}}&0&0\\\\0&0&0&0&{\\frac {1}{G_{zx}}}&0\\\\0&0&0&0&0&{\\frac {1}{G_{xy}}}\\\\\\end{bmatrix}}{\\begin{bmatrix}\\sigma _{xx}\\\\\\sigma _{yy}\\\\\\sigma _{zz}\\\\\\sigma _{yz}\\\\\\sigma _{zx}\\\\\\sigma _{xy}\\end{bmatrix}}}\n  \n\nwhere\n\nUnder plane stress conditions, σzz = σzx = σyz = 0, Hooke's law for an orthotropic material takes the form\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          y\n                          x\n                        \n                      \n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          x\n                          y\n                        \n                      \n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          x\n                          y\n                        \n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\varepsilon _{xx}\\\\\\varepsilon _{yy}\\\\2\\varepsilon _{xy}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}{\\frac {1}{E_{x}}}&-{\\frac {\\nu _{yx}}{E_{y}}}&0\\\\-{\\frac {\\nu _{xy}}{E_{x}}}&{\\frac {1}{E_{y}}}&0\\\\0&0&{\\frac {1}{G_{xy}}}\\end{bmatrix}}{\\begin{bmatrix}\\sigma _{xx}\\\\\\sigma _{yy}\\\\\\sigma _{xy}\\end{bmatrix}}\\,.}\n  \n\nThe inverse relation is\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            1\n            \n              1\n              −\n              \n                ν\n                \n                  x\n                  y\n                \n              \n              \n                ν\n                \n                  y\n                  x\n                \n              \n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                \n                \n                  \n                    ν\n                    \n                      y\n                      x\n                    \n                  \n                  \n                    E\n                    \n                      x\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    ν\n                    \n                      x\n                      y\n                    \n                  \n                  \n                    E\n                    \n                      y\n                    \n                  \n                \n                \n                  \n                    E\n                    \n                      y\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    G\n                    \n                      x\n                      y\n                    \n                  \n                  (\n                  1\n                  −\n                  \n                    ν\n                    \n                      x\n                      y\n                    \n                  \n                  \n                    ν\n                    \n                      y\n                      x\n                    \n                  \n                  )\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{xx}\\\\\\sigma _{yy}\\\\\\sigma _{xy}\\end{bmatrix}}\\,=\\,{\\frac {1}{1-\\nu _{xy}\\nu _{yx}}}{\\begin{bmatrix}E_{x}&\\nu _{yx}E_{x}&0\\\\\\nu _{xy}E_{y}&E_{y}&0\\\\0&0&G_{xy}(1-\\nu _{xy}\\nu _{yx})\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{xx}\\\\\\varepsilon _{yy}\\\\2\\varepsilon _{xy}\\end{bmatrix}}\\,.}\n  \n\nThe transposed form of the above stiffness matrix is also often used.\n\nA transversely isotropic material is symmetric with respect to a rotation about an axis of symmetry. For such a material, if e3 is the axis of symmetry, Hooke's law can be expressed as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    C\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    C\n                    \n                      12\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      13\n                    \n                  \n                \n                \n                  \n                    C\n                    \n                      33\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    C\n                    \n                      44\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    C\n                    \n                      44\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      \n                        \n                          C\n                          \n                            11\n                          \n                        \n                        −\n                        \n                          C\n                          \n                            12\n                          \n                        \n                      \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      6\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{1}\\\\\\sigma _{2}\\\\\\sigma _{3}\\\\\\sigma _{4}\\\\\\sigma _{5}\\\\\\sigma _{6}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}C_{11}&C_{12}&C_{13}&0&0&0\\\\C_{12}&C_{11}&C_{13}&0&0&0\\\\C_{13}&C_{13}&C_{33}&0&0&0\\\\0&0&0&C_{44}&0&0\\\\0&0&0&0&C_{44}&0\\\\0&0&0&0&0&{\\frac {C_{11}-C_{12}}{2}}\\end{bmatrix}}{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\end{bmatrix}}}\n\nMore frequently, the x ≡ e1 axis is taken to be the axis of symmetry and the inverse Hooke's law is written as\n[15]\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    ε\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    ε\n                    \n                      z\n                      z\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      y\n                      z\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      z\n                      x\n                    \n                  \n                \n              \n              \n                \n                  2\n                  \n                    ε\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          y\n                          x\n                        \n                      \n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          z\n                          x\n                        \n                      \n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          x\n                          y\n                        \n                      \n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          z\n                          y\n                        \n                      \n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          x\n                          z\n                        \n                      \n                      \n                        E\n                        \n                          x\n                        \n                      \n                    \n                  \n                \n                \n                  −\n                  \n                    \n                      \n                        ν\n                        \n                          y\n                          z\n                        \n                      \n                      \n                        E\n                        \n                          y\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      1\n                      \n                        E\n                        \n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          y\n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          x\n                          z\n                        \n                      \n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \n                    \n                      1\n                      \n                        G\n                        \n                          x\n                          y\n                        \n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    σ\n                    \n                      x\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      z\n                      z\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      y\n                      z\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      z\n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    σ\n                    \n                      x\n                      y\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\varepsilon _{xx}\\\\\\varepsilon _{yy}\\\\\\varepsilon _{zz}\\\\2\\varepsilon _{yz}\\\\2\\varepsilon _{zx}\\\\2\\varepsilon _{xy}\\end{bmatrix}}\\,=\\,{\\begin{bmatrix}{\\frac {1}{E_{x}}}&-{\\frac {\\nu _{yx}}{E_{y}}}&-{\\frac {\\nu _{zx}}{E_{z}}}&0&0&0\\\\-{\\frac {\\nu _{xy}}{E_{x}}}&{\\frac {1}{E_{y}}}&-{\\frac {\\nu _{zy}}{E_{z}}}&0&0&0\\\\-{\\frac {\\nu _{xz}}{E_{x}}}&-{\\frac {\\nu _{yz}}{E_{y}}}&{\\frac {1}{E_{z}}}&0&0&0\\\\0&0&0&{\\frac {1}{G_{yz}}}&0&0\\\\0&0&0&0&{\\frac {1}{G_{xz}}}&0\\\\0&0&0&0&0&{\\frac {1}{G_{xy}}}\\\\\\end{bmatrix}}{\\begin{bmatrix}\\sigma _{xx}\\\\\\sigma _{yy}\\\\\\sigma _{zz}\\\\\\sigma _{yz}\\\\\\sigma _{zx}\\\\\\sigma _{xy}\\end{bmatrix}}}\n\nTo grasp the degree of anisotropy of any class, a universal elastic anisotropy index (AU)[16] was formulated. It replaces the Zener ratio, which is suited for cubic crystals.\n\nLinear deformations of elastic materials can be approximated as adiabatic. Under these conditions and for quasistatic processes the first law of thermodynamics for a deformed body can be expressed as\n\n  \n    \n      \n        δ\n        W\n        =\n        δ\n        U\n      \n    \n    {\\displaystyle \\delta W=\\delta U}\n  \n\nwhere δU is the increase in internal energy and δW is the work done by external forces. The work can be split into two terms\n\n  \n    \n      \n        δ\n        W\n        =\n        δ\n        \n          W\n          \n            \n              s\n            \n          \n        \n        +\n        δ\n        \n          W\n          \n            \n              b\n            \n          \n        \n      \n    \n    {\\displaystyle \\delta W=\\delta W_{\\mathrm {s} }+\\delta W_{\\mathrm {b} }}\n  \n\nwhere δWs is the work done by surface forces while δWb is the work done by body forces. If δu is a variation of the displacement field u in the body, then the two external work terms can be expressed as\n\n  \n    \n      \n        δ\n        \n          W\n          \n            \n              s\n            \n          \n        \n        =\n        \n          ∫\n          \n            ∂\n            Ω\n          \n        \n        \n          t\n        \n        ⋅\n        δ\n        \n          u\n        \n        \n        d\n        S\n        \n        ;\n        \n        δ\n        \n          W\n          \n            \n              b\n            \n          \n        \n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        \n          b\n        \n        ⋅\n        δ\n        \n          u\n        \n        \n        d\n        V\n      \n    \n    {\\displaystyle \\delta W_{\\mathrm {s} }=\\int _{\\partial \\Omega }\\mathbf {t} \\cdot \\delta \\mathbf {u} \\,dS\\,;\\qquad \\delta W_{\\mathrm {b} }=\\int _{\\Omega }\\mathbf {b} \\cdot \\delta \\mathbf {u} \\,dV}\n  \n\nwhere t is the surface traction vector, b is the body force vector, Ω represents the body and ∂Ω represents its surface. Using the relation between the Cauchy stress and the surface traction, t = n · σ (where n is the unit outward normal to ∂Ω), we have\n\n  \n    \n      \n        δ\n        W\n        =\n        δ\n        U\n        =\n        \n          ∫\n          \n            ∂\n            Ω\n          \n        \n        (\n        \n          n\n        \n        ⋅\n        \n          σ\n        \n        )\n        ⋅\n        δ\n        \n          u\n        \n        \n        d\n        S\n        +\n        \n          ∫\n          \n            Ω\n          \n        \n        \n          b\n        \n        ⋅\n        δ\n        \n          u\n        \n        \n        d\n        V\n        \n        .\n      \n    \n    {\\displaystyle \\delta W=\\delta U=\\int _{\\partial \\Omega }(\\mathbf {n} \\cdot {\\boldsymbol {\\sigma }})\\cdot \\delta \\mathbf {u} \\,dS+\\int _{\\Omega }\\mathbf {b} \\cdot \\delta \\mathbf {u} \\,dV\\,.}\n  \n\nConverting the surface integral into a volume integral via the divergence theorem gives\n\n  \n    \n      \n        δ\n        U\n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        \n          \n            (\n          \n        \n        ∇\n        ⋅\n        (\n        \n          σ\n        \n        ⋅\n        δ\n        \n          u\n        \n        )\n        +\n        \n          b\n        \n        ⋅\n        δ\n        \n          u\n        \n        \n          \n            )\n          \n        \n        \n        d\n        V\n        \n        .\n      \n    \n    {\\displaystyle \\delta U=\\int _{\\Omega }{\\big (}\\nabla \\cdot ({\\boldsymbol {\\sigma }}\\cdot \\delta \\mathbf {u} )+\\mathbf {b} \\cdot \\delta \\mathbf {u} {\\big )}\\,dV\\,.}\n  \n\nUsing the symmetry of the Cauchy stress and the identity\n\n  \n    \n      \n        ∇\n        ⋅\n        (\n        \n          a\n        \n        ⋅\n        \n          b\n        \n        )\n        =\n        (\n        ∇\n        ⋅\n        \n          a\n        \n        )\n        ⋅\n        \n          b\n        \n        +\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          (\n          \n            \n              \n                a\n              \n              \n                \n                  T\n                \n              \n            \n            :\n            ∇\n            \n              b\n            \n            +\n            \n              a\n            \n            :\n            (\n            ∇\n            \n              b\n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\nabla \\cdot (\\mathbf {a} \\cdot \\mathbf {b} )=(\\nabla \\cdot \\mathbf {a} )\\cdot \\mathbf {b} +{\\tfrac {1}{2}}\\left(\\mathbf {a} ^{\\mathsf {T}}:\\nabla \\mathbf {b} +\\mathbf {a} :(\\nabla \\mathbf {b} )^{\\mathsf {T}}\\right)}\n  \n\nwe have the following\n\nδ\n        U\n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        \n          (\n          \n            \n              σ\n            \n            :\n            \n              \n                \n                  1\n                  2\n                \n              \n            \n            \n              (\n              \n                ∇\n                δ\n                \n                  u\n                \n                +\n                (\n                ∇\n                δ\n                \n                  u\n                \n                \n                  )\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              )\n            \n            +\n            \n              (\n              \n                ∇\n                ⋅\n                \n                  σ\n                \n                +\n                \n                  b\n                \n              \n              )\n            \n            ⋅\n            δ\n            \n              u\n            \n          \n          )\n        \n        \n        d\n        V\n        \n        .\n      \n    \n    {\\displaystyle \\delta U=\\int _{\\Omega }\\left({\\boldsymbol {\\sigma }}:{\\tfrac {1}{2}}\\left(\\nabla \\delta \\mathbf {u} +(\\nabla \\delta \\mathbf {u} )^{\\mathsf {T}}\\right)+\\left(\\nabla \\cdot {\\boldsymbol {\\sigma }}+\\mathbf {b} \\right)\\cdot \\delta \\mathbf {u} \\right)\\,dV\\,.}\n  \n\nFrom the definition of strain and from the equations of equilibrium we have\n\n  \n    \n      \n        δ\n        \n          ε\n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          (\n          \n            ∇\n            δ\n            \n              u\n            \n            +\n            (\n            ∇\n            δ\n            \n              u\n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n          \n          )\n        \n        \n        ;\n        \n        ∇\n        ⋅\n        \n          σ\n        \n        +\n        \n          b\n        \n        =\n        \n          0\n        \n        \n        .\n      \n    \n    {\\displaystyle \\delta {\\boldsymbol {\\varepsilon }}={\\tfrac {1}{2}}\\left(\\nabla \\delta \\mathbf {u} +(\\nabla \\delta \\mathbf {u} )^{\\mathsf {T}}\\right)\\,;\\qquad \\nabla \\cdot {\\boldsymbol {\\sigma }}+\\mathbf {b} =\\mathbf {0} \\,.}\n  \n\nHence we can write\n\n  \n    \n      \n        δ\n        U\n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        \n          σ\n        \n        :\n        δ\n        \n          ε\n        \n        \n        d\n        V\n      \n    \n    {\\displaystyle \\delta U=\\int _{\\Omega }{\\boldsymbol {\\sigma }}:\\delta {\\boldsymbol {\\varepsilon }}\\,dV}\n  \n\nand therefore the variation in the internal energy density is given by\n\n  \n    \n      \n        δ\n        \n          U\n          \n            0\n          \n        \n        =\n        \n          σ\n        \n        :\n        δ\n        \n          ε\n        \n        \n        .\n      \n    \n    {\\displaystyle \\delta U_{0}={\\boldsymbol {\\sigma }}:\\delta {\\boldsymbol {\\varepsilon }}\\,.}\n  \n\nAn elastic material is defined as one in which the total internal energy is equal to the potential energy of the internal forces (also called the elastic strain energy). Therefore, the internal energy density is a function of the strains, U0 = U0(ε) and the variation of the internal energy can be expressed as\n\n  \n    \n      \n        δ\n        \n          U\n          \n            0\n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                U\n                \n                  0\n                \n              \n            \n            \n              ∂\n              \n                ε\n              \n            \n          \n        \n        :\n        δ\n        \n          ε\n        \n        \n        .\n      \n    \n    {\\displaystyle \\delta U_{0}={\\frac {\\partial U_{0}}{\\partial {\\boldsymbol {\\varepsilon }}}}:\\delta {\\boldsymbol {\\varepsilon }}\\,.}\n  \n\nSince the variation of strain is arbitrary, the stress–strain relation of an elastic material is given by\n\n  \n    \n      \n        \n          σ\n        \n        =\n        \n          \n            \n              ∂\n              \n                U\n                \n                  0\n                \n              \n            \n            \n              ∂\n              \n                ε\n              \n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}={\\frac {\\partial U_{0}}{\\partial {\\boldsymbol {\\varepsilon }}}}\\,.}\n  \n\nFor a linear elastic material, the quantity ⁠∂U0/∂ε⁠ is a linear function of ε, and can therefore be expressed as\n\n  \n    \n      \n        \n          σ\n        \n        =\n        \n          \n            c\n          \n        \n        :\n        \n          ε\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}={\\mathsf {c}}:{\\boldsymbol {\\varepsilon }}}\n  \n\nwhere c is a fourth-rank tensor of material constants, also called the stiffness tensor. We can see why c must be a fourth-rank tensor by noting that, for a linear elastic material,\n\n  \n    \n      \n        \n          \n            ∂\n            \n              ∂\n              \n                ε\n              \n            \n          \n        \n        \n          σ\n        \n        (\n        \n          ε\n        \n        )\n        =\n        \n          constant\n        \n        =\n        \n          \n            c\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\partial }{\\partial {\\boldsymbol {\\varepsilon }}}}{\\boldsymbol {\\sigma }}({\\boldsymbol {\\varepsilon }})={\\text{constant}}={\\mathsf {c}}\\,.}\n  \n\nIn index notation\n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                σ\n                \n                  i\n                  j\n                \n              \n            \n            \n              ∂\n              \n                ε\n                \n                  k\n                  l\n                \n              \n            \n          \n        \n        =\n        \n          constant\n        \n        =\n        \n          c\n          \n            i\n            j\n            k\n            l\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\partial \\sigma _{ij}}{\\partial \\varepsilon _{kl}}}={\\text{constant}}=c_{ijkl}\\,.}\n\nThe right-hand side constant requires four indices and is a fourth-rank quantity. We can also see that this quantity must be a tensor because it is a linear transformation that takes the strain tensor to the stress tensor. We can also show that the constant obeys the tensor transformation rules for fourth-rank tensors.\n\nThere are two valid solutions. \nThe plus sign leads to \n  \n    \n      \n        ν\n        ≥\n        0\n      \n    \n    {\\displaystyle \\nu \\geq 0}\n  \n.",
        pageTitle: "Hooke's law",
    },
    {
        title: "Hotelling's law",
        link: "https://en.wikipedia.org/wiki/Hotelling%27s_law",
        content:
            "Hotelling's law is an observation in economics that in many markets it is rational for producers to make their products as similar as possible. This is also referred to as the principle of minimum differentiation as well as Hotelling's linear city model. The observation was made by Harold Hotelling (1895–1973) in the article \"Stability in Competition\" in the Economic Journal in 1929.[1]\n\nThe opposing phenomenon is product differentiation, which is usually considered to be a business advantage if executed properly.\n\nSuppose there are two competing shops located along the length of a street diverse running north and south, with customers spread equally along the street. Both shop owners want their shops to be where they will get most market share of customers. If both shops sell the same range of goods at the same prices then the locations of the shops are themselves the 'products'. Each customer will always choose the nearer shop as it is disadvantageous to travel to the farther.\n\nFor a single shop, the optimal location is anywhere along the length of the street. The shop owner is completely indifferent about the location of the shop since it will draw all customers to it, by default. However, from the point of view of a social welfare function that tries to minimize the distance that people need to travel, the optimal point is halfway along the length of the street.\n\nHotelling's law predicts that a street with two shops will also find both shops right next to each other at the same halfway point. Each shop will serve half the market; one will draw all customers from the north, the other all customers from the south.\n\nAnother example of the law in action is that of two takeaway food pushcarts, one at each end of a beach. If there is an equal distribution of rational consumers along the beach, each pushcart will get half the customers, divided by an invisible line equidistant from the carts. But, each pushcart owner will be tempted to push his cart slightly towards the other, moving the invisible line so that the owner is on the side with more than half the beach. Eventually, the pushcart operators will end up next to each other in the center of the beach.\n\nIn the case of three pushcarts an unstable equilibrium is reached. Imagine carts A and B are adjacent and each have access to half the potential customers (A’s to the left, B’s to the right). A new cart C can locate to the right of B, blocking B’s access to customers. B would then move to the right of C, blocking all of C’s customers, and the cycle would continue. Despite the constant battle for position, pushcarts follow Hotelling’s Law and aggregate near the center of the street.\n\nUnlike a linear road, in the case of a circular path two pushcarts A and B can locate anywhere along the path and equally share customers (indicated by red arrows). However, introducing a third pushcart again forces all three into an unstable equilibrium where two carts can entirely block a third’s access to customers, and all three carts cluster somewhere along the circle in a battle for position.\n\nIt would be more socially beneficial if the shops separated themselves and moved to one quarter of the way along the street from each end — each would still draw half the customers but customers would, on average, make a shorter journey. However, neither shop would be willing to do this independently, as it would then allow the other to relocate and capture more than half the market.\n\nWhen people along the street, or along the range of possible different product positions, consume more than a minimum number of goods (i.e. have discretionary income), companies can position their products to sections where consumers exist to maximize profit; this will often mean that companies will position themselves in different sections of the street, occupying niche markets. When prices are not fixed, companies can modify their prices to compete for customers; in those cases it is in the company's best interest to differentiate themselves as far away from each other as possible so they face less competition from each other.[2]\n\nIn a democracy, and especially in the American two-party system, political parties want to maximize the vote share allocated to their candidate. In theory, this means that political parties will adjust their platform to comply with the median voters' preferences. The Comparative Midpoints Model represents this idea best: Both political parties will get as close as possible to the competing party's platform while preserving its own identity.[3] However, party primaries can complicate this dynamic and make the stable points harder to find.[4]\n\nThe street is a metaphor for product differentiation; in the specific case of a street, the stores differentiate themselves from each other by location. The example can be generalized to all other types of horizontal product differentiation in almost any product characteristic, such as sweetness, colour, or size. The above case where the two stores are side by side would translate into products that are identical to each other. This phenomenon is present in many markets, particularly in those considered to be primarily commodities, and results in less variety for the consumer.\n\nThis phenomenon can be observed in real life, where commodity businesses like bars, restaurants, and gas stations, as well as large, branded chains, are located close to their rivals:[5]",
        pageTitle: "Hotelling's law",
    },
    {
        title: "Hubble's law",
        link: "https://en.wikipedia.org/wiki/Hubble%27s_law",
        content:
            'Hubble\'s law, also known as the Hubble–Lemaître law,[1] is the observation in physical cosmology that galaxies are moving away from Earth at speeds proportional to their distance. In other words, the farther a galaxy is from the Earth, the faster it moves away. A galaxy\'s recessional velocity is typically determined by measuring its redshift, a shift in the frequency of light emitted by the galaxy.\n\nThe discovery of Hubble\'s law is attributed to work published by Edwin Hubble in 1929,[2][3][4] but the notion of the universe expanding at a calculable rate was first derived from general relativity equations in 1922 by Alexander Friedmann. The Friedmann equations showed the universe might be expanding, and presented the expansion speed if that were the case.[5] Before Hubble, astronomer Carl Wilhelm Wirtz had, in 1922[6] and 1924,[7] deduced with his own data that galaxies that appeared smaller and dimmer had larger redshifts and thus that more distant galaxies recede faster from the observer. In 1927, Georges Lemaître concluded that the universe might be expanding by noting the proportionality of the recessional velocity of distant bodies to their respective distances. He estimated a value for this ratio, which—after Hubble confirmed cosmic expansion and determined a more precise value for it two years later—became known as the Hubble constant.[8][9][10][11][12] Hubble inferred the recession velocity of the objects from their redshifts, many of which were earlier measured and related to velocity by Vesto Slipher in 1917.[13][14][15] Combining Slipher\'s velocities with Henrietta Swan Leavitt\'s intergalactic distance calculations and methodology allowed Hubble to better calculate an expansion rate for the universe.[16]\n\nHubble\'s law is considered the first observational basis for the expansion of the universe, and is one of the pieces of evidence most often cited in support of the Big Bang model.[8][17] The motion of astronomical objects due solely to this expansion is known as the Hubble flow.[18] It is described by the equation v = H0D, with H0 the constant of proportionality—the Hubble constant—between the "proper distance" D to a galaxy (which can change over time, unlike the comoving distance) and its speed of separation v, i.e. the derivative of proper distance with respect to the cosmic time coordinate.[a] Though the Hubble constant H0 is constant at any given moment in time, the Hubble parameter H, of which the Hubble constant is the current value, varies with time, so the term constant is sometimes thought of as somewhat of a misnomer.[19][20]\n\nThe Hubble constant is most frequently quoted in km/s/Mpc, which gives the speed of a galaxy 1 megaparsec (3.09×1019 km) away as 70 km/s. Simplifying the units of the generalized form reveals that H0 specifies a frequency (SI unit: s−1), leading the reciprocal of H0 to be known as the Hubble time (14.4 billion years). The Hubble constant can also be stated as a relative rate of expansion. In this form H0 = 7%/Gyr, meaning that, at the current rate of expansion, it takes one billion years for an unbound structure to grow by 7%.\n\nA decade before Hubble made his observations, a number of physicists and mathematicians had established a consistent theory of an expanding universe by using Einstein field equations of general relativity. Applying the most general principles to the nature of the universe yielded a dynamic solution that conflicted with the then-prevalent notion of a static universe.\n\nIn 1912, Vesto M. Slipher measured the first Doppler shift of a "spiral nebula" (the obsolete term for spiral galaxies) and soon discovered that almost all such objects were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were "island universes" outside the Milky Way galaxy.[22][23]\n\nIn 1922, Alexander Friedmann derived his Friedmann equations from Einstein field equations, showing that the universe might expand at a rate calculable by the equations.[24] The parameter used by Friedmann is known today as the scale factor and can be considered as a scale invariant form of the proportionality constant of Hubble\'s law. Georges Lemaître independently found a similar solution in his 1927 paper discussed in the following section. The Friedmann equations are derived by inserting the metric for a homogeneous and isotropic universe into Einstein\'s field equations for a fluid with a given density and pressure. This idea of an expanding spacetime would eventually lead to the Big Bang and Steady State theories of cosmology.\n\nIn 1927, two years before Hubble published his own article, the Belgian priest and astronomer Georges Lemaître was the first to publish research deriving what is now known as Hubble\'s law. According to the Canadian astronomer Sidney van den Bergh, "the 1927 discovery of the expansion of the universe by Lemaître was published in French in a low-impact journal. In the 1931 high-impact English translation of this article, a critical equation was changed by omitting reference to what is now known as the Hubble constant."[25] It is now known that the alterations in the translated paper were carried out by Lemaître himself.[10][26]\n\nBefore the advent of modern cosmology, there was considerable talk about the size and shape of the universe. In 1920, the Shapley–Curtis debate took place between Harlow Shapley and Heber D. Curtis over this issue. Shapley argued for a small universe the size of the Milky Way galaxy, and Curtis argued that the universe was much larger. The issue was resolved in the coming decade with Hubble\'s improved observations.\n\nEdwin Hubble did most of his professional astronomical observing work at Mount Wilson Observatory,[27] home to the world\'s most powerful telescope at the time. His observations of Cepheid variable stars in "spiral nebulae" enabled him to calculate the distances to these objects. Surprisingly, these objects were discovered to be at distances which placed them well outside the Milky Way. They continued to be called nebulae, and it was only gradually that the term galaxies replaced it.\n\nThe velocities and distances that appear in Hubble\'s law are not directly measured. The velocities are inferred from the redshift z = ∆λ/λ of radiation and distance is inferred from brightness. Hubble sought to correlate brightness with parameter z.\n\nCombining his measurements of galaxy distances with Vesto Slipher and Milton Humason\'s measurements of the redshifts associated with the galaxies, Hubble discovered a rough proportionality between redshift of an object and its distance. Though there was considerable scatter (now known to be caused by peculiar velocities—the \'Hubble flow\' is used to refer to the region of space far enough out that the recession velocity is larger than local peculiar velocities), Hubble was able to plot a trend line from the 46 galaxies he studied and obtain a value for the Hubble constant of 500 (km/s)/Mpc (much higher than the currently accepted value due to errors in his distance calibrations; see cosmic distance ladder for details).[29]\n\nHubble\'s law can be easily depicted in a "Hubble diagram" in which the velocity (assumed approximately proportional to the redshift) of an object is plotted with respect to its distance from the observer.[30] A straight line of positive slope on this diagram is the visual depiction of Hubble\'s law.\n\nAfter Hubble\'s discovery was published, Albert Einstein abandoned his work on the cosmological constant, a term he had inserted into his equations of general relativity to coerce them into producing the static solution he previously considered the correct state of the universe. The Einstein equations in their simplest form model either an expanding or contracting universe, so Einstein introduced the constant to counter expansion or contraction and lead to a static and flat universe.[31] After Hubble\'s discovery that the universe was, in fact, expanding, Einstein called his faulty assumption that the universe is static his "greatest mistake".[31] On its own, general relativity could predict the expansion of the universe, which (through observations such as the bending of light by large masses, or the precession of the orbit of Mercury) could be experimentally observed and compared to his theoretical calculations using particular solutions of the equations he had originally formulated.\n\nIn 1931, Einstein went to Mount Wilson Observatory to thank Hubble for providing the observational basis for modern cosmology.[32]\n\nThe cosmological constant has regained attention in recent decades as a hypothetical explanation for dark energy.[33]\n\nThe discovery of the linear relationship between redshift and distance, coupled with a supposed linear relation between recessional velocity and redshift, yields a straightforward mathematical expression for Hubble\'s law as follows:\n\nv\n        =\n        \n          H\n          \n            0\n          \n        \n        \n        D\n      \n    \n    {\\displaystyle v=H_{0}\\,D}\n\nHubble\'s law is considered a fundamental relation between recessional velocity and distance. However, the relation between recessional velocity and redshift depends on the cosmological model adopted and is not established except for small redshifts.\n\nFor distances D larger than the radius of the Hubble sphere rHS, objects recede at a rate faster than the speed of light (See Uses of the proper distance for a discussion of the significance of this):\n\nr\n          \n            HS\n          \n        \n        =\n        \n          \n            c\n            \n              H\n              \n                0\n              \n            \n          \n        \n         \n        .\n      \n    \n    {\\displaystyle r_{\\text{HS}}={\\frac {c}{H_{0}}}\\ .}\n\nSince the Hubble "constant" is a constant only in space, not in time, the radius of the Hubble sphere may increase or decrease over various time intervals. The subscript \'0\' indicates the value of the Hubble constant today.[28] Current evidence suggests that the expansion of the universe is accelerating (see Accelerating universe), meaning that for any given galaxy, the recession velocity dD/dt is increasing over time as the galaxy moves to greater and greater distances; however, the Hubble parameter is actually thought to be decreasing with time, meaning that if we were to look at some fixed distance D and watch a series of different galaxies pass that distance, later galaxies would pass that distance at a smaller velocity than earlier ones.[35]\n\nRedshift can be measured by determining the wavelength of a known transition, such as hydrogen α-lines for distant quasars, and finding the fractional shift compared to a stationary reference. Thus, redshift is a quantity unambiguously acquired from observation. Care is required, however, in translating these to recessional velocities: for small redshift values, a linear relation of redshift to recessional velocity applies, but more generally the redshift-distance law is nonlinear, meaning the co-relation must be derived specifically for each given model and epoch.[36]\n\nThe redshift z is often described as a redshift velocity, which is the recessional velocity that would produce the same redshift if it were caused by a linear Doppler effect (which, however, is not the case, as the velocities involved are too large to use a non-relativistic formula for Doppler shift). This redshift velocity can easily exceed the speed of light.[37] In other words, to determine the redshift velocity vrs, the relation:\n\nv\n          \n            rs\n          \n        \n        ≡\n        c\n        z\n        ,\n      \n    \n    {\\displaystyle v_{\\text{rs}}\\equiv cz,}\n\nis used.[38][39] That is, there is no fundamental difference between redshift velocity and redshift: they are rigidly proportional, and not related by any theoretical reasoning. The motivation behind the "redshift velocity" terminology is that the redshift velocity agrees with the velocity from a low-velocity simplification of the so-called Fizeau–Doppler formula[40]\n\nz\n        =\n        \n          \n            \n              λ\n              \n                o\n              \n            \n            \n              λ\n              \n                e\n              \n            \n          \n        \n        −\n        1\n        =\n        \n          \n            \n              \n                1\n                +\n                \n                  \n                    v\n                    c\n                  \n                \n              \n              \n                1\n                −\n                \n                  \n                    v\n                    c\n                  \n                \n              \n            \n          \n        \n        −\n        1\n        ≈\n        \n          \n            v\n            c\n          \n        \n        .\n      \n    \n    {\\displaystyle z={\\frac {\\lambda _{\\text{o}}}{\\lambda _{\\text{e}}}}-1={\\sqrt {\\frac {1+{\\frac {v}{c}}}{1-{\\frac {v}{c}}}}}-1\\approx {\\frac {v}{c}}.}\n\nHere, λo, λe are the observed and emitted wavelengths respectively. The "redshift velocity" vrs is not so simply related to real velocity at larger velocities, however, and this terminology leads to confusion if interpreted as a real velocity. Next, the connection between redshift or redshift velocity and recessional velocity is discussed.[41]\n\nSuppose R(t) is called the scale factor of the universe, and increases as the universe expands in a manner that depends upon the cosmological model selected. Its meaning is that all measured proper distances D(t) between co-moving points increase proportionally to R. (The co-moving points are not moving relative to their local environments.) In other words:\n\nD\n              (\n              t\n              )\n            \n            \n              D\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n          \n        \n        =\n        \n          \n            \n              R\n              (\n              t\n              )\n            \n            \n              R\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {D(t)}{D(t_{0})}}={\\frac {R(t)}{R(t_{0})}},}\n\nwhere t0 is some reference time.[42] If light is emitted from a galaxy at time te and received by us at t0, it is redshifted due to the expansion of the universe, and this redshift z is simply:\n\nz\n        =\n        \n          \n            \n              R\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n            \n              R\n              (\n              \n                t\n                \n                  e\n                \n              \n              )\n            \n          \n        \n        −\n        1.\n      \n    \n    {\\displaystyle z={\\frac {R(t_{0})}{R(t_{\\text{e}})}}-1.}\n\nSuppose a galaxy is at distance D, and this distance changes with time at a rate dtD. We call this rate of recession the "recession velocity" vr:\n\nv\n          \n            r\n          \n        \n        =\n        \n          d\n          \n            t\n          \n        \n        D\n        =\n        \n          \n            \n              \n                d\n                \n                  t\n                \n              \n              R\n            \n            R\n          \n        \n        D\n        .\n      \n    \n    {\\displaystyle v_{\\text{r}}=d_{t}D={\\frac {d_{t}R}{R}}D.}\n\nH\n        ≡\n        \n          \n            \n              \n                d\n                \n                  t\n                \n              \n              R\n            \n            R\n          \n        \n        ,\n      \n    \n    {\\displaystyle H\\equiv {\\frac {d_{t}R}{R}},}\n\nv\n          \n            r\n          \n        \n        =\n        H\n        D\n        .\n      \n    \n    {\\displaystyle v_{\\text{r}}=HD.}\n\nFrom this perspective, Hubble\'s law is a fundamental relation between (i) the recessional velocity associated with the expansion of the universe and (ii) the distance to an object; the connection between redshift and distance is a crutch used to connect Hubble\'s law with observations. This law can be related to redshift z approximately by making a Taylor series expansion:\n\nz\n        =\n        \n          \n            \n              R\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n            \n              R\n              (\n              \n                t\n                \n                  e\n                \n              \n              )\n            \n          \n        \n        −\n        1\n        ≈\n        \n          \n            \n              R\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n            \n              R\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n              \n                (\n                \n                  1\n                  +\n                  (\n                  \n                    t\n                    \n                      e\n                    \n                  \n                  −\n                  \n                    t\n                    \n                      0\n                    \n                  \n                  )\n                  H\n                  (\n                  \n                    t\n                    \n                      0\n                    \n                  \n                  )\n                \n                )\n              \n            \n          \n        \n        −\n        1\n        ≈\n        (\n        \n          t\n          \n            0\n          \n        \n        −\n        \n          t\n          \n            e\n          \n        \n        )\n        H\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle z={\\frac {R(t_{0})}{R(t_{e})}}-1\\approx {\\frac {R(t_{0})}{R(t_{0})\\left(1+(t_{e}-t_{0})H(t_{0})\\right)}}-1\\approx (t_{0}-t_{e})H(t_{0}),}\n\nIf the distance is not too large, all other complications of the model become small corrections, and the time interval is simply the distance divided by the speed of light:\n\nz\n        ≈\n        (\n        \n          t\n          \n            0\n          \n        \n        −\n        \n          t\n          \n            e\n          \n        \n        )\n        H\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ≈\n        \n          \n            D\n            c\n          \n        \n        H\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle z\\approx (t_{0}-t_{\\text{e}})H(t_{0})\\approx {\\frac {D}{c}}H(t_{0}),}\n\nc\n        z\n        ≈\n        D\n        H\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        =\n        \n          v\n          \n            r\n          \n        \n        .\n      \n    \n    {\\displaystyle cz\\approx DH(t_{0})=v_{r}.}\n\nAccording to this approach, the relation cz = vr is an approximation valid at low redshifts, to be replaced by a relation at large redshifts that is model-dependent. See velocity-redshift figure.\n\nStrictly speaking, neither v nor D in the formula are directly observable, because they are properties now of a galaxy, whereas our observations refer to the galaxy in the past, at the time that the light we currently see left it.\n\nFor relatively nearby galaxies (redshift z much less than one), v and D will not have changed much, and v can be estimated using the formula v = zc where c is the speed of light. This gives the empirical relation found by Hubble.\n\nFor distant galaxies, v (or D) cannot be calculated from z without specifying a detailed model for how H changes with time. The redshift is not even directly related to the recession velocity at the time the light set out, but it does have a simple interpretation: (1 + z) is the factor by which the universe has expanded while the photon was traveling towards the observer.\n\nIn using Hubble\'s law to determine distances, only the velocity due to the expansion of the universe can be used. Since gravitationally interacting galaxies move relative to each other independent of the expansion of the universe,[43] these relative velocities, called peculiar velocities, need to be accounted for in the application of Hubble\'s law. Such peculiar velocities give rise to redshift-space distortions.\n\nThe parameter H is commonly called the "Hubble constant", but that is a misnomer since it is constant in space only at a fixed time; it varies with time in nearly all cosmological models, and all observations of far distant objects are also observations into the distant past, when the "constant" had a different value. "Hubble parameter" is a more correct term, with H0 denoting the present-day value.\n\nAnother common source of confusion is that the accelerating universe does not imply that the Hubble parameter is actually increasing with time; since \n  \n    \n      \n        H\n        (\n        t\n        )\n        ≡\n        \n          \n            \n              a\n              ˙\n            \n          \n        \n        (\n        t\n        )\n        \n          /\n        \n        a\n        (\n        t\n        )\n      \n    \n    {\\displaystyle H(t)\\equiv {\\dot {a}}(t)/a(t)}\n  \n, in most accelerating models \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n increases relatively faster than \n  \n    \n      \n        \n          \n            \n              a\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {a}}}\n  \n, so H decreases with time. (The recession velocity of one chosen galaxy does increase, but different galaxies passing a sphere of fixed radius cross the sphere more slowly at later times.)\n\nOn defining the dimensionless deceleration parameter \n  \n    \n      \n        q\n        ≡\n        −\n        \n          \n            \n              \n                \n                  \n                    a\n                    ¨\n                  \n                \n              \n              \n              a\n            \n            \n              \n                \n                  \n                    a\n                    ˙\n                  \n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\textstyle q\\equiv -{\\frac {{\\ddot {a}}\\,a}{{\\dot {a}}^{2}}}}\n  \n, it follows that\n\nd\n              H\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          H\n          \n            2\n          \n        \n        (\n        1\n        +\n        q\n        )\n      \n    \n    {\\displaystyle {\\frac {dH}{dt}}=-H^{2}(1+q)}\n\nFrom this it is seen that the Hubble parameter is decreasing with time, unless q < -1; the latter can only occur if the universe contains phantom energy, regarded as theoretically somewhat improbable.\n\nHowever, in the standard Lambda cold dark matter model (Lambda-CDM or ΛCDM model), q will tend to −1 from above in the distant future as the cosmological constant becomes increasingly dominant over matter; this implies that H will approach from above to a constant value of ≈ 57 (km/s)/Mpc, and the scale factor of the universe will then grow exponentially in time.\n\nThe mathematical derivation of an idealized Hubble\'s law for a uniformly expanding universe is a fairly elementary theorem of geometry in 3-dimensional Cartesian/Newtonian coordinate space, which, considered as a metric space, is entirely homogeneous and isotropic (properties do not vary with location or direction). Simply stated, the theorem is this:\n\nAny two points which are moving away from the origin, each along straight lines and with speed proportional to distance from the origin, will be moving away from each other with a speed proportional to their distance apart.\n\nIn fact, this applies to non-Cartesian spaces as long as they are locally homogeneous and isotropic, specifically to the negatively and positively curved spaces frequently considered as cosmological models (see shape of the universe).\n\nAn observation stemming from this theorem is that seeing objects recede from us on Earth is not an indication that Earth is near to a center from which the expansion is occurring, but rather that every observer in an expanding universe will see objects receding from them.\n\nThe value of the Hubble parameter changes over time, either increasing or decreasing depending on the value of the so-called deceleration parameter q, which is defined by\n\nq\n        =\n        −\n        \n          (\n          \n            1\n            +\n            \n              \n                \n                  \n                    H\n                    ˙\n                  \n                \n                \n                  H\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle q=-\\left(1+{\\frac {\\dot {H}}{H^{2}}}\\right).}\n\nIn a universe with a deceleration parameter equal to zero, it follows that H = 1/t, where t is the time since the Big Bang. A non-zero, time-dependent value of q simply requires integration of the Friedmann equations backwards from the present time to the time when the comoving horizon size was zero.\n\nIt was long thought that q was positive, indicating that the expansion is slowing down due to gravitational attraction. This would imply an age of the universe less than 1/H (which is about 14 billion years). For instance, a value for q of 1/2 (once favoured by most theorists) would give the age of the universe as 2/(3H). The discovery in 1998 that q is apparently negative means that the universe could actually be older than 1/H. However, estimates of the age of the universe are very close to 1/H.\n\nThe expansion of space summarized by the Big Bang interpretation of Hubble\'s law is relevant to the old conundrum known as Olbers\' paradox: If the universe were infinite in size, static, and filled with a uniform distribution of stars, then every line of sight in the sky would end on a star, and the sky would be as bright as the surface of a star. However, the night sky is largely dark.[44][45]\n\nSince the 17th century, astronomers and other thinkers have proposed many possible ways to resolve this paradox, but the currently accepted resolution depends in part on the Big Bang theory, and in part on the Hubble expansion: in a universe that existed for a finite amount of time, only the light of a finite number of stars has had enough time to reach us, and the paradox is resolved. Additionally, in an expanding universe, distant objects recede from us, which causes the light emanated from them to be redshifted and diminished in brightness by the time we see it.[44][45]\n\nInstead of working with Hubble\'s constant, a common practice is to introduce the dimensionless Hubble constant, usually denoted by h and commonly referred to as "little h",[29] then to write Hubble\'s constant H0 as h × 100 km⋅s−1⋅Mpc−1, all the relative uncertainty of the true value of H0 being then relegated to h.[46] The dimensionless Hubble constant is often used when giving distances that are calculated from redshift z using the formula d ≈ .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠c/H0⁠ × z. Since H0 is not precisely known, the distance is expressed as:\n\nc\n        z\n        \n          /\n        \n        \n          H\n          \n            0\n          \n        \n        ≈\n        (\n        2998\n        ×\n        z\n        )\n        \n           Mpc \n        \n        \n          h\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle cz/H_{0}\\approx (2998\\times z){\\text{ Mpc }}h^{-1}}\n\nIn other words, one calculates 2998 × z and one gives the units as Mpc h-1 or h-1 Mpc.\n\nOccasionally a reference value other than 100 may be chosen, in which case a subscript is presented after h to avoid confusion; e.g. h70 denotes H0 = 70 h70 (km/s)/Mpc, which implies h70 = h / 0.7.\n\nThis should not be confused with the dimensionless value of Hubble\'s constant, usually expressed in terms of Planck units, obtained by multiplying H0 by 1.75×10−63 (from definitions of parsec and tP), for example for H0 = 70, a Planck unit version of 1.2×10−61 is obtained.\n\nA value for q measured from standard candle observations of Type Ia supernovae, which was determined in 1998 to be negative, surprised many astronomers with the implication that the expansion of the universe is currently "accelerating"[47] (although the Hubble factor is still decreasing with time, as mentioned above in the Interpretation section; see the articles on dark energy and the ΛCDM model).\n\nH\n          \n            2\n          \n        \n        ≡\n        \n          \n            (\n            \n              \n                \n                  \n                    a\n                    ˙\n                  \n                \n                a\n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              8\n              π\n              G\n            \n            3\n          \n        \n        ρ\n        −\n        \n          \n            \n              k\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              a\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            \n              Λ\n              \n                c\n                \n                  2\n                \n              \n            \n            3\n          \n        \n        ,\n      \n    \n    {\\displaystyle H^{2}\\equiv \\left({\\frac {\\dot {a}}{a}}\\right)^{2}={\\frac {8\\pi G}{3}}\\rho -{\\frac {kc^{2}}{a^{2}}}+{\\frac {\\Lambda c^{2}}{3}},}\n\nwhere H is the Hubble parameter, a is the scale factor, G is the gravitational constant, k is the normalised spatial curvature of the universe and equal to −1, 0, or 1, and Λ is the cosmological constant.\n\nIf the universe is matter-dominated, then the mass density of the universe ρ can be taken to include just matter so\n\nρ\n        =\n        \n          ρ\n          \n            m\n          \n        \n        (\n        a\n        )\n        =\n        \n          \n            \n              ρ\n              \n                \n                  m\n                  \n                    0\n                  \n                \n              \n            \n            \n              a\n              \n                3\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho =\\rho _{m}(a)={\\frac {\\rho _{m_{0}}}{a^{3}}},}\n\nwhere ρm0 is the density of matter today. From the Friedmann equation and thermodynamic principles we know for non-relativistic particles that their mass density decreases proportional to the inverse volume of the universe, so the equation above must be true. We can also define (see density parameter for Ωm)\n\nρ\n                  \n                    c\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      3\n                      \n                        H\n                        \n                          0\n                        \n                        \n                          2\n                        \n                      \n                    \n                    \n                      8\n                      π\n                      G\n                    \n                  \n                \n                ;\n              \n            \n            \n              \n                \n                  Ω\n                  \n                    m\n                  \n                \n              \n              \n                \n                ≡\n                \n                  \n                    \n                      ρ\n                      \n                        \n                          m\n                          \n                            0\n                          \n                        \n                      \n                    \n                    \n                      ρ\n                      \n                        c\n                      \n                    \n                  \n                \n                =\n                \n                  \n                    \n                      8\n                      π\n                      G\n                    \n                    \n                      3\n                      \n                        H\n                        \n                          0\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                \n                  ρ\n                  \n                    \n                      m\n                      \n                        0\n                      \n                    \n                  \n                \n                ;\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho _{c}&={\\frac {3H_{0}^{2}}{8\\pi G}};\\\\\\Omega _{m}&\\equiv {\\frac {\\rho _{m_{0}}}{\\rho _{c}}}={\\frac {8\\pi G}{3H_{0}^{2}}}\\rho _{m_{0}};\\end{aligned}}}\n\nρ\n        =\n        \n          \n            \n              \n                ρ\n                \n                  c\n                \n              \n              \n                Ω\n                \n                  m\n                \n              \n            \n            \n              a\n              \n                3\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\rho ={\\frac {\\rho _{c}\\Omega _{m}}{a^{3}}}.}\n\nAlso, by definition,\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  Ω\n                  \n                    k\n                  \n                \n              \n              \n                \n                ≡\n                \n                  \n                    \n                      −\n                      k\n                      \n                        c\n                        \n                          2\n                        \n                      \n                    \n                    \n                      (\n                      \n                        a\n                        \n                          0\n                        \n                      \n                      \n                        H\n                        \n                          0\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  Ω\n                  \n                    Λ\n                  \n                \n              \n              \n                \n                ≡\n                \n                  \n                    \n                      Λ\n                      \n                        c\n                        \n                          2\n                        \n                      \n                    \n                    \n                      3\n                      \n                        H\n                        \n                          0\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Omega _{k}&\\equiv {\\frac {-kc^{2}}{(a_{0}H_{0})^{2}}}\\\\\\Omega _{\\Lambda }&\\equiv {\\frac {\\Lambda c^{2}}{3H_{0}^{2}}},\\end{aligned}}}\n\nwhere the subscript 0 refers to the values today, and a0 = 1. Substituting all of this into the Friedmann equation at the start of this section and replacing a with a = 1/(1+z) gives\n\nH\n          \n            2\n          \n        \n        (\n        z\n        )\n        =\n        \n          H\n          \n            0\n          \n          \n            2\n          \n        \n        \n          (\n          \n            \n              Ω\n              \n                m\n              \n            \n            (\n            1\n            +\n            z\n            \n              )\n              \n                3\n              \n            \n            +\n            \n              Ω\n              \n                k\n              \n            \n            (\n            1\n            +\n            z\n            \n              )\n              \n                2\n              \n            \n            +\n            \n              Ω\n              \n                Λ\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}(1+z)^{3}+\\Omega _{k}(1+z)^{2}+\\Omega _{\\Lambda }\\right).}\n\nIf the universe is both matter-dominated and dark energy-dominated, then the above equation for the Hubble parameter will also be a function of the equation of state of dark energy. So now:\n\nρ\n        =\n        \n          ρ\n          \n            m\n          \n        \n        (\n        a\n        )\n        +\n        \n          ρ\n          \n            d\n            e\n          \n        \n        (\n        a\n        )\n        ,\n      \n    \n    {\\displaystyle \\rho =\\rho _{m}(a)+\\rho _{de}(a),}\n\nwhere ρde is the mass density of the dark energy. By definition, an equation of state in cosmology is P = wρc2, and if this is substituted into the fluid equation, which describes how the mass density of the universe evolves with time, then\n\nρ\n                      ˙\n                    \n                  \n                \n                +\n                3\n                \n                  \n                    \n                      \n                        a\n                        ˙\n                      \n                    \n                    a\n                  \n                \n                \n                  (\n                  \n                    ρ\n                    +\n                    \n                      \n                        P\n                        \n                          c\n                          \n                            2\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                =\n                0\n                ;\n              \n            \n            \n              \n                \n                  \n                    \n                      d\n                      ρ\n                    \n                    ρ\n                  \n                \n                =\n                −\n                3\n                \n                  \n                    \n                      d\n                      a\n                    \n                    a\n                  \n                \n                (\n                1\n                +\n                w\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\dot {\\rho }}+3{\\frac {\\dot {a}}{a}}\\left(\\rho +{\\frac {P}{c^{2}}}\\right)=0;\\\\{\\frac {d\\rho }{\\rho }}=-3{\\frac {da}{a}}(1+w).\\end{aligned}}}\n\nln\n        ⁡\n        \n          ρ\n        \n        =\n        −\n        3\n        (\n        1\n        +\n        w\n        )\n        ln\n        ⁡\n        \n          a\n        \n        ;\n      \n    \n    {\\displaystyle \\ln {\\rho }=-3(1+w)\\ln {a};}\n\nρ\n        =\n        \n          a\n          \n            −\n            3\n            (\n            1\n            +\n            w\n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle \\rho =a^{-3(1+w)}.}\n\nTherefore, for dark energy with a constant equation of state w, \n  \n    \n      \n        \n          ρ\n          \n            d\n            e\n          \n        \n        (\n        a\n        )\n        =\n        \n          ρ\n          \n            d\n            e\n            0\n          \n        \n        \n          a\n          \n            −\n            3\n            (\n            1\n            +\n            w\n            )\n          \n        \n      \n    \n    {\\displaystyle \\rho _{de}(a)=\\rho _{de0}a^{-3(1+w)}}\n  \n. If this is substituted into the Friedman equation in a similar way as before, but this time set k = 0, which assumes a spatially flat universe, then (see shape of the universe)\n\nH\n          \n            2\n          \n        \n        (\n        z\n        )\n        =\n        \n          H\n          \n            0\n          \n          \n            2\n          \n        \n        \n          (\n          \n            \n              Ω\n              \n                m\n              \n            \n            (\n            1\n            +\n            z\n            \n              )\n              \n                3\n              \n            \n            +\n            \n              Ω\n              \n                d\n                e\n              \n            \n            (\n            1\n            +\n            z\n            \n              )\n              \n                3\n                (\n                1\n                +\n                w\n                )\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}(1+z)^{3}+\\Omega _{de}(1+z)^{3(1+w)}\\right).}\n\nIf the dark energy derives from a cosmological constant such as that introduced by Einstein, it can be shown that w = −1. The equation then reduces to the last equation in the matter-dominated universe section, with Ωk set to zero. In that case the initial dark energy density ρde0 is given by[48]\n\nρ\n                  \n                    d\n                    e\n                    0\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      Λ\n                      \n                        c\n                        \n                          2\n                        \n                      \n                    \n                    \n                      8\n                      π\n                      G\n                    \n                  \n                \n                \n                ,\n              \n            \n            \n              \n                \n                  Ω\n                  \n                    d\n                    e\n                  \n                \n              \n              \n                \n                =\n                \n                  Ω\n                  \n                    Λ\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho _{de0}&={\\frac {\\Lambda c^{2}}{8\\pi G}}\\,,\\\\\\Omega _{de}&=\\Omega _{\\Lambda }.\\end{aligned}}}\n\nIf dark energy does not have a constant equation-of-state w, then\n\nρ\n          \n            d\n            e\n          \n        \n        (\n        a\n        )\n        =\n        \n          ρ\n          \n            d\n            e\n            0\n          \n        \n        \n          e\n          \n            −\n            3\n            ∫\n            \n              \n                \n                  d\n                  a\n                \n                a\n              \n            \n            \n              (\n              \n                1\n                +\n                w\n                (\n                a\n                )\n              \n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\rho _{de}(a)=\\rho _{de0}e^{-3\\int {\\frac {da}{a}}\\left(1+w(a)\\right)},}\n\nand to solve this, w(a) must be parametrized, for example if w(a) = w0 + wa(1−a), giving[49]\n\nH\n          \n            2\n          \n        \n        (\n        z\n        )\n        =\n        \n          H\n          \n            0\n          \n          \n            2\n          \n        \n        \n          (\n          \n            \n              Ω\n              \n                m\n              \n            \n            \n              a\n              \n                −\n                3\n              \n            \n            +\n            \n              Ω\n              \n                d\n                e\n              \n            \n            \n              a\n              \n                −\n                3\n                \n                  (\n                  \n                    1\n                    +\n                    \n                      w\n                      \n                        0\n                      \n                    \n                    +\n                    \n                      w\n                      \n                        a\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              e\n              \n                −\n                3\n                \n                  w\n                  \n                    a\n                  \n                \n                (\n                1\n                −\n                a\n                )\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}a^{-3}+\\Omega _{de}a^{-3\\left(1+w_{0}+w_{a}\\right)}e^{-3w_{a}(1-a)}\\right).}\n\nThe Hubble constant H0 has units of inverse time; the Hubble time tH is simply defined as the inverse of the Hubble constant,[50] i.e.\n\nt\n          \n            H\n          \n        \n        ≡\n        \n          \n            1\n            \n              H\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              67.8\n              \n                 \n                (\n                k\n                m\n                \n                  /\n                \n                s\n                )\n                \n                  /\n                \n                M\n                p\n                c\n              \n            \n          \n        \n        =\n        4.55\n        ×\n        \n          10\n          \n            17\n          \n        \n        \n           \n          s\n        \n        =\n        14.4\n        \n           billion years\n        \n        .\n      \n    \n    {\\displaystyle t_{H}\\equiv {\\frac {1}{H_{0}}}={\\frac {1}{67.8\\mathrm {~(km/s)/Mpc} }}=4.55\\times 10^{17}\\mathrm {~s} =14.4{\\text{ billion years}}.}\n\nThis is slightly different from the age of the universe, which is approximately 13.8 billion years. The Hubble time is the age it would have had if the expansion had been linear,[51] and it is different from the real age of the universe because the expansion is not linear; it depends on the energy content of the universe (see § Derivation of the Hubble parameter).\n\nWe currently appear to be approaching a period where the expansion of the universe is exponential due to the increasing dominance of vacuum energy. In this regime, the Hubble parameter is constant, and the universe grows by a factor e each Hubble time:\n\nH\n        ≡\n        \n          \n            \n              \n                a\n                ˙\n              \n            \n            a\n          \n        \n        =\n        \n          \n            constant\n          \n        \n        \n        ⟹\n        \n        a\n        ∝\n        \n          e\n          \n            H\n            t\n          \n        \n        =\n        \n          e\n          \n            \n              t\n              \n                t\n                \n                  H\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle H\\equiv {\\frac {\\dot {a}}{a}}={\\textrm {constant}}\\quad \\Longrightarrow \\quad a\\propto e^{Ht}=e^{\\frac {t}{t_{H}}}}\n\nLikewise, the generally accepted value of 2.27 Es−1 means that (at the current rate) the universe would grow by a factor of e2.27 in one exasecond.\n\nOver long periods of time, the dynamics are complicated by general relativity, dark energy, inflation, etc., as explained above.\n\nThe Hubble length or Hubble distance is a unit of distance in cosmology, defined as cH−1 — the speed of light multiplied by the Hubble time. It is equivalent to 4,420 million parsecs or 14.4 billion light years. (The numerical value of the Hubble length in light years is, by definition, equal to that of the Hubble time in years.) Substituting D = cH−1 into the equation for Hubble\'s law, v = H0D reveals that the Hubble distance specifies the distance from our location to those galaxies which are currently receding from us at the speed of light.\n\nThe Hubble volume is sometimes defined as a volume of the universe with a comoving size of cH−1. The exact definition varies: it is sometimes defined as the volume of a sphere with radius cH−1, or alternatively, a cube of side cH−1. Some cosmologists even use the term Hubble volume to refer to the volume of the observable universe, although this has a radius approximately three times larger.\n\nThe value of the Hubble constant, H0, cannot be measured directly, but is derived from a combination of astronomical observations and model-dependent assumptions. Increasingly accurate observations and new models over many decades have led to two sets of highly precise values which do not agree. This difference is known as the "Hubble tension".[8][53]\n\nFor the original 1929 estimate of the constant now bearing his name, Hubble used observations of Cepheid variable stars as "standard candles" to measure distance.[54] The result he obtained was 500 (km/s)/Mpc, much larger than the value astronomers currently calculate. Later observations by astronomer Walter Baade led him to realize that there were distinct "populations" for stars (Population I and Population II) in a galaxy. The same observations led him to discover that there are two types of Cepheid variable stars with different luminosities. Using this discovery, he recalculated Hubble constant and the size of the known universe, doubling the previous calculation made by Hubble in 1929.[55][56][54] He announced this finding to considerable astonishment at the 1952 meeting of the International Astronomical Union in Rome.\n\nFor most of the second half of the 20th century, the value of H0 was estimated to be between 50 and 90 (km/s)/Mpc.\n\nThe value of the Hubble constant was the topic of a long and rather bitter controversy between Gérard de Vaucouleurs, who claimed the value was around 100, and Allan Sandage, who claimed the value was near 50.[57] In one demonstration of vitriol shared between the parties, when Sandage and Gustav Andreas Tammann (Sandage\'s research colleague) formally acknowledged the shortcomings of confirming the systematic error of their method in 1975, Vaucouleurs responded "It is unfortunate that this sober warning was so soon forgotten and ignored by most astronomers and textbook writers".[58] In 1996, a debate moderated by John Bahcall between Sidney van den Bergh and Gustav Tammann was held in similar fashion to the earlier Shapley–Curtis debate over these two competing values.\n\nThis previously wide variance in estimates was partially resolved with the introduction of the ΛCDM model of the universe in the late 1990s. Incorporating the ΛCDM model, observations of high-redshift clusters at X-ray and microwave wavelengths using the Sunyaev–Zel\'dovich effect, measurements of anisotropies in the cosmic microwave background radiation, and optical surveys all gave a value of around 50–70 km/s/Mpc for the constant.[59]\n\nBy the late 1990s, advances in ideas and technology allowed higher precision measurements.[60]\nHowever, two major categories of methods, each with high precision, fail to agree.\n"Late universe" measurements using calibrated distance ladder techniques have converged on a value of approximately 73 (km/s)/Mpc. Since 2000, "early universe" techniques based on measurements of the cosmic microwave background have become available, and these agree on a value near 67.7 (km/s)/Mpc.[61] (This accounts for the change in the expansion rate since the early universe, so is comparable to the first number.) Initially, this discrepancy was within the estimated measurement uncertainties and thus no cause for concern.  However, as techniques have improved, the estimated measurement uncertainties have shrunk, but the discrepancies have not, to the point that the disagreement is now highly statistically significant. This discrepancy is called the Hubble tension.[62][63]\n\nAn example of an "early" measurement, the Planck mission published in 2018 gives a value for H0 = of 67.4±0.5 (km/s)/Mpc.[64] In the "late" camp is the higher value of 74.03±1.42 (km/s)/Mpc determined by the  Hubble Space Telescope[65] \nand confirmed by the James Webb Space Telescope in 2023.[66][67]\nThe "early" and "late" measurements disagree at the >5 σ level, beyond a plausible level of chance.[68][69] The resolution to this disagreement is an ongoing area of active research.[70]\n\nSince 2013 much effort has gone in to new measurements to check for possible systematic errors and improved reproducibility.[53]\n\nThe "late universe" or distance ladder measurements typically employ three stages or "rungs". In the first rung distances to Cepheids are determined while trying to reduce luminosity errors from dust and correlations of metallicity with luminosity. The second rung uses \nType Ia supernova, explosions of almost constant amount of mass and thus very similar amounts of light; the primary source of systematic error is the limited number of objects that can be observed. The third rung of the distance ladder measures the red-shift of supernova to extract the Hubble flow and from that the constant. At this rung corrections due to motion other than expansion are applied.[53]: 2.1  \nAs an example of the kind of work needed to reduce systematic errors, photometry on observations from the James Webb Space Telescope of extra-galactic Cepheids confirm the findings from the HST. The higher resolution avoided confusion from crowding of stars in the field of view but came to the same value for H0.[71][53]\n\nThe "early universe" or inverse distance ladder measures the observable consequences of spherical sound waves on primordial plasma density. These pressure waves – called baryon acoustic oscillations (BAO) – cease once the universe cooled enough for electrons to stay bound to nuclei, ending the plasma and allowing the photons trapped by interaction with the plasma to escape. The pressure waves then become very small perturbations in density imprinted on the cosmic microwave background and on the large scale density of galaxies across the sky. Detailed structure in high precision measurements of the CMB can be matched to physics models of the oscillations. These models depend upon the Hubble constant such that a match reveals a value for the constant. Similarly, the BAO affects the statistical distribution of matter, observed as distant galaxies across the sky. These two independent kinds of measurements produce similar values for the constant from the current models, giving strong evidence that systematic errors in the measurements themselves do not affect the result.[53]: Sup. B\n\nIn addition to measurements based on calibrated distance ladder techniques or measurements of the CMB, other methods have been used to determine the Hubble constant.\n\nIn October 2018, scientists used information from gravitational wave events (especially those involving the merger of neutron stars, like GW170817), of determining the Hubble constant.[72][73]\n\nIn July 2019, astronomers reported that a new method to determine the Hubble constant, and resolve the discrepancy of earlier methods, has been proposed based on the mergers of pairs of neutron stars, following the detection of the neutron star merger of GW170817, an event known as a dark siren.[74][75] Their measurement of the Hubble constant is 73.3+5.3−5.0 (km/s)/Mpc.[76]\n\nAlso in July 2019, astronomers reported another new method, using data from the Hubble Space Telescope and based on distances to red giant stars calculated using the tip of the red-giant branch (TRGB) distance indicator. Their measurement of the Hubble constant is 69.8+1.9−1.9 (km/s)/Mpc.[77][78][79]\n\nIn February 2020, the Megamaser Cosmology Project published independent results based on astrophysical masers visible at cosmological distances and which do not require multi-step calibration.  That work confirmed the distance ladder results and differed from the early-universe results at a statistical significance level of 95%.[80]\n\nIn July 2020, measurements of the cosmic background radiation by the Atacama Cosmology Telescope predict that the Universe should be expanding more slowly than is currently observed.[81]\n\nIn July 2023, an independent estimate of the Hubble constant was derived from a kilonova, the optical afterglow of a neutron star merger, using the expanding photosphere method.[82] Due to the blackbody nature of early kilonova spectra,[83] such systems provide strongly constraining estimators of cosmic distance. Using the kilonova AT2017gfo (the aftermath of, once again, GW170817), these measurements indicate a local-estimate of the Hubble constant of 67.0±3.6 (km/s)/Mpc.[84][82]\n\nThe cause of the Hubble tension is unknown,[85] and there are many possible proposed solutions. The most conservative is that there is an unknown systematic error affecting either early-universe or late-universe observations. Although intuitively appealing, this explanation requires multiple unrelated effects regardless of whether early-universe or late-universe observations are incorrect, and there are no obvious candidates. Furthermore, any such systematic error would need to affect multiple different instruments, since both the early-universe and late-universe observations come from several different telescopes.[53]\n\nAlternatively, it could be that the observations are correct, but some unaccounted-for effect is causing the discrepancy. If the cosmological principle fails (see Lambda-CDM model § Violations of the cosmological principle), then the existing interpretations of the Hubble constant and the Hubble tension have to be revised, which might resolve the Hubble tension.[86] In particular, we would need to be located within a very large void, up to about a redshift of 0.5, for such an explanation to conflate with supernovae and baryon acoustic oscillation observations.[63] Yet another possibility is that the uncertainties in the measurements could have been underestimated, but given the internal agreements this is neither likely, nor resolves the overall tension.[53]\n\nFinally, another possibility is new physics beyond the currently accepted cosmological model of the universe, the ΛCDM model.[63][87] There are very many theories in this category, for example, replacing general relativity with a modified theory of gravity could potentially resolve the tension,[88][89] as can a dark energy component in the early universe,[b][90] dark energy with a time-varying equation of state,[c][91] or dark matter that decays into dark radiation.[92] A problem faced by all these theories is that both early-universe and late-universe measurements rely on multiple independent lines of physics, and it is difficult to modify any of those lines while preserving their successes elsewhere. The scale of the challenge can be seen from how some authors have argued that new early-universe physics alone is not sufficient;[93][94] while other authors argue that new late-universe physics alone is also not sufficient.[95] Nonetheless, astronomers are trying, with interest in the Hubble tension growing strongly since the mid 2010s.[63]',
        pageTitle: "Hubble's law",
    },
    {
        title: "Hume's law",
        link: "https://en.wikipedia.org/wiki/Hume%27s_law",
        content:
            'The is–ought problem, as articulated by the Scottish philosopher and historian David Hume, arises when one makes claims about what ought to be that are based solely on statements about what is. Hume found that there seems to be a significant difference between descriptive statements (about what is) and prescriptive statements (about what ought to be), and that it is not obvious how one can coherently transition from descriptive statements to prescriptive ones.\n\nHume\'s law or Hume\'s guillotine[1] is the thesis that an ethical or judgmental conclusion cannot be inferred from purely descriptive factual statements.[2]\n\nA similar view is defended by G. E. Moore\'s open-question argument, intended to refute any identification of moral properties with natural properties, which is asserted by ethical naturalists, who do not deem the naturalistic fallacy a fallacy.\n\nThe is–ought problem is closely related to the fact–value distinction in epistemology. Though the terms are often used interchangeably, academic discourse concerning the latter may encompass aesthetics in addition to ethics.\n\nHume discusses the problem in book III, part I, section I of his book, A Treatise of Human Nature (1739):\n\nIn every system of morality, which I have hitherto met with, I have always remarked, that the author proceeds for some time in the ordinary way of reasoning, and establishes the being of a God, or makes observations concerning human affairs; when of a sudden I am surprised to find, that instead of the usual copulations of propositions, is, and is not, I meet with no proposition that is not connected with an ought, or an ought not. This change is imperceptible; but is, however, of the last consequence. For as this ought, or ought not, expresses some new relation or affirmation, it\'s necessary that it should be observed and explained; and at the same time that a reason should be given, for what seems altogether inconceivable, how this new relation can be a deduction from others, which are entirely different from it. But as authors do not commonly use this precaution, I shall presume to recommend it to the readers; and am persuaded, that this small attention would subvert all the vulgar systems of morality, and let us see, that the distinction of vice and virtue is not founded merely on the relations of objects, nor is perceived by reason.[3][4]\n\nHume calls for caution against such inferences in the absence of any explanation of how the ought-statements follow from the is-statements. But how exactly can an "ought" be derived from an "is"? The question, prompted by Hume\'s small paragraph, has become one of the central questions of ethical theory, and Hume is usually assigned the position that such a derivation is impossible.[5]\n\nIn modern times, "Hume\'s law" often denotes the informal thesis that, if a reasoner only has access to non-moral factual premises, the reasoner cannot logically infer the truth of moral statements; or, more broadly, that one cannot infer evaluative statements (including aesthetic statements) from non-evaluative statements.[2] An alternative definition of Hume\'s law is that "If P implies Q, and Q is moral, then P is moral". This interpretation-driven definition avoids a loophole with the principle of explosion.[6] Other versions state that the is–ought gap can technically be formally bridged without a moral premise, but only in ways that are formally "vacuous" or "irrelevant", and that provide no "guidance". For example, one can infer from "The Sun is yellow" that "Either the Sun is yellow, or it is wrong to murder". But this provides no relevant moral guidance; absent a contradiction, one cannot deductively infer that "it is wrong to murder" solely from non-moral premises alone, adherents argue.[7]\n\nThe apparent gap between "is" statements and "ought" statements, when combined with Hume\'s fork, renders "ought" statements of dubious validity. Hume\'s fork is the idea that all items of knowledge are based either on logic and definitions, or else on observation. If the is–ought problem holds, then "ought" statements do not seem to be known in either of these two ways, and it would seem that there can be no moral knowledge. Moral skepticism and non-cognitivism work with such conclusions.\n\nEthical naturalists contend that moral truths exist, and that their truth value relates to facts about physical reality. Many modern naturalistic philosophers see no impenetrable barrier in deriving "ought" from "is", believing it can be done whenever we analyze goal-directed behavior. They suggest that a statement of the form "In order for agent A to achieve goal B, A reasonably ought to do C" exhibits no category error and may be factually verified or refuted. "Oughts" exist, then, in light of the existence of goals. A counterargument to this response is that it merely pushes back the "ought" to the subjectively valued "goal" and thus provides no fundamentally objective basis to one\'s goals which, consequentially, provides no basis of distinguishing moral value of fundamentally different goals. A dialectical naturalist response to this objection is that although it is true that individual goals have a degree of subjectivity, the process through which the existence of goals is made possible is not subjective—that is, the advent of organisms capable of subjectivity, having occurred through the objective process of evolution. This dialectical approach goes further to state that subjectivity should be conceptualized as objectivity at its highest point, having been the result of an unfolding developmental process.[citation needed]\n\nThis is similar to work done by the moral philosopher Alasdair MacIntyre, who attempts to show that because ethical language developed in the West in the context of a belief in a human telos—an end or goal—our inherited moral language, including terms such as good and bad, have functioned, and function, to evaluate the way in which certain behaviors facilitate the achievement of that telos. In an evaluative capacity, therefore, good and bad carry moral weight without committing a category error. For instance, a pair of scissors that cannot easily cut through paper can legitimately be called bad since it cannot fulfill its purpose effectively. Likewise, if a person is understood as having a particular purpose, then behaviour can be evaluated as good or bad in reference to that purpose. In plainer words, a person is acting good when that person fulfills that person\'s purpose.[8]\n\nEven if the concept of an "ought" is meaningful, this need not involve morality. This is because some goals may be morally neutral, or (if they exist) against what is moral. A poisoner might realize his victim has not died and say, for example, "I ought to have used more poison," since his goal is to murder. The next challenge of a moral realist is thus to explain what is meant by a "moral ought".[9]\n\nProponents of discourse ethics argue that the very act of discourse implies certain "oughts", that is, certain presuppositions that are necessarily accepted by the participants in discourse, and can be used to further derive prescriptive statements. They therefore argue that it is incoherent to argumentatively advance an ethical position on the basis of the is–ought problem, which contradicts these implied assumptions.\n\nAs MacIntyre explained, someone may be called a good person if people have an inherent purpose. Many ethical systems appeal to such a purpose. This is true of some forms of moral realism, which states that something can be wrong, even if every thinking person believes otherwise (the idea of brute fact about morality). The ethical realist might suggest that humans were created for a purpose (e.g. to serve God), especially if they are an ethical non-naturalist. If the ethical realist is instead an ethical naturalist, they may start with the fact that humans have evolved and pursue some sort of evolutionary ethics (which risks “committing” the moralistic fallacy).\nNot all moral systems appeal to a human telos or purpose. This is because it is not obvious that people even have any sort of natural purpose, or what that purpose would be. Although many scientists do recognize teleonomy (a tendency in nature), few philosophers appeal to it (this time, to avoid the naturalistic fallacy).\n\nGoal-dependent oughts run into problems even without an appeal to an innate human purpose. Consider cases where one has no desire to be good—whatever it is. If, for instance, a person wants to be good, and good means washing one\'s hands, then it seems one morally ought to wash their hands. The bigger problem in moral philosophy is what happens if someone does not want to be good, whatever its origins? Put simply, in what sense ought we to hold the goal of being good? It seems one can ask "how am I rationally required to hold \'good\' as a value, or to pursue it?"[10]\n\nThe issue above mentioned is a result of an important ethical relativist critique. Even if "oughts" depend on goals, the ought seems to vary with the person\'s goal. This is the conclusion of the ethical subjectivist, who says a person can only be called good according to whether they fulfill their own, self-assigned goal. Alasdair MacIntyre himself suggests that a person\'s purpose comes from their culture, making him a sort of ethical relativist.[11] Ethical relativists acknowledge local, institutional facts about what is right, but these are facts that can still vary by society. Thus, without an objective "moral goal", a moral ought is difficult to establish. G. E. M. Anscombe was particularly critical of the word "ought" for this reason; understood as "We need such-and-such, and will only get it this way"—for somebody may need something immoral, or else find that their noble need requires immoral action.[12]: 19  Anscombe would even go as far to suggest that "the concepts of obligation, and duty—moral obligation and moral duty, that is to say—and of what is morally right and wrong, and of the moral sense of \'ought,\' ought to be jettisoned if this is psychologically possible".[12]: 1\n\nIf moral goals depend on private assumptions or public agreement, so may morality as a whole. For example, Canada might call it good to maximize global welfare, where a citizen, Alice, calls it good to focus on herself, and then her family, and finally her friends (with little empathy for strangers). It does not seem that Alice can be objectively or rationally bound—without regard to her personal values nor those of groups of other people—to act a certain way. In other words, we may not be able to say "You just should do this". Moreover, persuading her to help strangers would necessarily mean appealing to values she already possesses (or else we would never even have a hope of persuading her).[13] This is another interest of normative ethics—questions of binding forces.\n\nThere may be responses to the above relativistic critiques. As mentioned above, ethical realists that are non-natural can appeal to God\'s purpose for humankind. On the other hand, naturalistic thinkers may posit that valuing people\'s well-being is somehow "obviously" the purpose of ethics, or else the only relevant purpose worth talking about. This is the move made by natural law, scientific moralists and some utilitarians.\n\nJohn Searle also attempts to derive "ought" from "is".[14] He tries to show that the act of making a promise places one under an obligation by definition, and that such an obligation amounts to an "ought". This view is still widely debated, and to answer criticisms, Searle has further developed the concept of institutional facts, for example, that a certain building is in fact a bank and that certain paper is in fact money, which would seem to depend upon general recognition of those institutions and their value.[15]\n\nIndefinables are concepts so global that they cannot be defined; rather, in a sense, they themselves, and the objects to which they refer, define our reality and our ideas. Their meanings cannot be stated in a true definition, but their meanings can be referred to instead by being placed with their incomplete definitions in self-evident statements, the truth of which can be tested by whether or not it is impossible to think the opposite without a contradiction. Thus, the truth of indefinable concepts and propositions using them is entirely a matter of logic.\n\nAn example of the above is that of the concepts "finite parts" and "wholes"; they cannot be defined without reference to each other and thus with some amount of circularity, but we can make the self-evident statement that "the whole is greater than any of its parts", and thus establish a meaning particular to the two concepts.\n\nThese two notions being granted, it can be said that statements of "ought" are measured by their prescriptive truth, just as statements of "is" are measured by their descriptive truth; and the descriptive truth of an "is" judgment is defined by its correspondence to reality (actual or in the mind), while the prescriptive truth of an "ought" judgment is defined according to a more limited scope—its correspondence to right desire (conceivable in the mind and able to be found in the rational appetite, but not in the more "actual" reality of things independent of the mind or rational appetite).[16]\n\nTo some, this may immediately suggest the question: "How can we know what is a right desire if it is already admitted that it is not based on the more actual reality of things independent of the mind?" The beginning of the answer is found when we consider that the concepts "good", "bad", "right" and "wrong" are indefinables. Thus, right desire cannot be defined properly, but a way to refer to its meaning may be found through a self-evident prescriptive truth.[17]\n\nThat self-evident truth which the moral cognitivist claims to exist upon which all other prescriptive truths are ultimately based is: One ought to desire what is really good for one and nothing else. The terms "real good" and "right desire" cannot be defined apart from each other, and thus their definitions would contain some degree of circularity, but the stated self-evident truth indicates a meaning particular to the ideas sought to be understood, and it is (the moral cognitivist might claim) impossible to think the opposite without a contradiction. Thus combined with other descriptive truths of what is good (goods in particular considered in terms of whether they suit a particular end and the limits to the possession of such particular goods being compatible with the general end of the possession of the total of all real goods throughout a whole life), a valid body of knowledge of right desire is generated.[18]\n\nSeveral counterexamples have been offered by philosophers claiming to show that there are cases when an "ought" logically follows from an "is." First of all, Hilary Putnam, by tracing back the quarrel to Hume\'s dictum, claims fact/value entanglement as an objection, since the distinction between them entails a value.[clarification needed] A. N. Prior points out, from the statement "He is a sea captain," it logically follows, "He ought to do what a sea captain ought to do."[19] Alasdair MacIntyre points out, from the statement "This watch is grossly inaccurate and irregular in time-keeping and too heavy to carry about comfortably," the evaluative conclusion validly follows, "This is a bad watch."[20] John Searle points out, from the statement "Jones promised to pay Smith five dollars," it logically follows that "Jones ought to pay Smith five dollars." The act of promising by definition places the promiser under obligation.[21]\n\nPhilippa Foot adopts a moral realist position, criticizing the idea that when evaluation is superposed on fact there has been a "committal in a new dimension."[22] She introduces, by analogy, the practical implications of using the word "injury." Not just anything counts as an injury. There must be some impairment. If one supposes a man wants the things the injury prevents him from obtaining, has one fallen into the old naturalist fallacy? She states the following:\n\nIt may seem that the only way to make a necessary connection between "injury" and the things that are to be avoided, is to say that it is only used in an "action-guiding sense" when applied to something the speaker intends to avoid. But we should look carefully at the crucial move in that argument, and query the suggestion that someone might happen not to want anything for which he would need the use of hands or eyes. Hands and eyes, like ears and legs, play a part in so many operations that a man could only be said not to need them if he had no wants at all.[23]\n\nFoot argues that the virtues, like hands and eyes in the analogy, play so large a part in so many operations that it is implausible to suppose that a committal in a non-naturalist dimension is necessary to demonstrate their goodness.\n\nPhilosophers who have supposed that actual action was required if "good" were to be used in a sincere evaluation have got into difficulties over weakness of will, and they should surely agree that enough has been done if we can show that any man has reason to aim at virtue and avoid vice. But is this impossibly difficult if we consider the kinds of things that count as virtue and vice? Consider, for instance, the cardinal virtues, prudence, temperance, courage and justice. Obviously any man needs prudence, but does he not also need to resist the temptation of pleasure when there is harm involved? And how could it be argued that he would never need to face what was fearful for the sake of some good? It is not obvious what someone would mean if he said that temperance or courage were not good qualities, and this not because of the "praising" sense of these words, but because of the things that courage and temperance are.[24]\n\nHilary Putnam argues that philosophers who accept Hume\'s "is–ought" distinction reject his reasons in making it, and thus undermine the entire claim.[25]\n\nVarious scholars have also indicated that, in the very work where Hume argues for the is–ought problem, Hume himself derives an "ought" from an "is".[26] Such seeming inconsistencies in Hume have led to an ongoing debate over whether Hume actually held to the is–ought problem in the first place, or whether he meant that ought inferences can be made but only with good argumentation.[27]',
        pageTitle: "Is–ought problem",
    },
    {
        title: "Humphrey's law",
        link: "https://en.wikipedia.org/wiki/Humphrey%27s_law",
        content:
            '"The Centipede\'s Dilemma" is a short poem that has lent its name to a psychological effect called the centipede effect or centipede syndrome. The centipede effect occurs when a normally automatic or unconscious activity is disrupted by consciousness of it or reflection on it. For example, a golfer thinking too closely about their swing or someone thinking too much about how they knot their tie may find their performance of the task impaired. The effect is also known as hyperreflection or Humphrey\'s law[1] after English psychologist George Humphrey (1889–1966), who propounded it in 1923. As he wrote of the poem, "This is a most psychological rhyme. It contains a profound truth which is illustrated daily in the lives of all of us". The effect is the reverse of a solvitur ambulando.\n\nThe short poem is usually attributed to Katherine Craster (1841–1874)[1] in Pinafore Poems, 1871.[2] By 1881, it had begun appearing in journals such as The Spectator[3] and Littell\'s Living Age.[4]\n\nOn May 23, 1889, the poem appeared in an article by British zoologist Ray Lankester, published in the scientific journal Nature,[5] which discussed the work of photographer Eadweard Muybridge in capturing the motion of animals: "For my own part," wrote Lankester, "I should greatly like to apply Mr. Muybridge\'s cameras, or a similar set of batteries, to the investigation of a phenomenon more puzzling even than that of \'the galloping horse\'. I allude to the problem of \'the running centipede\'". Lankester finished the article on a fanciful note by imagining the "disastrous results in the way of perplexity" that could result from such an investigation, quoting the poem and mentioning that the author was unknown to him or to the friend who sent it to him. It has since been variously attributed to specific authors, but without convincing evidence, and often appears under the title "The Centipede\'s Dilemma".\n\nEnglish psychologist George Humphrey (1889–1966) referred to the tale in his 1923 book The Story of Man\'s Mind:[6] "No man skilled at a trade needs to put his constant attention on the routine work", he wrote. "If he does, the job is apt to be spoiled". He went on to recount the centipede\'s story, commenting, "This is a most psychological rhyme. It contains a profound truth which is illustrated daily in the lives of all of us, for exactly the same thing happens if we pay conscious attention to any well-formed habit, such as walking". Thus, his eponymous "Humphrey\'s law" states that once a task has become automatized, conscious thought about the task, while performing it, impairs performance.[7] Whereas habit diminishes and then eliminates the attention required for routine tasks, this automaticity is disrupted by attention to a normally unconscious competence.\n\nIn 1994, philosopher Karl Popper referred to the centipede effect in his book Knowledge and the Body-Mind Problem: In Defence of Interaction:[8] "if we have learnt certain movements so that they have sunk below the level of conscious control, then if we try to follow them consciously, we very often interfere with them so badly that we stop them". He gives the example of  violinist Adolf Busch, who was asked by fellow violinist Bronisław Huberman how he played a certain passage of Beethoven\'s violin concerto. Busch told Huberman that it was quite simple, and then found that he could no longer play the passage.\n\nIn 1996, psychiatric psychoanalyst Theo L. Dorpat compares questions and interventions irrelevant to the patient\'s current thought process during psychotherapy in his book Gaslighting to "the story of the centipede who became disorganized and unable to walk after he was asked, \'What\'s wrong with your 34th left foot?\'."[9]\n\nIn 1903, Simplicissimus magazine printed an adaptation of the story "The Curse of The Toad" (Der Fluch der Kröte) by the Austrian author Gustav Meyrink. The fable was also published in Meyrink\'s 1903 collection of tales, The Hot Soldier and Other Stories.[10]\n\nSpider Robinson\'s short story "The Centipede\'s Dilemma", one in his story-sequence book "Callahan\'s Crosstime Saloon", concerns a psychic who uses instinctive telekinetic powers to cheat at darts, and is foiled when another character triggers hyperreflection in him.',
        pageTitle: "The Centipede's Dilemma",
    },
    {
        title: "Hutber's law",
        link: "https://en.wikipedia.org/wiki/Hutber%27s_law",
        content:
            "Hutber's law states that \"improvement means deterioration\". It is founded on the cynical observation that a stated improvement actually hides a deterioration.\n\nThe term has seen wide application in business, engineering, and risk analysis. It was invented in the 1970s by Patrick Hutber, an economist and journalist who was the City Editor for The Sunday Telegraph in London from 1966 to 1979.\n\nHis view was that if a company tells you it is 'improving' the service it provides, it almost always means that it will be doing less for you, or charging you more, or both.",
        pageTitle: "Hutber's law",
    },
    {
        title: "Hyrum's Law",
        link: "https://en.wikipedia.org/wiki/Hyrum%27s_Law",
        content:
            'An Application Programming Interface (API) is a connection between computers or between computer programs. It is a type of software interface, offering a service to other pieces of software.[1] A document or standard that describes how to build such a connection or interface is called an API specification. A computer system that meets this standard is said to implement or expose an API. The term API may refer either to the specification or to the implementation.\n\nIn contrast to a user interface, which connects a computer to a person, an application programming interface connects computers or pieces of software to each other. It is not intended to be used directly by a person (the end user) other than a computer programmer[1] who is incorporating it into software. An API is often made up of different parts which act as tools or services that are available to the programmer. A program or a programmer that uses one of these parts is said to call that portion of the API. The calls that make up the API are also known as subroutines, methods, requests, or endpoints. An API specification defines these calls, meaning that it explains how to use or implement them.\n\nOne purpose of APIs is to hide the internal details of how a system works, exposing only those parts a programmer will find useful and keeping them consistent even if the internal details later change. An API may be custom-built for a particular pair of systems, or it may be a shared standard allowing interoperability among many systems.\n\nThe term API is often used to refer to web APIs,[2] which allow communication between computers that are joined by the internet. There are also APIs for programming languages, software libraries, computer operating systems, and computer hardware. APIs originated in the 1940s, though the term did not emerge until the 1960s and 70s.\n\nAn API opens a software system to interactions from the outside. It allows two software systems to communicate across a boundary — an interface — using mutually agreed-upon signals.[3] In other words, an API connects software entities together. Unlike a user interface, an API is typically not visible to users. It is an "under the hood" portion of a software system, used for machine-to-machine communication.[4]\n\nA well-designed API exposes only objects or actions needed by software or software developers. It hides details that have no use. This abstraction simplifies programming.[5]\n\nBuilding software using APIs has been compared to using building-block toys, such as Lego bricks. Software services or software libraries are analogous to the bricks; they may be joined together via their APIs, composing a new software product.[6] The process of joining is called integration.[3]\n\nAs an example, consider a weather sensor that offers an API. When a certain message is transmitted to the sensor, it will detect the current weather conditions and reply with a weather report. The message that activates the sensor is an API call, and the weather report is an API response.[7] A weather forecasting app might integrate with a number of weather sensor APIs, gathering weather data from throughout a geographical area.\n\nAn API is often compared to a contract. It represents an agreement between parties: a service provider who offers the API and the software developers who rely upon it. If the API remains stable, or if it changes only in predictable ways, developers\' confidence in the API will increase. This may increase their use of the API.[8]\n\nThe term API initially described an interface only for end-user-facing programs, known as application programs. This origin is still reflected in the name "application programming interface." Today, the term is broader, including also utility software and even hardware interfaces.[10]\n\nThe idea of the API is much older than the term itself. British computer scientists Maurice Wilkes and David Wheeler worked on a modular software library in the 1940s for EDSAC, an early computer. The subroutines in this library were stored on punched paper tape organized in a filing cabinet. This cabinet also contained what Wilkes and Wheeler called a "library catalog" of notes about each subroutine and how to incorporate it into a program. Today, such a catalog would be called an API (or an API specification or API documentation) because it instructs a programmer on how to use (or "call") each subroutine that the programmer needs.[10]\n\nWilkes and Wheeler\'s book The Preparation of Programs for an Electronic Digital Computer contains the first published API specification. Joshua Bloch considers that Wilkes and Wheeler "latently invented" the API, because it is more of a concept that is discovered than invented.[10]\n\nThe term "application program interface" (without an -ing suffix) is first recorded in a paper called Data structures and techniques for remote computer graphics presented at an AFIPS conference in 1968.[12][10] The authors of this paper use the term to describe the interaction of an application—a graphics program in this case—with the rest of the computer system. A consistent application interface (consisting of Fortran subroutine calls) was intended to free the programmer from dealing with idiosyncrasies of the graphics display device, and to provide hardware independence if the computer or the display were replaced.[11]\n\nThe term was introduced to the field of databases by C. J. Date[13] in a 1974 paper called The Relational and Network Approaches: Comparison of the Application Programming Interface.[14] An API became a part of the ANSI/SPARC framework for database management systems. This framework treated the application programming interface separately from other interfaces, such as the query interface. Database professionals in the 1970s observed these different interfaces could be combined; a sufficiently rich application interface could support the other interfaces as well.[9]\n\nThis observation led to APIs that supported all types of programming, not just application programming. By 1990, the API was defined simply as "a set of services available to a programmer for performing certain tasks" by technologist Carl Malamud.[15]\n\nThe idea of the API was expanded again with the dawn of remote procedure calls and web APIs. As computer networks became common in the 1970s and 80s, programmers wanted to call libraries located not only on their local computers, but on computers located elsewhere. These remote procedure calls were well supported by the Java language in particular. In the 1990s, with the spread of the internet, standards like CORBA, COM, and DCOM competed to become the most common way to expose API services.[16]\n\nRoy Fielding\'s dissertation Architectural Styles and the Design of Network-based Software Architectures at UC Irvine in 2000 outlined Representational state transfer (REST) and described the idea of a "network-based Application Programming Interface" that Fielding contrasted with traditional "library-based" APIs.[17] XML and JSON web APIs saw widespread commercial adoption beginning in 2000 and continuing as of 2021. The web API is now the most common meaning of the term API.[2]\n\nThe Semantic Web proposed by Tim Berners-Lee in 2001 included "semantic APIs" that recast the API as an open, distributed data interface rather than a software behavior interface.[18] Proprietary interfaces and agents became more widespread than open ones, but the idea of the API as a data interface took hold. Because web APIs are widely used to exchange data of all kinds online, API has become a broad term describing much of the communication on the internet.[16] When used in this way, the term API has overlap in meaning with the term communication protocol.\n\nThe interface to a software library is one type of API. The API describes and prescribes the "expected behavior" (a specification) while the library is an "actual implementation" of this set of rules.\n\nA single API can have multiple implementations (or none, being abstract) in the form of different libraries that share the same programming interface.\n\nThe separation of the API from its implementation can allow programs written in one language to use a library written in another. For example, because Scala and Java compile to compatible bytecode, Scala developers can take advantage of any Java API.[19]\n\nAPI use can vary depending on the type of programming language involved.\nAn API for a procedural language such as Lua could consist primarily of basic routines to execute code, manipulate data or handle errors while an API for an object-oriented language, such as Java, would provide a specification of classes and its class methods.[20][21] .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#b1d2ff}@media screen{html.skin-theme-clientpref-night .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}Hyrum\'s law states that "With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody."[22] Meanwhile, several studies show that most applications that use an API tend to use a small part of the API.[23]\n\nLanguage bindings are also APIs. By mapping the features and capabilities of one language to an interface implemented in another language, a language binding allows a library or service written in one language to be used when developing in another language.[24] Tools such as SWIG and F2PY, a Fortran-to-Python interface generator, facilitate the creation of such interfaces.[25]\n\nAn API can also be related to a software framework: a framework can be based on several libraries implementing several APIs, but unlike the normal use of an API, the access to the behavior built into the framework is mediated by extending its content with new classes plugged into the framework itself.\n\nMoreover, the overall program flow of control can be out of the control of the caller and in the framework\'s hands by inversion of control or a similar mechanism.[26][27]\n\nAn API can specify the interface between an application and the operating system.[28] POSIX, for example, specifies a set of common APIs that aim to enable an application written for a POSIX conformant operating system to be compiled for another POSIX conformant operating system.\n\nLinux and Berkeley Software Distribution are examples of operating systems that implement the POSIX APIs.[29]\n\nMicrosoft has shown a strong commitment to a backward-compatible API, particularly within its Windows API (Win32) library, so older applications may run on newer versions of Windows using an executable-specific setting called "Compatibility Mode".[30]\n\nAn API differs from an application binary interface (ABI) in that an API is source code based while an ABI is binary based. For instance, POSIX provides APIs while the Linux Standard Base provides an ABI.[31][32]\n\nRemote APIs allow developers to manipulate remote resources through protocols, specific standards for communication that allow different technologies to work together, regardless of language or platform.\nFor example, the Java Database Connectivity API allows developers to query many different types of databases with the same set of functions, while the Java remote method invocation API uses the Java Remote Method Protocol to allow invocation of functions that operate remotely, but appear local to the developer.[33][34]\n\nTherefore, remote APIs are useful in maintaining the object abstraction in object-oriented programming; a method call, executed locally on a proxy object, invokes the corresponding method on the remote object, using the remoting protocol, and acquires the result to be used locally as a return value.\n\nA modification of the proxy object will also result in a corresponding modification of the remote object.[35]\n\nWeb APIs are the defined interfaces through which interactions happen between an enterprise and applications that use its assets, which also is a Service Level Agreement (SLA) to specify the functional provider and expose the service path or URL for its API users. An API approach is an architectural approach that revolves around providing a program interface to a set of services to different applications serving different types of consumers.[36]\n\nWhen used in the context of web development, an API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. An example might be a shipping company API that can be added to an eCommerce-focused website to facilitate ordering shipping services and automatically include current shipping rates, without the site developer having to enter the shipper\'s rate table into a web database. While "web API" historically has been virtually synonymous with web service, the recent trend (so-called Web 2.0) has been moving away from Simple Object Access Protocol (SOAP) based web services and service-oriented architecture (SOA) towards more direct representational state transfer (REST) style web resources and resource-oriented architecture (ROA).[37] Part of this trend is related to the Semantic Web movement toward Resource Description Framework (RDF), a concept to promote web-based ontology engineering technologies. Web APIs allow the combination of multiple APIs into new applications known as mashups.[38]\nIn the social media space, web APIs have allowed web communities to facilitate sharing content and data between communities and applications. In this way, content that is created in one place dynamically can be posted and updated to multiple locations on the web.[39] For example, Twitter\'s REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.[40]\n\nThe design of an API has significant impact on its usage.[5] The principle of information hiding describes the role of programming interfaces as enabling modular programming by hiding the implementation details of the modules so that users of modules need not understand the complexities inside the modules.[41] Thus, the design of an API attempts to provide only the tools a user would expect.[5] The design of programming interfaces represents an important part of software architecture, the organization of a complex piece of software.[42]\n\nAPIs are one of the more common ways technology companies integrate. Those that provide and use APIs are considered as being members of a business ecosystem.[43]\n\nAn important factor when an API becomes public is its "interface stability". Changes to the API—for example adding new parameters to a function call—could break compatibility with the clients that depend on that API.[48]\n\nWhen parts of a publicly presented API are subject to change and thus not stable, such parts of a particular API should be documented explicitly as "unstable". For example, in the Google Guava library, the parts that are considered unstable, and that might change soon, are marked with the Java annotation @Beta.[49]\n\nA public API can sometimes declare parts of itself as deprecated or rescinded. This usually means that part of the API should be considered a candidate for being removed, or modified in a backward incompatible way. Therefore, these changes allow developers to transition away from parts of the API that will be removed or not supported in the future.[50]\n\nClient code may contain innovative or opportunistic usages that were not intended by the API designers. In other words, for a library with a significant user base, when an element becomes part of the public API, it may be used in diverse ways.[51]\nOn February 19, 2020, Akamai published their annual “State of the Internet” report, showcasing the growing trend of cybercriminals targeting public API platforms at financial services worldwide. From December 2017 through November 2019, Akamai witnessed 85.42 billion credential violation attacks. About 20%, or 16.55 billion, were against hostnames defined as API endpoints. Of these, 473.5 million have targeted financial services sector organizations.[52]\n\nAPI documentation describes what services an API offers and how to use those services, aiming to cover everything a client would need to know for practical purposes.\n\nDocumentation is crucial for the development and maintenance of applications using the API.[53]\nAPI documentation is traditionally found in documentation files but can also be found in social media such as blogs, forums, and Q&A websites.[54]\n\nTraditional documentation files are often presented via a documentation system, such as Javadoc or Pydoc, that has a consistent appearance and structure.\nHowever, the types of content included in the documentation differs from API to API.[55]\n\nIn the interest of clarity, API documentation may include a description of classes and methods in the API as well as "typical usage scenarios, code snippets, design rationales, performance discussions, and contracts", but implementation details of the API services themselves are usually omitted. It can take a number of forms, including instructional documents, tutorials, and reference works. It\'ll also include a variety of information types, including guides and functionalities.\n\nRestrictions and limitations on how the API can be used are also covered by the documentation. For instance, documentation for an API function could note that its parameters cannot be null, that the function itself is not thread safe.[56] Because API documentation tends to be comprehensive, it is a challenge for writers to keep the documentation updated and for users to read it carefully, potentially yielding bugs.[48]\n\nAPI documentation can be enriched with metadata information like Java annotations. This metadata can be used by the compiler, tools, and by the run-time environment to implement custom behaviors or custom handling.[57]\n\nIt is possible to generate API documentation in a data-driven manner. By observing many programs that use a given API, it is possible to infer the typical usages, as well the required contracts and directives.[58] Then, templates can be used to generate natural language from the mined data.\n\nIn 2010, Oracle Corporation sued Google for having distributed a new implementation of Java embedded in the Android operating system.[59] Google had not acquired any permission to reproduce the Java API, although permission had been given to the similar OpenJDK project. Judge William Alsup ruled in the Oracle v. Google case that APIs cannot be copyrighted in the U.S. and that a victory for Oracle would have widely expanded copyright protection to a "functional set of symbols" and allowed the copyrighting of simple software commands:\n\nTo accept Oracle\'s claim would be to allow anyone to copyright one version of code to carry out a system of commands and thereby bar all others from writing its different versions to carry out all or part of the same commands.[60][61]\n\nAlsup\'s ruling was overturned in 2014 on appeal  to the Court of Appeals for the Federal Circuit, though the question of whether such use of APIs constitutes fair use was left unresolved.[62][63]\n\nIn 2016, following a two-week trial, a jury determined that Google\'s reimplementation of the Java API constituted fair use, but Oracle vowed to appeal the decision.[64] Oracle won on its appeal, with the Court of Appeals for the Federal Circuit ruling that Google\'s use of the APIs did not qualify for fair use.[65] In 2019, Google appealed to the Supreme Court of the United States over both the copyrightability and fair use rulings, and the Supreme Court granted review.[66] Due to the COVID-19 pandemic, the oral hearings in the case were delayed until October 2020.[67]\n\nThe case was decided by the Supreme Court in Google\'s favor.[68]',
        pageTitle: "API",
    },
    {
        title: "Isaac Bonewits's laws of magic",
        link: "https://en.wikipedia.org/wiki/Isaac_Bonewits#Contributions_to_Neopaganism",
        content:
            'Phillip Emmons Isaac Bonewits (October 1, 1949 – August 12, 2010) was an American Neo-Druid who wrote a number of books on the subject of Neopaganism and magic. Bonewits was a public speaker, liturgist, singer and songwriter, and founder of the Neopagan organizations Ár nDraíocht Féin (ADF) and the Aquarian Anti-Defamation League.\n\nBonewits was born on October 1, 1949,[1] in Royal Oak, Michigan, as the fourth of five children. His father was a Presbyterian while his mother a Catholic.[2][3] Spending much of his childhood in Ferndale, Michigan, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966 and graduated in 1970 with a Bachelor of Arts in magic,[4] perhaps becoming the first[1] and only person known to have ever received any kind of academic degree in magic from an accredited university.\n\nIn 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America (RDNA). Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18-year-old Bonewits was also recruited by the Church of Satan,[3] but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary Satanis: The Devil\'s Mass.[5] Bonewits, in his article "My Satanic Adventure", asserts that the rituals in Satanis were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.[6]\n\nBonewits\' first book, Real Magic, was published in 1971. Between 1973 and 1975 Bonewits was employed as the editor of Gnostica magazine in Minnesota (published by Llewellyn Publications). He established an offshoot group of the Reformed Druids of North America (RDNA) called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite, in his words, his "lifelong status as a gentile"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.[3]\n\nIn 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.[3]\n\nThroughout his life Bonewits had varying degrees of involvement with occult groups including Gardnerian Wicca and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn).[7] Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book Authentic Thaumaturgy to gamers as a way of organizing Dungeons & Dragons games.\n\nIn 1983, Bonewits founded Ár nDraíocht Féin (also known as "A Druid Fellowship" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization.[3]  Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.\n\nA songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).\n\nBonewits encouraged charity programs to help Neopagan seniors,[8] and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.[9]\n\nBonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years.  Paperwork and legalities caught up on December 31, 2007, making them legally married.[3][10]\n\nIn 1990, Bonewits was diagnosed with eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.\n\nOn October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer,[11] for which he underwent treatment.  He died at home, on August 12, 2010, surrounded by his family.[1]\n\nIn 2018, accusations of sexual abuse against a minor rose against ADF founder Bonewits relating to his relationship with Moira Greyland when she was six years old.[12] Greyland said in her book, \'The Last Closet: the Dark Side of Avalon\':\n\nSome people called him the Pagan pope […] I hated Isaac, and refused to be in the same room with him, even if the only way I could articulate my objections to him was to say \'he tickled me\'.[12]\n\nIn light of this accusation, ADF, the lead pagan organization that Issac Bonewits founded, removed his name from their website and repudiated him.\n\nTo preserve the health of our organization, we must cut out the blight that is Isaac Bonewits’ legacy. We sever the ties both historical and spiritual that bind us to him. For his actions against children, Isaac Bonewits will no longer be named as a beloved ancestor of ADF, nor is he welcome at our sacred fire.\n\nMay his memory and his dark actions fade with the rising of the sun.[13]\n\nIn his book Real Magic (1971), Bonewits proposed his "Laws of Magic". These "laws" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970s to be part of his publishing project Library of the Occult.\n\nBonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.',
        pageTitle: "Isaac Bonewits",
    },
    {
        title: "Joule's laws",
        link: "https://en.wikipedia.org/wiki/Joule%27s_laws",
        content:
            'Joule effect and Joule\'s law are any of several different physical effects discovered or characterized by English physicist James Prescott Joule. These physical effects are not the same, but all are frequently or occasionally referred to in the literature as the "Joule effect" or "Joule law" These physical effects include:\n\nBetween 1840 and 1843, Joule carefully studied the heat produced by an electric current.  From this study, he developed Joule\'s laws of heating, the first of which is commonly referred to as the Joule effect.  Joule\'s first law expresses the relationship between heat generated in a conductor and current flow, resistance, and time.[1]\n\nThe magnetostriction effect describes a property of ferromagnetic materials which causes them to change their shape when subjected to a magnetic field.  Joule first reported observing the change in the length of ferromagnetic rods in 1842.[2]\n\nIn 1845, Joule studied the free expansion of a gas into a larger volume. This became known as Joule expansion.[3]  The cooling of a gas by allowing it to expand freely is occasionally referred to as the Joule effect.[4]\n\nIf an elastic band is first stretched and then subjected to heating, it will shrink rather than expand.  This effect was first observed by John Gough in 1802, and was investigated further by Joule in the 1850s, when it then became known as the Gough–Joule effect.[5][6]\nExamples in Literature:',
        pageTitle: "Joule effect",
    },
    {
        title: "Joy's law",
        link: "https://en.wikipedia.org/wiki/Joy%27s_law_(management)",
        content:
            'In management, Joy\'s law is the principle that "no matter who you are, most of the smartest people work for someone else,” attributed to Sun Microsystems co-founder Bill Joy.[1] Joy was prompted to state this observation through his dislike of Bill Gates\' view of "Microsoft as an IQ monopolist." He argued that, instead, "It\'s better to create an ecology that gets all the world’s smartest people toiling in your garden for your goals. If you rely solely on your own employees, you’ll never solve all your customers\' needs."[2] Core to this principle is the definition of smart within the context of the quotation. Smart "refers to capability but not willingness to work for someone." Furthermore, "the fact that you are smart for one company does not make you smart for another." Richard Pettinger, Director of Information Management for Business, UCL [3] The law highlights an essential problem that is faced by many modern businesses, "that in any given sphere of activity most of the pertinent knowledge will reside outside the boundaries of any one organization, and the central challenge [is] to find ways to access that knowledge."[1]\n\nIn computing, the same Bill Joy devised a simple mathematical function regarding the increase in microprocessor speed over time[4] which is also referred to as Joy\'s Law.\n\nFriedrich Hayek, an economist and philosopher known for his defense of classical liberalism, observed that “knowledge is unevenly distributed.”[5] The ‘knowledge’ that Hayek refers to is the knowledge that the ‘smartest people’ possess in Joy’s law.\nHayek states that the problem of a rational economic order is because knowledge that we wish to grasp never exists in a “concentrated or integrated form but solely as the dispersed bits of incomplete and frequently contradictory knowledge which all the separate individuals possess.”[6] In other words, it is impossible to aggregate all the knowledge that exists. This explains that Joy is right in saying that “most of the smartest people work for someone else.”\n\nEric von Hippel, a professor of technological innovation in the MIT Sloan School of Management, is known partly for his principle of knowledge being ‘sticky’.[7] This highlights the difficulty of transporting knowledge from one place to another. Stickiness is defined as the cost required to “transfer a unit of information to a specified locus in a form usable by a given information seeker. When this cost is low, information stickiness is low; when it is high, stickiness is high.”[8] “When Joy says that most of the smart people work for someone else, it is not because companies are hiring dumb people. It is not because employees in any given firm are not smart. It is because of the nature of knowledge – getting hold of it is tough. It is unevenly distributed and sticky.”[5]\n\nOne interpretation of Joy’s Law is that of Todd Park, former Chief Technology Officer of the United States, through his summary of the challenge of open innovation in government: “Even if you get the best and the brightest to work for you, there will always be an infinite number of other, smarter people employed by others.”[9]',
        pageTitle: "Joy's law (management)",
    },
    {
        title: "Kepler's laws of planetary motion",
        link: "https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion",
        content:
            "In astronomy, Kepler's laws of planetary motion, published by Johannes Kepler in 1609 (except the third law, which was fully published in 1619), describe the orbits of planets around the Sun. These laws replaced circular orbits and epicycles in the heliocentric theory of Nicolaus Copernicus with elliptical orbits and explained how planetary velocities vary. The three laws state that:[1][2]\n\nThe elliptical orbits of planets were indicated by calculations of the orbit of Mars. From this, Kepler inferred that other bodies in the Solar System, including those farther away from the Sun, also have elliptical orbits. The second law establishes that when a planet is closer to the Sun, it travels faster. The third law expresses that the farther a planet is from the Sun, the longer its orbital period.\n\nIsaac Newton showed in 1687 that relationships like Kepler's would apply in the Solar System as a consequence of his own laws of motion and law of universal gravitation.\n\nA more precise historical approach is found in Astronomia nova and Epitome Astronomiae Copernicanae.\n\nJohannes Kepler's laws improved the model of Copernicus. According to Copernicus:[3][4]\n\nDespite being correct in saying that the planets revolved around the Sun, Copernicus was incorrect in defining their orbits. Introducing physical explanations for movement in space beyond just geometry, Kepler correctly defined the orbit of planets as follows:[1][2][5]: 53–54\n\nThe eccentricity of the orbit of the Earth makes the time from the March equinox to the September equinox, around 186 days, unequal to the time from the September equinox to the March equinox, around 179 days. A diameter would cut the orbit into equal parts, but the plane through the Sun parallel to the equator of the Earth cuts the orbit into two parts with areas in a 186 to 179 ratio, so the eccentricity of the orbit of the Earth is approximately\n\nwhich is close to the correct value (0.016710218). The accuracy of this calculation requires that the two dates chosen be along the elliptical orbit's minor axis and that the midpoints of each half be along the major axis. As the two dates chosen here are equinoxes, this will be correct when perihelion, the date the Earth is closest to the Sun, falls on a solstice. The current perihelion, near January 4, is fairly close to the solstice of December 21 or 22.\n\nIt took nearly two centuries for the current formulation of Kepler's work to take on its settled form. Voltaire's Eléments de la philosophie de Newton (Elements of Newton's Philosophy) of 1738 was the first publication to use the terminology of \"laws\".[6][7] The Biographical Encyclopedia of Astronomers in its article on Kepler (p. 620) states that the terminology of scientific laws for these discoveries was current at least from the time of Joseph de Lalande.[8] It was the exposition of Robert Small, in An account of the astronomical discoveries of Kepler (1814) that made up the set of three laws, by adding in the third.[9] Small also claimed, against the history, that these were empirical laws, based on inductive reasoning.[7][10]\n\nFurther, the current usage of \"Kepler's Second Law\" is something of a misnomer. Kepler had two versions, related in a qualitative sense: the \"distance law\" and the \"area law\". The \"area law\" is what became the Second Law in the set of three; but Kepler did himself not privilege it in that way.[11]\n\nKepler published his first two laws about planetary motion in 1609,[12] having found them by analyzing the astronomical observations of Tycho Brahe.[13][14][15][5]: 53  Kepler's third law was published in 1619.[16][14] Kepler had believed in the Copernican model of the Solar System, which called for circular orbits, but he could not reconcile Brahe's highly precise observations with a circular fit to Mars' orbit – Mars coincidentally having the highest eccentricity of all planets except Mercury.[17] His first law reflected this discovery.\n\nIn 1621, Kepler noted that his third law applies to the four brightest moons of Jupiter.[Nb 1] Godefroy Wendelin also made this observation in 1643.[Nb 2] The second law, in the \"area law\" form, was contested by Nicolaus Mercator in a book from 1664, but by 1670 his Philosophical Transactions were in its favour.[18][19] As the century proceeded it became more widely accepted.[20] The reception in Germany changed noticeably between 1688, the year in which Newton's Principia was published and was taken to be basically Copernican, and 1690, by which time work of Gottfried Leibniz on Kepler had been published.[21]\n\nNewton was credited with understanding that the second law is not special to the inverse square law of gravitation, being a consequence just of the radial nature of that law, whereas the other laws do depend on the inverse square form of the attraction. Carl Runge and Wilhelm Lenz much later identified a symmetry principle in the phase space of planetary motion (the orthogonal group O(4) acting) which accounts for the first and third laws in the case of Newtonian gravitation, as conservation of angular momentum does via rotational symmetry for the second law.[22]\n\nThe mathematical model of the kinematics of a planet subject to the laws allows a large range of further calculations.\n\nThe orbit of every planet is an ellipse with the sun at one of the two foci.\n\nMathematically, an ellipse can be represented by the formula:\n\nwhere  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is the semi-latus rectum, ε is the eccentricity of the ellipse, r is the distance from the Sun to the planet, and θ is the angle to the planet's current position from its closest approach, as seen from the Sun. So (r, θ) are polar coordinates.\n\nFor an ellipse 0 < ε < 1 ; in the limiting case ε = 0, the orbit is a circle with the Sun at the centre (i.e. where there is zero eccentricity).\n\nAt θ = 90° and at θ = 270° the distance is equal to \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n.\n\nAt θ = 180°, aphelion, the distance is maximum (by definition, aphelion is – invariably – perihelion plus 180°)\n\nThe semi-major axis a is the arithmetic mean between rmin and rmax:\n\nThe semi-minor axis b is the geometric mean between rmin and rmax:\n\nThe semi-latus rectum p is the harmonic mean between rmin and rmax:\n\nThe eccentricity ε is the coefficient of variation between rmin and rmax:\n\nThe special case of a circle is ε = 0, resulting in r = p =  rmin = rmax = a = b and A = πr2.\n\nA line joining a planet and the Sun sweeps out equal areas during equal intervals of time.[23]\n\nThe orbital radius and angular velocity of the planet in the elliptical orbit will vary. This is shown in the animation: the planet travels faster when closer to the Sun, then slower when farther from the Sun. Kepler's second law states that the blue sector has constant area.\n\nKepler notably arrived at this law through assumptions that were either only approximately true or outright false and can be outlined as follows:\n\nNevertheless, the result of the Second Law is exactly true, as it is logically equivalent to the conservation of angular momentum, which is true for any body experiencing a radially symmetric force.[24]  A correct proof can be shown through this. Since the cross product of two vectors gives the area of a parallelogram possessing sides of those vectors, the triangular area dA swept out in a short period of time is given by half the cross product of the r and dx vectors, for some short piece of the orbit, dx.\n\nd\n        A\n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            \n              r\n              →\n            \n          \n        \n        ×\n        \n          \n            \n              \n                d\n                x\n              \n              →\n            \n          \n        \n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            \n              r\n              →\n            \n          \n        \n        ×\n        \n          \n            \n              v\n              →\n            \n          \n        \n        d\n        t\n        )\n      \n    \n    {\\displaystyle dA={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {dx}})={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {v}}dt)}\n  \n\nfor a small piece of the orbit dx and time to cover it dt.\n\nThus \n  \n    \n      \n        \n          \n            \n              d\n              A\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            \n              r\n              →\n            \n          \n        \n        ×\n        \n          \n            \n              v\n              →\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {dA}{dt}}={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {v}}).}\n\nd\n              A\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            \n              r\n              →\n            \n          \n        \n        ×\n        \n          \n            \n              p\n              →\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {dA}{dt}}={\\frac {1}{m}}{\\frac {1}{2}}({\\vec {r}}\\times {\\vec {p}}).}\n\nSince the final expression is proportional to the total angular momentum \n  \n    \n      \n        (\n        \n          \n            \n              r\n              →\n            \n          \n        \n        ×\n        \n          \n            \n              p\n              →\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\vec {r}}\\times {\\vec {p}})}\n  \n, Kepler's equal area law will hold for any system that conserves angular momentum.  Since any radial force will produce no torque on the planet's motion, angular momentum will be conserved.\n\nIn a small time \n  \n    \n      \n        d\n        t\n      \n    \n    {\\displaystyle dt}\n  \n the planet sweeps out a small triangle having base line \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n and height \n  \n    \n      \n        r\n        \n        d\n        θ\n      \n    \n    {\\displaystyle r\\,d\\theta }\n  \n and area \n  \n    \n      \n        d\n        A\n        =\n        \n          \n            1\n            2\n          \n        \n        ⋅\n        r\n        ⋅\n        r\n        \n        d\n        θ\n      \n    \n    {\\textstyle dA={\\frac {1}{2}}\\cdot r\\cdot r\\,d\\theta }\n  \n, so the constant areal velocity is \n  \n    \n      \n        \n          \n            \n              d\n              A\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              r\n              \n                2\n              \n            \n            2\n          \n        \n        \n          \n            \n              d\n              θ\n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dA}{dt}}={\\frac {r^{2}}{2}}{\\frac {d\\theta }{dt}}.}\n\nThe area enclosed by the elliptical orbit is \n  \n    \n      \n        π\n        a\n        b\n      \n    \n    {\\displaystyle \\pi ab}\n  \n. So the period \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n satisfies\n\nAnd so, \n  \n    \n      \n        \n          \n            \n              d\n              A\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              a\n              b\n              n\n            \n            2\n          \n        \n        =\n        \n          \n            \n              π\n              a\n              b\n            \n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dA}{dt}}={\\frac {abn}{2}}={\\frac {\\pi ab}{T}}.}\n\nS: Sun at the primary focus, \nC: Centre of ellipse, \nS': The secondary focus.  \nIn each case, the area of all sectors depicted is identical.\n\nThe ratio of the square of an object's orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.\n\nThis captures the relationship between the distance of planets from the Sun, and their orbital periods.\n\nKepler enunciated in 1619[16] this third law in a laborious attempt to determine what he viewed as the \"music of the spheres\" according to precise laws, and express it in terms of musical notation.[25] It was therefore known as the harmonic law.[26] The original form of this law (referring to not the semi-major axis, but rather a \"mean distance\") holds true only for planets with small eccentricities near zero.[27]\n\nUsing Newton's law of gravitation (published 1687), this relation can be found in the case of a circular orbit by setting the centripetal force equal to the gravitational force:\n\nThen, expressing the angular velocity ω in terms of the orbital period \n  \n    \n      \n        \n          T\n        \n      \n    \n    {\\displaystyle {T}}\n  \n and then rearranging, results in Kepler's Third Law:\n\nA more detailed derivation can be done with general elliptical orbits, instead of circles, as well as orbiting the center of mass, instead of just the large mass. This results in replacing a circular radius, \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n, with the semi-major axis, \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n, of the elliptical relative motion of one mass relative to the other, as well as replacing the large mass \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n with \n  \n    \n      \n        M\n        +\n        m\n      \n    \n    {\\displaystyle M+m}\n  \n. However, with planet masses being so much smaller than the Sun, this correction is often ignored. The full corresponding formula is:\n\nwhere \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the mass of the Sun, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the mass of the planet, \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n is the gravitational constant, \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the orbital period and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is the elliptical semi-major axis, and \n  \n    \n      \n        \n          AU\n        \n      \n    \n    {\\displaystyle {\\text{AU}}}\n  \n is the astronomical unit, the average distance from earth to the sun.\n\nThe following table shows the data used by Kepler to empirically derive his law:\n\nKepler became aware of John Napier's recent invention of logarithms and log-log graphs before he discovered the pattern.[28]\n\nI first believed I was dreaming... But it is absolutely certain and exact that the ratio which exists between the period times of any two planets is precisely the ratio of the 3/2th power of the mean distance.\n\nFor comparison, here are modern estimates:[citation needed]\n\nIsaac Newton computed in his Philosophiæ Naturalis Principia Mathematica the acceleration of a planet moving according to Kepler's first and second laws.\n\nThis implies that the Sun may be the physical cause of the acceleration of planets. However, Newton states in his Principia that he considers forces from a mathematical point of view, not a physical, thereby taking an instrumentalist view.[30] Moreover, he does not assign a cause to gravity.[31]\n\nNewton defined the force acting on a planet to be the product of its mass and the acceleration (see Newton's laws of motion). So:\n\nThe Sun plays an unsymmetrical part, which is unjustified. So he assumed, in Newton's law of universal gravitation:\n\nAs the planets have small masses compared to that of the Sun, the orbits conform approximately to Kepler's laws. Newton's model improves upon Kepler's model, and fits actual observations more accurately. (See two-body problem.)\n\nBelow comes the detailed calculation of the acceleration of a planet moving according to Kepler's first and second laws.\n\nFrom the heliocentric point of view consider the vector to the planet \n  \n    \n      \n        \n          r\n        \n        =\n        r\n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} =r{\\hat {\\mathbf {r} }}}\n  \n where \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n is the distance to the planet and \n  \n    \n      \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {r} }}}\n  \n is a unit vector pointing towards the planet.\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                \n                  \n                    \n                      r\n                    \n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    r\n                  \n                  ^\n                \n              \n              ˙\n            \n          \n        \n        =\n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n        \n          \n            \n              θ\n              ^\n            \n          \n        \n        ,\n        \n        \n          \n            \n              d\n              \n                \n                  \n                    θ\n                    ^\n                  \n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  θ\n                  ^\n                \n              \n              ˙\n            \n          \n        \n        =\n        −\n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d{\\hat {\\mathbf {r} }}}{dt}}={\\dot {\\hat {\\mathbf {r} }}}={\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}},\\qquad {\\frac {d{\\hat {\\boldsymbol {\\theta }}}}{dt}}={\\dot {\\hat {\\boldsymbol {\\theta }}}}=-{\\dot {\\theta }}{\\hat {\\mathbf {r} }}}\n\nwhere \n  \n    \n      \n        \n          \n            \n              θ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\theta }}}}\n  \n is the unit vector whose direction is 90 degrees counterclockwise of \n  \n    \n      \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {r} }}}\n  \n, and \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is the polar angle, and where a dot on top of the variable signifies differentiation with respect to time.\n\nDifferentiate the position vector twice to obtain the velocity vector and the acceleration vector:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \n                        r\n                      \n                      ˙\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                \n                  \n                    \n                      \n                        r\n                      \n                      ^\n                    \n                  \n                \n                +\n                r\n                \n                  \n                    \n                      \n                        \n                          \n                            r\n                          \n                          ^\n                        \n                      \n                      ˙\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                \n                  \n                    \n                      \n                        r\n                      \n                      ^\n                    \n                  \n                \n                +\n                r\n                \n                  \n                    \n                      θ\n                      ˙\n                    \n                  \n                \n                \n                  \n                    \n                      θ\n                      ^\n                    \n                  \n                \n                ,\n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        r\n                      \n                      ¨\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  (\n                  \n                    \n                      \n                        \n                          r\n                          ¨\n                        \n                      \n                    \n                    \n                      \n                        \n                          \n                            r\n                          \n                          ^\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          r\n                          ˙\n                        \n                      \n                    \n                    \n                      \n                        \n                          \n                            \n                              \n                                r\n                              \n                              ^\n                            \n                          \n                          ˙\n                        \n                      \n                    \n                  \n                  )\n                \n                +\n                \n                  (\n                  \n                    \n                      \n                        \n                          r\n                          ˙\n                        \n                      \n                    \n                    \n                      \n                        \n                          θ\n                          ˙\n                        \n                      \n                    \n                    \n                      \n                        \n                          θ\n                          ^\n                        \n                      \n                    \n                    +\n                    r\n                    \n                      \n                        \n                          θ\n                          ¨\n                        \n                      \n                    \n                    \n                      \n                        \n                          θ\n                          ^\n                        \n                      \n                    \n                    +\n                    r\n                    \n                      \n                        \n                          θ\n                          ˙\n                        \n                      \n                    \n                    \n                      \n                        \n                          \n                            \n                              θ\n                              ^\n                            \n                          \n                          ˙\n                        \n                      \n                    \n                  \n                  )\n                \n                =\n                \n                  (\n                  \n                    \n                      \n                        \n                          r\n                          ¨\n                        \n                      \n                    \n                    −\n                    r\n                    \n                      \n                        \n                          \n                            θ\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      \n                        r\n                      \n                      ^\n                    \n                  \n                \n                +\n                \n                  (\n                  \n                    r\n                    \n                      \n                        \n                          θ\n                          ¨\n                        \n                      \n                    \n                    +\n                    2\n                    \n                      \n                        \n                          r\n                          ˙\n                        \n                      \n                    \n                    \n                      \n                        \n                          θ\n                          ˙\n                        \n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      θ\n                      ^\n                    \n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\dot {\\mathbf {r} }}&={\\dot {r}}{\\hat {\\mathbf {r} }}+r{\\dot {\\hat {\\mathbf {r} }}}={\\dot {r}}{\\hat {\\mathbf {r} }}+r{\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}},\\\\{\\ddot {\\mathbf {r} }}&=\\left({\\ddot {r}}{\\hat {\\mathbf {r} }}+{\\dot {r}}{\\dot {\\hat {\\mathbf {r} }}}\\right)+\\left({\\dot {r}}{\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}}+r{\\ddot {\\theta }}{\\hat {\\boldsymbol {\\theta }}}+r{\\dot {\\theta }}{\\dot {\\hat {\\boldsymbol {\\theta }}}}\\right)=\\left({\\ddot {r}}-r{\\dot {\\theta }}^{2}\\right){\\hat {\\mathbf {r} }}+\\left(r{\\ddot {\\theta }}+2{\\dot {r}}{\\dot {\\theta }}\\right){\\hat {\\boldsymbol {\\theta }}}.\\end{aligned}}}\n\nSo\n\n  \n    \n      \n        \n          \n            \n              \n                r\n              \n              ¨\n            \n          \n        \n        =\n        \n          a\n          \n            r\n          \n        \n        \n          \n            \n              r\n              ^\n            \n          \n        \n        +\n        \n          a\n          \n            θ\n          \n        \n        \n          \n            \n              θ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\ddot {\\mathbf {r} }}=a_{r}{\\hat {\\boldsymbol {r}}}+a_{\\theta }{\\hat {\\boldsymbol {\\theta }}}}\n  \n\nwhere the radial acceleration is\n\n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        =\n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        −\n        r\n        \n          \n            \n              \n                θ\n                ˙\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a_{r}={\\ddot {r}}-r{\\dot {\\theta }}^{2}}\n  \n\nand the transversal acceleration is\n\n  \n    \n      \n        \n          a\n          \n            θ\n          \n        \n        =\n        r\n        \n          \n            \n              θ\n              ¨\n            \n          \n        \n        +\n        2\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a_{\\theta }=r{\\ddot {\\theta }}+2{\\dot {r}}{\\dot {\\theta }}.}\n\nKepler's second law says that \n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n        =\n        n\n        a\n        b\n      \n    \n    {\\displaystyle r^{2}{\\dot {\\theta }}=nab}\n  \n is constant.\n\nThe transversal acceleration \n  \n    \n      \n        \n          a\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle a_{\\theta }}\n  \n is zero:\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                (\n                \n                  \n                    r\n                    \n                      2\n                    \n                  \n                  \n                    \n                      \n                        θ\n                        ˙\n                      \n                    \n                  \n                \n                )\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        r\n        \n          (\n          \n            2\n            \n              \n                \n                  r\n                  ˙\n                \n              \n            \n            \n              \n                \n                  θ\n                  ˙\n                \n              \n            \n            +\n            r\n            \n              \n                \n                  θ\n                  ¨\n                \n              \n            \n          \n          )\n        \n        =\n        r\n        \n          a\n          \n            θ\n          \n        \n        =\n        0.\n      \n    \n    {\\displaystyle {\\frac {d\\left(r^{2}{\\dot {\\theta }}\\right)}{dt}}=r\\left(2{\\dot {r}}{\\dot {\\theta }}+r{\\ddot {\\theta }}\\right)=ra_{\\theta }=0.}\n\nSo the acceleration of a planet obeying Kepler's second law is directed towards the Sun.\n\nThe radial acceleration \n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle a_{\\text{r}}}\n  \n is\n\n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        =\n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        −\n        r\n        \n          \n            \n              \n                θ\n                ˙\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        −\n        r\n        \n          \n            (\n            \n              \n                \n                  n\n                  a\n                  b\n                \n                \n                  r\n                  \n                    2\n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        −\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                3\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a_{\\text{r}}={\\ddot {r}}-r{\\dot {\\theta }}^{2}={\\ddot {r}}-r\\left({\\frac {nab}{r^{2}}}\\right)^{2}={\\ddot {r}}-{\\frac {n^{2}a^{2}b^{2}}{r^{3}}}.}\n\nKepler's first law states that the orbit is described by the equation:\n\n  \n    \n      \n        \n          \n            p\n            r\n          \n        \n        =\n        1\n        +\n        ε\n        cos\n        ⁡\n        (\n        θ\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {p}{r}}=1+\\varepsilon \\cos(\\theta ).}\n\nDifferentiating with respect to time\n\n  \n    \n      \n        −\n        \n          \n            \n              p\n              \n                \n                  \n                    r\n                    ˙\n                  \n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        =\n        −\n        ε\n        sin\n        ⁡\n        (\n        θ\n        )\n        \n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle -{\\frac {p{\\dot {r}}}{r^{2}}}=-\\varepsilon \\sin(\\theta )\\,{\\dot {\\theta }}}\n  \n\nor \n  \n    \n      \n        p\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        =\n        n\n        a\n        b\n        \n        ε\n        sin\n        ⁡\n        (\n        θ\n        )\n        .\n      \n    \n    {\\displaystyle p{\\dot {r}}=nab\\,\\varepsilon \\sin(\\theta ).}\n\nDifferentiating once more\n\n  \n    \n      \n        p\n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        =\n        n\n        a\n        b\n        ε\n        cos\n        ⁡\n        (\n        θ\n        )\n        \n        \n          \n            \n              θ\n              ˙\n            \n          \n        \n        =\n        n\n        a\n        b\n        ε\n        cos\n        ⁡\n        (\n        θ\n        )\n        \n        \n          \n            \n              n\n              a\n              b\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ε\n        cos\n        ⁡\n        (\n        θ\n        )\n        .\n      \n    \n    {\\displaystyle p{\\ddot {r}}=nab\\varepsilon \\cos(\\theta )\\,{\\dot {\\theta }}=nab\\varepsilon \\cos(\\theta )\\,{\\frac {nab}{r^{2}}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\varepsilon \\cos(\\theta ).}\n\nThe radial acceleration \n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle a_{\\text{r}}}\n  \n satisfies\n\n  \n    \n      \n        p\n        \n          a\n          \n            r\n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ε\n        cos\n        ⁡\n        (\n        θ\n        )\n        −\n        p\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                3\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          (\n          \n            ε\n            cos\n            ⁡\n            (\n            θ\n            )\n            −\n            \n              \n                p\n                r\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle pa_{\\text{r}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\varepsilon \\cos(\\theta )-p{\\frac {n^{2}a^{2}b^{2}}{r^{3}}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\left(\\varepsilon \\cos(\\theta )-{\\frac {p}{r}}\\right).}\n\nSubstituting the equation of the ellipse gives\n\n  \n    \n      \n        p\n        \n          a\n          \n            r\n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                b\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                p\n                r\n              \n            \n            −\n            1\n            −\n            \n              \n                p\n                r\n              \n            \n          \n          )\n        \n        =\n        −\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          b\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle pa_{\\text{r}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\left({\\frac {p}{r}}-1-{\\frac {p}{r}}\\right)=-{\\frac {n^{2}a^{2}}{r^{2}}}b^{2}.}\n\nThe relation \n  \n    \n      \n        \n          b\n          \n            2\n          \n        \n        =\n        p\n        a\n      \n    \n    {\\displaystyle b^{2}=pa}\n  \n gives the simple final result\n\n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        =\n        −\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                a\n                \n                  3\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a_{\\text{r}}=-{\\frac {n^{2}a^{3}}{r^{2}}}.}\n\nThis means that the acceleration vector \n  \n    \n      \n        \n          \n            \n              r\n              ¨\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\ddot {r}} }\n  \n of any planet obeying Kepler's first and second law satisfies the inverse square law\n\n  \n    \n      \n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        =\n        −\n        \n          \n            α\n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\ddot {r}} =-{\\frac {\\alpha }{r^{2}}}{\\hat {\\mathbf {r} }}}\n  \n\nwhere\n\n  \n    \n      \n        α\n        =\n        \n          n\n          \n            2\n          \n        \n        \n          a\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\alpha =n^{2}a^{3}}\n  \n\nis a constant, and \n  \n    \n      \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {r} }}}\n  \n is the unit vector pointing from the Sun towards the planet, and \n  \n    \n      \n        r\n        \n      \n    \n    {\\displaystyle r\\,}\n  \n is the distance between the planet and the Sun.\n\nSince mean motion \n  \n    \n      \n        n\n        =\n        \n          \n            \n              2\n              π\n            \n            T\n          \n        \n      \n    \n    {\\displaystyle n={\\frac {2\\pi }{T}}}\n  \n where \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the period, according to Kepler's third law, \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n has the same value for all the planets. So the inverse square law for planetary accelerations applies throughout the entire Solar System.\n\nThe inverse square law is a differential equation. The solutions to this differential equation include the Keplerian motions, as shown, but they also include motions where the orbit is a hyperbola or parabola or a straight line. (See Kepler orbit.)\n\nBy Newton's second law, the gravitational force that acts on the planet is:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          m\n          \n            planet\n          \n        \n        \n          \n            \n              r\n              ¨\n            \n          \n        \n        =\n        −\n        \n          m\n          \n            planet\n          \n        \n        α\n        \n          r\n          \n            −\n            2\n          \n        \n        \n          \n            \n              \n                r\n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} =m_{\\text{planet}}\\mathbf {\\ddot {r}} =-m_{\\text{planet}}\\alpha r^{-2}{\\hat {\\mathbf {r} }}}\n\nwhere \n  \n    \n      \n        \n          m\n          \n            planet\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{planet}}}\n  \n is the mass of the planet and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n has the same value for all planets in the Solar System. According to Newton's third law, the Sun is attracted to the planet by a force of the same magnitude. Since the force is proportional to the mass of the planet, under the symmetric consideration, it should also be proportional to the mass of the Sun, \n  \n    \n      \n        \n          m\n          \n            Sun\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{Sun}}}\n  \n. So\n\n  \n    \n      \n        α\n        =\n        G\n        \n          m\n          \n            Sun\n          \n        \n      \n    \n    {\\displaystyle \\alpha =Gm_{\\text{Sun}}}\n  \n\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n is the gravitational constant.\n\nThe acceleration of Solar System body number i is, according to Newton's laws:\n\n  \n    \n      \n        \n          \n            \n              \n                r\n                ¨\n              \n            \n          \n          \n            i\n          \n        \n        =\n        G\n        \n          ∑\n          \n            j\n            ≠\n            i\n          \n        \n        \n          m\n          \n            j\n          \n        \n        \n          r\n          \n            i\n            j\n          \n          \n            −\n            2\n          \n        \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\ddot {r}} _{i}=G\\sum _{j\\neq i}m_{j}r_{ij}^{-2}{\\hat {\\mathbf {r} }}_{ij}}\n  \n\nwhere \n  \n    \n      \n        \n          m\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle m_{j}}\n  \n is the mass of body j, \n  \n    \n      \n        \n          r\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle r_{ij}}\n  \n is the distance between body i and body j, \n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {r} }}_{ij}}\n  \n is the unit vector from body i towards body j, and the vector summation is over all bodies in the Solar System, besides i itself.\n\nIn the special case where there are only two bodies in the Solar System, Earth and Sun, the acceleration becomes\n\n  \n    \n      \n        \n          \n            \n              \n                r\n                ¨\n              \n            \n          \n          \n            Earth\n          \n        \n        =\n        G\n        \n          m\n          \n            Sun\n          \n        \n        \n          r\n          \n            \n              Earth\n            \n            ,\n            \n              Sun\n            \n          \n          \n            −\n            2\n          \n        \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            \n              Earth\n            \n            ,\n            \n              Sun\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\ddot {r}} _{\\text{Earth}}=Gm_{\\text{Sun}}r_{{\\text{Earth}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Sun}}}}\n  \n\nwhich is the acceleration of the Kepler motion. So this Earth moves around the Sun according to Kepler's laws.\n\nIf the two bodies in the Solar System are Moon and Earth the acceleration of the Moon becomes\n\n  \n    \n      \n        \n          \n            \n              \n                r\n                ¨\n              \n            \n          \n          \n            Moon\n          \n        \n        =\n        G\n        \n          m\n          \n            Earth\n          \n        \n        \n          r\n          \n            \n              Moon\n            \n            ,\n            \n              Earth\n            \n          \n          \n            −\n            2\n          \n        \n        \n          \n            \n              \n                \n                  r\n                \n                ^\n              \n            \n          \n          \n            \n              Moon\n            \n            ,\n            \n              Earth\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\ddot {r}} _{\\text{Moon}}=Gm_{\\text{Earth}}r_{{\\text{Moon}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Earth}}}}\n\nSo in this approximation, the Moon moves around the Earth according to Kepler's laws.\n\nIn the three-body case the accelerations are\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \n                        r\n                        ¨\n                      \n                    \n                  \n                  \n                    Sun\n                  \n                \n              \n              \n                \n                =\n                G\n                \n                  m\n                  \n                    Earth\n                  \n                \n                \n                  r\n                  \n                    \n                      Sun\n                    \n                    ,\n                    \n                      Earth\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Sun\n                    \n                    ,\n                    \n                      Earth\n                    \n                  \n                \n                +\n                G\n                \n                  m\n                  \n                    Moon\n                  \n                \n                \n                  r\n                  \n                    \n                      Sun\n                    \n                    ,\n                    \n                      Moon\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Sun\n                    \n                    ,\n                    \n                      Moon\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        r\n                        ¨\n                      \n                    \n                  \n                  \n                    Earth\n                  \n                \n              \n              \n                \n                =\n                G\n                \n                  m\n                  \n                    Sun\n                  \n                \n                \n                  r\n                  \n                    \n                      Earth\n                    \n                    ,\n                    \n                      Sun\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Earth\n                    \n                    ,\n                    \n                      Sun\n                    \n                  \n                \n                +\n                G\n                \n                  m\n                  \n                    Moon\n                  \n                \n                \n                  r\n                  \n                    \n                      Earth\n                    \n                    ,\n                    \n                      Moon\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Earth\n                    \n                    ,\n                    \n                      Moon\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        r\n                        ¨\n                      \n                    \n                  \n                  \n                    Moon\n                  \n                \n              \n              \n                \n                =\n                G\n                \n                  m\n                  \n                    Sun\n                  \n                \n                \n                  r\n                  \n                    \n                      Moon\n                    \n                    ,\n                    \n                      Sun\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Moon\n                    \n                    ,\n                    \n                      Sun\n                    \n                  \n                \n                +\n                G\n                \n                  m\n                  \n                    Earth\n                  \n                \n                \n                  r\n                  \n                    \n                      Moon\n                    \n                    ,\n                    \n                      Earth\n                    \n                  \n                  \n                    −\n                    2\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ^\n                      \n                    \n                  \n                  \n                    \n                      Moon\n                    \n                    ,\n                    \n                      Earth\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {\\ddot {r}} _{\\text{Sun}}&=Gm_{\\text{Earth}}r_{{\\text{Sun}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Sun}},{\\text{Earth}}}+Gm_{\\text{Moon}}r_{{\\text{Sun}},{\\text{Moon}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Sun}},{\\text{Moon}}}\\\\\\mathbf {\\ddot {r}} _{\\text{Earth}}&=Gm_{\\text{Sun}}r_{{\\text{Earth}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Sun}}}+Gm_{\\text{Moon}}r_{{\\text{Earth}},{\\text{Moon}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Moon}}}\\\\\\mathbf {\\ddot {r}} _{\\text{Moon}}&=Gm_{\\text{Sun}}r_{{\\text{Moon}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Sun}}}+Gm_{\\text{Earth}}r_{{\\text{Moon}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Earth}}}\\end{aligned}}}\n\nThese accelerations are not those of Kepler orbits, and the three-body problem is complicated. But Keplerian approximation is the basis for perturbation calculations. (See Lunar theory.)\n\nKepler used his two first laws to compute the position of a planet as a function of time. His method involves the solution of a transcendental equation called Kepler's equation.\n\nThe procedure for calculating the heliocentric polar coordinates (r,θ) of a planet as a function of the time t since perihelion, is the following five steps:\n\nThe position polar coordinates (r,θ) can now be written as a Cartesian vector \n  \n    \n      \n        \n          p\n        \n        =\n        r\n        \n          ⟨\n          \n            cos\n            ⁡\n            \n              θ\n            \n            ,\n            sin\n            ⁡\n            \n              θ\n            \n          \n          ⟩\n        \n      \n    \n    {\\displaystyle \\mathbf {p} =r\\left\\langle \\cos {\\theta },\\sin {\\theta }\\right\\rangle }\n  \n and the Cartesian velocity vector can then be calculated as \n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            \n              μ\n              a\n            \n            r\n          \n        \n        \n          ⟨\n          \n            −\n            sin\n            ⁡\n            \n              E\n            \n            ,\n            \n              \n                1\n                −\n                \n                  ε\n                  \n                    2\n                  \n                \n              \n            \n            cos\n            ⁡\n            \n              E\n            \n          \n          ⟩\n        \n      \n    \n    {\\displaystyle \\mathbf {v} ={\\frac {\\sqrt {\\mu a}}{r}}\\left\\langle -\\sin {E},{\\sqrt {1-\\varepsilon ^{2}}}\\cos {E}\\right\\rangle }\n  \n, where \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n is the standard gravitational parameter.[32]\n\nThe important special case of circular orbit, ε = 0, gives θ = E = M. Because the uniform circular motion was considered to be normal, a deviation from this motion was considered an anomaly.\n\nThe Keplerian problem assumes an elliptical orbit and the four points:\n\nThe problem is to compute the polar coordinates (r,θ) of the planet from the time since perihelion, t.\n\nIt is solved in steps. Kepler considered the circle with the major axis as a diameter, and\n\nThe sector areas are related by \n  \n    \n      \n        \n          |\n        \n        z\n        s\n        p\n        \n          |\n        \n        =\n        \n          \n            b\n            a\n          \n        \n        ⋅\n        \n          |\n        \n        z\n        s\n        x\n        \n          |\n        \n        .\n      \n    \n    {\\displaystyle |zsp|={\\frac {b}{a}}\\cdot |zsx|.}\n\nThe circular sector area \n  \n    \n      \n        \n          |\n        \n        z\n        c\n        y\n        \n          |\n        \n        =\n        \n          \n            \n              \n                a\n                \n                  2\n                \n              \n              M\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle |zcy|={\\frac {a^{2}M}{2}}.}\n\nThe area swept since perihelion,\n\n  \n    \n      \n        \n          |\n        \n        z\n        s\n        p\n        \n          |\n        \n        =\n        \n          \n            b\n            a\n          \n        \n        ⋅\n        \n          |\n        \n        z\n        s\n        x\n        \n          |\n        \n        =\n        \n          \n            b\n            a\n          \n        \n        ⋅\n        \n          |\n        \n        z\n        c\n        y\n        \n          |\n        \n        =\n        \n          \n            b\n            a\n          \n        \n        ⋅\n        \n          \n            \n              \n                a\n                \n                  2\n                \n              \n              M\n            \n            2\n          \n        \n        =\n        \n          \n            \n              a\n              b\n              M\n            \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle |zsp|={\\frac {b}{a}}\\cdot |zsx|={\\frac {b}{a}}\\cdot |zcy|={\\frac {b}{a}}\\cdot {\\frac {a^{2}M}{2}}={\\frac {abM}{2}},}\n  \n\nis by Kepler's second law proportional to time since perihelion. So the mean anomaly, M, is proportional to time since perihelion, t.\n\n  \n    \n      \n        M\n        =\n        n\n        t\n        ,\n      \n    \n    {\\displaystyle M=nt,}\n  \n\nwhere n is the mean motion.\n\nWhen the mean anomaly M is computed, the goal is to compute the true anomaly θ. The function θ = f(M) is, however, not elementary.[33] Kepler's solution is to use\n\n  \n    \n      \n        E\n        =\n        ∠\n        z\n        c\n        x\n        ,\n      \n    \n    {\\displaystyle E=\\angle zcx,}\n  \n x as seen from the centre, the eccentric anomaly\nas an intermediate variable, and first compute E as a function of M by solving Kepler's equation below, and then compute the true anomaly θ from the eccentric anomaly E. Here are the details.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  |\n                \n                z\n                c\n                y\n                \n                  |\n                \n              \n              \n                \n                =\n                \n                  |\n                \n                z\n                s\n                x\n                \n                  |\n                \n                =\n                \n                  |\n                \n                z\n                c\n                x\n                \n                  |\n                \n                −\n                \n                  |\n                \n                s\n                c\n                x\n                \n                  |\n                \n              \n            \n            \n              \n                w\n                i\n                t\n                h\n                \n                  |\n                \n                s\n                c\n                x\n                \n                  |\n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        |\n                      \n                      c\n                      s\n                      \n                        |\n                      \n                      .\n                      \n                        |\n                      \n                      d\n                      x\n                      \n                        |\n                      \n                    \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        a\n                        \n                          2\n                        \n                      \n                      M\n                    \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        a\n                        \n                          2\n                        \n                      \n                      E\n                    \n                    2\n                  \n                \n                −\n                \n                  \n                    \n                      a\n                      ε\n                      ⋅\n                      a\n                      sin\n                      ⁡\n                      E\n                    \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}|zcy|&=|zsx|=|zcx|-|scx|\\\\with|scx|&={\\frac {|cs|.|dx|}{2}}\\\\{\\frac {a^{2}M}{2}}&={\\frac {a^{2}E}{2}}-{\\frac {a\\varepsilon \\cdot a\\sin E}{2}}\\end{aligned}}}\n\nDivision by a2/2 gives Kepler's equation\n\n  \n    \n      \n        M\n        =\n        E\n        −\n        ε\n        sin\n        ⁡\n        E\n        .\n      \n    \n    {\\displaystyle M=E-\\varepsilon \\sin E.}\n\nThis equation gives M as a function of E. Determining E for a given M is the inverse problem. Iterative numerical algorithms are commonly used.\n\nHaving computed the eccentric anomaly E, the next step is to calculate the true anomaly θ.\n\nBut note:  Cartesian position coordinates with reference to the center of ellipse are (a cos E, b sin E)\n\nWith reference to the Sun (with coordinates (c,0) = (ae,0) ), r = (a cos E – ae, b sin E)\n\nTrue anomaly would be arctan(ry/rx), magnitude of r would be √r · r.\n\nNote from the figure that\n\n  \n    \n      \n        \n          |\n        \n        c\n        d\n        \n          |\n        \n        =\n        \n          |\n        \n        c\n        s\n        \n          |\n        \n        +\n        \n          |\n        \n        s\n        d\n        \n          |\n        \n      \n    \n    {\\displaystyle |cd|=|cs|+|sd|}\n  \n\nso that\n\n  \n    \n      \n        a\n        cos\n        ⁡\n        E\n        =\n        a\n        ε\n        +\n        r\n        cos\n        ⁡\n        θ\n        .\n      \n    \n    {\\displaystyle a\\cos E=a\\varepsilon +r\\cos \\theta .}\n\nDividing by \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and inserting from Kepler's first law\n\n  \n    \n      \n        \n          \n            r\n            a\n          \n        \n        =\n        \n          \n            \n              1\n              −\n              \n                ε\n                \n                  2\n                \n              \n            \n            \n              1\n              +\n              ε\n              cos\n              ⁡\n              θ\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {r}{a}}={\\frac {1-\\varepsilon ^{2}}{1+\\varepsilon \\cos \\theta }}}\n  \n\nto get\n\n  \n    \n      \n        cos\n        ⁡\n        E\n        =\n        ε\n        +\n        \n          \n            \n              1\n              −\n              \n                ε\n                \n                  2\n                \n              \n            \n            \n              1\n              +\n              ε\n              cos\n              ⁡\n              θ\n            \n          \n        \n        cos\n        ⁡\n        θ\n        =\n        \n          \n            \n              ε\n              (\n              1\n              +\n              ε\n              cos\n              ⁡\n              θ\n              )\n              +\n              \n                (\n                \n                  1\n                  −\n                  \n                    ε\n                    \n                      2\n                    \n                  \n                \n                )\n              \n              cos\n              ⁡\n              θ\n            \n            \n              1\n              +\n              ε\n              cos\n              ⁡\n              θ\n            \n          \n        \n        =\n        \n          \n            \n              ε\n              +\n              cos\n              ⁡\n              θ\n            \n            \n              1\n              +\n              ε\n              cos\n              ⁡\n              θ\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\cos E=\\varepsilon +{\\frac {1-\\varepsilon ^{2}}{1+\\varepsilon \\cos \\theta }}\\cos \\theta ={\\frac {\\varepsilon (1+\\varepsilon \\cos \\theta )+\\left(1-\\varepsilon ^{2}\\right)\\cos \\theta }{1+\\varepsilon \\cos \\theta }}={\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}.}\n\nThe result is a usable relationship between the eccentric anomaly E and the true anomaly θ.\n\nA computationally more convenient form follows by substituting into the trigonometric identity:\n\n  \n    \n      \n        \n          tan\n          \n            2\n          \n        \n        ⁡\n        \n          \n            x\n            2\n          \n        \n        =\n        \n          \n            \n              1\n              −\n              cos\n              ⁡\n              x\n            \n            \n              1\n              +\n              cos\n              ⁡\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\tan ^{2}{\\frac {x}{2}}={\\frac {1-\\cos x}{1+\\cos x}}.}\n\nGet\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  tan\n                  \n                    2\n                  \n                \n                ⁡\n                \n                  \n                    E\n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      −\n                      cos\n                      ⁡\n                      E\n                    \n                    \n                      1\n                      +\n                      cos\n                      ⁡\n                      E\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      1\n                      −\n                      \n                        \n                          \n                            ε\n                            +\n                            cos\n                            ⁡\n                            θ\n                          \n                          \n                            1\n                            +\n                            ε\n                            cos\n                            ⁡\n                            θ\n                          \n                        \n                      \n                    \n                    \n                      1\n                      +\n                      \n                        \n                          \n                            ε\n                            +\n                            cos\n                            ⁡\n                            θ\n                          \n                          \n                            1\n                            +\n                            ε\n                            cos\n                            ⁡\n                            θ\n                          \n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      (\n                      1\n                      +\n                      ε\n                      cos\n                      ⁡\n                      θ\n                      )\n                      −\n                      (\n                      ε\n                      +\n                      cos\n                      ⁡\n                      θ\n                      )\n                    \n                    \n                      (\n                      1\n                      +\n                      ε\n                      cos\n                      ⁡\n                      θ\n                      )\n                      +\n                      (\n                      ε\n                      +\n                      cos\n                      ⁡\n                      θ\n                      )\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      1\n                      −\n                      ε\n                    \n                    \n                      1\n                      +\n                      ε\n                    \n                  \n                \n                ⋅\n                \n                  \n                    \n                      1\n                      −\n                      cos\n                      ⁡\n                      θ\n                    \n                    \n                      1\n                      +\n                      cos\n                      ⁡\n                      θ\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      1\n                      −\n                      ε\n                    \n                    \n                      1\n                      +\n                      ε\n                    \n                  \n                \n                \n                  tan\n                  \n                    2\n                  \n                \n                ⁡\n                \n                  \n                    θ\n                    2\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\tan ^{2}{\\frac {E}{2}}&={\\frac {1-\\cos E}{1+\\cos E}}={\\frac {1-{\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}}{1+{\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}}}\\\\[8pt]&={\\frac {(1+\\varepsilon \\cos \\theta )-(\\varepsilon +\\cos \\theta )}{(1+\\varepsilon \\cos \\theta )+(\\varepsilon +\\cos \\theta )}}={\\frac {1-\\varepsilon }{1+\\varepsilon }}\\cdot {\\frac {1-\\cos \\theta }{1+\\cos \\theta }}={\\frac {1-\\varepsilon }{1+\\varepsilon }}\\tan ^{2}{\\frac {\\theta }{2}}.\\end{aligned}}}\n\nMultiplying by 1 + ε gives the result\n\n  \n    \n      \n        (\n        1\n        −\n        ε\n        )\n        \n          tan\n          \n            2\n          \n        \n        ⁡\n        \n          \n            θ\n            2\n          \n        \n        =\n        (\n        1\n        +\n        ε\n        )\n        \n          tan\n          \n            2\n          \n        \n        ⁡\n        \n          \n            E\n            2\n          \n        \n      \n    \n    {\\displaystyle (1-\\varepsilon )\\tan ^{2}{\\frac {\\theta }{2}}=(1+\\varepsilon )\\tan ^{2}{\\frac {E}{2}}}\n\nThis is the third step in the connection between time and position in the orbit.\n\nThe fourth step is to compute the heliocentric distance r from the true anomaly θ by Kepler's first law:\n\n  \n    \n      \n        r\n        (\n        1\n        +\n        ε\n        cos\n        ⁡\n        θ\n        )\n        =\n        a\n        \n          (\n          \n            1\n            −\n            \n              ε\n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle r(1+\\varepsilon \\cos \\theta )=a\\left(1-\\varepsilon ^{2}\\right)}\n\nUsing the relation above between θ and E the final equation for the distance r is:\n\n  \n    \n      \n        r\n        =\n        a\n        (\n        1\n        −\n        ε\n        cos\n        ⁡\n        E\n        )\n        .\n      \n    \n    {\\displaystyle r=a(1-\\varepsilon \\cos E).}",
        pageTitle: "Kepler's laws of planetary motion",
    },
    {
        title: "Kirchhoff's laws (disambiguation)",
        link: "https://en.wikipedia.org/wiki/Kirchhoff%27s_laws_(disambiguation)",
        content: "Kirchhoff's laws, named after Gustav Kirchhoff, may refer to:",
        pageTitle: "Kirchhoff's laws",
    },
    {
        title: "Kleiber's law",
        link: "https://en.wikipedia.org/wiki/Kleiber%27s_law",
        content:
            "Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}3⁄4 power of the animal's mass.[2]\n\nMore precisely : posing w = mass of the animal in kilograms, then BMR = 70w\n  \n    \n      \n        \n          \n          \n            3\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle ^{3/4}}\n  \n kilocalories per day, or BMR = 3.4w\n  \n    \n      \n        \n          \n          \n            3\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle ^{3/4}}\n  \n watts.[3]\n\nThus, over the same time span, a cat having a mass 100 times that of a mouse will consume only about 32 times the energy the mouse uses.\n\nPresently is unclear if the value of the exponent in Kleiber's law is correct, in part because the law currently lacks a single theoretical explanation that is entirely satisfactory.\n\nMore recently, Kleiber's law has also been shown to apply in plants,[4] suggesting that Kleiber's observation is much more general.\n\nKleiber's law, like many other biological allometric laws, is a consequence of the physics and/or geometry of circulatory systems in biology.[5] Max Kleiber first discovered the law when analyzing a large number of independent studies on respiration within individual species.[2] Kleiber expected to find an exponent of 2⁄3 (for reasons explained below), and was confounded by the discovery of a 3⁄4 exponent.\n\nBefore Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\",[6] which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.[7] Rubner (1883) first demonstrated the law in accurate respiration trials on dogs.[8]\n\nMax Kleiber challenged this notion in the early 1930s. Through extensive research on various animals' metabolic rates, he found that a 3/4 power scaling provided a better fit to the empirical data than the 2/3 power.[2] His findings provided the groundwork for understanding allometric scaling laws in biology, leading to the formulation of the Metabolic Scaling Theory and the later work by West, Brown, and Enquist, among others.\n\nSuch an argument does not address the fact that different organisms exhibit different shapes (and hence have different surface-area-to-volume ratios, even when scaled to the same size). Reasonable estimates for organisms' surface area do appear to scale linearly with the metabolic rate.[9]\n\nWest, Brown, and Enquist, (hereafter WBE) proposed a general theory for the origin of many allometric scaling laws in biology. According to the WBE theory, 3⁄4-scaling arises because of efficiency in nutrient distribution and transport throughout an organism. In most organisms, metabolism is supported by a circulatory system featuring branching tubules (i.e., plant vascular systems, insect tracheae, or the human cardiovascular system). WBE claim that (1) metabolism should scale proportionally to nutrient flow (or, equivalently, total fluid flow) in this circulatory system and (2) in order to minimize the energy dissipated in transport, the volume of fluid used to transport nutrients (i.e., blood volume) is a fixed fraction of body mass. [10] The model assumes that the energy dissipated is minimized and that the terminal tubes do not vary with body size. It provides a complete analysis of numerous anatomical and physiological scaling relations for circulatory systems in biology that generally agree with data.[10] More generally, the model predicts the structural and functional properties of vertebrate cardiovascular and respiratory systems, plant vascular systems, insect tracheal tubes, and other distribution networks.\n\nThey then analyze the consequences of these two claims at the level of the smallest circulatory tubules (capillaries, alveoli, etc.). Experimentally, the volume contained in those smallest tubules is constant across a wide range of masses. Because fluid flow through a tubule is determined by the volume thereof, the total fluid flow is proportional to the total number of smallest tubules. Thus, if B denotes the basal metabolic rate, Q the total fluid flow, and N the number of minimal tubules,\n  \n    \n      \n        B\n        ∝\n        Q\n        ∝\n        N\n        \n          .\n        \n      \n    \n    {\\displaystyle B\\propto Q\\propto N{\\text{.}}}\n  \n  Circulatory systems do not grow by simply scaling proportionally larger; they become more deeply nested. The depth of nesting depends on the self-similarity exponents of the tubule dimensions, and the effects of that depth depend on how many \"child\" tubules each branching produces. Connecting these values to macroscopic quantities depends (very loosely) on a precise model of tubules. WBE show that if the tubules are well-approximated by rigid cylinders, then, to prevent the fluid from \"getting clogged\" in small cylinders, the total fluid volume V satisfies[11]\n  \n    \n      \n        \n          N\n          \n            4\n          \n        \n        ∝\n        \n          V\n          \n            3\n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle N^{4}\\propto V^{3}{\\text{.}}}\n  \n  (Despite conceptual similarities, this condition is inconsistent with Murray's law)[12] Because blood volume is a fixed fraction of body mass,[10] \n  \n    \n      \n        B\n        ∝\n        \n          M\n          \n            \n              3\n              4\n            \n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle B\\propto M^{\\frac {3}{4}}{\\text{.}}}\n\nThe WBE theory predicts that the scaling of metabolism is not a strict power law but rather should be slightly curvilinear. The 3/4 exponent only holds exactly in the limit of organisms of infinite size. As body size increases, WBE predict that the scaling of metabolism will converge to a ~3/4 scaling exponent.[10]  Indeed, WBE predicts that the metabolic rates of the smallest animals tend to be greater than expected from the power-law scaling (see Fig. 2 in Savage et al. 2010 [13] ). Further, Metabolic rates for smaller animals (birds under 10 kg [22 lb], or insects) typically fit to 2⁄3 much better than 3⁄4; for larger animals, the reverse holds.[14] As a result, log-log plots of metabolic rate versus body mass can \"curve\" slightly upward, and fit better to quadratic models.[15] In all cases, local fits exhibit exponents in the [2⁄3,3⁄4] range.[16]\n\nElaborations of the WBE model predict larger scaling exponents, worsening the discrepancy with observed data.[17] see also, [14][18]). However, one can retain a similar theory by relaxing WBE's assumption of a nutrient transport network that is both fractal and circulatory. Different networks are less efficient in that they exhibit a lower scaling exponent. Still, a metabolic rate determined by nutrient transport will always exhibit scaling between 2⁄3 and 3⁄4.[16] WBE argued that fractal-like circulatory networks are likely under strong stabilizing selection to evolve to minimize energy used for transport. If selection for greater metabolic rates is favored, then smaller organisms will prefer to arrange their networks to scale as 2⁄3. Still, selection for larger-mass organisms will tend to result in networks that scale as 3⁄4, which produces the observed curvature.[19]\n\nAn alternative model notes that metabolic rate does not solely serve to generate heat. Metabolic rate contributing solely to useful work should scale with power 1 (linearly), whereas metabolic rate contributing to heat generation should be limited by surface area and scale with power 2⁄3. Basal metabolic rate is then the convex combination of these two effects: if the proportion of useful work is f, then the basal metabolic rate should scale as \n  \n    \n      \n        B\n        =\n        f\n        ⋅\n        k\n        M\n        +\n        (\n        1\n        −\n        f\n        )\n        ⋅\n        \n          k\n          ′\n        \n        \n          M\n          \n            \n              2\n              3\n            \n          \n        \n      \n    \n    {\\displaystyle B=f\\cdot kM+(1-f)\\cdot k'M^{\\frac {2}{3}}}\n  \n where k and k′ are constants of proportionality. k′ in particular describes the surface area ratio of organisms and is approximately 0.1 kJ·h−1·g−2/3;[20] typical values for f are 15-20%.[21] The theoretical maximum value of f is 21%, because the efficiency of glucose oxidation is only 42%, and half of the ATP so produced is wasted.[20]\n\nKozłowski and Konarzewski have argued against attempts to explain Kleiber's law via any sort of limiting factor because metabolic rates vary by factors of 4-5 between rest and activity. Hence, any limits that affect the scaling of the basal metabolic rate would make elevated metabolism — and hence all animal activity — impossible.[22] WBE conversely argue that natural selection can indeed select for minimal transport energy dissipation during rest, without abandoning the ability for less efficient function at other times.[23]\n\nOther researchers have also noted that Kozłowski and Konarzewski's criticism of the law tends to focus on precise structural details of the WBE circulatory networks but that the latter are not essential to the model.[11]\n\nAnalyses of variance for a variety of physical variables suggest that although most variation in basal metabolic rate is determined by mass, additional variables with significant effects include body temperature and taxonomic order.[24][25]\n\nA 1932 work by Brody calculated that the scaling was approximately 0.73.[9][26]\n\nA 2004 analysis of field metabolic rates for mammals conclude that they appear to scale with exponent 0.749.[19]\n\nKleiber's law has been reported to interspecific comparisons and has been claimed not to apply at the intraspecific level.[27] The taxonomic level that body mass metabolic allometry should be studied has been debated [28][29] Nonetheless, several analyses suggest that while the exponents of the Kleiber's relationship between body size and metabolism can vary at the intraspecific level, statistically, intraspecific exponents in both plants and animals tend to cluster around 3/4.[30]\n\nA 1999 analysis concluded that biomass production in a given plant scaled with the 3⁄4 power of the plant's mass during the plant's growth,[31] but a 2001 paper that included various types of unicellular photosynthetic organisms found scaling exponents intermediate between 0.75 and 1.00.[32] Similarly, a 2006 paper in Nature argued that the exponent of mass is close to 1 for plant seedlings, but that variation between species, phyla, and growth conditions overwhelm any \"Kleiber's law\"-like effects.[33] But, metabolic scaling theory can successfully resolve these apparent exceptions and deviations.  For finite-size corrections in networks with both area-preserving and area-increasing branching, the WBE model predicts that fits to data for plants yield scaling exponents that are steeper than 3/4 in small plants but then converge to 3/4 in larger plants (see [34][17]).\n\nBecause cell protoplasm appears to have constant density across a range of organism masses, a consequence of Kleiber's law is that, in larger species, less energy is available to each cell volume. Cells appear to cope with this difficulty via choosing one of the following two strategies: smaller cells or a slower cellular metabolic rate. Neurons and adipocytes exhibit the former; every other type of cell, the latter.[35] As a result, different organs exhibit different allometric scalings (see table).[9]",
        pageTitle: "Kleiber's law",
    },
    {
        title: "Kluge's law",
        link: "https://en.wikipedia.org/wiki/Kluge%27s_law",
        content:
            'Kluge\'s law is a controversial Proto-Germanic sound law formulated by Friedrich Kluge.[1] It purports to explain the origin of the Proto-Germanic long consonants *kk, *tt, and *pp (Proto-Indo-European lacked a phonemic length distinction for consonants) as originating in the assimilation of *n to a preceding voiced plosive consonant, under the condition that the *n was part of a suffix which was stressed in the ancestral Proto-Indo-European (PIE). The name "Kluge\'s law" was coined by Kauffmann (1887)[2] and revived by Frederik Kortlandt (1991).[3] As of 2006,[4] this law has not been generally accepted by historical linguists.\n\nThe resulting long consonants would subsequently have been shortened, except when they followed a short vowel; this is uncontroversial for *ss[4] (which has a different origin). Proponents of Kluge\'s law use this to explain why so many Proto-Germanic roots (especially of strong verbs) end in short *p, *t, or *k even though their likely cognates in other Indo-European languages point to final Proto-Indo-European (PIE) consonants other than the expected *b, *d, *g, or *ǵ. (Indeed, non-Germanic evidence for PIE *b is so rare that *b may not have been a phoneme at all; yet, in Proto-Germanic, *p was rare only at the beginnings of words.)\n\nMuch like Verner\'s law, Kluge\'s law would have created many consonant alternations in the grammatical paradigm of a word that were becoming only partially predictable. Analogical simplifications of these complexities are proposed[5][6] as an explanation for the many cases where closely related (often otherwise identical) words point to short, long, plosive, fricative, voiceless or voiced Proto-Germanic consonants in closely related Germanic languages or dialects, even sometimes the same dialect.\n\nThe origin of Proto-Germanic (PGmc) *ll, *rr, *nn, and *mm had already been explained in Kluge\'s time [7] as resulting from the assimilation of consonant clusters across earlier morpheme boundaries: *ll from earlier (Pre-Germanic) *l-n, *rr from earlier *r-n, *nn from earlier *n-n and *n-w, *mm from earlier *z-m and *n-m. This is uncontroversial today,[4][6][8] except that *r-n may not have given *rr in every case.[6][8] A few examples with *-n are:[6]\n\nKluge (1884)[1] proposed to explain *pp, *tt, and *kk the same way (examples cited after Kroonen 2011[6]):\n\nWithout Kluge\'s law, **-bn-, **-pn-, **-dn-, **-tn-, **-gn-, and **-kn- would be expected, respectively, in the Germanic forms (according to Grimm\'s law and Verner\'s law).\n\nKluge\'s law did not operate behind stressed vowels, only in the same environment as Verner\'s law.[1] Examples cited are after Kroonen (2009, 2011):[5][6]\n\nAlso, even when that condition was fulfilled, Kluge\'s law did not act on the descendants of PIE *s (PGmc *z following Verner\'s law). Examples cited after Kroonen (2009, 2011):[5][6]\n\nThe rise of long consonant phonemes left the Pre-Germanic language with three kinds of syllables:\n\nIn other words, syllables could be long because of the specific vowel (or a following *l/*m/*n/*r), or because a long consonant from the next syllable bled in. If both occurred, the syllable was overlong. See Mora (linguistics), which suggests that such overlong syllables are cross-linguistically rare.\n\nAll overlong syllables were then turned into long syllables by shortening the long consonant. This is uncontroversial for *ss, which derives from PIE *t-t, *d-t and *dʰ-t clusters across morpheme boundaries (which were probably pronounced [tst] in PIE):[4][8]\n\nKluge (1884: 183)[1] proposed to extend this explanation to cases where Proto-Germanic roots that constituted long syllables ended in *p, *t, or *k, while different consonants at the same places of articulation would be expected based on apparently related roots (in PGmc or other Indo-European branches). Examples cited after Kroonen (2011):[6]\n\nKluge\'s law had a noticeable effect on Proto-Germanic morphology. Because of its dependence on ablaut and accent, it operated in some parts of declension and conjugation, but not in others, giving rise to alternations of short and long consonants in both nominal and verbal paradigms. Kroonen (2009, 2011)[5][6] compared these alternations to grammatischer Wechsel (the alternation of voiced and voiceless fricatives in Proto-Germanic, caused by Verner\'s law) and especially to the consonant gradation of the neighboring Finnic and Sami languages. This is most conspicuous in the n-stem nouns and the "néh₂-presents" (imperfective verbs formed from perfective ones by adding the PIE suffix *néh₂-/*nh₂-), but also occurs in mn-stems and directional adverbs.[6]\n\nKluge\'s law created long consonants in the genitive singular, which ended in *-n-és in PIE, and in the genitive plural (*-n-óHom). It did not operate in the dative plural: although the *n of *-n̥-mis was in direct contact with the root in PIE, it was syllabic, so it became *-un- early on the way to Proto-Germanic[4] (soon assimilated to *-ummiz[4][8]), preventing the operation of Kluge\'s law.[6]\n\nSchematic (after Kroonen 2009: 32),[5] where C represents the initial and the final consonant of the root, and G represents its Verner variant if it had one:\n\nNaturally, this led to three different kinds of consonant alternation (examples after Kroonen 2009):[5]\n\nThe nominative singular of roots ending in plosives thus became difficult to predict from the cases where Kluge\'s law had operated; and the pure length opposition was more common than the others, because it was not limited to plosives.[6]\n\nIn PIE, such words regularly would have had a nominative singular in *-mḗn and a genitive in *-mn-és. However, in the genitive singular, it appears that the *-m- dropped out of the middle of the resulting three-consonant cluster already in PIE, making the mn-stems look like n-stems: PIE *bʰudʰ-mēn, *bʰudʰ-mn-és > *bʰudʰmēn, *bʰudʰnés ("bottom") > Greek πυθμήν /pytʰˈmɛ̌ːn/ from the nominative, but Sanskrit budʰná- and Latin fundus from the genitive. This would have allowed assimilation of *n to the now preceding consonant; Kroonen (2011)[6] proposed that this happened in such words, yielding e.g. PGmc *budmēn, *buttiz ("bottom").\n\nIn addition to prepositions that indicated relative locations (such as "in" or "over"), Proto-Germanic had a large set of directional adverbs: "locative" ones (with meanings such as "inside" or "on top"), "allative" ones (with meanings such as "into" or "up") and "ablative" ones (with meanings such as "out from the inside" or "down from above"). Many, but not all of these forms had long consonants. Kroonen (2011, 2012)[6][9] reconstructed examples like this and attributed them to Kluge\'s law:\n\nThe law has sparked[when?] discussions about its chronology in relation to Grimm\'s law and Verner\'s law. The problem is that the traditional ordering (1. Grimm, 2. Verner, 3. Kluge) cannot account for the absence of voice in the Proto-Germanic geminates.[citation needed] Therefore, it has been proposed[according to whom?] to rearrange the order of events so that the Proto-Germanic geminates\' loss of voice may be equated with that part of Grimm\'s law that turns mediae into voiceless tenues. This would mean that characteristics noted in Kluge\'s law happened before (or between different phases of) those of Grimm\'s law. If accepted, this has further consequences, because those characteristics of Verner\'s law must in fact, precede those of Kluge\'s law, or otherwise it can not be explained why both the reflexes of PIE voiced aspirated plosives and PIE voiceless plosives underwent the changes characteristic in Kluge\'s law. Consequently, this would put Verner\'s law chronologically in the first position, followed by Kluge\'s, and finally by Grimm\'s law.\n\nUnder the updated view, the processes may be summarized by the following table:\n\nSoon after the initial publications (Osthoff 1882; Kluge 1884[1]), Kluge\'s law came to be considered an unnecessary hypothesis by several authors. With rather few exceptions, introductory texts have ignored it, and more detailed works on Proto-Germanic have generally dismissed it rather briefly; "it has been seriously challenged throughout the 20th century, and nowadays even borders on the uncanonical in both Indo-European and Germanic linguistics" (Kroonen 2009: 53).[5]\n\nBeginning with Trautmann (1906),[10] several authors (e.g. Kuryłowicz 1957: 133–134;[11] Fagan 1989: 38;[12] Ringe 2006: 115[4]) have stated that there are very few or no cases where a Proto-Germanic root with a long plosive corresponds to, or is best explained as corresponding to, a PIE root followed by a suffix that began with n.\n\nLühr (1988)[13] and Kroonen (2011)[6] countered by presenting long lists of examples, especially (as they point out) of n-stem nouns.\n\nOnomatopoetic roots often end in a long plosive in Germanic languages. Examples (Kroonen 2011: 125)[6] include the Old Norse words klappa "to clap", okka "to sigh", and skvakka "to make a gurgling sound", Old Swedish kratta and modern German kratzen "to scratch", modern Norwegian tikka "to tap", Old Frisian kloppa and modern German klopfen "to knock", and Old English cluccian "to cluck". Long consonants more generally are ubiquitous in Germanic nicknames such as Old English Totta from Torhthelm, Beoffa from Beornfriþ, Blæcca for a black-haired man (note the short /k/ in blæc), Eadda (and German Otto) from all names with Proto-Germanic *Auda- (Gąsiorowski 2006[14] and references therein), a long list of Gothic ones whose referents are often difficult or impossible to reconstruct (Ibba, Faffo, Mammo, Oppa, Riggo, Wacca, etc.; possibly also atta, meaning "father"), German ones such as – accounting for the High German consonant shift – Fritz (*Fritta(n)-) from Friedrich, Lutz (*(H)lutta(n)-) from Ludwig, and Sicko (*Sikkan-) from Si(e)gmar, and finally Icelandic Solla from Sólrún, Magga from Margrét, Nonni from Jón, Stebbi from Stefán, Mogga from Morgunblaðið, and lögga "cop" from lögreglan "the police";[6] Gąsiorowski (2006)[14] further proposed to explain the otherwise enigmatic English words dog, pig, frog, stag, (ear)wig, and Old English sucga "dunnock" and *tacga ~ *tecga "young sheep" (not attested in the nominative singular) as nicknames formed to various nouns or adjectives. Some authors, such as Trautmann (1906)[10] and Fagan (1989),[12] have tried to ascribe all long plosives of Proto-Germanic to "intensive" or "expressive gemination" on the basis of the idea that the roots that contained them had meanings connected to emotions, including intensity and iteration; this idea, first formulated by Gerland (1869[15] – long before Kluge published), was accepted e.g. in the extremely influential, Indogermanisches etymologisches Wörterbuch (Pokorny 1959)[16] as well as the more specialist works of Seebold (1970)[17] and Kluge & Seebold (2002),[18] and was considered "still perhaps the most widely accepted explanation" by Ringe (2006: 115).[4]\n\nLühr (1988)[13] and Rasmussen (1989b), approvingly cited by Kroonen (2011),[6] as well as Kortlandt (1991),[3] countered that most nouns with long plosives or evidence of consonant gradation did not have meanings that would fit this hypothesis. The same works pointed out that "expressive gemination" does not explain why so many of these nouns are n-stems. Moreover, expressive gemination cannot explain the many cases where Proto-Germanic */pː tː kː/ correspond to PIE */bʱ dʱ ɡʲʱ ɡʱ/ (as in Old English liccian "to lick" from PIE *leiǵh-, where **licgian would be expected in OE; Gąsiorowski 2012: 17), it cannot explain Proto-Germanic */p t k/ corresponding to PIE */bʱ dʱ ɡʲʱ ɡʱ/ (as in Old English dēop from PIE *dheubh-; Kortlandt 1991: 3, Kroonen 2011: 128, Gąsiorowski 2012: 16[19]), and it cannot explain Proto-Germanic */p t k/ corresponding to PIE */p t kʲ k/ (as in Middle Dutch token "to push" from PIE *duk-), while Kluge\'s law followed by analogy has no problem with such phenomena (Kroonen 2011: 125). Kroonen (2011: 125) added: "Moreover, the Expressivity Theory [sic] seems to contain a critical theoretical fallacy. It is a priori implausible that a completely new range of phonemes (i.e. geminates) could be introduced into a linguistic system by extra-linguistic factors such as charged semantics. In this respect, some versions of the Expressivity Theory are truly comparable to what in biology is known as Aristotle\'s generatio spontanea hypothesis [...], which revolved around the idea that living organisms, such as flies and eels, come about spontaneously in decaying corpses." Finally, the nicknames with long consonants (including Gothic atta) are n-stems; n-stem nicknames occur in other Indo-European branches as well, such as Latin Catō, Varrō, Nerō and Greek Platōn, Strabōn,[6] and "Germanic has many personalizing or individualizing n-stems that are structurally identical with the hypocorisms [nicknames], e.g. OHG chresso \'groundling\' to chresan \'to crawl\' (Kuryłowicz 1957[11]) [...]" (Kroonen 2011: 82).[6]\n\nMost of the Proto-Germanic long plosives are voiceless; but while long voiced plosives were rare, they do have to be reconstructed in a few cases. The hypothesis of expressive gemination has trouble explaining this, as Trautmann (1906: 66)[10] admitted while rejecting Kluge\'s law: "Wie wir uns freilich das Nebeneinander von z. B. kk- gg- k- g- zu erklären haben, weiss ich nicht" – \'I do not know, however, how we ought to explain the coexistence of e.g. kk- gg- k- g-\'. Kroonen (2011: 124): "The only existing theory that is powerful enough   to explain  such root variations, is the one that acknowledges consonant gradation and the underlying mechanism of the paradigmatic contaminations. The co-occurrence of ON riga \'to lift heavily\' : MLG wriggen \'to twist\' : ME wricken \'to wiggle\', for instance, implies two different expressive formations within the Expressivity Theory, the choice between a voiced and voiceless geminate being arbitrary, erratic, or, in other words, scientifically unfalsifiable. By reconstructing a paradigm *wrikkōþi, *wrigunanþi < *uriḱ-néh₂-ti, *uriḱ-nh₂-énti, on the other hand, the only irregular form is *wrigg-, which may be explained readily by contamination of *wrig- and *wrikk-."\n\nSimilarly, Gąsiorowski (2012: 21)[19] felt that it was "methodologically unsound to invoke" "psycholinguistic factors" and other hypotheses of irregular development "until we have tried everything else", in this case, a regular sound law such as Kluge\'s. Kroonen (2009: 53)[5] pointed out that, by virtue of having first been published in 1869,[15] the hypothesis of expressive gemination "basically stems from the time before the rise of the Neogrammarian doctrine of Ausnahmslosigkeit der Lautgesetze" (\'exceptionlessness of the sound laws\').\n\nAs pointed out above, long consonants did not exist in Proto-Indo-European, and many Germanic roots are attested with a long consonant in some of the ancient languages, but with a short one in others (often together with a short or a long vowel, respectively). This led the "Leiden School" to postulate that the Germanic roots with long plosives were not inherited from PIE, but borrowed from a substrate language. Kroonen (2011: 12)[6] reported that his doctorate at the University of Leiden was originally intended\n\nto investigate the influence of lost non-Indo-European languages on the Proto-Germanic lexicon. [...] During the course of time, however, my dissertation gradually developed into a study of the Proto-Germanic n-stems and their typical morphology. The reason for this change of direction was that the most important formal criterion that had been used in order to isolate non-Indo-European words from the rest of the lexicon – the Proto-Germanic geminates – turned out to be significantly overrepresented in this morphological category.\n     The advocates of the Leiden Substrate Theory had defined the typical Germanic cross-dialectal interchange of singulate and geminate roots as the prime indicator of prehistoric language contact. For this reason, this substrate language had even been dubbed the "Language of the Geminates". Yet, beside the fact that geminates were not at all distributed randomly across the vocabulary, as would be expected in the case of language contact, the interchanges proved to be far from erratic. In fact, they turned out to be strikingly predictable in nature.\n\nWhile it is by no means impossible that there was "a substrate language with geminates", or even "that Kluge\'s law was triggered by the absorption of speakers of this substrate language into the PIE dialect that ultimately became known as Germanic", Kroonen (2009: 62)[5] found no evidence for such hypotheses and stressed that a long consonant in a Germanic root may not be taken as evidence that this root was borrowed.\n\nLong plosives are very rare in the known Gothic material; other than the abovementioned nicknames (including atta), they are attested only in skatts ("money"), smakka ("fig"; n-stem) and the Latin loanword, sakkus ("sack") (Kroonen 2011).[6] Therefore, Kuryłowicz (1957)[11] and Fagan (1989)[12] argued that long plosives were absent in Proto-Germanic and only arose in Proto-Northwest Germanic – so that, if Kluge\'s law exists at all, it must have operated between Proto-Germanic and Proto-Northwest Germanic, not between Proto-Indo-European and Proto-Germanic.\n\nLühr (1988)[13] and Kroonen (2011)[6] have pointed out that strong verbs with /p t k/ following a long vowel, diphthong or "resonant" are common in the Gothic Bible, and that many of these are clearly related to iteratives with long consonants that are attested in Northwest Germanic languages. Kroonen (2011: 82, 111)[6] further drew attention to the fact that the Old Saxon Heliand, an epic poem about the life of Jesus, contains only three words with long plosives of potentially Proto-Germanic origin (skatt "treasure, money", likkōn "to lick"; upp, uppa, uppan "on top", "up", "down from above"), while such words "are ever-present in Middle Low German", and approvingly cited the hypothesis by Kuryłowicz (1957: 140)[11] that words with long plosives were considered stylistically inappropriate for a Christian religious work because long plosives were so common in nicknames – they may have sounded too colloquial and informal. Gothic is almost exclusively known from the surviving parts of a Bible translation and from fragments of a commentary on the Gospel of John.',
        pageTitle: "Kluge's law",
    },
    {
        title: "Koomey's law",
        link: "https://en.wikipedia.org/wiki/Koomey%27s_law",
        content:
            'Koomey\'s law describes a trend in the history of computing hardware: for about a half-century, the number of computations per joule of energy dissipated doubled about every 1.57 years. Professor Jonathan Koomey described the trend in a 2010 paper in which he wrote that "at a fixed computing load, the amount of battery you need will fall by a factor of two every year and a half."[1]\n\nThis trend had been remarkably stable since the 1950s (R2 of over 98%). But in 2011, Koomey re-examined this data[2] and found that after 2000, the doubling slowed to about once every 2.6 years. This is related to the slowing[3] of Moore\'s law, the ability to build smaller transistors; and the end around 2005 of Dennard scaling, the ability to build smaller transistors with constant power density.\n\n"The difference between these two growth rates is substantial. A doubling every year and a half results in a 100-fold increase in efficiency every decade. A doubling every two and a half years yields just a 16-fold increase", Koomey wrote.[4]\n\nThe implications of Koomey\'s law are that the amount of battery needed for a fixed computing load will fall by a factor of 100 every decade.[5]  As computing devices become smaller and more mobile, this trend may be even more important than improvements in raw processing power for many applications.  Furthermore, energy costs are becoming an increasing factor in the economics of data centers, further increasing the importance of Koomey\'s law.\n\nThe slowing of Koomey\'s law has implications for energy use in information and communications technology. However, because computers do not run at peak output continuously, the effect of this slowing may not be seen for a decade or more.[6] Koomey writes that "as with any exponential trend, this one will eventually end...in a decade or so, energy use will once again be dominated by the power consumed when a computer is active. And that active power will still be hostage to the physics behind the slowdown in Moore\'s Law."\n\nKoomey was the lead author of the article in IEEE Annals of the History of Computing that first documented the trend.[1] At about the same time, Koomey published a short piece about it in IEEE Spectrum.[7]\n\nIt was further discussed in MIT Technology Review,[8] and in a post by Erik Brynjolfsson on the "Economics of Information" blog,[5] and at The Economist online.[9]\n\nThe trend was previously known for digital signal processors, and it was then named "Gene\'s law".  The name came from Gene Frantz, an electrical engineer at Texas Instruments. Frantz had documented that power dissipation in DSPs had been reduced by half every 18 months, over a 25-year period.[10][11]\n\nLatest studies indicate that Koomey\'s Law has slowed to doubling every 2.6 years.[2] This rate is a statistical average over many technologies and many years, but there are exceptions. For example, in 2020 AMD reported that, since 2014, the company has managed to improve the efficiency of its mobile processors by a factor of 31.7, which is a doubling rate of 1.2 years.[12] In June 2020, Koomey responded to the report, writing, "I have reviewed the data and can report that AMD exceeded the 25×20 goal it set in 2014 through improved design, superior optimization, and a laser-like focus on energy efficiency."[12]\n\nBy the second law of thermodynamics and Landauer\'s principle, irreversible computing cannot continue to be made more energy efficient forever.  Assuming that the energy efficiency of computing will continue to double every 2.6 years, and taking the most efficient super computer as of 2022,[13] the Landauer bound will be reached around 2080. Thus, after this point, Koomey\'s law can no longer hold. Landauer\'s principle, however, does not constrain the efficiency of reversible computing. This, in conjunction with other Beyond CMOS computing technologies, could permit continued advances in efficiency.',
        pageTitle: "Koomey's law",
    },
    {
        title: "Kopp's law",
        link: "https://en.wikipedia.org/wiki/Kopp%27s_law",
        content:
            "Kopp's law can refer to either of two relationships discovered by the German chemist Hermann Franz Moritz Kopp (1817–1892).\n\nThe Kopp–Neumann law, named for Kopp and Franz Ernst Neumann, is a common approach for determining the specific heat C  (in J·kg−1·K−1) of compounds using the following equation:[3]\n\n  \n    \n      \n        C\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          C\n          \n            i\n          \n        \n        \n          f\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle C=\\sum _{i=1}^{N}C_{i}f_{i},}\n  \n \nwhere N is the total number of compound constituents, and Ci and fi denote the specific heat and mass fraction of the i-th constituent. This law works surprisingly well at room-temperature conditions, but poorly at elevated temperatures.[3]\n\nThis thermodynamics-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Kopp's law",
    },
    {
        title: "Korte's law",
        link: "https://en.wikipedia.org/wiki/Korte%27s_law",
        content:
            "In psychophysics, Korte's third law of apparent motion[1] is an observation relating the phenomenon of apparent motion to the distance and duration between two successively presented stimuli.[2]\n\nKorte's four laws were first proposed in 1915 by Adolf Korte.[3] The third law, particularly, describes how the increase in distance between two stimuli narrows the range of interstimulus intervals (ISI), which produce the apparent motion.[4] It holds that there is a requirement for the proportional decrease in the frequency in which two stimulators are activated in alternation with the increase in ISI to ensure the quality of apparent motion.[4] One identified violation of the Korte's law occurs if the shortest path between seen arm positions is not possible anatomically.[5] This was demonstrated by Maggie Shiffrar and Jennifer Freyd using a picture that showed a woman demonstrating two positions. This highlighted the problem in taking the shortest path to perform the alternating postures.[5]\n\nThe laws were composed of general statements (laws) describing beta movement in the sense of \"optimal motion\".[6] These outlined several constraints for obtaining the percept of apparent motion between flashes: \"(1) larger separations require higher intensities, (2) slower presentation rates require higher intensities, (3) larger separations require slower presentation rates, (4) longer flash durations require shorter intervals .[7]\n\nA modern formulation of the law is that the greater the length of a path between two successively presented stimuli, the greater the stimulus onset asynchrony (SOA) must be for an observer to perceive the two stimuli as a single mobile object. Typically, the relationship between distance and minimal SOA is linear.[2]\n\nArguably, Korte's third law is counterintuitive. One might expect that successive stimuli are less likely to be perceived as a single object as both distance and interval increase, and therefore, a negative relationship should be observed instead. In fact, such a negative relationship can be observed as well as Korte's law. Which relationship holds depends on speed.[1] Korte's law also involves a constancy of velocity through apparent motion and it is said that data do not support it.[8]\n\nThis psychology-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Korte's third law of apparent motion",
    },
    {
        title: "Kranzberg's laws of technology",
        link: "https://en.wikipedia.org/wiki/Kranzberg%27s_laws_of_technology",
        content:
            "Melvin Kranzberg (November 22, 1917 – December 6, 1995) was an American historian, and professor of history at Case Western Reserve University from 1952 until 1971. He was a Callaway professor of the history of technology at Georgia Tech from 1972 to 1988.\n\nBorn in St. Louis, Missouri, Kranzberg graduated from Amherst College, received a master's and a Ph.D. from Harvard University and served in the U.S. Army in Europe during World War II. He received a Bronze Star for interrogating captured German prisoners and learning the location of Nazi gun emplacements. He was one of two interrogators out of nine in Patton's army who were not killed during the conflict. He received his interrogation training at Camp Ritchie in Maryland, making him one of the Ritchie Boys.\n\nKranzberg is known for his laws of technology, the first of which states \"Technology is neither good nor bad; nor is it neutral.\"[1]\n\nHe was one of the founders of the Society for the History of Technology in the United States and long-time editor of its journal Technology and Culture. Kranzberg served as president of the society from 1983 to 1984, and edited the society's journal from 1959 to 1981, when he turned it over to Robert C. Post of the Smithsonian Institution. The society awards a yearly $4000 fellowship named after Kranzberg to doctoral students engaged in the preparation of dissertations on the history of technology. The award is available to students all over the world. In 1967 Kranzberg was awarded the Leonardo da Vinci Medal by the Society for the History of Technology.\n\nHoward P. Segal wrote an informative semi-biographical tribute to Kranzberg in the Virginia Quarterly Review.[2]\n\nThere are two biographical articles by Robert C. Post in Technology and Culture:\n\nKranzberg helped found the International Committee for the History of Technology.[3]\n\nMelvin Kranzberg's six laws of technology[1][4] state:",
        pageTitle: "Melvin Kranzberg",
    },
    {
        title: "Kryder's law",
        link: "https://en.wikipedia.org/wiki/Kryder%27s_law",
        content:
            'Mark Howard Kryder (born October 7, 1943 in Portland, Oregon) was Seagate Corp.\'s senior vice president of research and chief technology officer.[1] Kryder holds a Bachelor of Science degree in electrical engineering from Stanford University and a Ph.D. in electrical engineering and physics from the California Institute of Technology.[1]\n\nKryder was elected a member of the National Academy of Engineering in 1994 for contributions to the understanding of magnetic domain behavior and for leadership in information storage research.\n\nHe is known for "Kryder\'s law", an observation from the mid-2000s about the increasing capacity of magnetic hard drives.\n\nA 2005 Scientific American article, titled "Kryder\'s Law", described Kryder\'s observation that magnetic disk areal storage density was then increasing at a rate exceeding Moore\'s Law.[2] The pace was then much faster than the two-year doubling time of semiconductor chip density posited by Moore\'s law.\n\nInside of a decade and a half, hard disks had increased their capacity 1,000-fold, a rate that Intel founder Gordon Moore himself has called "flabbergasting."\n\nIn 2005, commodity drive density of 110 Gbit/in2 (170 Mbit/mm2) had been reached, up from 100 Mbit/in2 (155 Kbit/mm2) circa 1990.[2] This does not extrapolate back to the initial 2 kilobit/in2 (3.1 bit/mm2) drives introduced in 1956, as growth rates surged during the latter 15-year period.[2][3]\n\nIn 2009, Kryder[4] projected that if hard drives were to continue to progress at their then-current pace of about 40% per year, then in 2020 a two-platter, 2.5-inch disk drive would store approximately 40 terabytes (TB) and cost about $40.\n\nThe validity of the Kryder\'s law projection of 2009 was questioned halfway into the forecast period, and some called the actual rate of areal density progress the "Kryder rate". As of 2014, the observed Kryder rate had fallen well short of the 2009 forecast of 40% per year. A single 2.5-inch platter stored around 0.3 terabytes in 2009 and this reached 0.6 terabytes in 2014. The Kryder rate over the five years ending in 2014 was around 15% per year. To reach 20 terabytes by 2020, starting in 2014, would have required an implausibly high Kryder rate of better than 80% per year.[5]\n\nBy 2019, it was observed that Kryder\'s law "has proven to be outdated as the cost of media storage is decreasing at a slower pace than in the past and is now stabilising."[6]\n\nMark H. Kryder is an elected member of the National Academy of Engineering, a Fellow of the American Physical Society. [7]  and a Fellow of the Institute of Electrical and Electronics Engineers (IEEE).[1] He was Distinguished Lecturer for the IEEE Magnetics Society, and has been awarded the IEEE Magnetics Society Achievement Award and IEEE Reynold B. Johnson Information Storage Systems Award.[8] Kryder received the Pingat Bakti Masyarakat[9] from Singapore in their 2007 National Day Awards.',
        pageTitle: "Mark Kryder",
    },
    {
        title: "Moore's law",
        link: "https://en.wikipedia.org/wiki/Moore%27s_law",
        content:
            "Moore's law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship. It is an experience-curve law, a type of law quantifying efficiency gains from experience in production.\n\nThe observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel and former CEO of the latter, who in 1965 noted that the number of components per integrated circuit had been doubling every year,[a] and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41%. Moore's empirical evidence did not directly imply that the historical trend would continue, nevertheless, his prediction has held since 1975 and has since become known as a law.\n\nMoore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development (R&D). Advancements in digital electronics, such as the reduction in quality-adjusted prices of microprocessors, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.\n\nIndustry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore's law. In September 2022, Nvidia CEO Jensen Huang considered Moore's law dead,[2] while Intel CEO Pat Gelsinger was of the opposite view.[3]\n\nIn 1959, Douglas Engelbart studied the projected downscaling of integrated circuit (IC) size, publishing his results in the article \"Microelectronics, and the Art of Similitude\".[4][5][6] Engelbart presented his findings at the 1960 International Solid-State Circuits Conference, where Moore was present in the audience.[7]\n\nIn 1965, Gordon Moore, who at the time was working as the director of research and development at Fairchild Semiconductor, was asked to contribute to the thirty-fifth-anniversary issue of Electronics magazine with a prediction on the future of the semiconductor components industry over the next ten years.[8] His response was a brief article entitled \"Cramming more components onto integrated circuits\".[1][9][b] Within his editorial, he speculated that by 1975 it would be possible to contain as many as 65000 components on a single quarter-square-inch (~ 1.6 cm2) semiconductor.\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.[1]\n\nMoore posited a log–linear relationship between device complexity (higher circuit density at reduced cost) and time.[12][13] In a 2015 interview, Moore noted of the 1965 article: \"... I just did a wild extrapolation saying it's going to continue to double every year for the next 10 years.\"[14] One historian of the law cites Stigler's law of eponymy, to introduce the fact that the regular doubling of components was known to many working in the field.[13]\n\nIn 1974, Robert H. Dennard at IBM recognized the rapid MOSFET scaling technology and formulated what became known as Dennard scaling, which describes that as MOS transistors get smaller, their power density stays constant such that the power use remains in proportion with area.[15][16] Evidence from the semiconductor industry shows that this inverse relationship between power density and areal density broke down in the mid-2000s.[17]\n\nAt the 1975 IEEE International Electron Devices Meeting, Moore revised his forecast rate,[18][19] predicting semiconductor complexity would continue to double annually until about 1980, after which it would decrease to a rate of doubling approximately every two years.[19][20][21] He outlined several contributing factors for this exponential behavior:[12][13]\n\nShortly after 1975, Caltech professor Carver Mead popularized the term Moore's law.[22][23] Moore's law eventually came to be widely accepted as a goal for the semiconductor industry, and it was cited by competitive semiconductor manufacturers as they strove to increase processing power. Moore viewed his eponymous law as surprising and optimistic: \"Moore's law is a violation of Murphy's law. Everything gets better and better.\"[24] The observation was even seen as a self-fulfilling prophecy.[25][26]\n\nThe doubling period is often misquoted as 18 months because of a separate prediction by Moore's colleague, Intel executive David House.[27] In 1975, House noted that Moore's revised law of doubling transistor count every 2 years in turn implied that computer chip performance would roughly double every 18 months,[28] with no increase in power consumption.[29] Mathematically, Moore's law predicted that transistor count would double every 2 years due to shrinking transistor dimensions and other improvements.[30] As a consequence of shrinking dimensions, Dennard scaling predicted that power consumption per unit area would remain constant. Combining these effects, David House deduced that computer chip performance would roughly double every 18 months. Also due to Dennard scaling, this increased performance would not be accompanied by increased power, i.e., the energy-efficiency of silicon-based computer chips roughly doubles every 18 months. Dennard scaling ended in the 2000s.[17] Koomey later showed that a similar rate of efficiency improvement predated silicon chips and Moore's law, for technologies such as vacuum tubes.\n\nMicroprocessor architects report that since around 2010, semiconductor advancement has slowed industry-wide below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, cited Moore's 1975 revision as a precedent for the current deceleration, which results from technical challenges and is \"a natural part of the history of Moore's law\".[31][32][33] The rate of improvement in physical dimensions known as Dennard scaling also ended in the mid-2000s. As a result, much of the semiconductor industry has shifted its focus to the needs of major computing applications rather than semiconductor scaling.[25][34][17] Nevertheless, as of 2019, leading semiconductor manufacturers TSMC and Samsung Electronics claimed to keep pace with Moore's law[35][36][37][38][39][40] with 10, 7, and 5 nm nodes in mass production.[35][36][41][42][43]\n\nAs the cost of computer power to the consumer falls, the cost for producers to fulfill Moore's law follows an opposite trend: R&D, manufacturing, and test costs have increased steadily with each new generation of chips. The cost of the tools, principally EUVL (Extreme ultraviolet lithography), used to manufacture chips doubles every 4 years.[44] Rising manufacturing costs are an important consideration for the sustaining of Moore's law.[45] This led to the formulation of Moore's second law, also called Rock's law (named after Arthur Rock), which is that the capital cost of a semiconductor fabrication plant also increases exponentially over time.[46][47]\n\nNumerous innovations by scientists and engineers have sustained Moore's law since the beginning of the IC era. Some of the key innovations are listed below, as examples of breakthroughs that have advanced integrated circuit and semiconductor device fabrication technology, allowing transistor counts to grow by more than seven orders of magnitude in less than five decades.\n\nComputer industry technology road maps predicted in 2001 that Moore's law would continue for several generations of semiconductor chips.[71]\n\nOne of the key technical challenges of engineering future nanoscale transistors is the design of gates. As device dimensions shrink, controlling the current flow in the thin channel becomes more difficult. Modern nanoscale transistors typically take the form of multi-gate MOSFETs, with the FinFET being the most common nanoscale transistor. The FinFET has gate dielectric on three sides of the channel. In comparison, the gate-all-around MOSFET (GAAFET) structure has even better gate control.\n\nMicroprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, announced, \"Our cadence today is closer to two and a half years than two.\"[103] Intel stated in 2015 that improvements in MOSFET devices have slowed, starting at the 22 nm feature width around 2012, and continuing at 14 nm.[104] Pat Gelsinger, Intel CEO, stated at the end of 2023 that \"we're no longer in the golden era of Moore's Law, it's much, much harder now, so we're probably doubling effectively closer to every three years now, so we've definitely seen a slowing.\"[105]\n\nThe physical limits to transistor scaling have been reached due to source-to-drain leakage, limited gate metals and limited options for channel material. Other approaches are being investigated, which do not rely on physical scaling. These include the spin state of electron spintronics, tunnel junctions, and advanced confinement of channel materials via nano-wire geometry.[106] Spin-based logic and memory options are being developed actively in labs.[107][108]\n\nThe vast majority of current transistors on ICs are composed principally of doped silicon and its alloys. As silicon is fabricated into single nanometer transistors, short-channel effects adversely changes desired material properties of silicon as a functional transistor. Below are several non-silicon substitutes in the fabrication of small nanometer transistors.\n\nOne proposed material is indium gallium arsenide, or InGaAs. Compared to their silicon and germanium counterparts, InGaAs transistors are more promising for future high-speed, low-power logic applications. Because of intrinsic characteristics of III–V compound semiconductors, quantum well and tunnel effect transistors based on InGaAs have been proposed as alternatives to more traditional MOSFET designs.\n\nBiological computing research shows that biological material has superior information density and energy efficiency compared to silicon-based computing.[116]\n\nVarious forms of graphene are being studied for graphene electronics, e.g. graphene nanoribbon transistors have shown promise since its appearance in publications in 2008. (Bulk graphene has a band gap of zero and thus cannot be used in transistors because of its constant conductivity, an inability to turn off. The zigzag edges of the nanoribbons introduce localized energy states in the conduction and valence bands and thus a bandgap that enables switching when fabricated as a transistor. As an example, a typical GNR of width of 10 nm has a desirable bandgap energy of 0.4 eV.[117][118]) More research will need to be performed, however, on sub-50 nm graphene layers, as its resistivity value increases and thus electron mobility decreases.[117]\n\nIn April 2005, Gordon Moore stated in an interview that the projection cannot be sustained indefinitely: \"It can't continue forever. The nature of exponentials is that you push them out and eventually disaster happens.\" He also noted that transistors eventually would reach the limits of miniaturization at atomic levels:\n\nIn terms of size [of transistors] you can see that we're approaching the size of atoms which is a fundamental barrier, but it'll be two or three generations before we get that far—but that's as far out as we've ever been able to see. We have another 10 to 20 years before we reach a fundamental limit. By then they'll be able to make bigger chips and have transistor budgets in the billions.[119]\n\nIn 2016 the International Technology Roadmap for Semiconductors, after using Moore's Law to drive the industry since 1998, produced its final roadmap. It no longer centered its research and development plan on Moore's law. Instead, it outlined what might be called the More than Moore strategy in which the needs of applications drive chip development, rather than a focus on semiconductor scaling. Application drivers range from smartphones to AI to data centers.[120]\n\nIEEE began a road-mapping initiative in 2016, Rebooting Computing, named the International Roadmap for Devices and Systems (IRDS).[121]\n\nSome forecasters, including Gordon Moore,[122] predict that Moore's law will end by around 2025.[123][120][124] Although Moore's Law will reach a physical limit, some forecasters are optimistic about the continuation of technological progress in a variety of other areas, including new chip architectures, quantum computing, and AI and machine learning.[125][126] Nvidia CEO Jensen Huang declared Moore's law dead in 2022;[2] several days later, Intel CEO Pat Gelsinger countered with the opposite claim.[3]\n\nDigital electronics have contributed to world economic growth in the late twentieth and early twenty-first centuries.[127] The primary driving force of economic growth is the growth of productivity,[128] which Moore's law factors into. Moore (1995) expected that \"the rate of technological progress is going to be controlled from financial realities\".[129] The reverse could and did occur around the late-1990s, however, with economists reporting that \"Productivity growth is the key economic indicator of innovation.\"[130] Moore's law describes a driving force of technological and social change, productivity, and economic growth.[131][132][128]\n\nAn acceleration in the rate of semiconductor progress contributed to a surge in U.S. productivity growth,[133][134][135] which reached 3.4% per year in 1997–2004, outpacing the 1.6% per year during both 1972–1996 and 2005–2013.[136] As economist Richard G. Anderson notes, \"Numerous studies have traced the cause of the productivity acceleration to technological innovations in the production of semiconductors that sharply reduced the prices of such components and of the products that contain them (as well as expanding the capabilities of such products).\"[137]\n\nThe primary negative implication of Moore's law is that obsolescence pushes society up against the Limits to Growth. As technologies continue to rapidly improve, they render predecessor technologies obsolete. In situations in which security and survivability of hardware or data are paramount, or in which resources are limited, rapid obsolescence often poses obstacles to smooth or continued operations.[138]\n\nSeveral measures of digital technology are improving at exponential rates related to Moore's law, including the size, cost, density, and speed of components. Moore wrote only about the density of components, \"a component being a transistor, resistor, diode or capacitor\",[129] at minimum cost.\n\nTransistors per integrated circuit – The most popular formulation is of the doubling of the number of transistors on ICs every two years. At the end of the 1970s, Moore's law became known as the limit for the number of transistors on the most complex chips. The graph at the top of this article shows this trend holds true today. As of 2022[update], the commercially available processor possessing one of the highest numbers of transistors is an AD102 graphics processor with more than 76,3 billion transistors.[139]\n\nDensity at minimum cost per transistor – This is the formulation given in Moore's 1965 paper.[1] It is not just about the density of transistors that can be achieved, but about the density of transistors at which the cost per transistor is the lowest.[140]\n\nAs more transistors are put on a chip, the cost to make each transistor decreases, but the chance that the chip will not work due to a defect increases. In 1965, Moore examined the density of transistors at which cost is minimized, and observed that, as transistors were made smaller through advances in photolithography, this number would increase at \"a rate of roughly a factor of two per year\".[1]\n\nDennard scaling – This posits that power usage would decrease in proportion to area (both voltage and current being proportional to length) of transistors. Combined with Moore's law, performance per watt would grow at roughly the same rate as transistor density, doubling every 1–2 years. According to Dennard scaling transistor dimensions would be scaled by 30% (0.7×) every technology generation, thus reducing their area by 50%. This would reduce the delay by 30% (0.7×) and therefore increase operating frequency by about 40% (1.4×). Finally, to keep electric field constant, voltage would be reduced by 30%, reducing energy by 65% and power (at 1.4× frequency) by 50%.[c] Therefore, in every technology generation transistor density would double, circuit becomes 40% faster, while power consumption (with twice the number of transistors) stays the same.[141] Dennard scaling ended in 2005–2010, due to leakage currents.[17]\n\nThe exponential processor transistor growth predicted by Moore does not always translate into exponentially greater practical CPU performance. Since around 2005–2007, Dennard scaling has ended, so even though Moore's law continued after that, it has not yielded proportional dividends in improved performance.[15][142] The primary reason cited for the breakdown is that at small sizes, current leakage poses greater challenges, and also causes the chip to heat up, which creates a threat of thermal runaway and therefore, further increases energy costs.[15][142][17]\n\nThe breakdown of Dennard scaling prompted a greater focus on multicore processors, but the gains offered by switching to more cores are lower than the gains that would be achieved had Dennard scaling continued.[143][144] In another departure from Dennard scaling, Intel microprocessors adopted a non-planar tri-gate FinFET at 22 nm in 2012 that is faster and consumes less power than a conventional planar transistor.[145] The rate of performance improvement for single-core microprocessors has slowed significantly.[146] Single-core performance was improving by 52% per year in 1986–2003 and 23% per year in 2003–2011, but slowed to just seven percent per year in 2011–2018.[146]\n\nQuality adjusted price of IT equipment – The price of information technology (IT), computers and peripheral equipment, adjusted for quality and inflation, declined 16% per year on average over the five decades from 1959 to 2009.[147][148] The pace accelerated, however, to 23% per year in 1995–1999 triggered by faster IT innovation,[130] and later, slowed to 2% per year in 2010–2013.[147][149]\n\nWhile quality-adjusted microprocessor price improvement continues,[150] the rate of improvement likewise varies, and is not linear on a log scale. Microprocessor price improvement accelerated during the late 1990s, reaching 60% per year (halving every nine months) versus the typical 30% improvement rate (halving every two years) during the years earlier and later.[151][152] Laptop microprocessors in particular improved 25–35% per year in 2004–2010, and slowed to 15–25% per year in 2010–2013.[153]\n\nThe number of transistors per chip cannot explain quality-adjusted microprocessor prices fully.[151][154][155] Moore's 1995 paper does not limit Moore's law to strict linearity or to transistor count, \"The definition of 'Moore's Law' has come to refer to almost anything related to the semiconductor industry that on a semi-log plot approximates a straight line. I hesitate to review its origins and by doing so restrict its definition.\"[129]\n\nHard disk drive areal density – A similar prediction (sometimes called Kryder's law) was made in 2005 for hard disk drive areal density.[156] The prediction was later viewed as over-optimistic. Several decades of rapid progress in areal density slowed around 2010, from 30 to 100% per year to 10–15% per year, because of noise related to smaller grain size of the disk media, thermal stability, and writability using available magnetic fields.[157][158]\n\nFiber-optic capacity – The number of bits per second that can be sent down an optical fiber increases exponentially, faster than Moore's law. Keck's law, in honor of Donald Keck.[159]\n\nNetwork capacity – According to Gerald Butters,[160][161] the former head of Lucent's Optical Networking Group at Bell Labs, there is another version, called Butters' Law of Photonics,[162] a formulation that deliberately parallels Moore's law. Butters' law says that the amount of data coming out of an optical fiber is doubling every nine months.[163] Thus, the cost of transmitting a bit over an optical network decreases by half every nine months. The availability of wavelength-division multiplexing (sometimes called WDM) increased the capacity that could be placed on a single fiber by as much as a factor of 100. Optical networking and dense wavelength-division multiplexing (DWDM) is rapidly bringing down the cost of networking, and further progress seems assured. As a result, the wholesale price of data traffic collapsed in the dot-com bubble. Nielsen's Law says that the bandwidth available to users increases by 50% annually.[164]\n\nPixels per dollar – Similarly, Barry Hendy of Kodak Australia has plotted pixels per dollar as a basic measure of value for a digital camera, demonstrating the historical linearity (on a log scale) of this market and the opportunity to predict the future trend of digital camera price, LCD and LED screens, and resolution.[165][166][167][168]\n\nThe great Moore's law compensator (TGMLC), also known as Wirth's law – generally is referred to as software bloat and is the principle that successive generations of computer software increase in size and complexity, thereby offsetting the performance gains predicted by Moore's law. In a 2008 article in InfoWorld, Randall C. Kennedy,[169] formerly of Intel, introduces this term using successive versions of Microsoft Office between the year 2000 and 2007 as his premise. Despite the gains in computational performance during this time period according to Moore's law, Office 2007 performed the same task at half the speed on a prototypical year 2007 computer as compared to Office 2000 on a year 2000 computer.\n\nLibrary expansion – was calculated in 1945 by Fremont Rider to double in capacity every 16 years, if sufficient space were made available.[170] He advocated replacing bulky, decaying printed works with miniaturized microform analog photographs, which could be duplicated on-demand for library patrons or other institutions. He did not foresee the digital technology that would follow decades later to replace analog microform with digital imaging, storage, and transmission media. Automated, potentially lossless digital technologies allowed vast increases in the rapidity of information growth in an era that now sometimes is called the Information Age.\n\nCarlson curve – is a term coined by The Economist[171] to describe the biotechnological equivalent of Moore's law, and is named after author Rob Carlson.[172] Carlson accurately predicted that the doubling time of DNA sequencing technologies (measured by cost and performance) would be at least as fast as Moore's law.[173] Carlson Curves illustrate the rapid (in some cases hyperexponential) decreases in cost, and increases in performance, of a variety of technologies, including DNA sequencing, DNA synthesis, and a range of physical and computational tools used in protein expression and in determining protein structures.\n\nEroom's law – is a pharmaceutical drug development observation that was deliberately written as Moore's Law spelled backward in order to contrast it with the exponential advancements of other forms of technology (such as transistors) over time. It states that the cost of developing a new drug roughly doubles every nine years.\n\nExperience curve effects says that each doubling of the cumulative production of virtually any product or service is accompanied by an approximate constant percentage reduction in the unit cost. The acknowledged first documented qualitative description of this dates from 1885.[174][175] A power curve was used to describe this phenomenon in a 1936 discussion of the cost of airplanes.[176]\n\nEdholm's law – Phil Edholm observed that the bandwidth of telecommunication networks (including the Internet) is doubling every 18 months.[177] The bandwidths of online communication networks has risen from bits per second to terabits per second. The rapid rise in online bandwidth is largely due to the same MOSFET scaling that enabled Moore's law, as telecommunications networks are built from MOSFETs.[178]\n\nHaitz's law predicts that the brightness of LEDs increases as their manufacturing cost goes down.\n\nSwanson's law is the observation that the price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. At present rates, costs go down 75% about every 10 years.",
        pageTitle: "Moore's law",
    },
    {
        title: "Lambert's cosine law",
        link: "https://en.wikipedia.org/wiki/Lambert%27s_cosine_law",
        content:
            'In optics, Lambert\'s cosine law says that the observed radiant intensity or luminous intensity from an ideal diffusely reflecting surface or ideal diffuse radiator is directly proportional to the cosine of the angle θ between the observer\'s line of sight and the surface normal; I = I0 cos θ.[1][2] The law is also known as the cosine emission law[3]  or Lambert\'s emission law. It is named after Johann Heinrich Lambert, from his Photometria, published in 1760.[4]\n\nA surface which obeys Lambert\'s law is said to be Lambertian, and exhibits Lambertian reflectance. Such a surface has a constant radiance/luminance, regardless of the angle from which it is observed; a single human eye perceives such a surface as having a constant brightness, regardless of the angle from which the eye observes the surface. It has the same radiance because, although the emitted power from a given area element is reduced by the cosine of the emission angle, the solid angle, subtended by surface visible to the viewer, is reduced by the very same amount. Because the ratio between power and solid angle is constant, radiance (power per unit solid angle per unit projected source area) stays the same.\n\nWhen an area element is radiating as a result of being illuminated by an external source, the irradiance (energy or photons /time/area) landing on that area element will be proportional to the cosine of the angle between the illuminating source and the normal. A Lambertian scatterer will then scatter this light according to the same cosine law as a Lambertian emitter. This means that although the radiance of the surface depends on the angle from the normal to the illuminating source, it will not depend on the angle from the normal to the observer. For example, if the moon were a Lambertian scatterer, one would expect to see its scattered brightness appreciably diminish towards the terminator due to the increased angle at which sunlight hit the surface. The fact that it does not diminish illustrates that the moon is not a Lambertian scatterer, and in fact tends to scatter more light into the oblique angles than a Lambertian scatterer.\n\nThe emission of a Lambertian radiator does not depend on the amount of incident radiation, but rather from radiation originating in the emitting body itself. For example, if the sun were a Lambertian radiator, one would expect to see a constant brightness across the entire solar disc. The fact that the sun exhibits limb darkening in the visible region illustrates that it is not a Lambertian radiator. A black body is an example of a Lambertian radiator.\n\nThe situation for a Lambertian surface (emitting or scattering) is illustrated in Figures 1 and 2.  For conceptual clarity we will think in terms of photons rather than energy or luminous energy. The wedges in the circle each represent an equal angle dΩ, of an arbitrarily chosen size, and for a Lambertian surface, the number of photons per second emitted into each wedge is proportional to the area of the wedge.\n\nThe length of each wedge is the product of the diameter of the circle and cos(θ). The maximum rate of photon emission per unit solid angle is along the normal, and diminishes to zero for θ = 90°. In mathematical terms, the radiance along the normal is I photons/(s·m2·sr) and the number of photons per second emitted into the vertical wedge is I dΩ dA. The number of photons per second emitted into the wedge at angle θ is I cos(θ) dΩ dA.\n\nFigure 2 represents what an observer sees. The observer directly above the area element will be seeing the scene through an aperture of area dA0 and the area element dA will subtend a (solid) angle of dΩ0, which is a portion of the observer\'s total angular field-of-view of the scene. Since the wedge size dΩ was chosen arbitrarily, for convenience we may assume without loss of generality that it coincides with the solid angle subtended by the aperture when "viewed" from the locus of the emitting area element dA. Thus the normal observer will then be recording the same I dΩ dA photons per second emission derived above and will measure a radiance of\n\nThe observer at angle θ to the normal will be seeing the scene through the same aperture of area dA0 (still corresponding to a dΩ wedge) and from this oblique vantage the area element dA is foreshortened and will subtend a (solid) angle of dΩ0 cos(θ).  This observer will be recording I cos(θ) dΩ dA photons per second, and so will be measuring a radiance of\n\nIn general, the luminous intensity of a point on a surface varies by direction; for a Lambertian surface, that distribution is defined by the cosine law, with peak luminous intensity in the normal direction. Thus when the Lambertian assumption holds, we can calculate the total luminous flux, \n  \n    \n      \n        \n          F\n          \n            tot\n          \n        \n      \n    \n    {\\displaystyle F_{\\text{tot}}}\n  \n, from the peak luminous intensity, \n  \n    \n      \n        \n          I\n          \n            max\n          \n        \n      \n    \n    {\\displaystyle I_{\\max }}\n  \n, by integrating the cosine law:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    tot\n                  \n                \n              \n              \n                \n                =\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    2\n                    π\n                  \n                \n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    π\n                    \n                      /\n                    \n                    2\n                  \n                \n                cos\n                ⁡\n                (\n                θ\n                )\n                \n                \n                  I\n                  \n                    max\n                  \n                \n                \n                sin\n                ⁡\n                (\n                θ\n                )\n                \n                d\n                θ\n                \n                d\n                ϕ\n              \n            \n            \n              \n              \n                \n                =\n                2\n                π\n                ⋅\n                \n                  I\n                  \n                    max\n                  \n                \n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    π\n                    \n                      /\n                    \n                    2\n                  \n                \n                cos\n                ⁡\n                (\n                θ\n                )\n                sin\n                ⁡\n                (\n                θ\n                )\n                \n                d\n                θ\n              \n            \n            \n              \n              \n                \n                =\n                2\n                π\n                ⋅\n                \n                  I\n                  \n                    max\n                  \n                \n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    π\n                    \n                      /\n                    \n                    2\n                  \n                \n                \n                  \n                    \n                      sin\n                      ⁡\n                      (\n                      2\n                      θ\n                      )\n                    \n                    2\n                  \n                \n                \n                d\n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{\\text{tot}}&=\\int _{0}^{2\\pi }\\int _{0}^{\\pi /2}\\cos(\\theta )\\,I_{\\max }\\,\\sin(\\theta )\\,d\\theta \\,d\\phi \\\\&=2\\pi \\cdot I_{\\max }\\int _{0}^{\\pi /2}\\cos(\\theta )\\sin(\\theta )\\,d\\theta \\\\&=2\\pi \\cdot I_{\\max }\\int _{0}^{\\pi /2}{\\frac {\\sin(2\\theta )}{2}}\\,d\\theta \\end{aligned}}}\n  \n\nand so\n\nwhere \n  \n    \n      \n        sin\n        ⁡\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle \\sin(\\theta )}\n  \n is the determinant of the Jacobian matrix for the unit sphere, and realizing that \n  \n    \n      \n        \n          I\n          \n            max\n          \n        \n      \n    \n    {\\displaystyle I_{\\max }}\n  \n is luminous flux per steradian.[5] Similarly, the peak intensity will be \n  \n    \n      \n        1\n        \n          /\n        \n        (\n        π\n        \n        \n          s\n          r\n        \n        )\n      \n    \n    {\\displaystyle 1/(\\pi \\,\\mathrm {sr} )}\n  \n of the total radiated luminous flux. For Lambertian surfaces, the same factor of \n  \n    \n      \n        π\n        \n        \n          s\n          r\n        \n      \n    \n    {\\displaystyle \\pi \\,\\mathrm {sr} }\n  \n relates luminance to luminous emittance, radiant intensity to radiant flux, and radiance to radiant emittance.[citation needed] Radians and steradians are, of course, dimensionless and so "rad" and "sr" are included only for clarity.\n\nExample: A surface with a luminance of say 100 cd/m2 (= 100 nits, typical PC monitor) will, if it is a perfect Lambert emitter, have a luminous emittance of 100π lm/m2. If its area is 0.1 m2 (~19" monitor) then the total light emitted, or luminous flux, would thus be 31.4 lm.',
        pageTitle: "Lambert's cosine law",
    },
    {
        title: "Lanchester's laws",
        link: "https://en.wikipedia.org/wiki/Lanchester%27s_laws",
        content:
            "Lanchester's laws are mathematical formulas for calculating the relative strengths of military forces. The Lanchester equations are differential equations describing the time dependence of two armies' strengths A and B as a function of time, with the function depending only on A and B.[1][2]\n\nIn 1915 and 1916 during World War I, M. Osipov[3]: vii–viii  and Frederick Lanchester independently devised a series of differential equations to demonstrate the power relationships between opposing forces.[4] Among these are what is known as Lanchester's linear law (for ancient combat) and Lanchester's square law (for modern combat with long-range weapons such as firearms).\n\nAs of 2017 modified variations of the Lanchester equations continue to form the basis of analysis in many of the US Army’s combat simulations,[5] and in 2016 a RAND Corporation report examined by these laws the probable outcome in the event of a Russian invasion into the Baltic nations of Estonia, Latvia, and Lithuania.[6]\n\nFor ancient combat, between phalanxes of soldiers with spears for example, one soldier could only ever fight exactly one other soldier at a time. If each soldier kills, and is killed by, exactly one other, then the number of soldiers remaining at the end of the battle is simply the difference between the larger army and the smaller, assuming identical weapons.\n\nThe linear law also applies to unaimed fire into an enemy-occupied area. The rate of attrition depends on the density of the available targets in the target area as well as the number of weapons shooting. If two forces, occupying the same land area and using the same weapons, shoot randomly into the same target area, they will both suffer the same rate and number of casualties, until the smaller force is eventually eliminated: the greater probability of any one shot hitting the larger force is balanced by the greater number of shots directed at the smaller force.\n\nLanchester's square law is also known as the N-square law.\n\nWith firearms engaging each other directly with aimed shooting from a distance, they can attack multiple targets and can receive fire from multiple directions. The rate of attrition now depends only on the number of weapons shooting. Lanchester determined that the power of such a force is proportional not to the number of units it has, but to the square of the number of units. This is known as Lanchester's square law.\n\nMore precisely, the law specifies the casualties a shooting force will inflict over a period of time, relative to those inflicted by the opposing force. In its basic form, the law is only useful to predict outcomes and casualties by attrition. It does not apply to whole armies, where tactical deployment means not all troops will be engaged all the time. It only works where each unit (soldier, ship, etc.) can kill only one equivalent unit at a time.  For this reason, the law does not apply to machine guns, artillery with unguided munitions, or nuclear weapons. The law requires an assumption that casualties accumulate over time: it does not work in situations in which opposing troops kill each other instantly, either by shooting simultaneously or by one side getting off the first shot and inflicting multiple casualties.\n\nNote that Lanchester's square law does not apply to technological force, only numerical force; so it requires an N-squared-fold increase in quality to compensate for an N-fold decrease in quantity.\n\nSuppose that two armies, Red and Blue, are engaging each other in combat. Red is shooting a continuous stream of bullets at Blue. Meanwhile, Blue is shooting a continuous stream of bullets at Red.\n\nLet symbol A represent the number of soldiers in the Red force. Each one has offensive firepower α, which is the number of enemy soldiers it can incapacitate (e.g., kill or injure) per unit time. Likewise, Blue has B soldiers, each with offensive firepower β.\n\nLanchester's square law calculates the number of soldiers lost on each side using the following pair of equations.[7]  Here, dA/dt represents the rate at which the number of Red soldiers is changing at a particular instant. A negative value indicates the loss of soldiers. Similarly, dB/dt represents the rate of change of the number of Blue soldiers.\n\nThe first three of these conclusions are obvious. The final one is the origin of the name \"square law\".\n\nLanchester's equations are related to the more recent salvo combat model equations, with two main differences.\n\nFirst, Lanchester's original equations form a continuous time model, whereas the basic salvo equations form a discrete time model.  In a gun battle, bullets or shells are typically fired in large quantities. Each round has a relatively low chance of hitting its target, and does a relatively small amount of damage. Therefore, Lanchester's equations model gunfire as a stream of firepower that continuously weakens the enemy force over time.\n\nBy comparison, cruise missiles typically are fired in relatively small quantities. Each one has a high probability of hitting its target, and carries a relatively powerful warhead. Therefore, it makes more sense to model them as a discrete pulse (or salvo) of firepower in a discrete time model.\n\nSecond, Lanchester's equations include only offensive firepower, whereas the salvo equations also include defensive firepower. Given their small size and large number, it is not practical to intercept bullets and shells in a gun battle. By comparison, cruise missiles can be intercepted (shot down) by surface-to-air missiles and anti-aircraft guns.  Therefore, missile combat models include those active defenses.\n\nLanchester's laws have been used to model historical battles for research purposes.  Examples include Pickett's Charge of Confederate infantry against Union infantry during the 1863 Battle of Gettysburg,[8] the 1940 Battle of Britain between the British and German air forces,[9] and the Battle of Kursk.[10]\n\nIn modern warfare, to take into account that to some extent both linear and the square apply often, an exponent of 1.5 is used.[11][12][3]: 7-5–7-8  Lanchester's laws have also been used to model guerrilla warfare.[13] The laws have also been applied to repeat battles with a range of inter-battle reinforcement strategies.[14]\n\nAttempts have been made to apply Lanchester's laws to conflicts between animal groups.[15] Examples include tests with chimpanzees[16] and ants. The chimpanzee application was relatively successful. A study of Australian meat ants and Argentine ants confirmed the square law,[17] but a study of fire ants did not confirm the square law.[18]\n\nThe Helmbold Parameters offer precise numerical indices, grounded in historical data, for quickly and accurately comparing battles in terms of bitterness and the degree of advantage held by each side. While their definition is modeled after a solution of the Lanchester Square Law's differential equations, their numerical values are based entirely on the initial and final strengths of the opponents and in no way depend upon the validity of Lanchester's Square Law as a model of attrition during the course of a battle.\n\nThe solution of Lanchester's Square Law used here can be written as:\n\na\n                (\n                t\n                )\n              \n              \n                \n                =\n                cosh\n                ⁡\n                (\n                λ\n                t\n                )\n                −\n                μ\n                sinh\n                ⁡\n                (\n                λ\n                t\n                )\n              \n            \n            \n              \n                d\n                (\n                t\n                )\n              \n              \n                \n                =\n                cosh\n                ⁡\n                (\n                λ\n                t\n                )\n                −\n                \n                  μ\n                  \n                    −\n                    1\n                  \n                \n                sinh\n                ⁡\n                (\n                λ\n                t\n                )\n              \n            \n            \n              \n                ε\n              \n              \n                \n                =\n                λ\n                T\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}a(t)&=\\cosh(\\lambda t)-\\mu \\sinh(\\lambda t)\\\\d(t)&=\\cosh(\\lambda t)-\\mu ^{-1}\\sinh(\\lambda t)\\\\\\varepsilon &=\\lambda T\\end{aligned}}}\n\nIf the initial and final strengths of the two sides are known it is possible to solve for the parameters \n  \n    \n      \n        a\n        (\n        T\n        )\n      \n    \n    {\\displaystyle a(T)}\n  \n, \n  \n    \n      \n        d\n        (\n        T\n        )\n      \n    \n    {\\displaystyle d(T)}\n  \n, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n, and \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n. If the battle duration \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is also known, then it is possible to solve for \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n.[19][20][21]\n\nIf, as is normally the case, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is small enough that the hyperbolic functions can, without any significant error, be replaced by their series expansion up to terms in the first power of \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, and if abbreviations adopted for the casualty fractions are \n  \n    \n      \n        \n          F\n          \n            A\n          \n        \n        =\n        1\n        −\n        a\n        (\n        T\n        )\n      \n    \n    {\\displaystyle F_{A}=1-a(T)}\n  \n and \n  \n    \n      \n        \n          F\n          \n            D\n          \n        \n        =\n        1\n        −\n        d\n        (\n        T\n        )\n      \n    \n    {\\displaystyle F_{D}=1-d(T)}\n  \n, then the approximate relations that hold include \n  \n    \n      \n        ε\n        =\n        \n          \n            \n              F\n              \n                A\n              \n            \n            \n              F\n              \n                D\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\varepsilon ={\\sqrt {F_{A}F_{D}}}}\n  \n and \n  \n    \n      \n        μ\n        =\n        \n          F\n          \n            A\n          \n        \n        \n          /\n        \n        \n          F\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle \\mu =F_{A}/F_{D}}\n  \n.[22] That \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is a kind of \"average\" (specifically, the geometric mean) of the casualty fractions justifies using it as an index of the bitterness of the battle.\n\nStatistical work prefers natural logarithms of the Helmbold Parameters. They are noted \n  \n    \n      \n        log\n        ⁡\n        μ\n      \n    \n    {\\displaystyle \\log \\mu }\n  \n, \n  \n    \n      \n        log\n        ⁡\n        ε\n      \n    \n    {\\displaystyle \\log \\varepsilon }\n  \n, and \n  \n    \n      \n        log\n        ⁡\n        λ\n      \n    \n    {\\displaystyle \\log \\lambda }\n  \n.\n\nSome observers have noticed a similar post-WWII decline in casualties at the level of wars instead of battles.[28][29][30][31]",
        pageTitle: "Lanchester's laws",
    },
    {
        title: "Leavitt's law",
        link: "https://en.wikipedia.org/wiki/Leavitt%27s_law",
        content:
            'In astronomy, a period-luminosity relation is a relationship linking the luminosity of pulsating variable stars with their pulsation period.\nThe best-known relation is the direct proportionality law holding for Classical Cepheid variables, sometimes called the Leavitt Law.[2][3][4]  Discovered in 1908 by Henrietta Swan Leavitt, the relation established Cepheids as foundational indicators of cosmic benchmarks for scaling galactic and extragalactic distances.[5][6][7][8][9][10]\nThe physical model explaining the Leavitt\'s law for classical cepheids is called kappa mechanism.\n\nLeavitt, a graduate of Radcliffe College, worked at the Harvard College Observatory as a "computer", tasked with examining photographic plates in order to measure and catalog the brightness of stars. Observatory Director Edward Charles Pickering assigned Leavitt to the study of variable stars of the Small and Large Magellanic Clouds, as recorded on photographic plates taken with the Bruce Astrograph of the Boyden Station of the Harvard Observatory in Arequipa, Peru. She identified 1777 variable stars, of which she classified 47 as Cepheids.  In 1908 she published her results in the Annals of the Astronomical Observatory of Harvard College, noting that the brighter variables had the longer period.[13] Building on this work, Leavitt looked carefully at the relation between the periods and the brightness of a sample of 25 of the Cepheids variables in the Small Magellanic Cloud, published in 1912.[11]  This paper was communicated and signed by Edward Pickering, but the first sentence indicates that it was "prepared by Miss Leavitt".\n\nIn the 1912 paper, Leavitt graphed the stellar magnitude versus the logarithm of the period and determined that, in her own words,\n\nA straight line can be readily drawn among each of the two series of points corresponding to maxima and minima, thus showing that there is a simple relation between the brightness of the Cepheid variables and their periods.[11]\n\nUsing the simplifying assumption that all of the Cepheids within the Small Magellanic Cloud were at approximately the same distance, the apparent magnitude of each star is equivalent to its absolute magnitude offset by a fixed quantity depending on that distance. This reasoning allowed Leavitt to establish that the logarithm of the period is linearly related to the logarithm of the star\'s average intrinsic optical luminosity (which is the amount of power radiated by the star in the visible spectrum).[14]\n\nAt the time, there was an unknown scale factor in this brightness since the distances to the Magellanic Clouds were unknown. Leavitt expressed the hope that parallaxes to some Cepheids would be measured; one year after she reported her results, Ejnar Hertzsprung determined the distances of several Cepheids in the Milky Way and that, with this calibration, the distance to any Cepheid could then be determined.[14]\n\nThe relation was used by Harlow Shapley in 1918 to investigate the distances of globular clusters and the absolute magnitudes of the cluster variables found in them.  It was hardly noted at the time that there was a discrepancy in the relations found for several types of pulsating variable all known generally as Cepheids.  This discrepancy was confirmed by Edwin Hubble\'s 1931 study of the globular clusters around the Andromeda Galaxy.  The solution was not found until the 1950s, when it was shown that population II Cepheids were systematically fainter than population I Cepheids.  The cluster variables (RR Lyrae variables) were fainter still.[15]\n\nPeriod-luminosity relations are known for several types of pulsating variable stars: type I Cepheids; type II Cepheids; RR Lyrae variables; Mira variables; and other long-period variable stars.[16]\n\nThe Classical Cepheid period-luminosity relation has been calibrated by many astronomers throughout the twentieth century, beginning with Hertzsprung.[17]  Calibrating the period-luminosity relation has been problematic; however, a firm Galactic calibration was established by Benedict et al. 2007 using precise HST parallaxes for 10 nearby classical Cepheids.[18] Also, in 2008, ESO astronomers estimated with a precision within 1% the distance to the Cepheid RS Puppis, using light echos from a nebula in which it is embedded.[19]  However, that latter finding has been actively debated in the literature.[20]\n\nThe following relationship between a Population I Cepheid\'s period P and its mean absolute magnitude Mv was established from Hubble Space Telescope trigonometric parallaxes for 10 nearby Cepheids:\n\nwith P measured in days.\n[21][18] The following relations can also be used to calculate the distance to classical Cepheids.\n\nClassical Cepheids (also known as Population I Cepheids, type I Cepheids, or Delta Cepheid variables) undergo pulsations with very regular periods on the order of days to months. Cepheid variables were discovered in 1784 by Edward Pigott, first with the variability of Eta Aquilae,[22] and a few months later by John Goodricke with the variability of Delta Cephei, the eponymous star for classical Cepheids.[23] Most of the Cepheids were identified by the distinctive light curve shape with a rapid increase in brightness and a sharp turnover.\n\nClassical Cepheids are 4–20 times more massive than the Sun[24] and up to 100,000 times more luminous.[25] These Cepheids are yellow bright giants and supergiants of spectral class F6 – K2 and their radii change by of the order of 10% during a pulsation cycle.[26]\n\nLeavitt\'s work on Cepheids in the Magellanic Clouds led her to discover the relation between the luminosity and the period of Cepheid variables.  \nHer discovery provided astronomers with the first "standard candle" with which to measure the distance to faraway galaxies. Cepheids were soon detected in other galaxies, such as Andromeda (notably by Edwin Hubble in 1923–24), and they became an important part of the evidence that "spiral nebulae" are independent galaxies located far outside of the Milky Way.  Leavitt\'s discovery provided the basis for a fundamental shift in cosmology, as it prompted Harlow Shapley to move the Sun from the center of the galaxy in the "Great Debate" and Hubble to move the Milky Way galaxy from the center of the universe.  With the period-luminosity relation providing a way to accurately measure distances on an inter-galactic scale, a new era in modern astronomy unfolded with an understanding of the structure and scale of the universe.[27]  The discovery of the expanding universe by Georges Lemaître and Hubble were made possible by Leavitt\'s groundbreaking research. Hubble often said that Leavitt deserved the Nobel Prize for her work,[28] and indeed she was nominated by a member of the Swedish Academy of Sciences in 1924, although as she had died of cancer three years earlier she was not eligible.[29][30] (The Nobel Prize is not awarded posthumously.)',
        pageTitle: "Period-luminosity relation",
    },
    {
        title: "Lehman's laws of software evolution",
        link: "https://en.wikipedia.org/wiki/Lehman%27s_laws_of_software_evolution",
        content:
            "In software engineering, the laws of software evolution refer to a series of laws that Lehman and Belady formulated starting in 1974 with respect to software evolution.[1][2]\nThe laws describe a balance between forces driving new developments on one hand, and forces that slow down progress on the other hand.  Over the past decades the laws have been revised and extended several times.[3]\n\nObserving that most software is subject to change in the course of its existence, the authors set out to determine laws that these changes will typically obey, or must obey for the software to survive.[1]\n\nIn his 1980 article,[1] Lehman qualified the application of such laws by distinguishing between three categories of software:\n\nThe laws are said to apply only to the last category of systems.",
        pageTitle: "Lehman's laws of software evolution",
    },
    {
        title: "Leibniz's law",
        link: "https://en.wikipedia.org/wiki/Identity_of_indiscernibles",
        content:
            'The identity of indiscernibles is an ontological principle that states that there cannot be separate objects or entities that have all their properties in common. That is, entities x and y are identical if every predicate possessed by x is also possessed by y and vice versa. It states that no two distinct things (such as snowflakes) can be exactly alike, but this is intended as a metaphysical principle rather than one of natural science. A related principle is the indiscernibility of identicals, discussed below.\n\nA form of the principle is attributed to the German philosopher Gottfried Wilhelm Leibniz. While some think that Leibniz\'s version of the principle is meant to be only the indiscernibility of identicals, others have interpreted it as the conjunction of the identity of indiscernibles and the indiscernibility of identicals (the converse principle). Because of its association with Leibniz, the indiscernibility of identicals is sometimes known as Leibniz\'s law. It is considered to be one of his great metaphysical principles, the other being the principle of noncontradiction and the principle of sufficient reason (famously used in his disputes with Newton and Clarke in the Leibniz–Clarke correspondence).\n\nSome philosophers have decided, however, that it is important to exclude certain predicates (or purported predicates) from the principle in order to avoid either triviality or contradiction. An example (detailed below) is the predicate that denotes whether an object is equal to x (often considered a valid predicate). As a consequence, there are a few different versions of the principle in the philosophical literature, of varying logical strength—and some of them are termed "the strong principle" or "the weak principle" by particular authors, in order to distinguish between them.[1]\n\nThe identity of indiscernibles has been used to motivate notions of noncontextuality within quantum mechanics.\n\nAssociated with this principle is also the question as to whether it is a logical principle, or merely an empirical principle.\n\nBoth identity and indiscernibility are expressed by the word "same".[2][3] Identity is about numerical sameness, and is expressed by the equality sign ("="). It is the relation each object bears only to itself.[4] Indiscernibility, on the other hand, concerns qualitative sameness: two objects are indiscernible if they have all their properties in common.[1] Formally, this can be expressed as "\n  \n    \n      \n        ∀\n        F\n        (\n        F\n        x\n        ↔\n        F\n        y\n        )\n      \n    \n    {\\displaystyle \\forall F(Fx\\leftrightarrow Fy)}\n  \n". The two senses of sameness are linked by two principles: the principle of indiscernibility of identicals and the principle of identity of indiscernibles. The principle of indiscernibility of identicals is uncontroversial and states that if two entities are identical with each other then they have the same properties.[3] The principle of identity of indiscernibles, on the other hand, is more controversial in making the converse claim that if two entities have the same properties then they must be identical.[3] This entails that "no two distinct things exactly resemble each other".[1] Note that these are all second-order expressions. Neither of these principles can be expressed in first-order logic (are nonfirstorderizable). Formally, the two principles can be expressed in the following way:\n\nThe indiscernibility of identicals is usually taken to be uncontroversially true, whereas the identity of indiscernibles is more controversial,[5] having been famously disputed by Max Black.[6]\n\nThe conjunction of these two principles is sometimes called "Leibniz\'s Law",[7][1] although this name has sometimes been used for either of the two other principles,[5] or for other principles.[8] It may be stated as a biconditional:\n\nSome logicians have regarded this principle as essential to identity and equality: Alfred Tarski listed it among the logical axioms governing the notion of identity,[9] and Rudolf Carnap defined the equals sign for identity (=) in terms of this biconditional.[10]\n\nIn a universe of two distinct objects A and B, all predicates F are materially equivalent to one of the following properties:\n\nIf ∀F applies to all such predicates, then the second principle as formulated above reduces trivially and uncontroversially to a logical tautology. In that case, the objects are distinguished by  IsA, IsB, and all predicates that are materially equivalent to either of these. This argument can combinatorially be extended to universes containing any number of distinct objects.\n\nThe equality relation expressed by the sign "=" is an equivalence relation in being reflexive (everything is equal to itself), symmetric (if x is equal to y then y is equal to x) and transitive (if x is equal to y and y is equal to z then x is equal to z). The indiscernibility of identicals and identity of indiscernables can jointly be used to define the equality relation. The symmetry and transitivity of equality follow from the first principle, whereas reflexivity follows from the second. Both principles can be combined into a single axiom by using a biconditional operator (\n  \n    \n      \n        ↔\n      \n    \n    {\\displaystyle \\leftrightarrow }\n  \n) in place of material implication (\n  \n    \n      \n        →\n      \n    \n    {\\displaystyle \\rightarrow }\n  \n).[11][citation needed]\n\nIndiscernibility is usually defined in terms of shared properties: two objects are indiscernible if they have all their properties in common.[12] The plausibility and strength of the principle of identity of indiscernibles depend on the conception of properties used to define indiscernibility.[12][13]\n\nOne important distinction in this regard is between pure and impure properties. Impure properties are properties that, unlike pure properties, involve reference to a particular substance in their definition.[12] So, for example, being a wife is a pure property while being the wife of Socrates is an impure property due to the reference to the particular "Socrates".[14] Sometimes, the terms qualitative and non-qualitative are used instead of pure and impure.[15] Discernibility is usually defined in terms of pure properties only. The reason for this is that taking impure properties into consideration would result in the principle being trivially true since any entity has the impure property of being identical to itself, which it does not share with any other entity.[12][13]\n\nAnother important distinction concerns the difference between intrinsic and extrinsic properties.[13] A property is extrinsic to an object if having this property depends on other objects (with or without reference to particular objects), otherwise it is intrinsic. For example, the property of being an aunt is extrinsic while the property of having a mass of 60 kg is intrinsic.[16][17] If the identity of indiscernibles is defined only in terms of intrinsic pure properties, one cannot regard two books lying on a table as distinct when they are intrinsically identical. But if extrinsic and impure properties are also taken into consideration, the same books become distinct so long as they are discernible through the latter properties.[12][13]\n\nMax Black has argued against the identity of indiscernibles by counterexample. Notice that to show that the identity of indiscernibles is false, it is sufficient that one provide a model in which there are two distinct (numerically nonidentical) things that have all the same properties. He claimed that in a symmetric universe wherein only two symmetrical spheres exist, the two spheres are two distinct objects even though they have all their properties in common.[18]\n\nBlack argues that even relational properties (properties specifying distances between objects in space-time) fail to distinguish two identical objects in a symmetrical universe. Per his argument, two objects are, and will remain, equidistant from the universe\'s plane of symmetry and each other. Even bringing in an external observer to label the two spheres distinctly does not solve the problem, because it violates the symmetry of the universe.\n\nAs stated above, the principle of indiscernibility of identicals—that if two objects are in fact one and the same, they have all the same properties—is mostly uncontroversial. However, one famous application of the indiscernibility of identicals was by René Descartes in his Meditations on First Philosophy. Descartes concluded that he could not doubt the existence of himself (the famous cogito argument), but that he could doubt the existence of his body.\n\nThis argument is criticized by some modern philosophers on the grounds that it allegedly derives a conclusion about what is true from a premise about what people know.  What people know or believe about an entity, they argue, is not really a characteristic of that entity. A response may be that the argument in the Meditations on First Philosophy is that the inability of Descartes to doubt the existence of his mind is part of his mind\'s essence. One may then argue that identical things should have identical essences.[19]\n\nNumerous counterexamples are given to debunk Descartes\' reasoning via reductio ad absurdum, such as the following argument based on a secret identity:',
        pageTitle: "Identity of indiscernibles",
    },
    {
        title: "Lenz's law",
        link: "https://en.wikipedia.org/wiki/Lenz%27s_law",
        content:
            "Lenz's law states that the direction of the electric current induced in a conductor by a changing magnetic field is such that the magnetic field created by the induced current opposes changes in the initial magnetic field. It is named after physicist Heinrich Lenz, who formulated it in 1834.[1]\n\nThe Induced current is the current generated in a wire due to change in magnetic flux. An example of the induced current is the current produced in the generator which involves rapidly rotating a coil of wire in a magnetic field.\n\nIt is a qualitative law that specifies the direction of induced current, but states nothing about its magnitude. Lenz's law predicts the direction of many effects in electromagnetism, such as the direction of voltage induced in an inductor or wire loop by a changing current, or the drag force of eddy currents exerted on moving objects in the magnetic field.\n\nLenz's law may be seen as analogous to Newton's third law in classical mechanics[2][3] and Le Chatelier's principle in chemistry.[4]\n\nThe current induced in a circuit due to a change in a magnetic field is directed to oppose the change in flux and to exert a mechanical force which opposes the motion.\n\nLenz's law is contained in the rigorous treatment of Faraday's law of induction (the magnitude of EMF induced in a coil is proportional to the rate of change of the magnetic flux),[5] where it finds expression by the negative sign:\n\nE\n          \n        \n        =\n        −\n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  \n                    B\n                  \n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\mathcal {E}}=-{\\frac {\\mathrm {d} \\Phi _{\\mathbf {B} }}{\\mathrm {d} t}},}\n\nwhich indicates that the induced electromotive force \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n and the rate of change in magnetic flux \n  \n    \n      \n        \n          Φ\n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi _{\\mathbf {B} }}\n  \n have opposite signs.[6]\n\nThis means that the direction of the back EMF of an induced field opposes the changing current that is its cause. D.J. Griffiths summarized it as follows: Nature abhors a change in flux.[7]\n\nIf a change in the magnetic field of current i1 induces another electric current, i2, the direction of i2 is opposite that of the change in i1. If these currents are in two coaxial circular conductors ℓ1 and ℓ2 respectively, and both are initially 0, then the currents i1 and i2 must counter-rotate. The opposing currents will repel each other as a result.\n\nMagnetic fields from strong magnets can create counter-rotating currents in a copper or aluminium pipe. This is shown by dropping the magnet through the pipe. The descent of the magnet inside the pipe is observably slower than when dropped outside the pipe.\n\nWhen a voltage is generated by a change in magnetic flux according to Faraday's law, the polarity of the induced voltage is such that it produces a current whose magnetic field opposes the change which produces it. The induced magnetic field inside any loop of wire always acts to keep the magnetic flux in the loop constant. The direction of an induced current can be determined using the right-hand rule to show which direction of current flow would create a magnetic field that would oppose the direction of changing flux through the loop.[8] In the examples above, if the flux is increasing, the induced field acts in opposition to it. If it is decreasing, the induced field acts in the direction of the applied field to oppose the change.\n\nIn electromagnetism, when charges move along electric field lines work is done on them, whether it involves storing potential energy (negative work) or increasing kinetic energy (positive work).\n\nWhen net positive work is applied to a charge q1, it gains speed and momentum. The net work on q1 thereby generates a magnetic field whose strength (in units of magnetic flux density (1 tesla = 1 volt-second per square meter)) is proportional to the speed increase of q1. This magnetic field can interact with a neighboring charge q2, passing on this momentum to it, and in return,  q1 loses momentum.\n\nThe charge q2 can also act on  q1 in a similar manner, by which it returns some of the momentum that it received from  q1. This back-and-forth component of momentum contributes to magnetic inductance. The closer that q1 and q2 are, the greater the effect. When q2 is inside a conductive medium such as a thick slab made of copper or aluminum, it more readily responds to the force applied to it by q1. The energy of q1 is not instantly consumed as heat generated by the current of q2 but is also stored in two opposing magnetic fields. The energy density of magnetic fields tends to vary with the square of the magnetic field's intensity; however, in the case of magnetically non-linear materials such as ferromagnets and superconductors, this relationship breaks down.\n\nMomentum must be conserved in the process, so if q1 is pushed in one direction, then q2 ought to be pushed in the other direction by the same force at the same time. However, the situation becomes more complicated when the finite speed of electromagnetic wave propagation is introduced (see retarded potential). This means that for a brief period the total momentum of the two charges is not conserved, implying that the difference should be accounted for by momentum in the fields, as asserted by Richard P. Feynman.[9] Famous 19th century electrodynamicist James Clerk Maxwell called this the \"electromagnetic momentum\".[10] Yet, such a treatment of fields may be necessary when Lenz's law is applied to opposite charges. It is normally assumed that the charges in question have the same sign. If they do not, such as a proton and an electron, the interaction is different. An electron generating a magnetic field would generate an EMF that causes a proton to accelerate in the same direction as the electron. At first, this might seem to violate the law of conservation of momentum, but such an interaction is seen to conserve momentum if the momentum of electromagnetic fields is taken into account.",
        pageTitle: "Lenz's law",
    },
    {
        title: "Lem's Law",
        link: "https://en.wikipedia.org/wiki/Lem%27s_Law",
        content:
            'Lem\'s law (Polish: Prawo Lema[1]) is an adage suggested  by the Polish science fiction writer and philosopher Stanisław Lem. It is best known from his faux review "Jedna Minuta" ("One Minute") of the non-existing book One Human Minute (1984),[2] but he formulated it in his correspondence already in 1978.\n\nLem\'s law, as translated into English, is stated as follows:[nb 1]\n\n"No one reads; if someone does read, he doesn\'t understand; if he understands, he immediately forgets."[6]\n\nThe "reviewed" fictional book One Human Minute is supposedly an ideal book that addresses the concern expressed in "Lem\'s law".[2]\n\nLem\'s law follows the structure of the argument about the non-existence of the world and the impossibility of knowledge and communication about it (even if it could exist) presented by Gorgias of Leontinoi.[4]\n\nIn an interview with Marek Oramus, who asked Lem how he came up with his law, Lem said that it resulted from his pondering upon the immense flood of publications with an inevitable repetitiveness of various conclusions. And the third part is valid because a person has to free some space in their head for yet another piece of information.[7]\n\nLem\'s law is related to what Juliusz Łukasiewicz [pl] called the "ignorance explosion".[8] \nIn Poland, Lem\'s law is often referred to as an expression of the conviction that the overall level of literacy and general education declines.[9][10][11][12][13]\nMore generally, it has also been used as a humorous description of Lem\'s critique of the contemporary state of our civilization.[14][15]',
        pageTitle: "Lem's law",
    },
    {
        title: "Lewis's law",
        link: "https://en.wikipedia.org/wiki/Lewis%27s_law",
        content:
            "Helen Alexandra Lewis (born 30 September 1983)[1] is an English journalist, women's activist,[2][3][4][5][6] and a staff writer at The Atlantic.[7][8] She is a former deputy editor of the New Statesman,[9] and has also written for The Guardian and The Sunday Times.[10]\n\nLewis was born in Worcester, England.[11] She attended St Mary's School, Worcester until 2001 and completed a Bachelor of Arts with honours in English at St Peter's College, Oxford in 2004.[12][13] In 2011, Lewis completed a Master of Arts in English at the Open University.[13]\n\nAfter graduating from Oxford,[14] Lewis gained a post-graduate diploma in newspaper journalism from London's City University.[citation needed] Subsequently, she was accepted on the Daily Mail's programme for trainee sub-editors, working in the job for a few years, and later joining the team responsible for commissioning features for the newspaper. She was appointed the Women in the Humanities Honorary Writing Fellow at Oxford University for 2018/2019,[15] and since 2019 has been on the steering committee for the Reuters Institute for Journalism at Oxford University,[16] where she delivered a lecture on \"The Failures of Political Journalism\",[17] subsequently adapted as a New Statesman cover story.[18]\n\nLewis was appointed as deputy editor of the New Statesman in 2012,[19] after becoming assistant editor in 2010.[20] Since July 2019, she has been a staff writer at The Atlantic.[21]\n\nIn 2018, Lewis interviewed Jordan Peterson for GQ,[22][23] in a video which has been viewed over 70 million times.[24] In 2019, April 2020, October 2020, April 2021, November 2021, June 2023, October 2023, May 2024, and October 2024, Lewis was a panellist on BBC's Have I Got News for You.[25][26][27][28]\n\nLewis's first book Difficult Women: A History of Feminism in 11 Fights, a history of the battles for women's rights, was published by Jonathan Cape on 27 February 2020. Difficult Women was featured in the New Statesman under \"Books to Read in 2020\", and in the Observer list of \"Non-fiction Books to Look Out for in 2020\".[29]\n\nIn 2019, Lewis launched her Radio 4 series, The Spark, a longform interview series with each episode dedicated to a single guest (or, in one case, two co-authors).[30][31] The first four series have been collected by Penguin as an audiobook.[32] In 2021, the BBC aired her comedy documentary series Great Wives.[33] In 2022, Helen Lewis's eight-part podcast called The New Gurus aired on BBC Radio 4.[34] In it, she investigated the popularity and influence of charismatic individuals from Russell Brand to Jordan Peterson.[35][36][37]\n\nLewis presents the current affairs podcasts Page 94: The Private Eye Podcast, with Ian Hislop, Andrew Hunter Murray, and Adam Macqueen,[38] and Strong Message Here with Armando Iannucci.[39]\n\nIn 2012, Lewis coined what she herself referred to as  \".mw-parser-output .vanchor>:target~.vanchor-text{background-color:#b1d2ff}@media screen{html.skin-theme-clientpref-night .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}Lewis's Law\": \"the comments on any article about feminism justify feminism.\"[40]\nIn January 2013, Lewis edited a week of articles dedicated to transgender issues in the New Statesman, featuring articles by transgender and non-binary writers including Juliet Jacques, Jane Fae and Sky Yarlett. In the introduction, she wrote: \"For anyone interested in equality, it should be obvious that trans people are subject to harassment simply for the way they express their gender identity.\"[41]\n\nWhile supporting transgender people's right to freedom from harassment and abuse,[42] in July 2017, Lewis wrote about her concerns that gender self-identification would make rape shelters unsafe for women and would lead to an increase in sexual assaults in women's changing rooms, writing: \"In this climate, who would challenge someone with a beard exposing their penis in a women's changing room?\"[43][44]\n\nIn response to criticism for those comments, Lewis said \"I've had two tedious years of being abused online as a transphobe and a 'TERF' or 'trans-exclusionary radical feminist'—despite my belief that trans women are women, and trans men are men—because I have expressed concerns about self-ID and its impact on single-sex spaces\".[45] In November 2020, due to her statements, game developer Ubisoft removed from Watch Dogs: Legion two in-game political podcasts featuring Lewis's voice.[46][47][48]\n\nLewis married Guardian journalist Jonathan Haynes in 2015. She was previously married in 2010 and divorced her first husband in 2013.[49]",
        pageTitle: "Helen Lewis (journalist)",
    },
    {
        title: "Lightwood's law",
        link: "https://en.wikipedia.org/wiki/Lightwood%27s_law",
        content:
            "Lightwood's law is the principle that, in medicine, bacterial infections will tend to localise while viral infections will tend to spread.[1] This is based on the observation that while bacterial sepsis tends, despite affecting the whole body, to have a clear site of origin or 'focus', the opposite may be true of viral infections.[2]  There may be multiple sites across the body which are affected including dermatological manifestations, respiratory symptoms and gastrointestinal symptoms.[citation needed] It is named for Reginald Cyril Lightwood.\n\nThis principle is by no means infallible and in clinical practice a variety of diagnostic tests are used to distinguish between bacterial and viral infections.[citation needed]\n\nThis infectious disease article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Lightwood's law",
    },
    {
        title: "Liebig's law of the minimum",
        link: "https://en.wikipedia.org/wiki/Liebig%27s_law_of_the_minimum",
        content:
            'Liebig\'s law of the minimum, often simply called Liebig\'s law or the law of the minimum, is a principle developed in agricultural science by Carl Sprengel (1840) and later popularized by Justus von Liebig. It states that growth is dictated not by total resources available, but by the scarcest resource (limiting factor). The law has also been applied to biological populations and ecosystem models for factors such as sunlight or mineral nutrients.\n\nThis  was originally applied to plant or crop growth, where it was found that increasing the amount of plentiful nutrients did not increase plant growth. Only by increasing the amount of the limiting nutrient (the one most scarce in relation to "need") was the growth of a plant or crop improved. This principle can be summed up in the aphorism, "The availability of the most abundant nutrient in the soil is only as good as the availability of the least abundant nutrient in the soil." Or the rough analog, "A chain is only as strong as its weakest link." Though diagnosis of limiting factors to crop yields is a common study, the approach has been criticized.[1]\n\nLiebig\'s law has been extended to biological populations (and is commonly used in ecosystem modelling). For example, the growth of an organism such as a plant may be dependent on a number of different factors, such as sunlight or mineral nutrients (e.g., nitrate or phosphate). The availability of these may vary, such that at any given time one is more limiting than the others. Liebig\'s law states that growth only occurs at the rate permitted by the most limiting factor.[2]\n\nFor instance, in the equation below, the growth of population \n  \n    \n      \n        O\n      \n    \n    {\\displaystyle O}\n  \n is a function of the minimum of three Michaelis-Menten terms representing limitation by factors \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  \n, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n.\n\nWhere\nO is the biomass concentration or population density.\nμI,μN,μP are the specific growth rates in response to the concentrations of three different limiting nutrients, represented by I,N,P respectively.\nkI,kN,kP are the half-saturation constants for the three nutrients I,N,P respectively. These constants represent the concentration of the nutrient at which the growth rate is half of its maximum.\nI,N,P are the concentrations of the three nutrients /factors.\nm is the mortality rate or decay constant.\n\nThe use of the equation is limited to a situation where there are steady state ceteris paribus conditions, and factor interactions are tightly controlled.\n\nIn human nutrition, the law of the minimum was used by William Cumming Rose to determine the essential amino acids. In 1931 he published his study "Feeding experiments with mixtures of highly refined amino acids".[3] Knowledge of the essential amino acids has enabled vegetarians to enhance their protein nutrition by protein combining from various vegetable sources. One practitioner was Nevin S. Scrimshaw fighting protein deficiency in India and Guatemala. Frances Moore Lappé published Diet for a Small Planet in 1971 which popularized protein combining using grains, legumes, and dairy products.\n\nThe law of the minimum was tested at University of Southern California in 1947.[4] "The formation of protein molecules is a coordinated tissue function and can be accomplished only when all amino acids which take part in the formation are present at the same time." It was further  concluded, that "\'incomplete\' amino acid mixtures are not stored in the body, but are irreversibly further metabolized." Robert Bruce Merrifield was a laboratory assistant for the experiments. When he wrote his autobiography he recounted in 1993 the finding:\n\nMore recently Liebig\'s law is starting to find an application in natural resource management where it surmises that growth in markets dependent upon natural resource inputs is restricted by the most limited input. As the natural capital upon which growth depends is limited in supply due to the finite nature of the planet, Liebig\'s law encourages scientists and natural resource managers to calculate the scarcity of essential resources in order to allow for a multi-generational approach to resource consumption.\n\nNeoclassical economic theory has sought to refute the issue of resource scarcity by application of the law of substitutability and technological innovation. The substitutability "law" states that as one resource is exhausted—and prices rise due to a lack of surplus—new markets based on alternative resources appear at certain prices in order to satisfy demand. Technological innovation implies that humans are able to use technology to fill the gaps in situations where resources are imperfectly substitutable.\n\nA market-based theory depends on proper pricing. Where resources such as clean air and water are not accounted for, there will be a "market failure". These failures may be addressed with Pigovian taxes and subsidies, such as a carbon tax. While the theory of the law of substitutability is a useful rule of thumb, some resources may be so fundamental that there exist no substitutes. For example, Isaac Asimov noted, "We may be able to substitute nuclear power for coal power, and plastics for wood ... but for phosphorus there is neither substitute nor replacement."[6]\n\nWhere no substitutes exist, such as phosphorus, recycling will be necessary. This may require careful long-term planning and governmental intervention, in part to create Pigovian taxes to allow efficient market allocation of resources, in part to address other market failures such as excessive time discounting.\n\nDobenecks used the image of a barrel — often called "Liebig\'s barrel" — to explain Liebig\'s law.[7] Just as the maximum practical capacity of a barrel with staves of unequal length is limited by the length of the shortest stave. Similarly, a plant\'s growth is limited by the nutrient in shortest supply.\n\nIf a system satisfies the law of the minimum then adaptation will equalize the load of different factors because the adaptation resource will be allocated for compensation of limitation.[8] Adaptation systems act as the cooper of Liebig\'s barrel and lengthens the shortest stave to improve barrel capacity. Indeed, in well-adapted systems the limiting factor should be compensated as far as possible. This observation follows the concept of resource competition and fitness maximization.[9]\n\nDue to the law of the minimum paradoxes, if we observe the Law of the Minimum in artificial systems, then under natural conditions adaptation will equalize the load of different factors and we can expect a violation of the law of the minimum. Inversely, if artificial systems demonstrate significant violation of the law of the minimum, then we can expect that under natural conditions adaptation will compensate this violation. In a limited system life will adjust as an evolution of what came before.[8]\n\nOne example of technological innovation is in plant genetics whereby the biological characteristics of species can be changed by employing genetic modification to alter biological dependence on the most limiting resource. Biotechnological innovations are thus able to extend the limits for growth in species by an increment until a new limiting factor is established, which can then be challenged through technological innovation.\n\nTheoretically there is no limit to the number of possible increments towards an unknown productivity limit.[10] This would be either the point where the increment to be advanced is so small it cannot be justified economically or where technology meets an invulnerable natural barrier. It may be worth adding that biotechnology itself is totally dependent on external sources of natural capital.',
        pageTitle: "Liebig's law of the minimum",
    },
    {
        title: "Lindy's Law",
        link: "https://en.wikipedia.org/wiki/Lindy%27s_Law",
        content:
            'The Lindy effect (also known as Lindy\'s law[1]) is a theorized phenomenon by which the future life expectancy of some non-perishable things, like a technology or an idea, is proportional to their current age. Thus, the Lindy effect proposes the longer a period something has survived to exist or be used in the present, the longer its remaining life expectancy. Longevity implies a resistance to change, obsolescence, or competition, and greater odds of continued existence into the future.[2] Where the Lindy effect applies, mortality rate decreases with time. Mathematically, the Lindy effect corresponds to lifetimes following a Pareto probability distribution.\n\nThe concept is named after Lindy\'s delicatessen in New York City, where the concept was informally theorized by comedians.[3][4] The Lindy effect has subsequently been theorized by mathematicians and statisticians.[5][6][1] Nassim Nicholas Taleb has expressed the Lindy effect in terms of "distance from an absorbing barrier".[7]\n\nThe Lindy effect applies to "non-perishable" items, those that do not have an "unavoidable expiration date".[2] For example, human beings are perishable: the life expectancy at birth in developed countries is about 80 years. So the Lindy effect does not apply to individual human lifespan: all else being equal, it is less likely for a 10-year-old human to die within the next year than for a 100-year-old, while the Lindy effect would predict the opposite.\n\nThe origin of the term can be traced to Albert Goldman and a 1964 article he had written in The New Republic titled "Lindy\'s Law."[3][4] The term Lindy refers to Lindy\'s delicatessen in New York, where comedians "foregather every night [to] conduct post-mortems on recent show business \'action.\'" In this article, Goldman describes a folkloric belief among New York City media observers that the amount of material comedians have is constant, and therefore, the frequency of output predicts how long their series will last:[8]\n\n... the life expectancy of a television comedian is [inversely] proportional to the total amount of his exposure on the medium. If, pathetically deluded by hubris, he undertakes a regular weekly or even monthly program, his chances of survival beyond the first season are slight; but if he adopts the conservation of resources policy favored by these senescent philosophers of "the Business," and confines himself to "specials" and "guest shots," he may last to the age of Ed Wynn [d. age 79 in 1966 while still acting in movies]\n\nBenoit Mandelbrot defined a different concept with the same name in his 1982 book The Fractal Geometry of Nature.[5] In Mandelbrot\'s version, comedians do not have a fixed amount of comedic material to spread over TV appearances, but rather, the more appearances they make, the more future appearances they are predicted to make: Mandelbrot expressed mathematically that for certain things bounded by the life of the producer, like human promise, future life expectancy is proportional to the past. He references Lindy\'s Law and a parable of the young poets\' cemetery and then applies to researchers and their publications: "However long a person\'s past collected works, it will on the average continue for an equal additional amount. When it eventually stops, it breaks off at precisely half of its promise."[5]\n\nIn Nassim Nicholas Taleb\'s 2012 book Antifragile: Things That Gain from Disorder he for the first time explicitly referred to his idea as the Lindy Effect, removed the bounds of the life of the producer to include anything which doesn\'t have a natural upper bound, and incorporated it into his broader theory of the Antifragile.\n\nIf a book has been in print for forty years, I can expect it to be in print for another forty years. But, and that is the main difference, if it survives another decade, then it will be expected to be in print another fifty years. This, simply, as a rule, tells you why things that have been around for a long time are not "aging" like persons, but "aging" in reverse. Every year that passes without extinction doubles the additional life expectancy. This is an indicator of some robustness. The robustness of an item is proportional to its life![9]\n\nAccording to Taleb, Mandelbrot agreed with the expanded definition of the Lindy Effect: "I [Taleb] suggested the boundary perishable/nonperishable and he [Mandelbrot] agreed that the nonperishable would be power-law distributed while the perishable (the initial Lindy story) worked as a mere metaphor."[10]\n\nTaleb further defined the term in Skin in the Game, where he linked Lindy with fragility, disorder and time.[11] To Taleb, "the theory of fragility directly leads to the Lindy effect," as he defines "fragility as sensitivity to disorder," and states that "time is equivalent to disorder, and resistance to the ravages of time, that is, what we gloriously call survival, is the ability to handle disorder."[11] As time operates through "skin in the game," Taleb believes that "[t]hings that have survived are hinting to us ‘ex post’ that they have some robustness." He concludes therefore that "the only effective judge of things is time," which in his view answers the "age-old meta-questions: Who will judge the expert? Who will guard the guard? [...] Well, survival will."[11] He further states that the Lindy effect in itself is "Lindy-proof," citing the words of pre-Socratic philosopher Periander ("Use laws that are old but food that is fresh") and Alfonso X of Castile ("Burn old logs. Drink old wine. Read old books. Keep old friends.")[11]\n\nContemporary proponents of the Lindy effect include Marc Andreessen, co-founder of Andreessen Horowitz, and Twitter user Paul Skallas, known as LindyMan. Skallas promotes a lifestyle inspired by the Lindy effect, an eclectic mix of Mediterranean writers such as Plutarch and Thomas Aquinas, and the presumed lifestyles of Mediterranean peoples. Writing for The New York Times, Ezra Marcus notes that Skallas\' approach to the Lindy effect breaks with Taleb\'s statistical analysis by focusing on lifestyle topics such as diet, dating, and exercise.[12]\n\nMathematically, the relation postulated by the Lindy effect can be expressed as the following statement about a random variable T corresponding to the lifetime of the object (e.g. a comedy show), which is assumed to take values in the range \n  \n    \n      \n        c\n        ≤\n        T\n        <\n        ∞\n      \n    \n    {\\displaystyle c\\leq T<\\infty }\n  \n (with a lower bound \n  \n    \n      \n        c\n        ≥\n        0\n      \n    \n    {\\displaystyle c\\geq 0}\n  \n):[1]\n\nE\n        \n        [\n        T\n        −\n        t\n        \n          |\n        \n        T\n        >\n        t\n        ]\n        =\n        p\n        ⋅\n        t\n      \n    \n    {\\displaystyle \\mathrm {E} [T-t|T>t]=p\\cdot t}\n\nHere the left hand side denotes the conditional expectation of the remaining lifetime \n  \n    \n      \n        T\n        −\n        t\n      \n    \n    {\\displaystyle T-t}\n  \n, given that \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n has exceeded \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, and the parameter \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n on the right hand side (called "Lindy proportion" by Iddo Eliazar) is a positive constant.[1]\n\nThis is equivalent to the survival function of T being\n\nΦ\n        (\n        t\n        )\n        :=\n        \n          Pr\n        \n        (\n        T\n        >\n        t\n        )\n        =\n        \n          \n            (\n            \n              \n                c\n                t\n              \n            \n            )\n          \n          \n            ϵ\n          \n        \n        \n           , where \n        \n        ϵ\n        =\n        1\n        +\n        \n          \n            1\n            p\n          \n        \n      \n    \n    {\\displaystyle \\Phi (t):={\\text{Pr}}(T>t)=\\left({\\frac {c}{t}}\\right)^{\\epsilon }{\\text{ , where }}\\epsilon =1+{\\frac {1}{p}}}\n\n−\n        \n          \n            \n              \n                Φ\n                ′\n              \n              (\n              t\n              )\n            \n            \n              Φ\n              (\n              t\n              )\n            \n          \n        \n        =\n        \n          \n            ϵ\n            t\n          \n        \n        =\n        \n          \n            \n              1\n              +\n              p\n            \n            p\n          \n        \n        \n          \n            1\n            t\n          \n        \n      \n    \n    {\\displaystyle -{\\frac {\\Phi \'(t)}{\\Phi (t)}}={\\frac {\\epsilon }{t}}={\\frac {1+p}{p}}{\\frac {1}{t}}}\n\nThis means that the lifetime \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n follows a Pareto distribution (a power-law distribution) with exponent \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n.[13][self-published source?][14][self-published source?][1]\n\nConversely, however, only Pareto distributions with exponent \n  \n    \n      \n        1\n        <\n        ϵ\n        <\n        ∞\n      \n    \n    {\\displaystyle 1<\\epsilon <\\infty }\n  \n correspond to a lifetime distribution that satisfies Lindy\'s Law, since the Lindy proportion \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is required to be positive and finite (in particular, the lifetime \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is assumed to have a finite expectation value).[1] Iddo Eliazar has proposed an alternative formulation of Lindy\'s Law involving the median instead of the mean (expected value) of the remaining lifetime \n  \n    \n      \n        T\n        −\n        t\n      \n    \n    {\\displaystyle T-t}\n  \n, which corresponds to Pareto distributions for the lifetime \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n with the full range of possible Pareto exponents \n  \n    \n      \n        0\n        <\n        ϵ\n        <\n        ∞\n      \n    \n    {\\displaystyle 0<\\epsilon <\\infty }\n  \n.[1] Eliazar also demonstrated a relation to Zipf’s Law, and to socioeconomic inequality, arguing that "Lindy’s Law, Pareto’s Law and Zipf’s Law are in effect synonymous laws."[1]',
        pageTitle: "Lindy effect",
    },
    {
        title: "Linus's law",
        link: "https://en.wikipedia.org/wiki/Linus%27s_law",
        content:
            'In software development, Linus\'s law is the assertion that "given enough eyeballs, all bugs are shallow".\nThe law was formulated by Eric S. Raymond in his essay and book The Cathedral and the Bazaar (1999), and was named in honor of Linus Torvalds.[1][2]\n\nA more formal statement is: "Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone." Presenting the code to multiple developers with the purpose of reaching consensus about its acceptance is a simple form of software reviewing. Researchers and practitioners have repeatedly shown the effectiveness of reviewing processes in finding bugs and security issues.[3]\n\nIn Facts and Fallacies about Software Engineering, Robert Glass refers to the law as a "mantra" of the open source movement, but calls it a fallacy due to the lack of supporting evidence and because research has indicated that the rate at which additional bugs are uncovered does not scale linearly with the number of reviewers; rather, there is a small maximum number of useful reviewers, between two and four, and additional reviewers above this number uncover bugs at a much lower rate.[4] While closed-source practitioners also promote stringent, independent code analysis during a software project\'s development, they focus on in-depth review by a few and not primarily the number of "eyeballs".[5]\n\nThe persistence of the Heartbleed security bug in a critical piece of code for two years has been considered as a refutation of Raymond\'s dictum.[6][7][8][9] Larry Seltzer suspects that the availability of source code may cause some developers and researchers to perform less extensive tests than they would with closed source software, making it easier for bugs to remain.[9]\nIn 2015, the Linux Foundation\'s executive director Jim Zemlin argued that the complexity of modern software has increased to such levels that specific resource allocation is desirable to improve its security. Regarding some of 2014\'s largest global open source software vulnerabilities, he says, "In these cases, the eyeballs weren\'t really looking".[8] Large scale experiments or peer-reviewed surveys to test how well the mantra holds in practice have not been performed.[10]\n\nEmpirical support of the validity of Linus\'s law[11] was obtained by comparing popular and unpopular projects of the same organization. Popular projects are projects with the top 5% of GitHub stars (7,481 stars or more). Bug identification was measured using the corrective commit probability, the ratio of commits determined to be related to fixing bugs. The analysis showed that popular projects had a higher ratio of bug fixes (e.g., Google\'s popular projects had a 27% higher bug fix rate than Google\'s less popular projects). Since it is unlikely that Google lowered its code quality standards in more popular projects, this is an indication of increased bug detection efficiency in popular projects.',
        pageTitle: "Linus's law",
    },
    {
        title: "Little's law",
        link: "https://en.wikipedia.org/wiki/Little%27s_law",
        content:
            "In mathematical queueing theory, Little's law (also result, theorem, lemma, or formula[1][2]) is a theorem by John Little which states that the long-term average number L of customers in a stationary system is equal to the long-term average effective arrival rate λ multiplied by the average time W that a customer spends in the system. Expressed algebraically the law is\n\nThe relationship is not influenced by the arrival process distribution, the service distribution, the service order, or practically anything else. In most queuing systems, service time is the bottleneck that creates the queue.[3]\n\nThe result applies to any system, and particularly, it applies to systems within systems.[4] For example in a bank branch, the customer line might be one subsystem, and each of the tellers another subsystem, and Little's result could be applied to each one, as well as the whole thing. The only requirements are that the system be stable and non-preemptive[vague]; this rules out transition states such as initial startup or shutdown.\n\nIn some cases it is possible not only to mathematically relate  the average number in the system to the average wait but even to relate the entire probability distribution (and moments) of the number in the system to the wait.[5]\n\nIn a 1954 paper, Little's law was assumed true and used without proof.[6][7] The form L = λW was first published by Philip M. Morse where he challenged readers to find a situation where the relationship did not hold.[6][8] Little published  in 1961 his proof of the law, showing that no such situation existed.[9] Little's proof was followed by a simpler version by Jewell[10] and another by Eilon.[11] Shaler Stidham published a different and more intuitive proof in 1972.[12][13]\n\nImagine an application that had no easy way to measure response time. If the mean number in the system and the throughput are known, the average response time can be found using Little’s Law:\n\nFor example: A queue depth meter shows an average of nine jobs waiting to be serviced. Add one for the job being serviced, so there is an average of ten jobs in the system. Another meter shows a mean throughput of 50 per second. The mean response time is calculated as 0.2 seconds = 10 / 50 per second.\n\nImagine a small store with a single counter and an area for browsing, where only one person can be at the counter at a time, and no one leaves without buying something.  So the system is:\n\nIf the rate at which people enter the store (called the arrival rate) is the rate at which they exit (called the exit rate), the system is stable.  By contrast, an arrival rate exceeding an exit rate would represent an unstable system, where the number of waiting customers in the store would gradually increase towards infinity.\n\nLittle's Law tells us that the average number of customers in the store L, is the effective arrival rate λ, times the average time that a customer spends in the store W, or simply:\n\nAssume customers arrive at the rate of 10 per hour and stay an average of 0.5 hour.  This means we should find the average number of customers in the store at any time to be 5.\n\nNow suppose the store is considering doing more advertising to raise the arrival rate to 20 per hour.  The store must either be prepared to host an average of 10 occupants or must reduce the time each customer spends in the store to 0.25 hour.  The store might achieve the latter by ringing up the bill faster or by adding more counters.\n\nWe can apply Little's Law to systems within the store.  For example, consider the counter and its queue.  Assume we notice that there are on average 2 customers in the queue and at the counter.  We know the arrival rate is 10 per hour, so customers must be spending 0.2 hours on average checking out.\n\nWe can even apply Little's Law to the counter itself.  The average number of people at the counter would be in the range (0, 1) since no more than one person can be at the counter at a time.  In that case, the average number of people at the counter is also known as the utilisation of the counter.\n\nHowever, because a store in reality generally has a limited amount of space, it can eventually become unstable. If the arrival rate is much greater than the exit rate, the store will eventually start to overflow, and thus any new arriving customers will simply be rejected (and forced to go somewhere else or try again later) until there is once again free space available in the store. This is also the difference between the arrival rate and the effective arrival rate, where the arrival rate roughly corresponds to the rate at which customers arrive at the store, whereas the effective arrival rate corresponds to the rate at which customers enter the store. However, in a system with an infinite size and no loss, the two are equal.\n\nTo use Little's law on data, formulas must be used to estimate the parameters, as the result does not necessarily directly apply over finite time intervals, due to problems like how to log customers already present at the start of the logging interval and those who have not yet departed when logging stops.[14]\n\nLittle's law is widely used in manufacturing to predict lead time based on the production rate and the amount of work-in-process.[15]\n\nSoftware-performance testers have used Little's law to ensure that the observed performance results are not due to bottlenecks imposed by the testing apparatus. [16][17]\n\nOther applications include staffing emergency departments in hospitals.[18][19]\n\nAn extension of Little's law provides a relationship between the steady state distribution of number of customers in the system and time spent in the system under a first come, first served service discipline.[20]",
        pageTitle: "Little's law",
    },
    {
        title: "Littlewood's law",
        link: "https://en.wikipedia.org/wiki/Littlewood%27s_law",
        content:
            'Littlewood\'s law states that a person can expect to experience events with odds of one in a million (referred to as a "miracle") at the rate of about one per month. It is named after the British mathematician John Edensor Littlewood.\n\nIt seeks, among other things, to debunk one element of supposed supernatural phenomenology and is related to the more general law of truly large numbers, which states that with a sample size large enough, any outrageous (in terms of probability model of single sample) thing is likely to happen.\n\nAn early formulation of the law appears in the 1953 collection of Littlewood\'s work, A Mathematician\'s Miscellany. In the chapter "Large Numbers", Littlewood states:\n\nLittlewood uses these remarks to illustrate that seemingly unlikely coincidences can be expected over long periods. He provides several anecdotes about improbable events that, given enough time, are likely to occur. For example, in the game of bridge, the probability that a player will be dealt 13 cards of the same suit is extremely low (Littlewood calculates it as \n  \n    \n      \n        2.4\n        ⋅\n        \n          10\n          \n            −\n            9\n          \n        \n      \n    \n    {\\displaystyle 2.4\\cdot 10^{-9}}\n  \n). While such a deal might seem miraculous, if one estimates that \n  \n    \n      \n        2\n        ⋅\n        \n          10\n          \n            6\n          \n        \n      \n    \n    {\\displaystyle 2\\cdot 10^{6}}\n  \n people in England each play an average of 30 bridge hands a week, it becomes quite expected that such a "miracle" would happen approximately once per year.\n\nThis statement was later reformulated as Littlewood\'s law of miracles by Freeman Dyson, in a 2004 review of the book Debunked! ESP, Telekinesis, and Other Pseudoscience, published in the New York Review of Books:',
        pageTitle: "Littlewood's law",
    },
    {
        title: "Llinás's law",
        link: "https://en.wikipedia.org/wiki/Llin%C3%A1s%27s_law",
        content:
            "Llinás's law, or law of no interchangeability of neurons, is a statement in neuroscience made by Rodolfo Llinás in 1989, during his Luigi Galvani Award Lecture at the Fidia Research Foundation Neuroscience Award Lectures.[1]\n\nA neuron of a given kind (e.g. a thalamic cell) cannot be functionally replaced by one of another type (e.g. an inferior ollivary cell) even if their synaptic connectivity and the type of neurotransmitter outputs are identical. (The difference is that the intrinsic electrophysiological properties of thalamic cells are extraordinarily different from those of inferior olivary neurons).[2][3]\n\nThe statement of this law is a consequence of an article written by Rodolfo Llinas himself in 1988 and published in Science with the title \"The Intrinsic Electrophysiological Properties of Mammalian Neurons: Insights into Central Nervous System Function\",[4] which is considered a watershed due to its more than 2000 citations in the scientific literature, marking a major shift in viewpoint in neuroscience around the functional aspect. Until then, the prevailing belief in neuroscience was that just the connections and neurotransmitters released by neurons was enough to determine their function. Research by Llinás and colleagues during the 80's with vertebrates revealed this previously held dogma was wrong.",
        pageTitle: "Llinás's law",
    },
    {
        title: "Lorentz force law",
        link: "https://en.wikipedia.org/wiki/Lorentz_force",
        content:
            "In physics, specifically in electromagnetism, the Lorentz force law is the combination of electric and magnetic force on a point charge due to electromagnetic fields. The Lorentz force, on the other hand, is a physical effect that occurs in the vicinity of electrically neutral, current-carrying conductors causing moving electrical charges to experience a magnetic force.\n\nThe Lorentz force law states that a particle of charge q moving with a velocity v in an electric field E and a magnetic field B experiences a force (in SI units[1][2]) of\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right).}\n  \n\nIt says that the electromagnetic force on a charge q is a combination of (1) a force in the direction of the electric field E (proportional to the magnitude of the field and the quantity of charge), and (2) a force at right angles to both the magnetic field B and the velocity v of the charge (proportional to the magnitude of the field, the charge, and the velocity).\n\nVariations on this basic formula describe the magnetic force on a current-carrying wire (sometimes called Laplace force), the electromotive force in a wire loop moving through a magnetic field (an aspect of Faraday's law of induction), and the force on a moving charged particle.[3]\n\nHistorians suggest that the law is implicit in a paper by James Clerk Maxwell, published in 1865.[4] Hendrik Lorentz arrived at a complete derivation in 1895,[5] identifying the contribution of the electric force a few years after Oliver Heaviside correctly identified the contribution of the magnetic force.[6]\n\nIn many textbook treatments of classical electromagnetism, the Lorentz force law is used as the definition of the electric and magnetic fields E and B.[7][8][9] To be specific, the Lorentz force is understood to be the following empirical statement:\n\nThe electromagnetic force F on a test charge at a given point and time is a certain function of its charge q and velocity v, which can be parameterized by exactly two vectors E and B, in the functional form: \n  \n    \n      \n        \n          F\n        \n        =\n        q\n        (\n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {F} =q(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} )}\n\nThis is valid, even for particles approaching the speed of light (that is, magnitude of v, |v| ≈ c).[10] So the two vector fields E and B are thereby defined throughout space and time, and these are called the \"electric field\" and \"magnetic field\". The fields are defined everywhere in space and time with respect to what force a test charge would receive regardless of whether a charge is present to experience the force.\n\nCoulomb's law is only valid for point charges at rest. In fact, the electromagnetic force between two point charges depends not only on the distance but also on the relative velocity. For small relative velocities and very small accelerations, instead of the Coulomb force, the Weber force can be applied. The sum of the Weber forces of all charge carriers in a closed DC loop on a single test charge produces – regardless of the shape of the current loop – the Lorentz force.\n\nThe interpretation of magnetism by means of a modified Coulomb law was first proposed by Carl Friedrich Gauss. In 1835, Gauss assumed that each segment of a DC loop contains an equal number of negative and positive point charges that move at different speeds.[11] If Coulomb's law were completely correct, no force should act between any two short segments of such current loops. However, around 1825, André-Marie Ampère demonstrated experimentally that this is not the case. Ampère also formulated a force law. Based on this law, Gauss concluded that the electromagnetic force between two point charges depends not only on the distance but also on the relative velocity.\n\nThe Weber force is a central force and complies with Newton's third law. This demonstrates not only the conservation of momentum but also that the conservation of energy and the conservation of angular momentum apply. Weber electrodynamics is only a quasistatic approximation, i.e. it should not be used for higher velocities and accelerations. However, the Weber force illustrates that the Lorentz force can be traced back to central forces between numerous point-like charge carriers.\n\nThe force F acting on a particle of electric charge q with instantaneous velocity v, due to an external electric field E and magnetic field B, is given by (SI definition of quantities[1]):[12]\n\nF\n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n\nwhere × is the vector cross product (all boldface quantities are vectors). In terms of Cartesian components, we have:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    x\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        x\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        y\n                      \n                    \n                    \n                      B\n                      \n                        z\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        z\n                      \n                    \n                    \n                      B\n                      \n                        y\n                      \n                    \n                  \n                  )\n                \n                ,\n              \n            \n            \n              \n                \n                  F\n                  \n                    y\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        y\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        z\n                      \n                    \n                    \n                      B\n                      \n                        x\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        x\n                      \n                    \n                    \n                      B\n                      \n                        z\n                      \n                    \n                  \n                  )\n                \n                ,\n              \n            \n            \n              \n                \n                  F\n                  \n                    z\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        z\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        x\n                      \n                    \n                    \n                      B\n                      \n                        y\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        y\n                      \n                    \n                    \n                      B\n                      \n                        x\n                      \n                    \n                  \n                  )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{x}&=q\\left(E_{x}+v_{y}B_{z}-v_{z}B_{y}\\right),\\\\[0.5ex]F_{y}&=q\\left(E_{y}+v_{z}B_{x}-v_{x}B_{z}\\right),\\\\[0.5ex]F_{z}&=q\\left(E_{z}+v_{x}B_{y}-v_{y}B_{x}\\right).\\end{aligned}}}\n\nIn general, the electric and magnetic fields are functions of the position and time. Therefore, explicitly, the Lorentz force can be written as:\n\n  \n    \n      \n        \n          F\n        \n        \n          (\n          \n            \n              r\n            \n            (\n            t\n            )\n            ,\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            (\n            t\n            )\n            ,\n            t\n            ,\n            q\n          \n          )\n        \n        =\n        q\n        \n          [\n          \n            \n              E\n            \n            (\n            \n              r\n            \n            ,\n            t\n            )\n            +\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            (\n            t\n            )\n            ×\n            \n              B\n            \n            (\n            \n              r\n            \n            ,\n            t\n            )\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbf {F} \\left(\\mathbf {r} (t),{\\dot {\\mathbf {r} }}(t),t,q\\right)=q\\left[\\mathbf {E} (\\mathbf {r} ,t)+{\\dot {\\mathbf {r} }}(t)\\times \\mathbf {B} (\\mathbf {r} ,t)\\right]}\n  \n\nin which r is the position vector of the charged particle, t is time, and the overdot is a time derivative.\n\nA positively charged particle will be accelerated in the same linear orientation as the E field, but will curve perpendicularly to both the instantaneous velocity vector v and the B field according to the right-hand rule (in detail, if the fingers of the right hand are extended to point in the direction of v and are then curled to point in the direction of B, then the extended thumb will point in the direction of F).\n\nThe term qE is called the electric force, while the term q(v × B) is called the magnetic force.[13] According to some definitions, the term \"Lorentz force\" refers specifically to the formula for the magnetic force,[14] with the total electromagnetic force (including the electric force) given some other (nonstandard) name. This article will not follow this nomenclature: in what follows, the term Lorentz force will refer to the expression for the total force.\n\nThe magnetic force component of the Lorentz force manifests itself as the force that acts on a current-carrying wire in a magnetic field. In that context, it is also called the Laplace force.\n\nThe Lorentz force is a force exerted by the electromagnetic field on the charged particle, that is, it is the rate at which linear momentum is transferred from the electromagnetic field to the particle. Associated with it is the power which is the rate at which energy is transferred from the electromagnetic field to the particle. That power is\n\n  \n    \n      \n        \n          v\n        \n        ⋅\n        \n          F\n        \n        =\n        q\n        \n        \n          v\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} \\cdot \\mathbf {F} =q\\,\\mathbf {v} \\cdot \\mathbf {E} .}\n  \n\nNotice that the magnetic field does not contribute to the power because the magnetic force is always perpendicular to the velocity of the particle.\n\nFor a continuous charge distribution in motion, the Lorentz force equation becomes:\n\n  \n    \n      \n        \n          d\n        \n        \n          F\n        \n        =\n        \n          d\n        \n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {F} =\\mathrm {d} q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n  \n\nwhere \n  \n    \n      \n        \n          d\n        \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {F} }\n  \n is the force on a small piece of the charge distribution with charge \n  \n    \n      \n        \n          d\n        \n        q\n      \n    \n    {\\displaystyle \\mathrm {d} q}\n  \n. If both sides of this equation are divided by the volume of this small piece of the charge distribution \n  \n    \n      \n        \n          d\n        \n        V\n      \n    \n    {\\displaystyle \\mathrm {d} V}\n  \n, the result is:\n\n  \n    \n      \n        \n          f\n        \n        =\n        ρ\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\rho \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n  \n\nwhere \n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle \\mathbf {f} }\n  \n is the force density (force per unit volume) and \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is the charge density (charge per unit volume). Next, the current density corresponding to the motion of the charge continuum is\n\n  \n    \n      \n        \n          J\n        \n        =\n        ρ\n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {J} =\\rho \\mathbf {v} }\n  \n\nso the continuous analogue to the equation is[15]\n\nf\n        \n        =\n        ρ\n        \n          E\n        \n        +\n        \n          J\n        \n        ×\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\rho \\mathbf {E} +\\mathbf {J} \\times \\mathbf {B} }\n\nThe total force is the volume integral over the charge distribution:\n\n  \n    \n      \n        \n          F\n        \n        =\n        ∫\n        \n          (\n          \n            ρ\n            \n              E\n            \n            +\n            \n              J\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        \n          d\n        \n        V\n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =\\int \\left(\\rho \\mathbf {E} +\\mathbf {J} \\times \\mathbf {B} \\right)\\mathrm {d} V.}\n\nBy eliminating \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n and \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n, using Maxwell's equations, and manipulating using the theorems of vector calculus, this form of the equation can be used to derive the Maxwell stress tensor \n  \n    \n      \n        \n          σ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}}\n  \n, in turn this can be combined with the Poynting vector \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\mathbf {S} }\n  \n to obtain the electromagnetic stress–energy tensor T used in general relativity.[15]\n\nIn terms of \n  \n    \n      \n        \n          σ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}}\n  \n and \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\mathbf {S} }\n  \n, another way to write the Lorentz force (per unit volume) is[15]\n\n  \n    \n      \n        \n          f\n        \n        =\n        ∇\n        ⋅\n        \n          σ\n        \n        −\n        \n          \n            \n              1\n              \n                c\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                ∂\n                \n                  S\n                \n              \n              \n                ∂\n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\nabla \\cdot {\\boldsymbol {\\sigma }}-{\\dfrac {1}{c^{2}}}{\\dfrac {\\partial \\mathbf {S} }{\\partial t}}}\n  \n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the speed of light and ∇· (nabla followed by a middle dot) denotes the divergence of a tensor field. Rather than the amount of charge and its velocity in electric and magnetic fields, this equation relates the energy flux (flow of energy per unit time per unit distance) in the fields to the force exerted on a charge distribution. See Covariant formulation of classical electromagnetism for more details.\n\nThe density of power associated with the Lorentz force in a material medium is\n\n  \n    \n      \n        \n          J\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {J} \\cdot \\mathbf {E} .}\n\nIf we separate the total charge and total current into their free and bound parts, we get that the density of the Lorentz force is\n\n  \n    \n      \n        \n          f\n        \n        =\n        \n          (\n          \n            \n              ρ\n              \n                f\n              \n            \n            −\n            ∇\n            ⋅\n            \n              P\n            \n          \n          )\n        \n        \n          E\n        \n        +\n        \n          (\n          \n            \n              \n                J\n              \n              \n                f\n              \n            \n            +\n            ∇\n            ×\n            \n              M\n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    P\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n          \n          )\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {f} =\\left(\\rho _{f}-\\nabla \\cdot \\mathbf {P} \\right)\\mathbf {E} +\\left(\\mathbf {J} _{f}+\\nabla \\times \\mathbf {M} +{\\frac {\\partial \\mathbf {P} }{\\partial t}}\\right)\\times \\mathbf {B} .}\n\nwhere: \n  \n    \n      \n        \n          ρ\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle \\rho _{f}}\n  \n is the density of free charge; \n  \n    \n      \n        \n          P\n        \n      \n    \n    {\\displaystyle \\mathbf {P} }\n  \n is the polarization density; \n  \n    \n      \n        \n          \n            J\n          \n          \n            f\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {J} _{f}}\n  \n is the density of free current; and \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n  \n is the magnetization density. In this way, the Lorentz force can explain the torque applied to a permanent magnet by the magnetic field. The density of the associated power is\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                J\n              \n              \n                f\n              \n            \n            +\n            ∇\n            ×\n            \n              M\n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    P\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n          \n          )\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\left(\\mathbf {J} _{f}+\\nabla \\times \\mathbf {M} +{\\frac {\\partial \\mathbf {P} }{\\partial t}}\\right)\\cdot \\mathbf {E} .}\n\nThe above-mentioned formulae use the conventions for the definition of the electric and magnetic field used with the SI, which is the most common. However, other conventions with the same physics (i.e. forces on e.g. an electron) are possible and used. In the conventions used with the older CGS-Gaussian units, which are somewhat more common among some theoretical physicists as well as condensed matter experimentalists, one has instead\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          q\n          \n            \n              G\n            \n          \n        \n        \n          (\n          \n            \n              \n                E\n              \n              \n                \n                  G\n                \n              \n            \n            +\n            \n              \n                \n                  v\n                \n                c\n              \n            \n            ×\n            \n              \n                B\n              \n              \n                \n                  G\n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} =q_{\\mathrm {G} }\\left(\\mathbf {E} _{\\mathrm {G} }+{\\frac {\\mathbf {v} }{c}}\\times \\mathbf {B} _{\\mathrm {G} }\\right),}\n  \n\nwhere c is the speed of light. Although this equation looks slightly different, it is equivalent, since one has the following relations:[1]\n\n  \n    \n      \n        \n          q\n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            \n              q\n              \n                \n                  S\n                  I\n                \n              \n            \n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        ,\n        \n        \n          \n            E\n          \n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            4\n            π\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        \n        \n          \n            E\n          \n          \n            \n              S\n              I\n            \n          \n        \n        ,\n        \n        \n          \n            B\n          \n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            4\n            π\n            \n              /\n            \n            \n              μ\n              \n                0\n              \n            \n          \n        \n        \n        \n          \n            \n              B\n            \n            \n              \n                S\n                I\n              \n            \n          \n        \n        ,\n        \n        c\n        =\n        \n          \n            1\n            \n              \n                ε\n                \n                  0\n                \n              \n              \n                μ\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle q_{\\mathrm {G} }={\\frac {q_{\\mathrm {SI} }}{\\sqrt {4\\pi \\varepsilon _{0}}}},\\quad \\mathbf {E} _{\\mathrm {G} }={\\sqrt {4\\pi \\varepsilon _{0}}}\\,\\mathbf {E} _{\\mathrm {SI} },\\quad \\mathbf {B} _{\\mathrm {G} }={\\sqrt {4\\pi /\\mu _{0}}}\\,{\\mathbf {B} _{\\mathrm {SI} }},\\quad c={\\frac {1}{\\sqrt {\\varepsilon _{0}\\mu _{0}}}}.}\n  \n\nwhere ε0 is the vacuum permittivity and μ0 the vacuum permeability. In practice, the subscripts \"G\" and \"SI\" are omitted, and the used convention (and unit)  must be determined from context.\n\nEarly attempts to quantitatively describe the electromagnetic force were made in the mid-18th century. It was proposed that the force on magnetic poles, by Johann Tobias Mayer and others in 1760,[16] and electrically charged objects, by Henry Cavendish in 1762,[17] obeyed an inverse-square law. However, in both cases the experimental proof was neither complete nor conclusive. It was not until 1784 when Charles-Augustin de Coulomb, using a torsion balance, was able to definitively show through experiment that this was true.[18] Soon after the discovery in 1820 by Hans Christian Ørsted that a magnetic needle is acted on by a voltaic current, André-Marie Ampère that same year was able to devise through experimentation the formula for the angular dependence of the force between two current elements.[19][20] In all these descriptions, the force was always described in terms of the properties of the matter involved and the distances between two masses or charges rather than in terms of electric and magnetic fields.[21]\n\nThe modern concept of electric and magnetic fields first arose in the theories of Michael Faraday, particularly his idea of lines of force, later to be given full mathematical description by Lord Kelvin and James Clerk Maxwell.[22] From a modern perspective it is possible to identify in Maxwell's 1865 formulation of his field equations a form of the Lorentz force equation in relation to electric currents,[4] although in the time of Maxwell it was not evident how his equations related to the forces on moving charged objects. J. J. Thomson was the first to attempt to derive from Maxwell's field equations the electromagnetic forces on a moving charged object in terms of the object's properties and external fields. Interested in determining the electromagnetic behavior of the charged particles in cathode rays, Thomson published a paper in 1881 wherein he gave the force on the particles due to an external magnetic field as[6][23]\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            q\n            2\n          \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\frac {q}{2}}\\mathbf {v} \\times \\mathbf {B} .}\n  \n\nThomson derived the correct basic form of the formula, but, because of some miscalculations and an incomplete description of the displacement current, included an incorrect scale-factor of a half in front of the formula. Oliver Heaviside invented the modern vector notation and applied it to Maxwell's field equations; he also (in 1885 and 1889) had fixed the mistakes of Thomson's derivation and arrived at the correct form of the magnetic force on a moving charged object.[6][24][25] Finally, in 1895,[5][26] Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. Lorentz began by abandoning the Maxwellian descriptions of the ether and conduction. Instead, Lorentz made a distinction between matter and the luminiferous aether and sought to apply the Maxwell equations at a microscopic scale. Using Heaviside's version of the Maxwell equations for a stationary ether and applying Lagrangian mechanics (see below), Lorentz arrived at the correct and complete form of the force law that now bears his name.[27][28]\n\nIn many cases of practical interest, the motion in a magnetic field of an electrically charged particle (such as an electron or ion in a plasma) can be treated as the superposition of a relatively fast circular motion around a point called the guiding center and a relatively slow drift of this point. The drift speeds may differ for various species depending on their charge states, masses, or temperatures, possibly resulting in electric currents or chemical separation.\n\nWhile the modern Maxwell's equations describe how electrically charged particles and currents or moving charged particles give rise to electric and magnetic fields, the Lorentz force law completes that picture by describing the force acting on a moving point charge q in the presence of electromagnetic fields.[12][29] The Lorentz force law describes the effect of E and B upon a point charge, but such electromagnetic forces are not the entire picture. Charged particles are possibly coupled to other forces, notably gravity and nuclear forces. Thus, Maxwell's equations do not stand separate from other physical laws, but are coupled to them via the charge and current densities. The response of a point charge to the Lorentz law is one aspect; the generation of E and B by currents and charges is another.\n\nIn real materials the Lorentz force is inadequate to describe the collective behavior of charged particles, both in principle and as a matter of computation. The charged particles in a material medium not only respond to the E and B fields but also generate these fields. Complex transport equations must be solved to determine the time and spatial response of charges, for example, the Boltzmann equation or the Fokker–Planck equation or the Navier–Stokes equations. For example, see magnetohydrodynamics, fluid dynamics, electrohydrodynamics, superconductivity, stellar evolution. An entire physical apparatus for dealing with these matters has developed. See for example, Green–Kubo relations and Green's function (many-body theory).\n\nWhen a wire carrying an electric current is placed in a magnetic field, each of the moving charges, which comprise the current, experiences the Lorentz force, and together they can create a macroscopic force on the wire (sometimes called the Laplace force). By combining the Lorentz force law above with the definition of electric current, the following equation results, in the case of a straight stationary wire in a homogeneous field:[30]\n\n  \n    \n      \n        \n          F\n        \n        =\n        I\n        \n          ℓ\n        \n        ×\n        \n          B\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} =I{\\boldsymbol {\\ell }}\\times \\mathbf {B} ,}\n  \n\nwhere ℓ is a vector whose magnitude is the length of the wire, and whose direction is along the wire, aligned with the direction of the conventional current I.\n\nIf the wire is not straight, the force on it can be computed by applying this formula to each infinitesimal segment of wire \n  \n    \n      \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n, then adding up all these forces by integration. This results in the same formal expression, but ℓ should now be understood as the vector connecting the end points of the curved wire with direction from starting to end point of conventional current. Usually, there will also be a net torque.\n\nIf, in addition, the magnetic field is inhomogeneous, the net force on a stationary rigid wire carrying a steady current I is given by integration along the wire,\n\n  \n    \n      \n        \n          F\n        \n        =\n        I\n        ∫\n        \n          d\n        \n        \n          ℓ\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =I\\int \\mathrm {d} {\\boldsymbol {\\ell }}\\times \\mathbf {B} .}\n\nOne application of this is Ampère's force law, which describes how two current-carrying wires can attract or repel each other, since each experiences a Lorentz force from the other's magnetic field.\n\nThe magnetic force (qv × B) component of the Lorentz force is responsible for motional electromotive force (or motional EMF), the phenomenon underlying many electrical generators. When a conductor is moved through a magnetic field, the magnetic field exerts opposite forces on electrons and nuclei in the wire, and this creates the EMF. The term \"motional EMF\" is applied to this phenomenon, since the EMF is due to the motion of the wire.\n\nIn other electrical generators, the magnets move, while the conductors do not. In this case, the EMF is due to the electric force (qE) term in the Lorentz Force equation. The electric field in question is created by the changing magnetic field, resulting in an induced EMF called the transformer EMF  , as described by the Maxwell–Faraday equation (one of the four modern Maxwell's equations).[31][32]\n\nBoth of these EMFs, despite their apparently distinct origins, are described by the same equation, namely, the EMF is the rate of change of magnetic flux through the wire. (This is Faraday's law of induction, see below.) Einstein's special theory of relativity was partially motivated by the desire to better understand this link between the two effects.[31] In fact, the electric and magnetic fields are different facets of the same electromagnetic field, and in moving from one inertial frame to another, the solenoidal vector field portion of the E-field can change in whole or in part to a B-field or vice versa.[33]\n\nGiven a loop of wire in a magnetic field, Faraday's law of induction states the induced electromotive force (EMF) in the wire is:\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        −\n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}=-{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}}\n  \n\nwhere\n\n  \n    \n      \n        \n          Φ\n          \n            B\n          \n        \n        =\n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\Phi _{B}=\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot \\mathbf {B} (\\mathbf {r} ,t)}\n  \n\nis the magnetic flux through the loop, B is the magnetic field, Σ(t) is a surface bounded by the closed contour ∂Σ(t), at time t, dA is an infinitesimal vector area element of Σ(t) (magnitude is the area of an infinitesimal patch of surface, direction is orthogonal to that surface patch).\n\nThe sign of the EMF is determined by Lenz's law. Note that this is valid for not only a stationary wire – but also for a moving wire.\n\nFrom Faraday's law of induction (that is valid for a moving wire, for instance in a motor) and the Maxwell Equations, the Lorentz Force can be deduced. The reverse is also true, the Lorentz force and the Maxwell Equations can be used to derive the Faraday Law.\n\nLet Σ(t) be the moving wire, moving together without rotation and with constant velocity v and Σ(t) be the internal surface of the wire. The EMF around the closed path ∂Σ(t) is given by:[34]\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle {\\mathcal {E}}=\\oint _{\\partial \\Sigma (t)}\\!\\!\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q}\n  \n\nwhere \n  \n    \n      \n        \n          E\n        \n        =\n        \n          F\n        \n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle \\mathbf {E} =\\mathbf {F} /q}\n  \n is the electric field and dℓ is an infinitesimal vector element of the contour ∂Σ(t).\n\nNB: Both dℓ and dA have a sign ambiguity; to get the correct sign, the right-hand rule is used, as explained in the article Kelvin–Stokes theorem.\n\nThe above result can be compared with the version of Faraday's law of induction that appears in the modern Maxwell's equations, called here the Maxwell–Faraday equation:\n\n  \n    \n      \n        ∇\n        ×\n        \n          E\n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                B\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {E} =-{\\frac {\\partial \\mathbf {B} }{\\partial t}}\\,.}\n\nThe Maxwell–Faraday equation also can be written in an integral form using the Kelvin–Stokes theorem.[35]\n\nSo we have, the Maxwell–Faraday equation:\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        −\n         \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          \n            \n              ∂\n              \n                B\n              \n              (\n              \n                r\n              \n              ,\n              \n              t\n              )\n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {E} (\\mathbf {r} ,\\ t)=-\\ \\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot {\\frac {\\partial \\mathbf {B} (\\mathbf {r} ,\\,t)}{\\partial t}}}\n  \n\nand the Faraday law,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        −\n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        .\n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,\\ t)=-{\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot \\mathbf {B} (\\mathbf {r} ,\\ t).}\n\nThe two are equivalent if the wire is not moving. Using the Leibniz integral rule and that div B = 0, results in,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n        t\n        )\n        =\n        −\n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          \n            ∂\n            \n              ∂\n              t\n            \n          \n        \n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n        +\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,t)=-\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot {\\frac {\\partial }{\\partial t}}\\mathbf {B} (\\mathbf {r} ,t)+\\oint _{\\partial \\Sigma (t)}\\!\\!\\!\\!\\mathbf {v} \\times \\mathbf {B} \\,\\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n\nand using the Maxwell Faraday equation,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        +\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,\\ t)=\\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {E} (\\mathbf {r} ,\\ t)+\\oint _{\\partial \\Sigma (t)}\\!\\!\\!\\!\\mathbf {v} \\times \\mathbf {B} (\\mathbf {r} ,\\ t)\\,\\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n\nsince this is valid for any wire position it implies that\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n        \n        t\n        )\n        +\n        q\n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        \n        t\n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\,\\mathbf {E} (\\mathbf {r} ,\\,t)+q\\,\\mathbf {v} \\times \\mathbf {B} (\\mathbf {r} ,\\,t).}\n\nFaraday's law of induction holds whether the loop of wire is rigid and stationary, or in motion or in process of deformation, and it holds whether the magnetic field is constant in time or changing. However, there are cases where Faraday's law is either inadequate or difficult to use, and application of the underlying Lorentz force law is necessary. See inapplicability of Faraday's law.\n\nIf the magnetic field is fixed in time and the conducting loop moves through the field, the magnetic flux ΦB linking the loop can change in several ways. For example, if the B-field varies with position, and the loop moves to a location with different B-field, ΦB will change. Alternatively, if the loop changes orientation with respect to the B-field, the B ⋅ dA differential element will change because of the different angle between B and dA, also changing ΦB. As a third example, if a portion of the circuit is swept through a uniform, time-independent B-field, and another portion of the circuit is held stationary, the flux linking the entire closed circuit can change due to the shift in relative position of the circuit's component parts with time (surface ∂Σ(t) time-dependent). In all three cases, Faraday's law of induction then predicts the EMF generated by the change in ΦB.\n\nNote that the Maxwell Faraday's equation implies that the Electric Field E is non conservative when the Magnetic Field B varies in time, and is not expressible as the gradient of a scalar field, and not subject to the gradient theorem since its curl is not zero.[34][36]\n\nThe E and B fields can be replaced by the magnetic vector potential A and (scalar) electrostatic potential ϕ by\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  E\n                \n              \n              \n                \n                =\n                −\n                ∇\n                ϕ\n                −\n                \n                  \n                    \n                      ∂\n                      \n                        A\n                      \n                    \n                    \n                      ∂\n                      t\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  B\n                \n              \n              \n                \n                =\n                ∇\n                ×\n                \n                  A\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {E} &=-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}\\\\[1ex]\\mathbf {B} &=\\nabla \\times \\mathbf {A} \\end{aligned}}}\n  \n\nwhere ∇ is the gradient, ∇⋅ is the divergence, and ∇× is the curl.\n\nThe force becomes\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            ϕ\n            −\n            \n              \n                \n                  ∂\n                  \n                    A\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              v\n            \n            ×\n            (\n            ∇\n            ×\n            \n              A\n            \n            )\n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}+\\mathbf {v} \\times (\\nabla \\times \\mathbf {A} )\\right].}\n\nUsing an identity for the triple product this can be rewritten as\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            ϕ\n            −\n            \n              \n                \n                  ∂\n                  \n                    A\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            ∇\n            \n              (\n              \n                \n                  v\n                \n                ⋅\n                \n                  A\n                \n              \n              )\n            \n            −\n            \n              (\n              \n                \n                  v\n                \n                ⋅\n                ∇\n              \n              )\n            \n            \n              A\n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}+\\nabla \\left(\\mathbf {v} \\cdot \\mathbf {A} \\right)-\\left(\\mathbf {v} \\cdot \\nabla \\right)\\mathbf {A} \\right].}\n\n(Notice that the coordinates and the velocity components should be treated as independent variables, so the del operator acts only on \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n, not on \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n; thus, there is no need of using Feynman's subscript notation in the equation above.) Using the chain rule, the total derivative of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n is:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                A\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          v\n        \n        ⋅\n        ∇\n        )\n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {A} }{\\mathrm {d} t}}={\\frac {\\partial \\mathbf {A} }{\\partial t}}+(\\mathbf {v} \\cdot \\nabla )\\mathbf {A} }\n  \n\nso that the above expression becomes:\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            (\n            ϕ\n            −\n            \n              v\n            \n            ⋅\n            \n              A\n            \n            )\n            −\n            \n              \n                \n                  \n                    d\n                  \n                  \n                    A\n                  \n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla (\\phi -\\mathbf {v} \\cdot \\mathbf {A} )-{\\frac {\\mathrm {d} \\mathbf {A} }{\\mathrm {d} t}}\\right].}\n\nWith v = ẋ, we can put the equation into the convenient Euler–Lagrange form\n\nF\n        \n        =\n        q\n        \n          [\n          \n            −\n            \n              ∇\n              \n                \n                  x\n                \n              \n            \n            (\n            ϕ\n            −\n            \n              \n                \n                  \n                    x\n                  \n                  ˙\n                \n              \n            \n            ⋅\n            \n              A\n            \n            )\n            +\n            \n              \n                \n                  d\n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            \n              ∇\n              \n                \n                  \n                    \n                      x\n                    \n                    ˙\n                  \n                \n              \n            \n            (\n            ϕ\n            −\n            \n              \n                \n                  \n                    x\n                  \n                  ˙\n                \n              \n            \n            ⋅\n            \n              A\n            \n            )\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla _{\\mathbf {x} }(\\phi -{\\dot {\\mathbf {x} }}\\cdot \\mathbf {A} )+{\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\nabla _{\\dot {\\mathbf {x} }}(\\phi -{\\dot {\\mathbf {x} }}\\cdot \\mathbf {A} )\\right]}\n\nwhere \n  \n    \n      \n        \n          ∇\n          \n            \n              x\n            \n          \n        \n        =\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                x\n              \n            \n          \n        \n        +\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                y\n              \n            \n          \n        \n        +\n        \n          \n            \n              z\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                z\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {x} }={\\hat {x}}{\\dfrac {\\partial }{\\partial x}}+{\\hat {y}}{\\dfrac {\\partial }{\\partial y}}+{\\hat {z}}{\\dfrac {\\partial }{\\partial z}}}\n  \n and \n  \n    \n      \n        \n          ∇\n          \n            \n              \n                \n                  x\n                \n                ˙\n              \n            \n          \n        \n        =\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      x\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      y\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              z\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      z\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\nabla _{\\dot {\\mathbf {x} }}={\\hat {x}}{\\dfrac {\\partial }{\\partial {\\dot {x}}}}+{\\hat {y}}{\\dfrac {\\partial }{\\partial {\\dot {y}}}}+{\\hat {z}}{\\dfrac {\\partial }{\\partial {\\dot {z}}}}.}\n\nThe Lagrangian for a charged particle of mass m and charge q in an electromagnetic field equivalently describes the dynamics of the particle in terms of its energy, rather than the force exerted on it. The classical expression is given by:[37]\n\n  \n    \n      \n        L\n        =\n        \n          \n            m\n            2\n          \n        \n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        +\n        q\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        −\n        q\n        ϕ\n      \n    \n    {\\displaystyle L={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} +q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} -q\\phi }\n  \n\nwhere A and ϕ are the potential fields as above. The quantity \n  \n    \n      \n        V\n        =\n        q\n        (\n        ϕ\n        −\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle V=q(\\phi -\\mathbf {A} \\cdot \\mathbf {\\dot {r}} )}\n  \n can be thought as a velocity-dependent potential function.[38] Using Lagrange's equations, the equation for the Lorentz force given above can be obtained again.\n\nFor an A field, a particle moving with velocity v = ṙ has potential momentum \n  \n    \n      \n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle q\\mathbf {A} (\\mathbf {r} ,t)}\n  \n, so its potential energy is \n  \n    \n      \n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle q\\mathbf {A} (\\mathbf {r} ,t)\\cdot \\mathbf {\\dot {r}} }\n  \n. For a ϕ field, the particle's potential energy is \n  \n    \n      \n        q\n        ϕ\n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle q\\phi (\\mathbf {r} ,t)}\n  \n.\n\nThe total potential energy is then:\n\n  \n    \n      \n        V\n        =\n        q\n        ϕ\n        −\n        q\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle V=q\\phi -q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} }\n  \n\nand the kinetic energy is:\n\n  \n    \n      \n        T\n        =\n        \n          \n            m\n            2\n          \n        \n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle T={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} }\n  \n\nhence the Lagrangian:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n              \n                \n                =\n                T\n                −\n                V\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    m\n                    2\n                  \n                \n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ⋅\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                +\n                q\n                \n                  A\n                \n                ⋅\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                −\n                q\n                ϕ\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    m\n                    2\n                  \n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          \n                            x\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      \n                        \n                          \n                            y\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      \n                        \n                          \n                            z\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                +\n                q\n                \n                  (\n                  \n                    \n                      \n                        \n                          x\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        x\n                      \n                    \n                    +\n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        y\n                      \n                    \n                    +\n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        z\n                      \n                    \n                  \n                  )\n                \n                −\n                q\n                ϕ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}L&=T-V\\\\[1ex]&={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} +q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} -q\\phi \\\\[1ex]&={\\frac {m}{2}}\\left({\\dot {x}}^{2}+{\\dot {y}}^{2}+{\\dot {z}}^{2}\\right)+q\\left({\\dot {x}}A_{x}+{\\dot {y}}A_{y}+{\\dot {z}}A_{z}\\right)-q\\phi \\end{aligned}}}\n\nLagrange's equations are\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                \n                  \n                    x\n                    ˙\n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}={\\frac {\\partial L}{\\partial x}}}\n  \n\n(same for y and z). So calculating the partial derivatives:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      d\n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n                \n                  \n                    \n                      ∂\n                      L\n                    \n                    \n                      ∂\n                      \n                        \n                          \n                            x\n                            ˙\n                          \n                        \n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  \n                    \n                      \n                        d\n                      \n                      \n                        A\n                        \n                          x\n                        \n                      \n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          x\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          y\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          y\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          z\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          z\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    \n                      \n                        \n                          x\n                          ˙\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          y\n                        \n                      \n                    \n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          z\n                        \n                      \n                    \n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {\\mathrm {d} }{\\mathrm {d} t}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}&=m{\\ddot {x}}+q{\\frac {\\mathrm {d} A_{x}}{\\mathrm {d} t}}\\\\&=m{\\ddot {x}}+q\\left[{\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\frac {dx}{dt}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\frac {dy}{dt}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\frac {dz}{dt}}\\right]\\\\[1ex]&=m{\\ddot {x}}+q\\left[{\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\dot {y}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\dot {z}}\\right]\\\\\\end{aligned}}}\n  \n\n\n  \n    \n      \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              x\n            \n          \n        \n        =\n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              x\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      y\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      z\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\partial L}{\\partial x}}=-q{\\frac {\\partial \\phi }{\\partial x}}+q\\left({\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{y}}{\\partial x}}{\\dot {y}}+{\\frac {\\partial A_{z}}{\\partial x}}{\\dot {z}}\\right)}\n  \n\nequating and simplifying:\n\n  \n    \n      \n        m\n        \n          \n            \n              x\n              ¨\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  y\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  z\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n        =\n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              x\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      y\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      z\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle m{\\ddot {x}}+q\\left({\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\dot {y}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\dot {z}}\\right)=-q{\\frac {\\partial \\phi }{\\partial x}}+q\\left({\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{y}}{\\partial x}}{\\dot {y}}+{\\frac {\\partial A_{z}}{\\partial x}}{\\dot {z}}\\right)}\n  \n\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    x\n                  \n                \n              \n              \n                \n                =\n                −\n                q\n                \n                  (\n                  \n                    \n                      \n                        \n                          ∂\n                          ϕ\n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                  \n                  )\n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    \n                      (\n                      \n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  y\n                                \n                              \n                            \n                            \n                              ∂\n                              x\n                            \n                          \n                        \n                        −\n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  x\n                                \n                              \n                            \n                            \n                              ∂\n                              y\n                            \n                          \n                        \n                      \n                      )\n                    \n                    +\n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                    \n                      (\n                      \n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  z\n                                \n                              \n                            \n                            \n                              ∂\n                              x\n                            \n                          \n                        \n                        −\n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  x\n                                \n                              \n                            \n                            \n                              ∂\n                              z\n                            \n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                [\n                \n                  \n                    \n                      y\n                      ˙\n                    \n                  \n                \n                (\n                ∇\n                ×\n                \n                  A\n                \n                \n                  )\n                  \n                    z\n                  \n                \n                −\n                \n                  \n                    \n                      z\n                      ˙\n                    \n                  \n                \n                (\n                ∇\n                ×\n                \n                  A\n                \n                \n                  )\n                  \n                    y\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                [\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ×\n                (\n                ∇\n                ×\n                \n                  A\n                \n                )\n                \n                  ]\n                  \n                    x\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                (\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ×\n                \n                  B\n                \n                \n                  )\n                  \n                    x\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{x}&=-q\\left({\\frac {\\partial \\phi }{\\partial x}}+{\\frac {\\partial A_{x}}{\\partial t}}\\right)+q\\left[{\\dot {y}}\\left({\\frac {\\partial A_{y}}{\\partial x}}-{\\frac {\\partial A_{x}}{\\partial y}}\\right)+{\\dot {z}}\\left({\\frac {\\partial A_{z}}{\\partial x}}-{\\frac {\\partial A_{x}}{\\partial z}}\\right)\\right]\\\\[1ex]&=qE_{x}+q[{\\dot {y}}(\\nabla \\times \\mathbf {A} )_{z}-{\\dot {z}}(\\nabla \\times \\mathbf {A} )_{y}]\\\\[1ex]&=qE_{x}+q[\\mathbf {\\dot {r}} \\times (\\nabla \\times \\mathbf {A} )]_{x}\\\\[1ex]&=qE_{x}+q(\\mathbf {\\dot {r}} \\times \\mathbf {B} )_{x}\\end{aligned}}}\n  \n\nand similarly for the y and z directions. Hence the force equation is:\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        (\n        \n          E\n        \n        +\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {F} =q(\\mathbf {E} +\\mathbf {\\dot {r}} \\times \\mathbf {B} )}\n\nThe potential energy depends on the velocity of the particle, so the force is velocity dependent, so it is not conservative.\n\nThe relativistic Lagrangian is\n\n  \n    \n      \n        L\n        =\n        −\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          \n            1\n            −\n            \n              \n                (\n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ˙\n                      \n                    \n                    c\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        +\n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        )\n        ⋅\n        \n          \n            \n              \n                r\n              \n              ˙\n            \n          \n        \n        −\n        q\n        ϕ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle L=-mc^{2}{\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}+q\\mathbf {A} (\\mathbf {r} )\\cdot {\\dot {\\mathbf {r} }}-q\\phi (\\mathbf {r} )}\n\nThe action is the relativistic arclength of the path of the particle in spacetime, minus the potential energy contribution, plus an extra contribution which quantum mechanically is an extra phase a charged particle gets when it is moving along a vector potential.\n\nThe equations of motion derived by extremizing the action (see matrix calculus for the notation):\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                P\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        =\n        q\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              \n                r\n              \n              ˙\n            \n          \n        \n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {P} }{\\mathrm {d} t}}={\\frac {\\partial L}{\\partial \\mathbf {r} }}=q{\\partial \\mathbf {A}  \\over \\partial \\mathbf {r} }\\cdot {\\dot {\\mathbf {r} }}-q{\\partial \\phi  \\over \\partial \\mathbf {r} }}\n  \n\n\n  \n    \n      \n        \n          P\n        \n        −\n        q\n        \n          A\n        \n        =\n        \n          \n            \n              m\n              \n                \n                  \n                    \n                      r\n                    \n                    ˙\n                  \n                \n              \n            \n            \n              1\n              −\n              \n                \n                  (\n                  \n                    \n                      \n                        \n                          \n                            r\n                          \n                          ˙\n                        \n                      \n                      c\n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {P} -q\\mathbf {A} ={\\frac {m{\\dot {\\mathbf {r} }}}{\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}}}\n  \n\nare the same as Hamilton's equations of motion:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            ∂\n            \n              ∂\n              \n                p\n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                (\n                \n                  P\n                \n                −\n                q\n                \n                  A\n                \n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                (\n                m\n                \n                  c\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            +\n            q\n            ϕ\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {r} }{\\mathrm {d} t}}={\\frac {\\partial }{\\partial \\mathbf {p} }}\\left({\\sqrt {(\\mathbf {P} -q\\mathbf {A} )^{2}+(mc^{2})^{2}}}+q\\phi \\right)}\n  \n\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          \n            ∂\n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                (\n                \n                  P\n                \n                −\n                q\n                \n                  A\n                \n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                (\n                m\n                \n                  c\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            +\n            q\n            ϕ\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} t}}=-{\\frac {\\partial }{\\partial \\mathbf {r} }}\\left({\\sqrt {(\\mathbf {P} -q\\mathbf {A} )^{2}+(mc^{2})^{2}}}+q\\phi \\right)}\n  \n\nboth are equivalent to the noncanonical form:\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          \n            \n              m\n              \n                \n                  \n                    \n                      r\n                    \n                    ˙\n                  \n                \n              \n            \n            \n              \n                1\n                −\n                \n                  \n                    (\n                    \n                      \n                        \n                          \n                            \n                              r\n                            \n                            ˙\n                          \n                        \n                        c\n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}{m{\\dot {\\mathbf {r} }} \\over {\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}}=q\\left(\\mathbf {E} +{\\dot {\\mathbf {r} }}\\times \\mathbf {B} \\right).}\n  \n\nThis formula is the Lorentz force, representing the rate at which the EM field adds relativistic momentum to the particle.\n\nUsing the metric signature (1, −1, −1, −1), the Lorentz force for a charge q can be written in[39] covariant form:\n\nd\n              \n              \n                p\n                \n                  α\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          F\n          \n            α\n            β\n          \n        \n        \n          U\n          \n            β\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{\\alpha }}{\\mathrm {d} \\tau }}=qF^{\\alpha \\beta }U_{\\beta }}\n\nwhere pα is the four-momentum, defined as\n\n  \n    \n      \n        \n          p\n          \n            α\n          \n        \n        =\n        \n          (\n          \n            \n              p\n              \n                0\n              \n            \n            ,\n            \n              p\n              \n                1\n              \n            \n            ,\n            \n              p\n              \n                2\n              \n            \n            ,\n            \n              p\n              \n                3\n              \n            \n          \n          )\n        \n        =\n        \n          (\n          \n            γ\n            m\n            c\n            ,\n            \n              p\n              \n                x\n              \n            \n            ,\n            \n              p\n              \n                y\n              \n            \n            ,\n            \n              p\n              \n                z\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle p^{\\alpha }=\\left(p_{0},p_{1},p_{2},p_{3}\\right)=\\left(\\gamma mc,p_{x},p_{y},p_{z}\\right),}\n  \n\nτ the proper time of the particle, Fαβ the contravariant electromagnetic tensor\n\n  \n    \n      \n        \n          F\n          \n            α\n            β\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  −\n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n              \n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n                \n                  0\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle F^{\\alpha \\beta }={\\begin{pmatrix}0&-E_{x}/c&-E_{y}/c&-E_{z}/c\\\\E_{x}/c&0&-B_{z}&B_{y}\\\\E_{y}/c&B_{z}&0&-B_{x}\\\\E_{z}/c&-B_{y}&B_{x}&0\\end{pmatrix}}}\n  \n\nand U is the covariant 4-velocity of the particle, defined as:\n\n  \n    \n      \n        \n          U\n          \n            β\n          \n        \n        =\n        \n          (\n          \n            \n              U\n              \n                0\n              \n            \n            ,\n            \n              U\n              \n                1\n              \n            \n            ,\n            \n              U\n              \n                2\n              \n            \n            ,\n            \n              U\n              \n                3\n              \n            \n          \n          )\n        \n        =\n        γ\n        \n          (\n          \n            c\n            ,\n            −\n            \n              v\n              \n                x\n              \n            \n            ,\n            −\n            \n              v\n              \n                y\n              \n            \n            ,\n            −\n            \n              v\n              \n                z\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle U_{\\beta }=\\left(U_{0},U_{1},U_{2},U_{3}\\right)=\\gamma \\left(c,-v_{x},-v_{y},-v_{z}\\right),}\n  \n\nin which\n\n  \n    \n      \n        γ\n        (\n        v\n        )\n        =\n        \n          \n            1\n            \n              1\n              −\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              1\n              −\n              \n                \n                  \n                    \n                      v\n                      \n                        x\n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        y\n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        z\n                      \n                      \n                        2\n                      \n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma (v)={\\frac {1}{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}={\\frac {1}{\\sqrt {1-{\\frac {v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}{c^{2}}}}}}}\n  \n\nis the Lorentz factor.\n\nThe fields are transformed to a frame moving with constant relative velocity by:\n\n  \n    \n      \n        \n          F\n          \n            ′\n            \n              μ\n              ν\n            \n          \n        \n        =\n        \n          \n            \n              Λ\n              \n                μ\n              \n            \n          \n          \n            α\n          \n        \n        \n          \n            \n              Λ\n              \n                ν\n              \n            \n          \n          \n            β\n          \n        \n        \n          F\n          \n            α\n            β\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle F'^{\\mu \\nu }={\\Lambda ^{\\mu }}_{\\alpha }{\\Lambda ^{\\nu }}_{\\beta }F^{\\alpha \\beta }\\,,}\n  \n\nwhere Λμα is the Lorentz transformation tensor.\n\nThe α = 1 component (x-component) of the force is\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          U\n          \n            β\n          \n        \n        \n          F\n          \n            1\n            β\n          \n        \n        =\n        q\n        \n          (\n          \n            \n              U\n              \n                0\n              \n            \n            \n              F\n              \n                10\n              \n            \n            +\n            \n              U\n              \n                1\n              \n            \n            \n              F\n              \n                11\n              \n            \n            +\n            \n              U\n              \n                2\n              \n            \n            \n              F\n              \n                12\n              \n            \n            +\n            \n              U\n              \n                3\n              \n            \n            \n              F\n              \n                13\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=qU_{\\beta }F^{1\\beta }=q\\left(U_{0}F^{10}+U_{1}F^{11}+U_{2}F^{12}+U_{3}F^{13}\\right).}\n\nSubstituting the components of the covariant electromagnetic tensor F yields\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          [\n          \n            \n              U\n              \n                0\n              \n            \n            \n              (\n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  c\n                \n              \n              )\n            \n            +\n            \n              U\n              \n                2\n              \n            \n            (\n            −\n            \n              B\n              \n                z\n              \n            \n            )\n            +\n            \n              U\n              \n                3\n              \n            \n            (\n            \n              B\n              \n                y\n              \n            \n            )\n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=q\\left[U_{0}\\left({\\frac {E_{x}}{c}}\\right)+U_{2}(-B_{z})+U_{3}(B_{y})\\right].}\n\nUsing the components of covariant four-velocity yields\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        γ\n        \n          [\n          \n            c\n            \n              (\n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  c\n                \n              \n              )\n            \n            +\n            (\n            −\n            \n              v\n              \n                y\n              \n            \n            )\n            (\n            −\n            \n              B\n              \n                z\n              \n            \n            )\n            +\n            (\n            −\n            \n              v\n              \n                z\n              \n            \n            )\n            (\n            \n              B\n              \n                y\n              \n            \n            )\n          \n          ]\n        \n        =\n        q\n        γ\n        \n          (\n          \n            \n              E\n              \n                x\n              \n            \n            +\n            \n              v\n              \n                y\n              \n            \n            \n              B\n              \n                z\n              \n            \n            −\n            \n              v\n              \n                z\n              \n            \n            \n              B\n              \n                y\n              \n            \n          \n          )\n        \n        =\n        q\n        γ\n        \n          [\n          \n            \n              E\n              \n                x\n              \n            \n            +\n            \n              \n                (\n                \n                  \n                    v\n                  \n                  ×\n                  \n                    B\n                  \n                \n                )\n              \n              \n                x\n              \n            \n          \n          ]\n        \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=q\\gamma \\left[c\\left({\\frac {E_{x}}{c}}\\right)+(-v_{y})(-B_{z})+(-v_{z})(B_{y})\\right]=q\\gamma \\left(E_{x}+v_{y}B_{z}-v_{z}B_{y}\\right)=q\\gamma \\left[E_{x}+\\left(\\mathbf {v} \\times \\mathbf {B} \\right)_{x}\\right]\\,.}\n\nThe calculation for α = 2, 3 (force components in the y and z directions) yields similar results, so collecting the three equations into one:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        γ\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} \\tau }}=q\\gamma \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right),}\n  \n\nand since differentials in coordinate time dt and proper time dτ are related by the Lorentz factor,\n\n  \n    \n      \n        d\n        t\n        =\n        γ\n        (\n        v\n        )\n        \n        d\n        τ\n        ,\n      \n    \n    {\\displaystyle dt=\\gamma (v)\\,d\\tau ,}\n  \n\nso we arrive at\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} t}}=q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right).}\n\nThis is precisely the Lorentz force law, however, it is important to note that p is the relativistic expression,\n\n  \n    \n      \n        \n          p\n        \n        =\n        γ\n        (\n        v\n        )\n        \n          m\n          \n            0\n          \n        \n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} =\\gamma (v)m_{0}\\mathbf {v} \\,.}\n\nThe electric and magnetic fields are dependent on the velocity of an observer, so the relativistic form of the Lorentz force law can best be exhibited starting from a coordinate-independent expression for the electromagnetic and magnetic fields \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n, and an arbitrary time-direction, \n  \n    \n      \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{0}}\n  \n. This can be settled through spacetime algebra (or the geometric algebra of spacetime), a type of Clifford algebra defined on a pseudo-Euclidean space,[40] as\n\n  \n    \n      \n        \n          E\n        \n        =\n        \n          (\n          \n            \n              \n                F\n              \n            \n            ⋅\n            \n              γ\n              \n                0\n              \n            \n          \n          )\n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} =\\left({\\mathcal {F}}\\cdot \\gamma _{0}\\right)\\gamma _{0}}\n  \n\nand\n\n  \n    \n      \n        i\n        \n          B\n        \n        =\n        \n          (\n          \n            \n              \n                F\n              \n            \n            ∧\n            \n              γ\n              \n                0\n              \n            \n          \n          )\n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle i\\mathbf {B} =\\left({\\mathcal {F}}\\wedge \\gamma _{0}\\right)\\gamma _{0}}\n  \n\n\n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n is a spacetime bivector (an oriented plane segment, just like a vector is an oriented line segment), which has six degrees of freedom corresponding to boosts (rotations in spacetime planes) and rotations (rotations in space-space planes). The dot product with the vector \n  \n    \n      \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{0}}\n  \n pulls a vector (in the space algebra) from the translational part, while the wedge-product creates a trivector (in the space algebra) who is dual to a vector which is the usual magnetic field vector. The relativistic velocity is given by the (time-like) changes in a time-position vector \n  \n    \n      \n        v\n        =\n        \n          \n            \n              x\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle v={\\dot {x}}}\n  \n, where\n\n  \n    \n      \n        \n          v\n          \n            2\n          \n        \n        =\n        1\n        ,\n      \n    \n    {\\displaystyle v^{2}=1,}\n  \n\n(which shows our choice for the metric) and the velocity is\n\n  \n    \n      \n        \n          v\n        \n        =\n        c\n        v\n        ∧\n        \n          γ\n          \n            0\n          \n        \n        \n          /\n        \n        (\n        v\n        ⋅\n        \n          γ\n          \n            0\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {v} =cv\\wedge \\gamma _{0}/(v\\cdot \\gamma _{0}).}\n\nThe proper form of the Lorentz force law ('invariant' is an inadequate term because no transformation has been defined) is simply\n\nF\n        =\n        q\n        \n          \n            F\n          \n        \n        ⋅\n        v\n      \n    \n    {\\displaystyle F=q{\\mathcal {F}}\\cdot v}\n\nNote that the order is important because between a bivector and a vector the dot product is anti-symmetric. Upon a spacetime split like one can obtain the velocity, and fields as above yielding the usual expression.\n\nIn the general theory of relativity the equation of motion for a particle with mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n and charge \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n, moving in a space with metric tensor \n  \n    \n      \n        \n          g\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle g_{ab}}\n  \n and electromagnetic field \n  \n    \n      \n        \n          F\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle F_{ab}}\n  \n, is given as\n\n  \n    \n      \n        m\n        \n          \n            \n              d\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        −\n        m\n        \n          \n            1\n            2\n          \n        \n        \n          g\n          \n            a\n            b\n            ,\n            c\n          \n        \n        \n          u\n          \n            a\n          \n        \n        \n          u\n          \n            b\n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {du_{c}}{ds}}-m{\\frac {1}{2}}g_{ab,c}u^{a}u^{b}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        \n          u\n          \n            a\n          \n        \n        =\n        d\n        \n          x\n          \n            a\n          \n        \n        \n          /\n        \n        d\n        s\n      \n    \n    {\\displaystyle u^{a}=dx^{a}/ds}\n  \n (\n  \n    \n      \n        d\n        \n          x\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle dx^{a}}\n  \n is taken along the trajectory), \n  \n    \n      \n        \n          g\n          \n            a\n            b\n            ,\n            c\n          \n        \n        =\n        ∂\n        \n          g\n          \n            a\n            b\n          \n        \n        \n          /\n        \n        ∂\n        \n          x\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle g_{ab,c}=\\partial g_{ab}/\\partial x^{c}}\n  \n, and \n  \n    \n      \n        d\n        \n          s\n          \n            2\n          \n        \n        =\n        \n          g\n          \n            a\n            b\n          \n        \n        d\n        \n          x\n          \n            a\n          \n        \n        d\n        \n          x\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle ds^{2}=g_{ab}dx^{a}dx^{b}}\n  \n.\n\nThe equation can also be written as\n\n  \n    \n      \n        m\n        \n          \n            \n              d\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        −\n        m\n        \n          Γ\n          \n            a\n            b\n            c\n          \n        \n        \n          u\n          \n            a\n          \n        \n        \n          u\n          \n            b\n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {du_{c}}{ds}}-m\\Gamma _{abc}u^{a}u^{b}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        \n          Γ\n          \n            a\n            b\n            c\n          \n        \n      \n    \n    {\\displaystyle \\Gamma _{abc}}\n  \n is the Christoffel symbol (of the torsion-free metric connection in general relativity), or as\n\n  \n    \n      \n        m\n        \n          \n            \n              D\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {Du_{c}}{ds}}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the covariant differential in general relativity (metric, torsion-free).\n\nThe Lorentz force occurs in many devices, including:\n\nIn its manifestation as the Laplace force on an electric current in a conductor, this force occurs in many devices, including:\n\nThe numbered references refer in part to the list immediately below.",
        pageTitle: "Lorentz force",
    },
    {
        title: "Lotka's law",
        link: "https://en.wikipedia.org/wiki/Lotka%27s_law",
        content:
            "Lotka's law,[1] named after Alfred J. Lotka, is one of a variety of special applications of Zipf's law.  It describes the frequency of publication by authors in any given field.\n\nLet \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n be the number of publications, \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  \n be the number of authors with \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n publications, and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n be a constants depending on the specific field. Lotka's law states that \n  \n    \n      \n        Y\n        ∝\n        \n          X\n          \n            −\n            k\n          \n        \n      \n    \n    {\\displaystyle Y\\propto X^{-k}}\n  \n.\n\nIn Lotka's original publication, he claimed \n  \n    \n      \n        k\n        =\n        2\n      \n    \n    {\\displaystyle k=2}\n  \n. Subsequent research showed that \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n varies depending on the discipline.\n\nEquivalently, Lotka's law can be stated as \n  \n    \n      \n        \n          Y\n          ′\n        \n        ∝\n        \n          X\n          \n            −\n            (\n            k\n            −\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle Y'\\propto X^{-(k-1)}}\n  \n, where \n  \n    \n      \n        \n          Y\n          ′\n        \n      \n    \n    {\\displaystyle Y'}\n  \n is the number of authors with at least \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n publications. Their equivalence can be proved by taking the derivative.\n\nAssume that n=2 in a discipline, then as the number of articles published increases, authors producing that many publications become less frequent. There are 1/4 as many authors publishing two articles within a specified time period as there are single-publication authors, 1/9 as many publishing three articles, 1/16 as many publishing four articles, etc.\n\nAnd if 100 authors wrote exactly one article each over a specific period in the discipline, then:\n\nThat would be a total of 294 articles and 155 writers, with an average of 1.9 articles for each writer.\n\nLotka's law may be described using the Zeta distribution:\n\nfor \n  \n    \n      \n        x\n        =\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n        ,\n        …\n      \n    \n    {\\displaystyle x=1,2,3,4,\\dots }\n  \n and where\n\nis the Riemann zeta function. It is the limiting case of Zipf's law where an individual's maximum number of publications is infinite.",
        pageTitle: "Lotka's law",
    },
    {
        title: "Maes–Garreau law",
        link: "https://en.wikipedia.org/wiki/Maes%E2%80%93Garreau_law",
        content:
            'The Maes–Garreau law is the statement that "most favorable predictions about future technology will fall within the Maes–Garreau point", defined as "the latest possible date a prediction can come true and still remain in the lifetime of the person making it".[1] Specifically, it relates to predictions of a technological singularity or other radical future technologies.[1]\n\nIt has been referred to as a "law of human nature",[2] although Kelly\'s evidence is anecdotal.\n\nKevin Kelly, editor of Wired magazine, created the law in 2007 after being influenced by Pattie Maes at MIT and Joel Garreau (author of Radical Evolution).[1]\n\nIn 1993, Maes listed a number of her colleagues at MIT that had publicly predicted mind uploading (the replication of a human brain on a computer), and noted that the innovations were generally slated to occur approximately 70 years after the birth of the predictor. As quoted by her colleague Rodney Brooks:[1]\n\nShe took as many people as she could find who had publicly predicted downloading of consciousness into silicon, and plotted the dates of their predictions, along with when they themselves would turn seventy years old. Not too surprisingly, the years matched up for each of them. Three score and ten years from their individual births, technology would be ripe for them to download their consciousnesses into a computer. Just in the nick of time! They were each, in their own minds, going to be remarkably lucky, to be in just the right place at the right time.\n\nThe Machine Intelligence Research Institute released a paper detailing a much larger set of 95 AI predictions extracted from a database of 257 AI predictions, which finds a broad array of estimates significantly before and after a predictor\'s estimated longevity, thus contradicting the law. MIRI states a stronger rule as being "Maes-Garreau": "the predictor expects AI to be developed at the exact end of their life."[3]\n\nThis article about futures studies is a stub. You can help Wikipedia by expanding it.',
        pageTitle: "Maes–Garreau law",
    },
    {
        title: "Marconi's law",
        link: "https://en.wikipedia.org/wiki/Marconi%27s_law",
        content:
            'Radio propagation is the behavior of radio waves as they travel, or are propagated, from one point to another in vacuum, or into various parts of the atmosphere.[1]: 26‑1  As a form of electromagnetic radiation, like light waves, radio waves are affected by the phenomena of reflection, refraction, diffraction, absorption, polarization, and scattering.[2] Understanding the effects of varying conditions on radio propagation has many practical applications, from choosing frequencies for amateur radio communications, international shortwave broadcasters, to designing reliable mobile telephone systems, to radio navigation, to operation of radar systems.\n\nSeveral different types of propagation are used in practical radio transmission systems. Line-of-sight propagation means radio waves which travel in a straight line from the transmitting antenna to the receiving antenna. Line of sight transmission is used for medium-distance radio transmission, such as cell phones, cordless phones, walkie-talkies, wireless networks, FM radio, television broadcasting, radar, and satellite communication (such as satellite television). Line-of-sight transmission on the surface of the Earth is limited to the distance to the visual horizon, which depends on the height of transmitting and receiving antennas. It is the only propagation method possible at microwave frequencies and above.[a]\n\nAt lower frequencies in the MF, LF, and VLF bands, diffraction allows radio waves to bend over hills and other obstacles, and travel beyond the horizon, following the contour of the Earth. These are called surface waves or ground wave propagation. AM broadcast and amateur radio stations use ground waves to cover their listening areas. As the frequency gets lower, the attenuation with distance decreases, so very low frequency (VLF) to extremely low frequency (ELF) ground waves can be used to communicate worldwide. VLF to ELF waves can penetrate significant distances through water and earth, and these frequencies are used for mine communication and military communication with submerged submarines.\n\nAt medium wave and shortwave frequencies (MF and HF bands), radio waves can refract from the ionosphere, a layer of charged particles (ions) high in the atmosphere. This means that medium and short radio waves transmitted at an angle into the sky can be refracted back to Earth at great distances beyond the horizon – even transcontinental distances. This is called skywave propagation. It is used by amateur radio operators to communicate with operators in distant countries, and by shortwave broadcast stations to transmit internationally.[b]\n\nIn addition, there are several less common radio propagation mechanisms, such as tropospheric scattering (troposcatter), tropospheric ducting (ducting) at VHF frequencies and near vertical incidence skywave (NVIS) which are used when HF communications are desired within a few hundred miles.\n\nAt different frequencies, radio waves travel through the atmosphere by different mechanisms or modes:[3]\n\nE, F layer ionospheric refraction at night, when D layer absorption weakens.\n\nInfrequent E ionospheric (Es) refraction. Uncommonly F2 layer ionospheric refraction during high sunspot activity up to 50 MHz and rarely to 80 MHz. Sometimes tropospheric ducting or meteor scatter\n\nIn free space, all electromagnetic waves (radio, light, X-rays, etc.) obey the inverse-square law which states that the power density \n  \n    \n      \n        ρ\n        \n      \n    \n    {\\displaystyle \\rho \\,}\n  \n of an electromagnetic wave is proportional to the inverse of the square of the distance \n  \n    \n      \n        r\n        \n      \n    \n    {\\displaystyle r\\,}\n  \n from a point source[1]: 26‑19  or:\n\nAt typical communication distances from a transmitter, the transmitting antenna usually can be approximated by a point source. Doubling the distance of a receiver from a transmitter means that the power density of the radiated wave at that new location is reduced to one-quarter of its previous value.\n\nThe power density per surface unit is proportional to the product of the electric and magnetic field strengths. Thus, doubling the propagation path distance from the transmitter reduces each of these received field strengths over a free-space path by one-half.\n\nRadio waves in vacuum travel at the speed of light. The Earth\'s atmosphere is thin enough that radio waves in the atmosphere travel very close to the speed of light, but variations in density and temperature can cause some slight refraction (bending) of waves over distances.\n\nLine-of-sight refers to radio waves which travel directly in a line from the transmitting antenna to the receiving antenna, often also called direct-wave. It does not necessarily require a cleared sight path; at lower frequencies radio waves can pass through buildings, foliage and other obstructions. This is the most common propagation mode at VHF and above, and the only possible mode at microwave frequencies and above. On the surface of the Earth, line of sight propagation is limited by the visual horizon to about 40 miles (64 km). This is the method used by cell phones,[c] cordless phones, walkie-talkies, wireless networks, point-to-point microwave radio relay links, FM and television broadcasting and radar. Satellite communication uses longer line-of-sight paths; for example home satellite dishes receive signals from communication satellites 22,000 miles (35,000 km) above the Earth, and ground stations can communicate with spacecraft billions of miles from Earth.\n\nGround plane reflection effects are an important factor in VHF line-of-sight propagation. The interference between the direct beam line-of-sight and the ground reflected beam often leads to an effective inverse-fourth-power (.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄distance4) law for ground-plane limited radiation.[citation needed]\n\nLower frequency (between 30 and 3,000 kHz) vertically polarized radio waves can travel as surface waves following the contour of the Earth; this is called ground wave propagation.\n\nIn this mode the radio wave propagates by interacting with the conductive surface of the Earth. The wave "clings" to the surface and thus follows the curvature of the Earth, so ground waves can travel over mountains and beyond the horizon. Ground waves propagate in vertical polarization so vertical antennas (monopoles) are required. Since the ground is not a perfect electrical conductor, ground waves are attenuated as they follow the Earth\'s surface. Attenuation is proportional to frequency, so ground waves are the main mode of propagation at lower frequencies, in the MF, LF and VLF bands. Ground waves are used by radio broadcasting stations in the MF and LF bands, and for time signals and radio navigation systems.\n\nAt even lower frequencies, in the VLF to ELF bands, an Earth-ionosphere waveguide mechanism allows even longer range transmission. These frequencies are used for secure military communications. They can also penetrate to a significant depth into seawater, and so are used for one-way military communication to submerged submarines.\n\nEarly long-distance radio communication (wireless telegraphy) before the mid-1920s used low frequencies in the longwave bands and relied exclusively on ground-wave propagation. Frequencies above 3 MHz were regarded as useless and were given to hobbyists (radio amateurs). The discovery around 1920 of the ionospheric reflection or skywave mechanism made the medium wave and short wave frequencies useful for long-distance communication and they were allocated to commercial and military users.[9]\n\nNon-line-of-sight (NLOS) radio propagation occurs outside of the typical line-of-sight (LOS) between the transmitter and receiver, such as in ground reflections. \nNear-line-of-sight (also NLOS) conditions refer to partial obstruction by a physical object present in the innermost Fresnel zone.\n\nObstacles that commonly cause NLOS propagation include buildings, trees, hills, mountains, and, in some cases, high voltage electric power lines.  Some of these obstructions reflect certain radio frequencies, while some simply absorb or garble the signals; but, in either case, they limit the use of many types of radio transmissions, especially when low on power budget.\n\nLower power levels at a receiver reduce the chance of successfully receiving a transmission. Low levels can be caused by at least three basic reasons: low transmit level, for example Wi-Fi power levels; far-away transmitter, such as 3G more than 5 miles (8.0 km) away or TV more than 31 miles (50 km) away; and obstruction between the transmitter and the receiver, leaving no clear path.\n\nNLOS lowers the effective received power. Near Line Of Sight can usually be dealt with using better antennas, but Non Line Of Sight usually requires alternative paths or multipath propagation methods.\n\nHow to achieve effective NLOS networking has become one of the major questions of modern computer networking.  Currently, the most common method for dealing with NLOS conditions on wireless computer networks is simply to circumvent the NLOS condition and place relays at additional locations, sending the content of the radio transmission around the obstructions.  Some more advanced NLOS transmission schemes now use multipath signal propagation, bouncing the radio signal off other nearby objects to get to the receiver.\n\nNon-Line-of-Sight (NLOS) is a term often used in radio communications to describe a radio channel or link where there is no visual line of sight (LOS) between the transmitting antenna and the receiving antenna. In this context LOS is taken\n\nThere are many electrical characteristics of the transmission media that affect the radio wave propagation and therefore the quality of operation of a  radio channel, if it is possible at all, over an NLOS path.\n\nThe acronym NLOS has become more popular in the context of wireless local area networks (WLANs) and wireless metropolitan area networks such as WiMAX because the capability of such links to provide a reasonable level of NLOS coverage greatly improves their marketability and versatility in the typical urban environments where they are most frequently used. However NLOS contains many other subsets of radio communications.\n\nThe influence of a visual obstruction on a NLOS link may be anything from negligible to complete suppression. An example might apply to a LOS  path between a television broadcast antenna and a roof mounted receiving antenna. If a cloud passed between the antennas the link could actually become NLOS but the quality of the radio channel could be virtually unaffected. If, instead, a large building was constructed in the path making it NLOS, the channel may be impossible to receive.\n\nHF propagation conditions can be simulated using radio propagation models, such as the Voice of America Coverage Analysis Program, and realtime measurements can be done using chirp transmitters. For radio amateurs the WSPR mode provides maps with real time propagation conditions between a network of transmitters and receivers.[10] Even without special beacons the realtime propagation conditions can be measured: A worldwide network of receivers decodes morse code signals on amateur radio frequencies in realtime and provides sophisticated search functions and propagation maps for every station received.[11]\n\nThe average person can notice the effects of changes in radio propagation in several ways.\n\nIn AM broadcasting, the dramatic ionospheric changes that occur overnight in the mediumwave band drive a unique broadcast license scheme in the United States, with entirely different transmitter power output levels and directional antenna patterns to cope with skywave propagation at night. Very few stations are allowed to run without modifications during dark hours, typically only those on clear channels in North America.[12] Many stations have no authorization to run at all outside of daylight hours.\n\nFor FM broadcasting (and the few remaining low-band TV stations), weather is the primary cause for changes in VHF propagation, along with some diurnal changes when the sky is mostly without cloud cover.[13] These changes are most obvious during temperature inversions, such as in the late-night and early-morning hours when it is clear, allowing the ground and the air near it to cool more rapidly. This not only causes dew, frost, or fog, but also causes a slight "drag" on the bottom of the radio waves, bending the signals down such that they can follow the Earth\'s curvature over the normal radio horizon. The result is typically several stations being heard from another media market – usually a neighboring one, but sometimes ones from a few hundred kilometers (miles) away.  Ice storms are also the result of inversions, but these normally cause more scattered omnidirection propagation, resulting mainly in interference, often among weather radio stations. In late spring and early summer, a combination of other atmospheric factors can occasionally cause skips that duct high-power signals to places well over 1000 km (600 miles) away.\n\nNon-broadcast signals are also affected. Mobile phone signals are in the UHF band, ranging from 700 to over 2600 MHz, a range which makes them even more prone to weather-induced propagation changes. In urban (and to some extent suburban) areas with a high population density, this is partly offset by the use of smaller cells, which use lower effective radiated power and beam tilt to reduce interference, and therefore increase frequency reuse and user capacity. However, since this would not be very cost-effective in more rural areas, these cells are larger and so more likely to cause interference over longer distances when propagation conditions allow.\n\nWhile this is generally transparent to the user thanks to the way that cellular networks handle cell-to-cell handoffs, when cross-border signals are involved, unexpected charges for international roaming may occur despite not having left the country at all. This often occurs between southern San Diego and northern Tijuana at the western end of the U.S./Mexico border, and between eastern Detroit and western  Windsor along the U.S./Canada border. Since signals can travel unobstructed over a body of water far larger than the Detroit River, and cool water temperatures also cause inversions in surface air, this "fringe roaming" sometimes occurs across the Great Lakes, and between islands in the Caribbean. Signals can skip from the Dominican Republic to a mountainside in Puerto Rico and vice versa, or between the U.S. and British Virgin Islands, among others. While unintended cross-border roaming is often automatically removed by mobile phone company billing systems, inter-island roaming is typically not.\n\nA radio propagation model, also known as the radio wave propagation model or the radio frequency propagation model, is an empirical mathematical formulation for the characterization of radio wave propagation as a function of frequency, distance and other conditions. A single model is usually developed to predict the behavior of propagation for all similar links under similar constraints. Created with the goal of formalizing the way radio waves are propagated from one place to another, such models typically predict the path loss along a link or the effective coverage area of a transmitter.\n\nThe inventor of radio communication, Guglielmo Marconi, before 1900 formulated the first crude empirical rule of radio propagation: the maximum  transmission distance varied as the square of the height of the antenna.\n\nAs the path loss encountered along any radio link serves as the dominant factor for characterization of propagation for the link, radio propagation models typically focus on realization of the path loss with the auxiliary task of predicting the area of coverage for a transmitter or modeling the distribution of signals over different regions.\n\nBecause each individual telecommunication link has to encounter different terrain, path, obstructions, atmospheric conditions and other phenomena, it is intractable to formulate the exact loss for all telecommunication systems in a single mathematical equation. As a result, different models exist for different types of radio links under different conditions. The models rely on computing the median path loss for a link under a certain probability that the considered conditions will occur.\n\nRadio propagation models are empirical in nature, which means, they are developed based on large collections of data collected for the specific scenario. For any model, the collection of data has to be sufficiently large to provide enough likeliness (or enough scope) to all kind of situations that can happen in that specific scenario. Like all empirical models, radio propagation models do not point out the exact behavior of a link, rather, they predict the most likely behavior the link may exhibit under the specified conditions.\n\nDifferent models have been developed to meet the needs of realizing the propagation behavior in different conditions. Types of models for radio propagation include:',
        pageTitle: "Radio propagation",
    },
    {
        title: "Lorentz force law",
        link: "https://en.wikipedia.org/wiki/Lorentz_force_law",
        content:
            "In physics, specifically in electromagnetism, the Lorentz force law is the combination of electric and magnetic force on a point charge due to electromagnetic fields. The Lorentz force, on the other hand, is a physical effect that occurs in the vicinity of electrically neutral, current-carrying conductors causing moving electrical charges to experience a magnetic force.\n\nThe Lorentz force law states that a particle of charge q moving with a velocity v in an electric field E and a magnetic field B experiences a force (in SI units[1][2]) of\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right).}\n  \n\nIt says that the electromagnetic force on a charge q is a combination of (1) a force in the direction of the electric field E (proportional to the magnitude of the field and the quantity of charge), and (2) a force at right angles to both the magnetic field B and the velocity v of the charge (proportional to the magnitude of the field, the charge, and the velocity).\n\nVariations on this basic formula describe the magnetic force on a current-carrying wire (sometimes called Laplace force), the electromotive force in a wire loop moving through a magnetic field (an aspect of Faraday's law of induction), and the force on a moving charged particle.[3]\n\nHistorians suggest that the law is implicit in a paper by James Clerk Maxwell, published in 1865.[4] Hendrik Lorentz arrived at a complete derivation in 1895,[5] identifying the contribution of the electric force a few years after Oliver Heaviside correctly identified the contribution of the magnetic force.[6]\n\nIn many textbook treatments of classical electromagnetism, the Lorentz force law is used as the definition of the electric and magnetic fields E and B.[7][8][9] To be specific, the Lorentz force is understood to be the following empirical statement:\n\nThe electromagnetic force F on a test charge at a given point and time is a certain function of its charge q and velocity v, which can be parameterized by exactly two vectors E and B, in the functional form: \n  \n    \n      \n        \n          F\n        \n        =\n        q\n        (\n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {F} =q(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} )}\n\nThis is valid, even for particles approaching the speed of light (that is, magnitude of v, |v| ≈ c).[10] So the two vector fields E and B are thereby defined throughout space and time, and these are called the \"electric field\" and \"magnetic field\". The fields are defined everywhere in space and time with respect to what force a test charge would receive regardless of whether a charge is present to experience the force.\n\nCoulomb's law is only valid for point charges at rest. In fact, the electromagnetic force between two point charges depends not only on the distance but also on the relative velocity. For small relative velocities and very small accelerations, instead of the Coulomb force, the Weber force can be applied. The sum of the Weber forces of all charge carriers in a closed DC loop on a single test charge produces – regardless of the shape of the current loop – the Lorentz force.\n\nThe interpretation of magnetism by means of a modified Coulomb law was first proposed by Carl Friedrich Gauss. In 1835, Gauss assumed that each segment of a DC loop contains an equal number of negative and positive point charges that move at different speeds.[11] If Coulomb's law were completely correct, no force should act between any two short segments of such current loops. However, around 1825, André-Marie Ampère demonstrated experimentally that this is not the case. Ampère also formulated a force law. Based on this law, Gauss concluded that the electromagnetic force between two point charges depends not only on the distance but also on the relative velocity.\n\nThe Weber force is a central force and complies with Newton's third law. This demonstrates not only the conservation of momentum but also that the conservation of energy and the conservation of angular momentum apply. Weber electrodynamics is only a quasistatic approximation, i.e. it should not be used for higher velocities and accelerations. However, the Weber force illustrates that the Lorentz force can be traced back to central forces between numerous point-like charge carriers.\n\nThe force F acting on a particle of electric charge q with instantaneous velocity v, due to an external electric field E and magnetic field B, is given by (SI definition of quantities[1]):[12]\n\nF\n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n\nwhere × is the vector cross product (all boldface quantities are vectors). In terms of Cartesian components, we have:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    x\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        x\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        y\n                      \n                    \n                    \n                      B\n                      \n                        z\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        z\n                      \n                    \n                    \n                      B\n                      \n                        y\n                      \n                    \n                  \n                  )\n                \n                ,\n              \n            \n            \n              \n                \n                  F\n                  \n                    y\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        y\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        z\n                      \n                    \n                    \n                      B\n                      \n                        x\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        x\n                      \n                    \n                    \n                      B\n                      \n                        z\n                      \n                    \n                  \n                  )\n                \n                ,\n              \n            \n            \n              \n                \n                  F\n                  \n                    z\n                  \n                \n              \n              \n                \n                =\n                q\n                \n                  (\n                  \n                    \n                      E\n                      \n                        z\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        x\n                      \n                    \n                    \n                      B\n                      \n                        y\n                      \n                    \n                    −\n                    \n                      v\n                      \n                        y\n                      \n                    \n                    \n                      B\n                      \n                        x\n                      \n                    \n                  \n                  )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{x}&=q\\left(E_{x}+v_{y}B_{z}-v_{z}B_{y}\\right),\\\\[0.5ex]F_{y}&=q\\left(E_{y}+v_{z}B_{x}-v_{x}B_{z}\\right),\\\\[0.5ex]F_{z}&=q\\left(E_{z}+v_{x}B_{y}-v_{y}B_{x}\\right).\\end{aligned}}}\n\nIn general, the electric and magnetic fields are functions of the position and time. Therefore, explicitly, the Lorentz force can be written as:\n\n  \n    \n      \n        \n          F\n        \n        \n          (\n          \n            \n              r\n            \n            (\n            t\n            )\n            ,\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            (\n            t\n            )\n            ,\n            t\n            ,\n            q\n          \n          )\n        \n        =\n        q\n        \n          [\n          \n            \n              E\n            \n            (\n            \n              r\n            \n            ,\n            t\n            )\n            +\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            (\n            t\n            )\n            ×\n            \n              B\n            \n            (\n            \n              r\n            \n            ,\n            t\n            )\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbf {F} \\left(\\mathbf {r} (t),{\\dot {\\mathbf {r} }}(t),t,q\\right)=q\\left[\\mathbf {E} (\\mathbf {r} ,t)+{\\dot {\\mathbf {r} }}(t)\\times \\mathbf {B} (\\mathbf {r} ,t)\\right]}\n  \n\nin which r is the position vector of the charged particle, t is time, and the overdot is a time derivative.\n\nA positively charged particle will be accelerated in the same linear orientation as the E field, but will curve perpendicularly to both the instantaneous velocity vector v and the B field according to the right-hand rule (in detail, if the fingers of the right hand are extended to point in the direction of v and are then curled to point in the direction of B, then the extended thumb will point in the direction of F).\n\nThe term qE is called the electric force, while the term q(v × B) is called the magnetic force.[13] According to some definitions, the term \"Lorentz force\" refers specifically to the formula for the magnetic force,[14] with the total electromagnetic force (including the electric force) given some other (nonstandard) name. This article will not follow this nomenclature: in what follows, the term Lorentz force will refer to the expression for the total force.\n\nThe magnetic force component of the Lorentz force manifests itself as the force that acts on a current-carrying wire in a magnetic field. In that context, it is also called the Laplace force.\n\nThe Lorentz force is a force exerted by the electromagnetic field on the charged particle, that is, it is the rate at which linear momentum is transferred from the electromagnetic field to the particle. Associated with it is the power which is the rate at which energy is transferred from the electromagnetic field to the particle. That power is\n\n  \n    \n      \n        \n          v\n        \n        ⋅\n        \n          F\n        \n        =\n        q\n        \n        \n          v\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} \\cdot \\mathbf {F} =q\\,\\mathbf {v} \\cdot \\mathbf {E} .}\n  \n\nNotice that the magnetic field does not contribute to the power because the magnetic force is always perpendicular to the velocity of the particle.\n\nFor a continuous charge distribution in motion, the Lorentz force equation becomes:\n\n  \n    \n      \n        \n          d\n        \n        \n          F\n        \n        =\n        \n          d\n        \n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {F} =\\mathrm {d} q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n  \n\nwhere \n  \n    \n      \n        \n          d\n        \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathrm {d} \\mathbf {F} }\n  \n is the force on a small piece of the charge distribution with charge \n  \n    \n      \n        \n          d\n        \n        q\n      \n    \n    {\\displaystyle \\mathrm {d} q}\n  \n. If both sides of this equation are divided by the volume of this small piece of the charge distribution \n  \n    \n      \n        \n          d\n        \n        V\n      \n    \n    {\\displaystyle \\mathrm {d} V}\n  \n, the result is:\n\n  \n    \n      \n        \n          f\n        \n        =\n        ρ\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\rho \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right)}\n  \n\nwhere \n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle \\mathbf {f} }\n  \n is the force density (force per unit volume) and \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is the charge density (charge per unit volume). Next, the current density corresponding to the motion of the charge continuum is\n\n  \n    \n      \n        \n          J\n        \n        =\n        ρ\n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {J} =\\rho \\mathbf {v} }\n  \n\nso the continuous analogue to the equation is[15]\n\nf\n        \n        =\n        ρ\n        \n          E\n        \n        +\n        \n          J\n        \n        ×\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\rho \\mathbf {E} +\\mathbf {J} \\times \\mathbf {B} }\n\nThe total force is the volume integral over the charge distribution:\n\n  \n    \n      \n        \n          F\n        \n        =\n        ∫\n        \n          (\n          \n            ρ\n            \n              E\n            \n            +\n            \n              J\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        \n          d\n        \n        V\n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =\\int \\left(\\rho \\mathbf {E} +\\mathbf {J} \\times \\mathbf {B} \\right)\\mathrm {d} V.}\n\nBy eliminating \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n and \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n, using Maxwell's equations, and manipulating using the theorems of vector calculus, this form of the equation can be used to derive the Maxwell stress tensor \n  \n    \n      \n        \n          σ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}}\n  \n, in turn this can be combined with the Poynting vector \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\mathbf {S} }\n  \n to obtain the electromagnetic stress–energy tensor T used in general relativity.[15]\n\nIn terms of \n  \n    \n      \n        \n          σ\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}}\n  \n and \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\mathbf {S} }\n  \n, another way to write the Lorentz force (per unit volume) is[15]\n\n  \n    \n      \n        \n          f\n        \n        =\n        ∇\n        ⋅\n        \n          σ\n        \n        −\n        \n          \n            \n              1\n              \n                c\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                ∂\n                \n                  S\n                \n              \n              \n                ∂\n                t\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {f} =\\nabla \\cdot {\\boldsymbol {\\sigma }}-{\\dfrac {1}{c^{2}}}{\\dfrac {\\partial \\mathbf {S} }{\\partial t}}}\n  \n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the speed of light and ∇· (nabla followed by a middle dot) denotes the divergence of a tensor field. Rather than the amount of charge and its velocity in electric and magnetic fields, this equation relates the energy flux (flow of energy per unit time per unit distance) in the fields to the force exerted on a charge distribution. See Covariant formulation of classical electromagnetism for more details.\n\nThe density of power associated with the Lorentz force in a material medium is\n\n  \n    \n      \n        \n          J\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {J} \\cdot \\mathbf {E} .}\n\nIf we separate the total charge and total current into their free and bound parts, we get that the density of the Lorentz force is\n\n  \n    \n      \n        \n          f\n        \n        =\n        \n          (\n          \n            \n              ρ\n              \n                f\n              \n            \n            −\n            ∇\n            ⋅\n            \n              P\n            \n          \n          )\n        \n        \n          E\n        \n        +\n        \n          (\n          \n            \n              \n                J\n              \n              \n                f\n              \n            \n            +\n            ∇\n            ×\n            \n              M\n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    P\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n          \n          )\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {f} =\\left(\\rho _{f}-\\nabla \\cdot \\mathbf {P} \\right)\\mathbf {E} +\\left(\\mathbf {J} _{f}+\\nabla \\times \\mathbf {M} +{\\frac {\\partial \\mathbf {P} }{\\partial t}}\\right)\\times \\mathbf {B} .}\n\nwhere: \n  \n    \n      \n        \n          ρ\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle \\rho _{f}}\n  \n is the density of free charge; \n  \n    \n      \n        \n          P\n        \n      \n    \n    {\\displaystyle \\mathbf {P} }\n  \n is the polarization density; \n  \n    \n      \n        \n          \n            J\n          \n          \n            f\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {J} _{f}}\n  \n is the density of free current; and \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n  \n is the magnetization density. In this way, the Lorentz force can explain the torque applied to a permanent magnet by the magnetic field. The density of the associated power is\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                J\n              \n              \n                f\n              \n            \n            +\n            ∇\n            ×\n            \n              M\n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    P\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n          \n          )\n        \n        ⋅\n        \n          E\n        \n        .\n      \n    \n    {\\displaystyle \\left(\\mathbf {J} _{f}+\\nabla \\times \\mathbf {M} +{\\frac {\\partial \\mathbf {P} }{\\partial t}}\\right)\\cdot \\mathbf {E} .}\n\nThe above-mentioned formulae use the conventions for the definition of the electric and magnetic field used with the SI, which is the most common. However, other conventions with the same physics (i.e. forces on e.g. an electron) are possible and used. In the conventions used with the older CGS-Gaussian units, which are somewhat more common among some theoretical physicists as well as condensed matter experimentalists, one has instead\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          q\n          \n            \n              G\n            \n          \n        \n        \n          (\n          \n            \n              \n                E\n              \n              \n                \n                  G\n                \n              \n            \n            +\n            \n              \n                \n                  v\n                \n                c\n              \n            \n            ×\n            \n              \n                B\n              \n              \n                \n                  G\n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} =q_{\\mathrm {G} }\\left(\\mathbf {E} _{\\mathrm {G} }+{\\frac {\\mathbf {v} }{c}}\\times \\mathbf {B} _{\\mathrm {G} }\\right),}\n  \n\nwhere c is the speed of light. Although this equation looks slightly different, it is equivalent, since one has the following relations:[1]\n\n  \n    \n      \n        \n          q\n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            \n              q\n              \n                \n                  S\n                  I\n                \n              \n            \n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        ,\n        \n        \n          \n            E\n          \n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            4\n            π\n            \n              ε\n              \n                0\n              \n            \n          \n        \n        \n        \n          \n            E\n          \n          \n            \n              S\n              I\n            \n          \n        \n        ,\n        \n        \n          \n            B\n          \n          \n            \n              G\n            \n          \n        \n        =\n        \n          \n            4\n            π\n            \n              /\n            \n            \n              μ\n              \n                0\n              \n            \n          \n        \n        \n        \n          \n            \n              B\n            \n            \n              \n                S\n                I\n              \n            \n          \n        \n        ,\n        \n        c\n        =\n        \n          \n            1\n            \n              \n                ε\n                \n                  0\n                \n              \n              \n                μ\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle q_{\\mathrm {G} }={\\frac {q_{\\mathrm {SI} }}{\\sqrt {4\\pi \\varepsilon _{0}}}},\\quad \\mathbf {E} _{\\mathrm {G} }={\\sqrt {4\\pi \\varepsilon _{0}}}\\,\\mathbf {E} _{\\mathrm {SI} },\\quad \\mathbf {B} _{\\mathrm {G} }={\\sqrt {4\\pi /\\mu _{0}}}\\,{\\mathbf {B} _{\\mathrm {SI} }},\\quad c={\\frac {1}{\\sqrt {\\varepsilon _{0}\\mu _{0}}}}.}\n  \n\nwhere ε0 is the vacuum permittivity and μ0 the vacuum permeability. In practice, the subscripts \"G\" and \"SI\" are omitted, and the used convention (and unit)  must be determined from context.\n\nEarly attempts to quantitatively describe the electromagnetic force were made in the mid-18th century. It was proposed that the force on magnetic poles, by Johann Tobias Mayer and others in 1760,[16] and electrically charged objects, by Henry Cavendish in 1762,[17] obeyed an inverse-square law. However, in both cases the experimental proof was neither complete nor conclusive. It was not until 1784 when Charles-Augustin de Coulomb, using a torsion balance, was able to definitively show through experiment that this was true.[18] Soon after the discovery in 1820 by Hans Christian Ørsted that a magnetic needle is acted on by a voltaic current, André-Marie Ampère that same year was able to devise through experimentation the formula for the angular dependence of the force between two current elements.[19][20] In all these descriptions, the force was always described in terms of the properties of the matter involved and the distances between two masses or charges rather than in terms of electric and magnetic fields.[21]\n\nThe modern concept of electric and magnetic fields first arose in the theories of Michael Faraday, particularly his idea of lines of force, later to be given full mathematical description by Lord Kelvin and James Clerk Maxwell.[22] From a modern perspective it is possible to identify in Maxwell's 1865 formulation of his field equations a form of the Lorentz force equation in relation to electric currents,[4] although in the time of Maxwell it was not evident how his equations related to the forces on moving charged objects. J. J. Thomson was the first to attempt to derive from Maxwell's field equations the electromagnetic forces on a moving charged object in terms of the object's properties and external fields. Interested in determining the electromagnetic behavior of the charged particles in cathode rays, Thomson published a paper in 1881 wherein he gave the force on the particles due to an external magnetic field as[6][23]\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            q\n            2\n          \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\frac {q}{2}}\\mathbf {v} \\times \\mathbf {B} .}\n  \n\nThomson derived the correct basic form of the formula, but, because of some miscalculations and an incomplete description of the displacement current, included an incorrect scale-factor of a half in front of the formula. Oliver Heaviside invented the modern vector notation and applied it to Maxwell's field equations; he also (in 1885 and 1889) had fixed the mistakes of Thomson's derivation and arrived at the correct form of the magnetic force on a moving charged object.[6][24][25] Finally, in 1895,[5][26] Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. Lorentz began by abandoning the Maxwellian descriptions of the ether and conduction. Instead, Lorentz made a distinction between matter and the luminiferous aether and sought to apply the Maxwell equations at a microscopic scale. Using Heaviside's version of the Maxwell equations for a stationary ether and applying Lagrangian mechanics (see below), Lorentz arrived at the correct and complete form of the force law that now bears his name.[27][28]\n\nIn many cases of practical interest, the motion in a magnetic field of an electrically charged particle (such as an electron or ion in a plasma) can be treated as the superposition of a relatively fast circular motion around a point called the guiding center and a relatively slow drift of this point. The drift speeds may differ for various species depending on their charge states, masses, or temperatures, possibly resulting in electric currents or chemical separation.\n\nWhile the modern Maxwell's equations describe how electrically charged particles and currents or moving charged particles give rise to electric and magnetic fields, the Lorentz force law completes that picture by describing the force acting on a moving point charge q in the presence of electromagnetic fields.[12][29] The Lorentz force law describes the effect of E and B upon a point charge, but such electromagnetic forces are not the entire picture. Charged particles are possibly coupled to other forces, notably gravity and nuclear forces. Thus, Maxwell's equations do not stand separate from other physical laws, but are coupled to them via the charge and current densities. The response of a point charge to the Lorentz law is one aspect; the generation of E and B by currents and charges is another.\n\nIn real materials the Lorentz force is inadequate to describe the collective behavior of charged particles, both in principle and as a matter of computation. The charged particles in a material medium not only respond to the E and B fields but also generate these fields. Complex transport equations must be solved to determine the time and spatial response of charges, for example, the Boltzmann equation or the Fokker–Planck equation or the Navier–Stokes equations. For example, see magnetohydrodynamics, fluid dynamics, electrohydrodynamics, superconductivity, stellar evolution. An entire physical apparatus for dealing with these matters has developed. See for example, Green–Kubo relations and Green's function (many-body theory).\n\nWhen a wire carrying an electric current is placed in a magnetic field, each of the moving charges, which comprise the current, experiences the Lorentz force, and together they can create a macroscopic force on the wire (sometimes called the Laplace force). By combining the Lorentz force law above with the definition of electric current, the following equation results, in the case of a straight stationary wire in a homogeneous field:[30]\n\n  \n    \n      \n        \n          F\n        \n        =\n        I\n        \n          ℓ\n        \n        ×\n        \n          B\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} =I{\\boldsymbol {\\ell }}\\times \\mathbf {B} ,}\n  \n\nwhere ℓ is a vector whose magnitude is the length of the wire, and whose direction is along the wire, aligned with the direction of the conventional current I.\n\nIf the wire is not straight, the force on it can be computed by applying this formula to each infinitesimal segment of wire \n  \n    \n      \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n, then adding up all these forces by integration. This results in the same formal expression, but ℓ should now be understood as the vector connecting the end points of the curved wire with direction from starting to end point of conventional current. Usually, there will also be a net torque.\n\nIf, in addition, the magnetic field is inhomogeneous, the net force on a stationary rigid wire carrying a steady current I is given by integration along the wire,\n\n  \n    \n      \n        \n          F\n        \n        =\n        I\n        ∫\n        \n          d\n        \n        \n          ℓ\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =I\\int \\mathrm {d} {\\boldsymbol {\\ell }}\\times \\mathbf {B} .}\n\nOne application of this is Ampère's force law, which describes how two current-carrying wires can attract or repel each other, since each experiences a Lorentz force from the other's magnetic field.\n\nThe magnetic force (qv × B) component of the Lorentz force is responsible for motional electromotive force (or motional EMF), the phenomenon underlying many electrical generators. When a conductor is moved through a magnetic field, the magnetic field exerts opposite forces on electrons and nuclei in the wire, and this creates the EMF. The term \"motional EMF\" is applied to this phenomenon, since the EMF is due to the motion of the wire.\n\nIn other electrical generators, the magnets move, while the conductors do not. In this case, the EMF is due to the electric force (qE) term in the Lorentz Force equation. The electric field in question is created by the changing magnetic field, resulting in an induced EMF called the transformer EMF  , as described by the Maxwell–Faraday equation (one of the four modern Maxwell's equations).[31][32]\n\nBoth of these EMFs, despite their apparently distinct origins, are described by the same equation, namely, the EMF is the rate of change of magnetic flux through the wire. (This is Faraday's law of induction, see below.) Einstein's special theory of relativity was partially motivated by the desire to better understand this link between the two effects.[31] In fact, the electric and magnetic fields are different facets of the same electromagnetic field, and in moving from one inertial frame to another, the solenoidal vector field portion of the E-field can change in whole or in part to a B-field or vice versa.[33]\n\nGiven a loop of wire in a magnetic field, Faraday's law of induction states the induced electromotive force (EMF) in the wire is:\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        −\n        \n          \n            \n              \n                d\n              \n              \n                Φ\n                \n                  B\n                \n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}=-{\\frac {\\mathrm {d} \\Phi _{B}}{\\mathrm {d} t}}}\n  \n\nwhere\n\n  \n    \n      \n        \n          Φ\n          \n            B\n          \n        \n        =\n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\Phi _{B}=\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot \\mathbf {B} (\\mathbf {r} ,t)}\n  \n\nis the magnetic flux through the loop, B is the magnetic field, Σ(t) is a surface bounded by the closed contour ∂Σ(t), at time t, dA is an infinitesimal vector area element of Σ(t) (magnitude is the area of an infinitesimal patch of surface, direction is orthogonal to that surface patch).\n\nThe sign of the EMF is determined by Lenz's law. Note that this is valid for not only a stationary wire – but also for a moving wire.\n\nFrom Faraday's law of induction (that is valid for a moving wire, for instance in a motor) and the Maxwell Equations, the Lorentz Force can be deduced. The reverse is also true, the Lorentz force and the Maxwell Equations can be used to derive the Faraday Law.\n\nLet Σ(t) be the moving wire, moving together without rotation and with constant velocity v and Σ(t) be the internal surface of the wire. The EMF around the closed path ∂Σ(t) is given by:[34]\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle {\\mathcal {E}}=\\oint _{\\partial \\Sigma (t)}\\!\\!\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q}\n  \n\nwhere \n  \n    \n      \n        \n          E\n        \n        =\n        \n          F\n        \n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle \\mathbf {E} =\\mathbf {F} /q}\n  \n is the electric field and dℓ is an infinitesimal vector element of the contour ∂Σ(t).\n\nNB: Both dℓ and dA have a sign ambiguity; to get the correct sign, the right-hand rule is used, as explained in the article Kelvin–Stokes theorem.\n\nThe above result can be compared with the version of Faraday's law of induction that appears in the modern Maxwell's equations, called here the Maxwell–Faraday equation:\n\n  \n    \n      \n        ∇\n        ×\n        \n          E\n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                B\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\nabla \\times \\mathbf {E} =-{\\frac {\\partial \\mathbf {B} }{\\partial t}}\\,.}\n\nThe Maxwell–Faraday equation also can be written in an integral form using the Kelvin–Stokes theorem.[35]\n\nSo we have, the Maxwell–Faraday equation:\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        −\n         \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          \n            \n              ∂\n              \n                B\n              \n              (\n              \n                r\n              \n              ,\n              \n              t\n              )\n            \n            \n              ∂\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {E} (\\mathbf {r} ,\\ t)=-\\ \\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot {\\frac {\\partial \\mathbf {B} (\\mathbf {r} ,\\,t)}{\\partial t}}}\n  \n\nand the Faraday law,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        −\n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        .\n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,\\ t)=-{\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot \\mathbf {B} (\\mathbf {r} ,\\ t).}\n\nThe two are equivalent if the wire is not moving. Using the Leibniz integral rule and that div B = 0, results in,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n        t\n        )\n        =\n        −\n        \n          ∫\n          \n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          A\n        \n        ⋅\n        \n          \n            ∂\n            \n              ∂\n              t\n            \n          \n        \n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n        +\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,t)=-\\int _{\\Sigma (t)}\\mathrm {d} \\mathbf {A} \\cdot {\\frac {\\partial }{\\partial t}}\\mathbf {B} (\\mathbf {r} ,t)+\\oint _{\\partial \\Sigma (t)}\\!\\!\\!\\!\\mathbf {v} \\times \\mathbf {B} \\,\\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n\nand using the Maxwell Faraday equation,\n\n  \n    \n      \n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          F\n        \n        \n          /\n        \n        q\n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        =\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n          d\n        \n        \n          ℓ\n        \n        ⋅\n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        +\n        \n          ∮\n          \n            ∂\n            Σ\n            (\n            t\n            )\n          \n        \n        \n        \n        \n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n         \n        t\n        )\n        \n        \n          d\n        \n        \n          ℓ\n        \n      \n    \n    {\\displaystyle \\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {F} /q(\\mathbf {r} ,\\ t)=\\oint _{\\partial \\Sigma (t)}\\mathrm {d} {\\boldsymbol {\\ell }}\\cdot \\mathbf {E} (\\mathbf {r} ,\\ t)+\\oint _{\\partial \\Sigma (t)}\\!\\!\\!\\!\\mathbf {v} \\times \\mathbf {B} (\\mathbf {r} ,\\ t)\\,\\mathrm {d} {\\boldsymbol {\\ell }}}\n  \n\nsince this is valid for any wire position it implies that\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n        \n          E\n        \n        (\n        \n          r\n        \n        ,\n        \n        t\n        )\n        +\n        q\n        \n        \n          v\n        \n        ×\n        \n          B\n        \n        (\n        \n          r\n        \n        ,\n        \n        t\n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\,\\mathbf {E} (\\mathbf {r} ,\\,t)+q\\,\\mathbf {v} \\times \\mathbf {B} (\\mathbf {r} ,\\,t).}\n\nFaraday's law of induction holds whether the loop of wire is rigid and stationary, or in motion or in process of deformation, and it holds whether the magnetic field is constant in time or changing. However, there are cases where Faraday's law is either inadequate or difficult to use, and application of the underlying Lorentz force law is necessary. See inapplicability of Faraday's law.\n\nIf the magnetic field is fixed in time and the conducting loop moves through the field, the magnetic flux ΦB linking the loop can change in several ways. For example, if the B-field varies with position, and the loop moves to a location with different B-field, ΦB will change. Alternatively, if the loop changes orientation with respect to the B-field, the B ⋅ dA differential element will change because of the different angle between B and dA, also changing ΦB. As a third example, if a portion of the circuit is swept through a uniform, time-independent B-field, and another portion of the circuit is held stationary, the flux linking the entire closed circuit can change due to the shift in relative position of the circuit's component parts with time (surface ∂Σ(t) time-dependent). In all three cases, Faraday's law of induction then predicts the EMF generated by the change in ΦB.\n\nNote that the Maxwell Faraday's equation implies that the Electric Field E is non conservative when the Magnetic Field B varies in time, and is not expressible as the gradient of a scalar field, and not subject to the gradient theorem since its curl is not zero.[34][36]\n\nThe E and B fields can be replaced by the magnetic vector potential A and (scalar) electrostatic potential ϕ by\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  E\n                \n              \n              \n                \n                =\n                −\n                ∇\n                ϕ\n                −\n                \n                  \n                    \n                      ∂\n                      \n                        A\n                      \n                    \n                    \n                      ∂\n                      t\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  B\n                \n              \n              \n                \n                =\n                ∇\n                ×\n                \n                  A\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {E} &=-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}\\\\[1ex]\\mathbf {B} &=\\nabla \\times \\mathbf {A} \\end{aligned}}}\n  \n\nwhere ∇ is the gradient, ∇⋅ is the divergence, and ∇× is the curl.\n\nThe force becomes\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            ϕ\n            −\n            \n              \n                \n                  ∂\n                  \n                    A\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              v\n            \n            ×\n            (\n            ∇\n            ×\n            \n              A\n            \n            )\n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}+\\mathbf {v} \\times (\\nabla \\times \\mathbf {A} )\\right].}\n\nUsing an identity for the triple product this can be rewritten as\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            ϕ\n            −\n            \n              \n                \n                  ∂\n                  \n                    A\n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            ∇\n            \n              (\n              \n                \n                  v\n                \n                ⋅\n                \n                  A\n                \n              \n              )\n            \n            −\n            \n              (\n              \n                \n                  v\n                \n                ⋅\n                ∇\n              \n              )\n            \n            \n              A\n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla \\phi -{\\frac {\\partial \\mathbf {A} }{\\partial t}}+\\nabla \\left(\\mathbf {v} \\cdot \\mathbf {A} \\right)-\\left(\\mathbf {v} \\cdot \\nabla \\right)\\mathbf {A} \\right].}\n\n(Notice that the coordinates and the velocity components should be treated as independent variables, so the del operator acts only on \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n, not on \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n; thus, there is no need of using Feynman's subscript notation in the equation above.) Using the chain rule, the total derivative of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n is:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                A\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          v\n        \n        ⋅\n        ∇\n        )\n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {A} }{\\mathrm {d} t}}={\\frac {\\partial \\mathbf {A} }{\\partial t}}+(\\mathbf {v} \\cdot \\nabla )\\mathbf {A} }\n  \n\nso that the above expression becomes:\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          [\n          \n            −\n            ∇\n            (\n            ϕ\n            −\n            \n              v\n            \n            ⋅\n            \n              A\n            \n            )\n            −\n            \n              \n                \n                  \n                    d\n                  \n                  \n                    A\n                  \n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla (\\phi -\\mathbf {v} \\cdot \\mathbf {A} )-{\\frac {\\mathrm {d} \\mathbf {A} }{\\mathrm {d} t}}\\right].}\n\nWith v = ẋ, we can put the equation into the convenient Euler–Lagrange form\n\nF\n        \n        =\n        q\n        \n          [\n          \n            −\n            \n              ∇\n              \n                \n                  x\n                \n              \n            \n            (\n            ϕ\n            −\n            \n              \n                \n                  \n                    x\n                  \n                  ˙\n                \n              \n            \n            ⋅\n            \n              A\n            \n            )\n            +\n            \n              \n                \n                  d\n                \n                \n                  \n                    d\n                  \n                  t\n                \n              \n            \n            \n              ∇\n              \n                \n                  \n                    \n                      x\n                    \n                    ˙\n                  \n                \n              \n            \n            (\n            ϕ\n            −\n            \n              \n                \n                  \n                    x\n                  \n                  ˙\n                \n              \n            \n            ⋅\n            \n              A\n            \n            )\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =q\\left[-\\nabla _{\\mathbf {x} }(\\phi -{\\dot {\\mathbf {x} }}\\cdot \\mathbf {A} )+{\\frac {\\mathrm {d} }{\\mathrm {d} t}}\\nabla _{\\dot {\\mathbf {x} }}(\\phi -{\\dot {\\mathbf {x} }}\\cdot \\mathbf {A} )\\right]}\n\nwhere \n  \n    \n      \n        \n          ∇\n          \n            \n              x\n            \n          \n        \n        =\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                x\n              \n            \n          \n        \n        +\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                y\n              \n            \n          \n        \n        +\n        \n          \n            \n              z\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                z\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\nabla _{\\mathbf {x} }={\\hat {x}}{\\dfrac {\\partial }{\\partial x}}+{\\hat {y}}{\\dfrac {\\partial }{\\partial y}}+{\\hat {z}}{\\dfrac {\\partial }{\\partial z}}}\n  \n and \n  \n    \n      \n        \n          ∇\n          \n            \n              \n                \n                  x\n                \n                ˙\n              \n            \n          \n        \n        =\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      x\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      y\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              z\n              ^\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                ∂\n                \n                  \n                    \n                      z\n                      ˙\n                    \n                  \n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\nabla _{\\dot {\\mathbf {x} }}={\\hat {x}}{\\dfrac {\\partial }{\\partial {\\dot {x}}}}+{\\hat {y}}{\\dfrac {\\partial }{\\partial {\\dot {y}}}}+{\\hat {z}}{\\dfrac {\\partial }{\\partial {\\dot {z}}}}.}\n\nThe Lagrangian for a charged particle of mass m and charge q in an electromagnetic field equivalently describes the dynamics of the particle in terms of its energy, rather than the force exerted on it. The classical expression is given by:[37]\n\n  \n    \n      \n        L\n        =\n        \n          \n            m\n            2\n          \n        \n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        +\n        q\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        −\n        q\n        ϕ\n      \n    \n    {\\displaystyle L={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} +q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} -q\\phi }\n  \n\nwhere A and ϕ are the potential fields as above. The quantity \n  \n    \n      \n        V\n        =\n        q\n        (\n        ϕ\n        −\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle V=q(\\phi -\\mathbf {A} \\cdot \\mathbf {\\dot {r}} )}\n  \n can be thought as a velocity-dependent potential function.[38] Using Lagrange's equations, the equation for the Lorentz force given above can be obtained again.\n\nFor an A field, a particle moving with velocity v = ṙ has potential momentum \n  \n    \n      \n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle q\\mathbf {A} (\\mathbf {r} ,t)}\n  \n, so its potential energy is \n  \n    \n      \n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        ,\n        t\n        )\n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle q\\mathbf {A} (\\mathbf {r} ,t)\\cdot \\mathbf {\\dot {r}} }\n  \n. For a ϕ field, the particle's potential energy is \n  \n    \n      \n        q\n        ϕ\n        (\n        \n          r\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle q\\phi (\\mathbf {r} ,t)}\n  \n.\n\nThe total potential energy is then:\n\n  \n    \n      \n        V\n        =\n        q\n        ϕ\n        −\n        q\n        \n          A\n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle V=q\\phi -q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} }\n  \n\nand the kinetic energy is:\n\n  \n    \n      \n        T\n        =\n        \n          \n            m\n            2\n          \n        \n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ⋅\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle T={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} }\n  \n\nhence the Lagrangian:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n              \n                \n                =\n                T\n                −\n                V\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    m\n                    2\n                  \n                \n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ⋅\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                +\n                q\n                \n                  A\n                \n                ⋅\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                −\n                q\n                ϕ\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    m\n                    2\n                  \n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          \n                            x\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      \n                        \n                          \n                            y\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      \n                        \n                          \n                            z\n                            ˙\n                          \n                        \n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                +\n                q\n                \n                  (\n                  \n                    \n                      \n                        \n                          x\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        x\n                      \n                    \n                    +\n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        y\n                      \n                    \n                    +\n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                    \n                      A\n                      \n                        z\n                      \n                    \n                  \n                  )\n                \n                −\n                q\n                ϕ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}L&=T-V\\\\[1ex]&={\\frac {m}{2}}\\mathbf {\\dot {r}} \\cdot \\mathbf {\\dot {r}} +q\\mathbf {A} \\cdot \\mathbf {\\dot {r}} -q\\phi \\\\[1ex]&={\\frac {m}{2}}\\left({\\dot {x}}^{2}+{\\dot {y}}^{2}+{\\dot {z}}^{2}\\right)+q\\left({\\dot {x}}A_{x}+{\\dot {y}}A_{y}+{\\dot {z}}A_{z}\\right)-q\\phi \\end{aligned}}}\n\nLagrange's equations are\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                \n                  \n                    x\n                    ˙\n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}={\\frac {\\partial L}{\\partial x}}}\n  \n\n(same for y and z). So calculating the partial derivatives:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      d\n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n                \n                  \n                    \n                      ∂\n                      L\n                    \n                    \n                      ∂\n                      \n                        \n                          \n                            x\n                            ˙\n                          \n                        \n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  \n                    \n                      \n                        d\n                      \n                      \n                        A\n                        \n                          x\n                        \n                      \n                    \n                    \n                      \n                        d\n                      \n                      t\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          x\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          y\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          y\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          z\n                        \n                      \n                    \n                    \n                      \n                        \n                          d\n                          z\n                        \n                        \n                          d\n                          t\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \n                  \n                    \n                      x\n                      ¨\n                    \n                  \n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    \n                      \n                        \n                          x\n                          ˙\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          y\n                        \n                      \n                    \n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          z\n                        \n                      \n                    \n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {\\mathrm {d} }{\\mathrm {d} t}}{\\frac {\\partial L}{\\partial {\\dot {x}}}}&=m{\\ddot {x}}+q{\\frac {\\mathrm {d} A_{x}}{\\mathrm {d} t}}\\\\&=m{\\ddot {x}}+q\\left[{\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\frac {dx}{dt}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\frac {dy}{dt}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\frac {dz}{dt}}\\right]\\\\[1ex]&=m{\\ddot {x}}+q\\left[{\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\dot {y}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\dot {z}}\\right]\\\\\\end{aligned}}}\n  \n\n\n  \n    \n      \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              x\n            \n          \n        \n        =\n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              x\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      y\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      z\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\partial L}{\\partial x}}=-q{\\frac {\\partial \\phi }{\\partial x}}+q\\left({\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{y}}{\\partial x}}{\\dot {y}}+{\\frac {\\partial A_{z}}{\\partial x}}{\\dot {z}}\\right)}\n  \n\nequating and simplifying:\n\n  \n    \n      \n        m\n        \n          \n            \n              x\n              ¨\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  y\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  z\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n        =\n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              x\n            \n          \n        \n        +\n        q\n        \n          (\n          \n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      x\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  x\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      y\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  y\n                  ˙\n                \n              \n            \n            +\n            \n              \n                \n                  ∂\n                  \n                    A\n                    \n                      z\n                    \n                  \n                \n                \n                  ∂\n                  x\n                \n              \n            \n            \n              \n                \n                  z\n                  ˙\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle m{\\ddot {x}}+q\\left({\\frac {\\partial A_{x}}{\\partial t}}+{\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{x}}{\\partial y}}{\\dot {y}}+{\\frac {\\partial A_{x}}{\\partial z}}{\\dot {z}}\\right)=-q{\\frac {\\partial \\phi }{\\partial x}}+q\\left({\\frac {\\partial A_{x}}{\\partial x}}{\\dot {x}}+{\\frac {\\partial A_{y}}{\\partial x}}{\\dot {y}}+{\\frac {\\partial A_{z}}{\\partial x}}{\\dot {z}}\\right)}\n  \n\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    x\n                  \n                \n              \n              \n                \n                =\n                −\n                q\n                \n                  (\n                  \n                    \n                      \n                        \n                          ∂\n                          ϕ\n                        \n                        \n                          ∂\n                          x\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          ∂\n                          \n                            A\n                            \n                              x\n                            \n                          \n                        \n                        \n                          ∂\n                          t\n                        \n                      \n                    \n                  \n                  )\n                \n                +\n                q\n                \n                  [\n                  \n                    \n                      \n                        \n                          y\n                          ˙\n                        \n                      \n                    \n                    \n                      (\n                      \n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  y\n                                \n                              \n                            \n                            \n                              ∂\n                              x\n                            \n                          \n                        \n                        −\n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  x\n                                \n                              \n                            \n                            \n                              ∂\n                              y\n                            \n                          \n                        \n                      \n                      )\n                    \n                    +\n                    \n                      \n                        \n                          z\n                          ˙\n                        \n                      \n                    \n                    \n                      (\n                      \n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  z\n                                \n                              \n                            \n                            \n                              ∂\n                              x\n                            \n                          \n                        \n                        −\n                        \n                          \n                            \n                              ∂\n                              \n                                A\n                                \n                                  x\n                                \n                              \n                            \n                            \n                              ∂\n                              z\n                            \n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                [\n                \n                  \n                    \n                      y\n                      ˙\n                    \n                  \n                \n                (\n                ∇\n                ×\n                \n                  A\n                \n                \n                  )\n                  \n                    z\n                  \n                \n                −\n                \n                  \n                    \n                      z\n                      ˙\n                    \n                  \n                \n                (\n                ∇\n                ×\n                \n                  A\n                \n                \n                  )\n                  \n                    y\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                [\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ×\n                (\n                ∇\n                ×\n                \n                  A\n                \n                )\n                \n                  ]\n                  \n                    x\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                q\n                \n                  E\n                  \n                    x\n                  \n                \n                +\n                q\n                (\n                \n                  \n                    \n                      r\n                      ˙\n                    \n                  \n                \n                ×\n                \n                  B\n                \n                \n                  )\n                  \n                    x\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{x}&=-q\\left({\\frac {\\partial \\phi }{\\partial x}}+{\\frac {\\partial A_{x}}{\\partial t}}\\right)+q\\left[{\\dot {y}}\\left({\\frac {\\partial A_{y}}{\\partial x}}-{\\frac {\\partial A_{x}}{\\partial y}}\\right)+{\\dot {z}}\\left({\\frac {\\partial A_{z}}{\\partial x}}-{\\frac {\\partial A_{x}}{\\partial z}}\\right)\\right]\\\\[1ex]&=qE_{x}+q[{\\dot {y}}(\\nabla \\times \\mathbf {A} )_{z}-{\\dot {z}}(\\nabla \\times \\mathbf {A} )_{y}]\\\\[1ex]&=qE_{x}+q[\\mathbf {\\dot {r}} \\times (\\nabla \\times \\mathbf {A} )]_{x}\\\\[1ex]&=qE_{x}+q(\\mathbf {\\dot {r}} \\times \\mathbf {B} )_{x}\\end{aligned}}}\n  \n\nand similarly for the y and z directions. Hence the force equation is:\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        (\n        \n          E\n        \n        +\n        \n          \n            \n              r\n              ˙\n            \n          \n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {F} =q(\\mathbf {E} +\\mathbf {\\dot {r}} \\times \\mathbf {B} )}\n\nThe potential energy depends on the velocity of the particle, so the force is velocity dependent, so it is not conservative.\n\nThe relativistic Lagrangian is\n\n  \n    \n      \n        L\n        =\n        −\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          \n            1\n            −\n            \n              \n                (\n                \n                  \n                    \n                      \n                        \n                          r\n                        \n                        ˙\n                      \n                    \n                    c\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        +\n        q\n        \n          A\n        \n        (\n        \n          r\n        \n        )\n        ⋅\n        \n          \n            \n              \n                r\n              \n              ˙\n            \n          \n        \n        −\n        q\n        ϕ\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle L=-mc^{2}{\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}+q\\mathbf {A} (\\mathbf {r} )\\cdot {\\dot {\\mathbf {r} }}-q\\phi (\\mathbf {r} )}\n\nThe action is the relativistic arclength of the path of the particle in spacetime, minus the potential energy contribution, plus an extra contribution which quantum mechanically is an extra phase a charged particle gets when it is moving along a vector potential.\n\nThe equations of motion derived by extremizing the action (see matrix calculus for the notation):\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                P\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        =\n        q\n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              \n                r\n              \n              ˙\n            \n          \n        \n        −\n        q\n        \n          \n            \n              ∂\n              ϕ\n            \n            \n              ∂\n              \n                r\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {P} }{\\mathrm {d} t}}={\\frac {\\partial L}{\\partial \\mathbf {r} }}=q{\\partial \\mathbf {A}  \\over \\partial \\mathbf {r} }\\cdot {\\dot {\\mathbf {r} }}-q{\\partial \\phi  \\over \\partial \\mathbf {r} }}\n  \n\n\n  \n    \n      \n        \n          P\n        \n        −\n        q\n        \n          A\n        \n        =\n        \n          \n            \n              m\n              \n                \n                  \n                    \n                      r\n                    \n                    ˙\n                  \n                \n              \n            \n            \n              1\n              −\n              \n                \n                  (\n                  \n                    \n                      \n                        \n                          \n                            r\n                          \n                          ˙\n                        \n                      \n                      c\n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {P} -q\\mathbf {A} ={\\frac {m{\\dot {\\mathbf {r} }}}{\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}}}\n  \n\nare the same as Hamilton's equations of motion:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            ∂\n            \n              ∂\n              \n                p\n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                (\n                \n                  P\n                \n                −\n                q\n                \n                  A\n                \n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                (\n                m\n                \n                  c\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            +\n            q\n            ϕ\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {r} }{\\mathrm {d} t}}={\\frac {\\partial }{\\partial \\mathbf {p} }}\\left({\\sqrt {(\\mathbf {P} -q\\mathbf {A} )^{2}+(mc^{2})^{2}}}+q\\phi \\right)}\n  \n\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          \n            ∂\n            \n              ∂\n              \n                r\n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                (\n                \n                  P\n                \n                −\n                q\n                \n                  A\n                \n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                (\n                m\n                \n                  c\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            +\n            q\n            ϕ\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} t}}=-{\\frac {\\partial }{\\partial \\mathbf {r} }}\\left({\\sqrt {(\\mathbf {P} -q\\mathbf {A} )^{2}+(mc^{2})^{2}}}+q\\phi \\right)}\n  \n\nboth are equivalent to the noncanonical form:\n\n  \n    \n      \n        \n          \n            \n              d\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          \n            \n              m\n              \n                \n                  \n                    \n                      r\n                    \n                    ˙\n                  \n                \n              \n            \n            \n              \n                1\n                −\n                \n                  \n                    (\n                    \n                      \n                        \n                          \n                            \n                              r\n                            \n                            ˙\n                          \n                        \n                        c\n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              \n                \n                  \n                    r\n                  \n                  ˙\n                \n              \n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} }{\\mathrm {d} t}}{m{\\dot {\\mathbf {r} }} \\over {\\sqrt {1-\\left({\\frac {\\dot {\\mathbf {r} }}{c}}\\right)^{2}}}}=q\\left(\\mathbf {E} +{\\dot {\\mathbf {r} }}\\times \\mathbf {B} \\right).}\n  \n\nThis formula is the Lorentz force, representing the rate at which the EM field adds relativistic momentum to the particle.\n\nUsing the metric signature (1, −1, −1, −1), the Lorentz force for a charge q can be written in[39] covariant form:\n\nd\n              \n              \n                p\n                \n                  α\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          F\n          \n            α\n            β\n          \n        \n        \n          U\n          \n            β\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{\\alpha }}{\\mathrm {d} \\tau }}=qF^{\\alpha \\beta }U_{\\beta }}\n\nwhere pα is the four-momentum, defined as\n\n  \n    \n      \n        \n          p\n          \n            α\n          \n        \n        =\n        \n          (\n          \n            \n              p\n              \n                0\n              \n            \n            ,\n            \n              p\n              \n                1\n              \n            \n            ,\n            \n              p\n              \n                2\n              \n            \n            ,\n            \n              p\n              \n                3\n              \n            \n          \n          )\n        \n        =\n        \n          (\n          \n            γ\n            m\n            c\n            ,\n            \n              p\n              \n                x\n              \n            \n            ,\n            \n              p\n              \n                y\n              \n            \n            ,\n            \n              p\n              \n                z\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle p^{\\alpha }=\\left(p_{0},p_{1},p_{2},p_{3}\\right)=\\left(\\gamma mc,p_{x},p_{y},p_{z}\\right),}\n  \n\nτ the proper time of the particle, Fαβ the contravariant electromagnetic tensor\n\n  \n    \n      \n        \n          F\n          \n            α\n            β\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  −\n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n              \n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n                \n                  0\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle F^{\\alpha \\beta }={\\begin{pmatrix}0&-E_{x}/c&-E_{y}/c&-E_{z}/c\\\\E_{x}/c&0&-B_{z}&B_{y}\\\\E_{y}/c&B_{z}&0&-B_{x}\\\\E_{z}/c&-B_{y}&B_{x}&0\\end{pmatrix}}}\n  \n\nand U is the covariant 4-velocity of the particle, defined as:\n\n  \n    \n      \n        \n          U\n          \n            β\n          \n        \n        =\n        \n          (\n          \n            \n              U\n              \n                0\n              \n            \n            ,\n            \n              U\n              \n                1\n              \n            \n            ,\n            \n              U\n              \n                2\n              \n            \n            ,\n            \n              U\n              \n                3\n              \n            \n          \n          )\n        \n        =\n        γ\n        \n          (\n          \n            c\n            ,\n            −\n            \n              v\n              \n                x\n              \n            \n            ,\n            −\n            \n              v\n              \n                y\n              \n            \n            ,\n            −\n            \n              v\n              \n                z\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle U_{\\beta }=\\left(U_{0},U_{1},U_{2},U_{3}\\right)=\\gamma \\left(c,-v_{x},-v_{y},-v_{z}\\right),}\n  \n\nin which\n\n  \n    \n      \n        γ\n        (\n        v\n        )\n        =\n        \n          \n            1\n            \n              1\n              −\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              1\n              −\n              \n                \n                  \n                    \n                      v\n                      \n                        x\n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        y\n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      v\n                      \n                        z\n                      \n                      \n                        2\n                      \n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma (v)={\\frac {1}{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}={\\frac {1}{\\sqrt {1-{\\frac {v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}{c^{2}}}}}}}\n  \n\nis the Lorentz factor.\n\nThe fields are transformed to a frame moving with constant relative velocity by:\n\n  \n    \n      \n        \n          F\n          \n            ′\n            \n              μ\n              ν\n            \n          \n        \n        =\n        \n          \n            \n              Λ\n              \n                μ\n              \n            \n          \n          \n            α\n          \n        \n        \n          \n            \n              Λ\n              \n                ν\n              \n            \n          \n          \n            β\n          \n        \n        \n          F\n          \n            α\n            β\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle F'^{\\mu \\nu }={\\Lambda ^{\\mu }}_{\\alpha }{\\Lambda ^{\\nu }}_{\\beta }F^{\\alpha \\beta }\\,,}\n  \n\nwhere Λμα is the Lorentz transformation tensor.\n\nThe α = 1 component (x-component) of the force is\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          U\n          \n            β\n          \n        \n        \n          F\n          \n            1\n            β\n          \n        \n        =\n        q\n        \n          (\n          \n            \n              U\n              \n                0\n              \n            \n            \n              F\n              \n                10\n              \n            \n            +\n            \n              U\n              \n                1\n              \n            \n            \n              F\n              \n                11\n              \n            \n            +\n            \n              U\n              \n                2\n              \n            \n            \n              F\n              \n                12\n              \n            \n            +\n            \n              U\n              \n                3\n              \n            \n            \n              F\n              \n                13\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=qU_{\\beta }F^{1\\beta }=q\\left(U_{0}F^{10}+U_{1}F^{11}+U_{2}F^{12}+U_{3}F^{13}\\right).}\n\nSubstituting the components of the covariant electromagnetic tensor F yields\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        \n          [\n          \n            \n              U\n              \n                0\n              \n            \n            \n              (\n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  c\n                \n              \n              )\n            \n            +\n            \n              U\n              \n                2\n              \n            \n            (\n            −\n            \n              B\n              \n                z\n              \n            \n            )\n            +\n            \n              U\n              \n                3\n              \n            \n            (\n            \n              B\n              \n                y\n              \n            \n            )\n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=q\\left[U_{0}\\left({\\frac {E_{x}}{c}}\\right)+U_{2}(-B_{z})+U_{3}(B_{y})\\right].}\n\nUsing the components of covariant four-velocity yields\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n                \n                  1\n                \n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        γ\n        \n          [\n          \n            c\n            \n              (\n              \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  c\n                \n              \n              )\n            \n            +\n            (\n            −\n            \n              v\n              \n                y\n              \n            \n            )\n            (\n            −\n            \n              B\n              \n                z\n              \n            \n            )\n            +\n            (\n            −\n            \n              v\n              \n                z\n              \n            \n            )\n            (\n            \n              B\n              \n                y\n              \n            \n            )\n          \n          ]\n        \n        =\n        q\n        γ\n        \n          (\n          \n            \n              E\n              \n                x\n              \n            \n            +\n            \n              v\n              \n                y\n              \n            \n            \n              B\n              \n                z\n              \n            \n            −\n            \n              v\n              \n                z\n              \n            \n            \n              B\n              \n                y\n              \n            \n          \n          )\n        \n        =\n        q\n        γ\n        \n          [\n          \n            \n              E\n              \n                x\n              \n            \n            +\n            \n              \n                (\n                \n                  \n                    v\n                  \n                  ×\n                  \n                    B\n                  \n                \n                )\n              \n              \n                x\n              \n            \n          \n          ]\n        \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} p^{1}}{\\mathrm {d} \\tau }}=q\\gamma \\left[c\\left({\\frac {E_{x}}{c}}\\right)+(-v_{y})(-B_{z})+(-v_{z})(B_{y})\\right]=q\\gamma \\left(E_{x}+v_{y}B_{z}-v_{z}B_{y}\\right)=q\\gamma \\left[E_{x}+\\left(\\mathbf {v} \\times \\mathbf {B} \\right)_{x}\\right]\\,.}\n\nThe calculation for α = 2, 3 (force components in the y and z directions) yields similar results, so collecting the three equations into one:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              τ\n            \n          \n        \n        =\n        q\n        γ\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} \\tau }}=q\\gamma \\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right),}\n  \n\nand since differentials in coordinate time dt and proper time dτ are related by the Lorentz factor,\n\n  \n    \n      \n        d\n        t\n        =\n        γ\n        (\n        v\n        )\n        \n        d\n        τ\n        ,\n      \n    \n    {\\displaystyle dt=\\gamma (v)\\,d\\tau ,}\n  \n\nso we arrive at\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        q\n        \n          (\n          \n            \n              E\n            \n            +\n            \n              v\n            \n            ×\n            \n              B\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} \\mathbf {p} }{\\mathrm {d} t}}=q\\left(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} \\right).}\n\nThis is precisely the Lorentz force law, however, it is important to note that p is the relativistic expression,\n\n  \n    \n      \n        \n          p\n        \n        =\n        γ\n        (\n        v\n        )\n        \n          m\n          \n            0\n          \n        \n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} =\\gamma (v)m_{0}\\mathbf {v} \\,.}\n\nThe electric and magnetic fields are dependent on the velocity of an observer, so the relativistic form of the Lorentz force law can best be exhibited starting from a coordinate-independent expression for the electromagnetic and magnetic fields \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n, and an arbitrary time-direction, \n  \n    \n      \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{0}}\n  \n. This can be settled through spacetime algebra (or the geometric algebra of spacetime), a type of Clifford algebra defined on a pseudo-Euclidean space,[40] as\n\n  \n    \n      \n        \n          E\n        \n        =\n        \n          (\n          \n            \n              \n                F\n              \n            \n            ⋅\n            \n              γ\n              \n                0\n              \n            \n          \n          )\n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} =\\left({\\mathcal {F}}\\cdot \\gamma _{0}\\right)\\gamma _{0}}\n  \n\nand\n\n  \n    \n      \n        i\n        \n          B\n        \n        =\n        \n          (\n          \n            \n              \n                F\n              \n            \n            ∧\n            \n              γ\n              \n                0\n              \n            \n          \n          )\n        \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle i\\mathbf {B} =\\left({\\mathcal {F}}\\wedge \\gamma _{0}\\right)\\gamma _{0}}\n  \n\n\n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n is a spacetime bivector (an oriented plane segment, just like a vector is an oriented line segment), which has six degrees of freedom corresponding to boosts (rotations in spacetime planes) and rotations (rotations in space-space planes). The dot product with the vector \n  \n    \n      \n        \n          γ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{0}}\n  \n pulls a vector (in the space algebra) from the translational part, while the wedge-product creates a trivector (in the space algebra) who is dual to a vector which is the usual magnetic field vector. The relativistic velocity is given by the (time-like) changes in a time-position vector \n  \n    \n      \n        v\n        =\n        \n          \n            \n              x\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle v={\\dot {x}}}\n  \n, where\n\n  \n    \n      \n        \n          v\n          \n            2\n          \n        \n        =\n        1\n        ,\n      \n    \n    {\\displaystyle v^{2}=1,}\n  \n\n(which shows our choice for the metric) and the velocity is\n\n  \n    \n      \n        \n          v\n        \n        =\n        c\n        v\n        ∧\n        \n          γ\n          \n            0\n          \n        \n        \n          /\n        \n        (\n        v\n        ⋅\n        \n          γ\n          \n            0\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {v} =cv\\wedge \\gamma _{0}/(v\\cdot \\gamma _{0}).}\n\nThe proper form of the Lorentz force law ('invariant' is an inadequate term because no transformation has been defined) is simply\n\nF\n        =\n        q\n        \n          \n            F\n          \n        \n        ⋅\n        v\n      \n    \n    {\\displaystyle F=q{\\mathcal {F}}\\cdot v}\n\nNote that the order is important because between a bivector and a vector the dot product is anti-symmetric. Upon a spacetime split like one can obtain the velocity, and fields as above yielding the usual expression.\n\nIn the general theory of relativity the equation of motion for a particle with mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n and charge \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n, moving in a space with metric tensor \n  \n    \n      \n        \n          g\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle g_{ab}}\n  \n and electromagnetic field \n  \n    \n      \n        \n          F\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle F_{ab}}\n  \n, is given as\n\n  \n    \n      \n        m\n        \n          \n            \n              d\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        −\n        m\n        \n          \n            1\n            2\n          \n        \n        \n          g\n          \n            a\n            b\n            ,\n            c\n          \n        \n        \n          u\n          \n            a\n          \n        \n        \n          u\n          \n            b\n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {du_{c}}{ds}}-m{\\frac {1}{2}}g_{ab,c}u^{a}u^{b}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        \n          u\n          \n            a\n          \n        \n        =\n        d\n        \n          x\n          \n            a\n          \n        \n        \n          /\n        \n        d\n        s\n      \n    \n    {\\displaystyle u^{a}=dx^{a}/ds}\n  \n (\n  \n    \n      \n        d\n        \n          x\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle dx^{a}}\n  \n is taken along the trajectory), \n  \n    \n      \n        \n          g\n          \n            a\n            b\n            ,\n            c\n          \n        \n        =\n        ∂\n        \n          g\n          \n            a\n            b\n          \n        \n        \n          /\n        \n        ∂\n        \n          x\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle g_{ab,c}=\\partial g_{ab}/\\partial x^{c}}\n  \n, and \n  \n    \n      \n        d\n        \n          s\n          \n            2\n          \n        \n        =\n        \n          g\n          \n            a\n            b\n          \n        \n        d\n        \n          x\n          \n            a\n          \n        \n        d\n        \n          x\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle ds^{2}=g_{ab}dx^{a}dx^{b}}\n  \n.\n\nThe equation can also be written as\n\n  \n    \n      \n        m\n        \n          \n            \n              d\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        −\n        m\n        \n          Γ\n          \n            a\n            b\n            c\n          \n        \n        \n          u\n          \n            a\n          \n        \n        \n          u\n          \n            b\n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {du_{c}}{ds}}-m\\Gamma _{abc}u^{a}u^{b}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        \n          Γ\n          \n            a\n            b\n            c\n          \n        \n      \n    \n    {\\displaystyle \\Gamma _{abc}}\n  \n is the Christoffel symbol (of the torsion-free metric connection in general relativity), or as\n\n  \n    \n      \n        m\n        \n          \n            \n              D\n              \n                u\n                \n                  c\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        =\n        e\n        \n          F\n          \n            c\n            b\n          \n        \n        \n          u\n          \n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle m{\\frac {Du_{c}}{ds}}=eF_{cb}u^{b},}\n  \n\nwhere \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the covariant differential in general relativity (metric, torsion-free).\n\nThe Lorentz force occurs in many devices, including:\n\nIn its manifestation as the Laplace force on an electric current in a conductor, this force occurs in many devices, including:\n\nThe numbered references refer in part to the list immediately below.",
        pageTitle: "Lorentz force",
    },
    {
        title: "Meadow's law",
        link: "https://en.wikipedia.org/wiki/Meadow%27s_law",
        content:
            "Meadow's Law is a now-discredited[1][2][3] legal concept once used to adjudicate cases involving multiple instances of sudden infant death syndrome (SIDS), also known as crib or cot deaths, linked to a single caregiver. Due to the rarity and often inexplicable nature of these deaths, the law posited that \"one sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise.\" Now recognized as fundamentally flawed and based on misunderstanding of statistics, Meadow's Law has been heavily criticized for leading to wrongful convictions and accusations.[4]\n\nThe name is derived from the controversial British paediatrician Roy Meadow, who until 2003 was seen by many as \"Britain's most eminent paediatrician\" and leading expert on child abuse.[5] Meadow's reputation went into decline with a series of legal reverses for his theories, and in July 2005 he was struck off the medical register by the General Medical Council for tendering misleading evidence.  Meadow's licence was reinstated in February 2006 by a London court.\n\nMeadow attributes many unexplained infant deaths to the disorder or condition in mothers called Munchausen syndrome by proxy. According to this diagnosis some parents, especially mothers, harm or even kill their children as a means of calling attention to themselves.  Its existence has been confirmed by cases where parents have been caught on video surveillance actively harming their children,[6] but its frequency is subject to debate as Meadow claimed to have destroyed the original data which he used to substantiate the law.[7]\n\nAs a result of the 1993 trial of Beverley Allitt, a paediatric nurse convicted of killing four children under her care and injuring five others, Meadow's ideas gained ascendancy in British child protection circles, and mothers were convicted of murder on the basis of his expert testimony.[8] Thousands of children[citation needed] were removed from their parents and taken into care or fostered out because they were deemed to be 'at risk'. From 2003, however, the tide of opinion turned: a number of high-profile acquittals cast doubt on the validity of 'Meadow's Law'. Several convictions were reversed, and many more came under review.\n\nIn a note to his mathematical analysis of the Sally Clark case, Professor Ray Hill endorses a claim that Meadow did not originate the rule:\n\nProfessor Meadow did not originate the law. It appears to be attributable to D. J. and V. J. M. Di Maio, two American pathologists who state in their book:[9] It is the authors' opinion that while a second SIDS death from a mother is improbable, it is possible and she should be given the benefit of the doubt. A third case, in our opinion, is not possible and is a case of homicide. It is clear that the statement is the authors' opinion. It is not a conclusion reached by analysis of their observations; no supportive data are presented and there are no illustrative case histories, or references to earlier publications. This is in striking contrast with the rest of the book which is replete with illustrative case histories and cites many references throughout. A recent examination of Meadow's own contributions to the medical literature has likewise failed to uncover supportive pathological evidence or references to it\n\nThe precept was published in the United States by DiMaio and DiMaio in 1989, without mention of Meadow. In ABC of Child Abuse, first published in the same year, Meadow wrote his formulation:\n\n'One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise' is a crude aphorism but a sensible working rule for anyone encountering these tragedies\n\nThe formula is \"clearly fallacious\" according to Bob Carpenter, Professor of Medical Statistics at the London School of Hygiene and Tropical Medicine, an expert witness in some of the trials where infant cot deaths were prosecuted as homicides.[12]\n\nCritics of Meadow's law state that it is based on a fundamental misunderstanding of statistics, particularly relating to probability, likelihood, and statistical independence.\n\nAt the trial in 1999 of solicitor Sally Clark, accused of murdering her two sons, Meadow testified that the odds against two such deaths happening naturally was 73,000,000:1, a figure which he obtained by squaring the observed ratio of births to cot-deaths in affluent non-smoking families (approximately 8,500:1).\n\nThis caused an uproar among professional statisticians, whose criticisms were twofold:\n\nFirstly, Meadow was accused of espousing the so-called prosecutor's fallacy in which the probability of \"cause given effect\" (i.e. the true likelihood of a suspect's innocence) is confused with that of \"effect given cause\" (the likelihood that innocence will result in the observed double-cot-death). In reality, these quantities can only be equated when the likelihood of the alternative hypothesis, in this case murder, is close to certainty. Since murder (and especially double murder) is itself a rare event, the probability of Clark's innocence was certainly far greater than Meadow's figure suggested.\n\nAn equivalent error is to accuse anybody who wins a lottery of fraud.\n\nThe second criticism was that Meadow's calculation had assumed that cot deaths within a single family were statistically independent events, governed by a probability common to the entire affluent non-smoking population. No account had been taken of conditions specific to individual families (such as a hypothesised \"cot death gene\") which might make some more vulnerable than others. The occurrence of one cot-death makes it likely that such conditions exist, and the probability of subsequent deaths is therefore greater than the group average (estimates are mostly in the region of 1:100).\n\nCombining these corrections with estimates of successive murder probabilities by affluent non-smokers, Mathematics Professor Ray Hill found that the probability of Clark's guilt could be as low as 10% (based solely on the fact of two unexplained child deaths, and before any other evidence was considered).[11]  In any case, a legal verdict is not to be rendered on the basis of statistics; Hill wrote, \"guilt must be proved on the basis of forensic and other evidence and not on the basis of these statistics alone.  My own personal view that she is innocent is based on my subjective assessment of all the aspects\".[13]",
        pageTitle: "Meadow's law",
    },
    {
        title: "Mendel's laws",
        link: "https://en.wikipedia.org/wiki/Mendelian_inheritance",
        content:
            'Mendelian inheritance (also known as Mendelism) is a type of biological inheritance following the principles originally proposed by Gregor Mendel in 1865 and 1866, re-discovered in 1900 by Hugo de Vries and Carl Correns, and later popularized by William Bateson.[1] These principles were initially controversial. When Mendel\'s theories were integrated with the Boveri–Sutton chromosome theory of inheritance by Thomas Hunt Morgan in 1915, they became the core of classical genetics. Ronald Fisher combined these ideas with the theory of natural selection in his 1930 book The Genetical Theory of Natural Selection, putting evolution onto a mathematical footing and forming the basis for population genetics within the modern evolutionary synthesis.[2]\n\nThe principles of Mendelian inheritance were named for and first derived by Gregor Johann Mendel,[3] a nineteenth-century Moravian monk who formulated his ideas after conducting simple hybridization experiments with pea plants (Pisum sativum) he had planted in the garden of his monastery.[4] Between 1856 and 1863, Mendel cultivated and tested some 5,000 pea plants. From these experiments, he induced two generalizations which later became known as Mendel\'s Principles of Heredity or Mendelian inheritance. He described his experiments in a two-part paper, Versuche über Pflanzen-Hybriden (Experiments on Plant Hybridization),[5] that he presented to the Natural History Society of Brno on 8 February and 8 March 1865, and which was published in 1866.[3][6][7][8]\n\nMendel\'s results were at first largely ignored. Although they were not completely unknown to biologists of the time, they were not seen as generally applicable, even by Mendel himself, who thought they only applied to certain categories of species or traits. A major roadblock to understanding their significance was the importance attached by 19th-century biologists to the apparent blending of many inherited traits in the overall appearance of the progeny,[citation needed] now known to be due to multi-gene interactions, in contrast to the organ-specific binary characters studied by Mendel.[4] In 1900, however, his work was "re-discovered" by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak. The exact nature of the "re-discovery" has been debated: De Vries published first on the subject, mentioning Mendel in a footnote, while Correns pointed out Mendel\'s priority after having read De Vries\' paper and realizing that he himself did not have priority. De Vries may not have acknowledged truthfully how much of his knowledge of the laws came from his own work and how much came only after reading Mendel\'s paper. Later scholars have accused Von Tschermak of not truly understanding the results at all.[9][10]\n\nRegardless, the "re-discovery" made Mendelism an important but controversial theory. Its most vigorous promoter in Europe was William Bateson, who coined the terms "genetics" and "allele" to describe many of its tenets.[11] The model of heredity was contested by other biologists because it implied that heredity was discontinuous, in opposition to the apparently continuous variation observable for many traits.[12] Many biologists also dismissed the theory because they were not sure it would apply to all species. However, later work by biologists and statisticians such as Ronald Fisher showed that if multiple Mendelian factors were involved in the expression of an individual trait, they could produce the diverse results observed, thus demonstrating that Mendelian genetics is compatible with natural selection.[13][14] Thomas Hunt Morgan and his assistants later integrated Mendel\'s theoretical model with the chromosome theory of inheritance, in which the chromosomes of cells were thought to hold the actual hereditary material, and created what is now known as classical genetics, a highly successful foundation which eventually cemented Mendel\'s place in history.[3][11]\n\nMendel\'s findings allowed scientists such as Fisher and J.B.S. Haldane to predict the expression of traits on the basis of mathematical probabilities. An important aspect of Mendel\'s success can be traced to his decision to start his crosses only with plants he demonstrated were true-breeding.[4][13] He only measured discrete (binary) characteristics, such as color, shape, and position of the seeds, rather than quantitatively variable characteristics. He expressed his results numerically and subjected them to statistical analysis. His method of data analysis and his large sample size gave credibility to his data. He had the foresight to follow several successive generations (P, F1, F2, F3) of pea plants and record their variations. Finally, he performed "test crosses" (backcrossing descendants of the initial hybridization to the initial true-breeding lines) to reveal the presence and proportions of recessive characters.[15]\n\nPunnett Squares are a well known genetics tool that was created by an English geneticist, Reginald Punnett, which can visually demonstrate all the possible genotypes that an offspring can receive, given the genotypes of their parents.[16][17][18] Each parent carries two alleles, which can be shown on the top and the side of the chart, and each contribute one of them towards reproduction at a time. Each of the squares in the middle demonstrates the number of times each pairing of parental alleles could combine to make potential offspring. Using probabilities, one can then determine which genotypes the parents can create, and at what frequencies they can be created.[16][18]\n\nFor example, if two parents both have a heterozygous genotype, then there would be a 50% chance for their offspring to have the same genotype, and a 50% chance they would have a homozygous genotype. Since they could possibly contribute two identical alleles, the 50% would be halved to 25% to account for each type of homozygote, whether this was a homozygous dominant genotype, or a homozygous recessive genotype.[16][17][18]\n\nPedigrees are visual tree like representations that demonstrate exactly how alleles are being passed from past generations to future ones.[19] They also provide a diagram displaying each individual that carries a desired allele, and exactly which side of inheritance it was received from, whether it was from their mother\'s side or their father\'s side.[19] Pedigrees can also be used to aid researchers in determining the inheritance pattern for the desired allele, because they share information such as the gender of all individuals, the phenotype, a predicted genotype, the potential sources for the alleles, and also based its history, how it could continue to spread in the future generations to come. By using pedigrees, scientists have been able to find ways to control the flow of alleles over time, so that alleles that act problematic can be resolved upon discovery.[20]\n\nFive parts of Mendel\'s discoveries were an important divergence from the common theories at the time and were the prerequisite for the establishment of his rules.\n\nAccording to customary terminology, the principles of inheritance discovered by Gregor Mendel are here referred to as Mendelian laws, although today\'s geneticists also speak of Mendelian rules or Mendelian principles,[21][22] as there are many exceptions summarized under the collective term Non-Mendelian inheritance. The laws were initially formulated by the geneticist Thomas Hunt Morgan in 1916.[23]\n\nMendel selected for the experiment the following characters of pea plants:\n\nWhen he crossed purebred white flower and purple flower pea plants (the parental or P generation) by artificial pollination, the resulting flower colour was not a blend. Rather than being a mix of the two, the offspring in the first generation (F1-generation) were all purple-flowered. Therefore, he called this biological trait dominant. When he allowed self-fertilization in the uniform looking F1-generation, he obtained both colours in the F2 generation with a purple flower to white flower ratio of 3 : 1. In some of the other characters also one of the traits was dominant.\n\nHe then conceived the idea of heredity units, which he called hereditary "factors". Mendel found that there are alternative forms of factors—now called genes—that account for variations in inherited characteristics. For example, the gene for flower color in pea plants exists in two forms, one for purple and the other for white. The alternative "forms" are now called alleles. For each trait, an organism inherits two alleles, one from each parent. These alleles may be the same or different. An organism that has two identical alleles for a gene is said to be homozygous for that gene (and is called a homozygote). An organism that has two different alleles for a gene is said to be heterozygous for that gene (and is called a heterozygote).\n\nMendel hypothesized that allele pairs separate randomly, or segregate, from each other during the production of the gametes in the seed plant (egg cell) and the pollen plant (sperm). Because allele pairs separate during gamete production, a sperm or egg carries only one allele for each inherited trait. When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. Mendel also found that each pair of alleles segregates independently of the other pairs of alleles during gamete formation.\n\nThe genotype of an individual is made up of the many alleles it possesses. The phenotype is the result of the expression of all characteristics that are genetically determined by its alleles as well as by its environment. The presence of an allele does not mean that the trait will be expressed in the individual that possesses it. If the two alleles of an inherited pair differ (the heterozygous condition), then one determines the organism\'s appearance and is called the dominant allele; the other has no noticeable effect on the organism\'s appearance and is called the recessive allele.\n\nIf two parents are mated with each other who differ in one genetic characteristic for which they are both homozygous (each pure-bred), all offspring in the first generation (F1) are equal to the examined characteristic in genotype and phenotype showing the dominant trait. This uniformity rule or reciprocity rule applies to all individuals of the F1-generation.[30]\n\nThe principle of dominant inheritance discovered by Mendel states that in a heterozygote the dominant allele will cause the recessive allele to be "masked": that is, not expressed in the phenotype. Only if an individual is homozygous with respect to the recessive allele will the recessive trait be expressed. Therefore, a cross between a homozygous dominant and a homozygous recessive organism yields a heterozygous organism whose phenotype displays only the dominant trait.\n\nThe F1 offspring of Mendel\'s pea crosses always looked like one of the two parental varieties. In this situation of "complete dominance", the dominant allele had the same phenotypic effect whether present in one or two copies.\n\nBut for some characteristics, the F1 hybrids have an appearance in between the phenotypes of the two parental varieties. A cross between two four o\'clock (Mirabilis jalapa) plants shows an exception to Mendel\'s principle, called incomplete dominance. Flowers of heterozygous plants have a phenotype somewhere between the two homozygous genotypes. In cases of intermediate inheritance (incomplete dominance) in the F1-generation Mendel\'s principle of uniformity in genotype and phenotype applies as well. Research about intermediate inheritance was done by other scientists. The first was Carl Correns with his studies about Mirabilis jalapa.[28][31][32][33][34]\n\nThe Law of Segregation of genes applies when two individuals, both heterozygous for a certain trait are crossed, for example, hybrids of the F1-generation. The offspring in the F2-generation differ in genotype and phenotype so that the characteristics of the grandparents (P-generation) regularly occur again. In a dominant-recessive inheritance, an average of 25% are homozygous with the dominant trait, 50% are heterozygous showing the dominant trait in the phenotype (genetic carriers), 25% are homozygous with the recessive trait and therefore express the recessive trait in the phenotype. The genotypic ratio is 1: 2 : 1, and the phenotypic ratio is 3: 1.\n\nIn the pea plant example, the capital "B" represents the dominant allele for purple blossom and lowercase "b" represents the recessive allele for white blossom. The pistil plant and the pollen plant are both F1-hybrids with genotype "B b". Each has one allele for purple and one allele for white. In the offspring, in the F2-plants in the Punnett-square, three combinations are possible. The genotypic ratio is 1 BB : 2 Bb : 1 bb. But the phenotypic ratio of plants with purple blossoms to those with white blossoms is 3 : 1 due to the dominance of the allele for purple. Plants with homozygous "b b" are white flowered like one of the grandparents in the P-generation.\n\nIn cases of incomplete dominance the same segregation of alleles takes place in the F2-generation, but here also the phenotypes show a ratio of 1 : 2 : 1, as the heterozygous are different in phenotype from the homozygous because the genetic expression of one allele compensates the missing expression of the other allele only partially. This results in an intermediate inheritance which was later described by other scientists.\n\nIn some literature sources, the principle of segregation is cited as the "first law". Nevertheless, Mendel did his crossing experiments with heterozygous plants after obtaining these hybrids by crossing two purebred plants, discovering the principle of dominance and uniformity first.[35][27]\n\nMolecular proof of segregation of genes was subsequently found through observation of meiosis by two scientists independently, the German botanist Oscar Hertwig in 1876, and the Belgian zoologist Edouard Van Beneden in 1883. Most alleles are located in chromosomes in the cell nucleus. Paternal and maternal chromosomes get separated in meiosis because during spermatogenesis the chromosomes are segregated on the four sperm cells that arise from one mother sperm cell, and during oogenesis the chromosomes are distributed between the polar bodies and the egg cell. Every individual organism contains two alleles for each trait. They segregate (separate) during meiosis such that each gamete contains only one of the alleles.[36] When the gametes unite in the zygote the alleles—one from the mother one from the father—get passed on to the offspring. An offspring thus receives a pair of alleles for a trait by inheriting homologous chromosomes from the parent organisms: one allele for each trait from each parent.[36] Heterozygous individuals with the dominant trait in the phenotype are genetic carriers of the recessive trait.\n\nThe Law of Independent Assortment proposes alleles for separate traits are passed independently of one another.[40][35] That is, the biological selection of an allele for one trait has nothing to do with the selection of an allele for any other trait. Mendel found support for this law in his dihybrid cross experiments. In his monohybrid crosses, an idealized 3:1 ratio between dominant and recessive phenotypes resulted. In dihybrid crosses, however, he found a 9:3:3:1 ratios. This shows that each of the two alleles is inherited independently from the other, with a 3:1 phenotypic ratio for each.\n\nIndependent assortment occurs in eukaryotic organisms during meiotic metaphase I, and produces a gamete with a mixture of the organism\'s chromosomes. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent chromosome along the metaphase plate with respect to the other bivalent chromosomes. Along with crossing over, independent assortment increases genetic diversity by producing novel genetic combinations.\n\nThere are many deviations from the principle of independent assortment due to genetic linkage.\n\nOf the 46 chromosomes in a normal diploid human cell, half are maternally derived (from the mother\'s egg) and half are paternally derived (from the father\'s sperm). This occurs as sexual reproduction involves the fusion of two haploid gametes (the egg and sperm) to produce a zygote and a new organism, in which every cell has two sets of chromosomes (diploid). During gametogenesis the normal complement of 46 chromosomes needs to be halved to 23 to ensure that the resulting haploid gamete can join with another haploid gamete to produce a diploid organism.\n\nIn independent assortment, the chromosomes that result are randomly sorted from all possible maternal and paternal chromosomes. Because zygotes end up with a mix instead of a pre-defined "set" from either parent, chromosomes are therefore considered assorted independently. As such, the zygote can end up with any combination of paternal or maternal chromosomes. For human gametes, with 23 chromosomes, the number of possibilities is 223 or 8,388,608 possible combinations.[41] This contributes to the genetic variability of progeny. Generally, the recombination of genes has important implications for many evolutionary processes.[42][43][44]\n\nA Mendelian trait is one whose inheritance follows Mendel\'s principles—namely, the trait depends only on a single locus, whose alleles are either dominant or recessive.\n\nMany traits are inherited in a non-Mendelian fashion.[45]\n\nMendel himself warned that care was needed in extrapolating his patterns to other organisms or traits. Indeed, many organisms have traits whose inheritance works differently from the principles he described; these traits are called non-Mendelian.[46][47]\n\nFor example, Mendel focused on traits whose genes have only two alleles, such as "A" and "a". However, many genes have more than two alleles. He also focused on traits determined by a single gene. But some traits, such as height, depend on many genes rather than just one. Traits dependent on multiple genes are called polygenic traits.',
        pageTitle: "Mendelian inheritance",
    },
    {
        title: "Menzerath's law",
        link: "https://en.wikipedia.org/wiki/Menzerath%27s_law",
        content:
            "Menzerath's law, also known as the Menzerath–Altmann law (named after Paul Menzerath and Gabriel Altmann), is a linguistic law according to which the increase of the size of a linguistic construct results in a decrease of the size of its constituents, and vice versa.[1][2]\n\nFor example, the longer a sentence (measured in terms of the number of clauses), the shorter the clauses (measured in terms of the number of words), or: the longer a word (in syllables or morphs), the shorter the syllables or morphs in sounds.\n\nIn the 19th century, Eduard Sievers observed that vowels in short words are pronounced longer than the same vowels in long words.[3][4]: 122  Menzerath & de Oleza (1928)[5] expanded this observation to state that, as the number of syllables in words increases, the syllables themselves become shorter on average.\n\nThe larger a linguistic construct, the smaller its constituents.\n\nIn the early 1980s, Altmann, Heups,[6] and Köhler[7] demonstrated using quantitative methods that this postulate can also be applied to larger constructs of natural language: the larger the sentence, the smaller the individual clauses, etc. A prerequisite for such relationships is that a relationship between units (here: sentence) and their direct constituents (here: clause) is examined.[8][9][1]: Übersichten\n\nAccording to Altmann (1980),[8] it can be mathematically stated as:\n\ny\n        =\n        a\n        ⋅\n        \n          x\n          \n            b\n          \n        \n        ⋅\n        \n          e\n          \n            −\n            c\n            x\n          \n        \n      \n    \n    {\\displaystyle y=a\\cdot x^{b}\\cdot e^{-cx}}\n\nThe law can be explained by assuming that linguistic segments contain information about their structure (besides the information that needs to be communicated).[7] The assumption that the length of the structure information is independent of the length of the other content of the segment yields the alternative formula that was also successfully empirically tested.[10]\n\nGerlach (1982)[11] checked a German dictionary[12] with about 15,000 entries:\n\nWhere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is the number of morphs per word, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the number of words in the dictionary with length \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n; \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is the observed average length of morphs (number of phonemes per morph); \n  \n    \n      \n        \n          y\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle y^{*}}\n  \n is the prediction according to \n  \n    \n      \n        y\n        =\n        a\n        \n          x\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle y=ax^{b}}\n  \n where \n  \n    \n      \n        a\n        ,\n        b\n      \n    \n    {\\displaystyle a,b}\n  \n are fited to data. The F-test has \n  \n    \n      \n        p\n        <\n        0.001\n      \n    \n    {\\displaystyle p<0.001}\n  \n.\n\nAs another example, the simplest form of Menzerath's law, \n  \n    \n      \n        y\n        =\n        a\n        \n          x\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle y=ax^{b}}\n  \n, holds for the duration of vowels in Hungarian words:[13]\n\nMore examples are on the German Wikipedia pages on phoneme duration, syllable duration, word length, clause length, and sentence length.\n\nThis law also seems to hold true for at least a subclass of Japanese Kanji characters.[14]\n\nBeyond quantitative linguistics, Menzerath's law can be discussed in any multi-level complex systems. Given three levels, \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is the number of middle-level units contained in a high-level unit, \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is the averaged number of low-level units contained in middle-level units, Menzerath's law claims a negative correlation between \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n and \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\n\nMenzerath's law is shown to be true for both the base-exon-gene levels in the human genome,[15] and base-chromosome-genome levels in genomes from a collection of species.[16] In addition, Menzerath's law was shown to accurately predict the distribution of protein lengths in terms of amino acid number in the proteome of ten organisms.[17]\n\nFurthermore, studies have shown that the social behavior of baboon groups also corresponds to Menzerath's Law: the larger the entire group, the smaller the subordinate social groups.[1]: 99 ff\n\nIn 2016, a research group at the University of Michigan found that the calls of geladas obey Menzerath's law, observing that calls are abbreviated when used in longer sequences.[18][19]",
        pageTitle: "Menzerath's law",
    },
    {
        title: "Metcalfe's law",
        link: "https://en.wikipedia.org/wiki/Metcalfe%27s_law",
        content:
            "Metcalfe's law states that the financial value or influence of a telecommunications network is proportional to the square of the number of connected users of the system (n2). The law is named after Robert Metcalfe and was first proposed in 1980, albeit not in terms of users, but rather of \"compatible communicating devices\" (e.g., fax machines, telephones).[1] It later became associated with users on the Ethernet after a September 1993 Forbes article by George Gilder.[2]\n\nMetcalfe's law characterizes many of the network effects of communication technologies and networks such as the Internet, social networking and the World Wide Web. Former Chairman of the U.S. Federal Communications Commission Reed Hundt said that this law gives the most understanding to the workings of the present-day Internet.[3] Mathematically, Metcalfe's Law shows that the number of unique possible connections in an \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-node connection can be expressed as the triangular number \n  \n    \n      \n        n\n        (\n        n\n        −\n        1\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n(n-1)/2}\n  \n, which is asymptotically proportional to \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n  \n.\n\nThe law has often been illustrated using the example of fax machines: a single fax machine on its own is useless, but the value of every fax machine increases with the total number of fax machines in the network, because the total number of people with whom each user may send and receive documents increases.[4] This is common illustration to explain network effect. Thus, in any social network, the greater the number of users with the service, the more valuable the service becomes to the community.\n\nMetcalfe's law was conceived in 1983 in a presentation to the 3Com sales force.[5] It stated V would be proportional to the total number of possible connections, or approximately n-squared.\n\nThe original incarnation was careful to delineate between a linear cost (Cn), non-linear growth(n2) and a non-constant proportionality factor affinity (A). The break-even point point where costs are recouped is given by:\n  \n    \n      \n        C\n        ×\n        n\n        =\n        A\n        ×\n        n\n        (\n        n\n        −\n        1\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle C\\times n=A\\times n(n-1)/2}\n  \nAt some size, the right-hand side of the equation V, value, exceeds the cost, and A describes the relationship between size and net value added. For large n, net network value is then:\n  \n    \n      \n        Π\n        =\n        n\n        (\n        A\n        ×\n        (\n        n\n        −\n        1\n        )\n        \n          /\n        \n        2\n        −\n        C\n        )\n      \n    \n    {\\displaystyle \\Pi =n(A\\times (n-1)/2-C)}\n  \nMetcalfe properly dimensioned A as \"value per user\". Affinity is also a function of network size, and Metcalfe correctly asserted that A must decline as n grows large. In a 2006 interview, Metcalfe stated:[6]\n\nThere may be diseconomies of network scale that eventually drive values down with increasing size. So, if V = An2, it could be that A (for “affinity,” value per connection) is also a function of n and heads down after some network size, overwhelming n2.\n\nNetwork size, and hence value, does not grow unbounded but is constrained by practical limitations such as infrastructure, access to technology, and bounded rationality such as Dunbar's number. It is almost always the case that user growth n reaches a saturation point. With technologies, substitutes, competitors and technical obsolescence constrain growth of n. Growth of n is typically assumed to follow a sigmoid function such as a logistic curve or Gompertz curve.\n\nA is also governed by the connectivity or density of the network topology. In an undirected network, every edge connects two nodes such that there are 2m nodes per edge. The proportion of nodes in actual contact are given by \n  \n    \n      \n        c\n        =\n        2\n        m\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle c=2m/n}\n  \n.\n\nThe maximum possible number of edges in a simple network (i.e. one with no multi-edges or self-edges) is \n  \n    \n      \n        \n          \n            \n              (\n            \n            \n              n\n              2\n            \n            \n              )\n            \n          \n        \n        =\n        n\n        (\n        n\n        −\n        1\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle {\\binom {n}{2}}=n(n-1)/2}\n  \n. \nTherefore the density ρ of a network is the faction of those edges that are actually present is:\n\nwhich for large networks is approximated by \n  \n    \n      \n        ρ\n        =\n        c\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle \\rho =c/n}\n  \n.[7]\n\nMetcalfe's law assumes that the value of each node \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is of equal benefit.[3] If this is not the case, for example because one fax machine serves 60 workers in a company, the second fax machine serves half of that, the third one third, and so on, then the relative value of an additional connection decreases. Likewise, in social networks, if users that join later use the network less than early adopters, then the benefit of each additional user may lessen, making the overall network less efficient if costs per users are fixed.\n\nWithin the context of social networks, many, including Metcalfe himself, have proposed modified models in which the value of the network grows as \n  \n    \n      \n        n\n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle n\\log n}\n  \n rather than \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n  \n.[8][3] Reed[non sequitur] and Andrew Odlyzko have sought out possible relationships to Metcalfe's Law in terms of describing the relationship of a network and one can read about how those are related. Tongia and Wilson also examine the related question of the costs to those excluded.[9]\n\nFor more than 30 years, there was little concrete evidence in support of the law. Finally, in July 2013, Dutch researchers analyzed European Internet-usage patterns over a long-enough time[specify] and found \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n  \n proportionality for small values of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n and \n  \n    \n      \n        n\n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle n\\log n}\n  \n proportionality for large values of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n.[10] A few months later, Metcalfe himself provided further proof by using Facebook's data over the past 10 years to show a good fit for Metcalfe's law.[11]\n\nIn 2015, Zhang, Liu, and Xu parameterized the Metcalfe function in data from Tencent and Facebook. Their work showed that Metcalfe's law held for both, despite differences in audience between the two sites (Facebook serving a worldwide audience and Tencent serving only Chinese users). The functions for the two sites were \n  \n    \n      \n        \n          V\n          \n            Tencent\n          \n        \n        =\n        7.39\n        ×\n        \n          10\n          \n            −\n            9\n          \n        \n        ×\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle V_{\\text{Tencent}}=7.39\\times 10^{-9}\\times n^{2}}\n  \n and \n  \n    \n      \n        \n          V\n          \n            Facebook\n          \n        \n        =\n        5.70\n        ×\n        \n          10\n          \n            −\n            9\n          \n        \n        ×\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle V_{\\text{Facebook}}=5.70\\times 10^{-9}\\times n^{2}}\n  \n respectively.[12]\nOne of the earliest mentions of the Metcalfe Law in the context of Bitcoin was by a Reddit post by Santostasi in 2014. He compared the observed generalized Metcalfe behavior for Bitcoin to the Zipf's Law and the theoretical Metcalfe result.[13]\nThe Metcalfe's Law is a critical component of Santostasi's Bitcoin Power Law Theory.[14]\nIn a working paper, Peterson linked time-value-of-money concepts to Metcalfe value using Bitcoin and Facebook as numerical examples of the proof,[15] and in 2018 applied Metcalfe's law to Bitcoin, showing that over 70% of variance in Bitcoin value was explained by applying Metcalfe's law to increases in Bitcoin network size.[16]\n\nIn a 2024 interview, mathematician Terence Tao emphasized the importance of universality and networking within the mathematics community, for which he cited the Metcalfe's Law. Tao believes that a larger audience leads to more connections, which ultimately results in positive developments within the community. For this, he cited Metcalfe's law to support this perspective. Tao further stated, \"my whole career experience has been sort of the more connections equals just better stuff happening\".[17]",
        pageTitle: "Metcalfe's law",
    },
    {
        title: "Miller's law",
        link: "https://en.wikipedia.org/wiki/Miller%27s_law",
        content:
            "Miller's law can refer to four unrelated principles.\n\nThe Miller's law used in communication was formulated by George Armitage Miller (1920–2012), a professor of psychology at Princeton University, as part of his theory of communication. According to it, one should suspend judgment about what someone else is saying to first understand them without imbuing their message with personal interpretations.\n\n...that in order to understand what someone is telling you, it is necessary for you to assume the person is being truthful, then imagine what could be true about it.\n\nThe Miller's law used in linguistics is a sound law of postnasal deaspiration in ancient Greek.\n\nThe Miller's law used in psychology is the observation, also by George Armitage Miller, that the number of objects the average person can hold in working memory is about seven.[4] It was put forward in a 1956 edition of Psychological Review in a paper titled \"The Magical Number Seven, Plus or Minus Two\".[5][6][7]\n\nThe Miller's law used in software development was formulated by Mike Beltzner and is named in respect of Dave Miller, long-standing owner of the Bugzilla product:\n\nAll discussions of incremental updates to Bugzilla will eventually trend towards proposals for large scale redesigns or feature additions or replacements for Bugzilla.",
        pageTitle: "Miller's law",
    },
    {
        title: "Mooers's law",
        link: "https://en.wikipedia.org/wiki/Mooers%27s_law",
        content:
            "Mooers's law is a comment about the use of information retrieval systems made by the American computer scientist Calvin Mooers in 1959:\n\nAn information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.\n\nMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any impliciations of the information that may conflict with the user's prior information. In learning new information, a user may end up proving their work incorrect or irrelevant. Mooers argued that users prefer to remain in a state of safety in which new information is ignored in an attempt to save potential embarrassment or reprisal from supervisors.[2]\n\nThe more common interpretation of Mooers's law is similar to Zipf's principle of least effort. It emphasizes the amount of effort needed to use and understand an information retrieval system before the information seeker gives up; it is often paraphrased to increase the focus on the retrieval system:\n\nThe more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.\n\nMooers's Law tells us that information will be used in direct proportion to how easy it is to obtain.\n\nIn this interpretation, \"painful and troublesome\" comes from using the retrieval system.",
        pageTitle: "Mooers's law",
    },
    {
        title: "Moore's law",
        link: "https://en.wikipedia.org/wiki/Moore%27s_law",
        content:
            "Moore's law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship. It is an experience-curve law, a type of law quantifying efficiency gains from experience in production.\n\nThe observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel and former CEO of the latter, who in 1965 noted that the number of components per integrated circuit had been doubling every year,[a] and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41%. Moore's empirical evidence did not directly imply that the historical trend would continue, nevertheless, his prediction has held since 1975 and has since become known as a law.\n\nMoore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development (R&D). Advancements in digital electronics, such as the reduction in quality-adjusted prices of microprocessors, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.\n\nIndustry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore's law. In September 2022, Nvidia CEO Jensen Huang considered Moore's law dead,[2] while Intel CEO Pat Gelsinger was of the opposite view.[3]\n\nIn 1959, Douglas Engelbart studied the projected downscaling of integrated circuit (IC) size, publishing his results in the article \"Microelectronics, and the Art of Similitude\".[4][5][6] Engelbart presented his findings at the 1960 International Solid-State Circuits Conference, where Moore was present in the audience.[7]\n\nIn 1965, Gordon Moore, who at the time was working as the director of research and development at Fairchild Semiconductor, was asked to contribute to the thirty-fifth-anniversary issue of Electronics magazine with a prediction on the future of the semiconductor components industry over the next ten years.[8] His response was a brief article entitled \"Cramming more components onto integrated circuits\".[1][9][b] Within his editorial, he speculated that by 1975 it would be possible to contain as many as 65000 components on a single quarter-square-inch (~ 1.6 cm2) semiconductor.\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.[1]\n\nMoore posited a log–linear relationship between device complexity (higher circuit density at reduced cost) and time.[12][13] In a 2015 interview, Moore noted of the 1965 article: \"... I just did a wild extrapolation saying it's going to continue to double every year for the next 10 years.\"[14] One historian of the law cites Stigler's law of eponymy, to introduce the fact that the regular doubling of components was known to many working in the field.[13]\n\nIn 1974, Robert H. Dennard at IBM recognized the rapid MOSFET scaling technology and formulated what became known as Dennard scaling, which describes that as MOS transistors get smaller, their power density stays constant such that the power use remains in proportion with area.[15][16] Evidence from the semiconductor industry shows that this inverse relationship between power density and areal density broke down in the mid-2000s.[17]\n\nAt the 1975 IEEE International Electron Devices Meeting, Moore revised his forecast rate,[18][19] predicting semiconductor complexity would continue to double annually until about 1980, after which it would decrease to a rate of doubling approximately every two years.[19][20][21] He outlined several contributing factors for this exponential behavior:[12][13]\n\nShortly after 1975, Caltech professor Carver Mead popularized the term Moore's law.[22][23] Moore's law eventually came to be widely accepted as a goal for the semiconductor industry, and it was cited by competitive semiconductor manufacturers as they strove to increase processing power. Moore viewed his eponymous law as surprising and optimistic: \"Moore's law is a violation of Murphy's law. Everything gets better and better.\"[24] The observation was even seen as a self-fulfilling prophecy.[25][26]\n\nThe doubling period is often misquoted as 18 months because of a separate prediction by Moore's colleague, Intel executive David House.[27] In 1975, House noted that Moore's revised law of doubling transistor count every 2 years in turn implied that computer chip performance would roughly double every 18 months,[28] with no increase in power consumption.[29] Mathematically, Moore's law predicted that transistor count would double every 2 years due to shrinking transistor dimensions and other improvements.[30] As a consequence of shrinking dimensions, Dennard scaling predicted that power consumption per unit area would remain constant. Combining these effects, David House deduced that computer chip performance would roughly double every 18 months. Also due to Dennard scaling, this increased performance would not be accompanied by increased power, i.e., the energy-efficiency of silicon-based computer chips roughly doubles every 18 months. Dennard scaling ended in the 2000s.[17] Koomey later showed that a similar rate of efficiency improvement predated silicon chips and Moore's law, for technologies such as vacuum tubes.\n\nMicroprocessor architects report that since around 2010, semiconductor advancement has slowed industry-wide below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, cited Moore's 1975 revision as a precedent for the current deceleration, which results from technical challenges and is \"a natural part of the history of Moore's law\".[31][32][33] The rate of improvement in physical dimensions known as Dennard scaling also ended in the mid-2000s. As a result, much of the semiconductor industry has shifted its focus to the needs of major computing applications rather than semiconductor scaling.[25][34][17] Nevertheless, as of 2019, leading semiconductor manufacturers TSMC and Samsung Electronics claimed to keep pace with Moore's law[35][36][37][38][39][40] with 10, 7, and 5 nm nodes in mass production.[35][36][41][42][43]\n\nAs the cost of computer power to the consumer falls, the cost for producers to fulfill Moore's law follows an opposite trend: R&D, manufacturing, and test costs have increased steadily with each new generation of chips. The cost of the tools, principally EUVL (Extreme ultraviolet lithography), used to manufacture chips doubles every 4 years.[44] Rising manufacturing costs are an important consideration for the sustaining of Moore's law.[45] This led to the formulation of Moore's second law, also called Rock's law (named after Arthur Rock), which is that the capital cost of a semiconductor fabrication plant also increases exponentially over time.[46][47]\n\nNumerous innovations by scientists and engineers have sustained Moore's law since the beginning of the IC era. Some of the key innovations are listed below, as examples of breakthroughs that have advanced integrated circuit and semiconductor device fabrication technology, allowing transistor counts to grow by more than seven orders of magnitude in less than five decades.\n\nComputer industry technology road maps predicted in 2001 that Moore's law would continue for several generations of semiconductor chips.[71]\n\nOne of the key technical challenges of engineering future nanoscale transistors is the design of gates. As device dimensions shrink, controlling the current flow in the thin channel becomes more difficult. Modern nanoscale transistors typically take the form of multi-gate MOSFETs, with the FinFET being the most common nanoscale transistor. The FinFET has gate dielectric on three sides of the channel. In comparison, the gate-all-around MOSFET (GAAFET) structure has even better gate control.\n\nMicroprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, below the pace predicted by Moore's law.[17] Brian Krzanich, the former CEO of Intel, announced, \"Our cadence today is closer to two and a half years than two.\"[103] Intel stated in 2015 that improvements in MOSFET devices have slowed, starting at the 22 nm feature width around 2012, and continuing at 14 nm.[104] Pat Gelsinger, Intel CEO, stated at the end of 2023 that \"we're no longer in the golden era of Moore's Law, it's much, much harder now, so we're probably doubling effectively closer to every three years now, so we've definitely seen a slowing.\"[105]\n\nThe physical limits to transistor scaling have been reached due to source-to-drain leakage, limited gate metals and limited options for channel material. Other approaches are being investigated, which do not rely on physical scaling. These include the spin state of electron spintronics, tunnel junctions, and advanced confinement of channel materials via nano-wire geometry.[106] Spin-based logic and memory options are being developed actively in labs.[107][108]\n\nThe vast majority of current transistors on ICs are composed principally of doped silicon and its alloys. As silicon is fabricated into single nanometer transistors, short-channel effects adversely changes desired material properties of silicon as a functional transistor. Below are several non-silicon substitutes in the fabrication of small nanometer transistors.\n\nOne proposed material is indium gallium arsenide, or InGaAs. Compared to their silicon and germanium counterparts, InGaAs transistors are more promising for future high-speed, low-power logic applications. Because of intrinsic characteristics of III–V compound semiconductors, quantum well and tunnel effect transistors based on InGaAs have been proposed as alternatives to more traditional MOSFET designs.\n\nBiological computing research shows that biological material has superior information density and energy efficiency compared to silicon-based computing.[116]\n\nVarious forms of graphene are being studied for graphene electronics, e.g. graphene nanoribbon transistors have shown promise since its appearance in publications in 2008. (Bulk graphene has a band gap of zero and thus cannot be used in transistors because of its constant conductivity, an inability to turn off. The zigzag edges of the nanoribbons introduce localized energy states in the conduction and valence bands and thus a bandgap that enables switching when fabricated as a transistor. As an example, a typical GNR of width of 10 nm has a desirable bandgap energy of 0.4 eV.[117][118]) More research will need to be performed, however, on sub-50 nm graphene layers, as its resistivity value increases and thus electron mobility decreases.[117]\n\nIn April 2005, Gordon Moore stated in an interview that the projection cannot be sustained indefinitely: \"It can't continue forever. The nature of exponentials is that you push them out and eventually disaster happens.\" He also noted that transistors eventually would reach the limits of miniaturization at atomic levels:\n\nIn terms of size [of transistors] you can see that we're approaching the size of atoms which is a fundamental barrier, but it'll be two or three generations before we get that far—but that's as far out as we've ever been able to see. We have another 10 to 20 years before we reach a fundamental limit. By then they'll be able to make bigger chips and have transistor budgets in the billions.[119]\n\nIn 2016 the International Technology Roadmap for Semiconductors, after using Moore's Law to drive the industry since 1998, produced its final roadmap. It no longer centered its research and development plan on Moore's law. Instead, it outlined what might be called the More than Moore strategy in which the needs of applications drive chip development, rather than a focus on semiconductor scaling. Application drivers range from smartphones to AI to data centers.[120]\n\nIEEE began a road-mapping initiative in 2016, Rebooting Computing, named the International Roadmap for Devices and Systems (IRDS).[121]\n\nSome forecasters, including Gordon Moore,[122] predict that Moore's law will end by around 2025.[123][120][124] Although Moore's Law will reach a physical limit, some forecasters are optimistic about the continuation of technological progress in a variety of other areas, including new chip architectures, quantum computing, and AI and machine learning.[125][126] Nvidia CEO Jensen Huang declared Moore's law dead in 2022;[2] several days later, Intel CEO Pat Gelsinger countered with the opposite claim.[3]\n\nDigital electronics have contributed to world economic growth in the late twentieth and early twenty-first centuries.[127] The primary driving force of economic growth is the growth of productivity,[128] which Moore's law factors into. Moore (1995) expected that \"the rate of technological progress is going to be controlled from financial realities\".[129] The reverse could and did occur around the late-1990s, however, with economists reporting that \"Productivity growth is the key economic indicator of innovation.\"[130] Moore's law describes a driving force of technological and social change, productivity, and economic growth.[131][132][128]\n\nAn acceleration in the rate of semiconductor progress contributed to a surge in U.S. productivity growth,[133][134][135] which reached 3.4% per year in 1997–2004, outpacing the 1.6% per year during both 1972–1996 and 2005–2013.[136] As economist Richard G. Anderson notes, \"Numerous studies have traced the cause of the productivity acceleration to technological innovations in the production of semiconductors that sharply reduced the prices of such components and of the products that contain them (as well as expanding the capabilities of such products).\"[137]\n\nThe primary negative implication of Moore's law is that obsolescence pushes society up against the Limits to Growth. As technologies continue to rapidly improve, they render predecessor technologies obsolete. In situations in which security and survivability of hardware or data are paramount, or in which resources are limited, rapid obsolescence often poses obstacles to smooth or continued operations.[138]\n\nSeveral measures of digital technology are improving at exponential rates related to Moore's law, including the size, cost, density, and speed of components. Moore wrote only about the density of components, \"a component being a transistor, resistor, diode or capacitor\",[129] at minimum cost.\n\nTransistors per integrated circuit – The most popular formulation is of the doubling of the number of transistors on ICs every two years. At the end of the 1970s, Moore's law became known as the limit for the number of transistors on the most complex chips. The graph at the top of this article shows this trend holds true today. As of 2022[update], the commercially available processor possessing one of the highest numbers of transistors is an AD102 graphics processor with more than 76,3 billion transistors.[139]\n\nDensity at minimum cost per transistor – This is the formulation given in Moore's 1965 paper.[1] It is not just about the density of transistors that can be achieved, but about the density of transistors at which the cost per transistor is the lowest.[140]\n\nAs more transistors are put on a chip, the cost to make each transistor decreases, but the chance that the chip will not work due to a defect increases. In 1965, Moore examined the density of transistors at which cost is minimized, and observed that, as transistors were made smaller through advances in photolithography, this number would increase at \"a rate of roughly a factor of two per year\".[1]\n\nDennard scaling – This posits that power usage would decrease in proportion to area (both voltage and current being proportional to length) of transistors. Combined with Moore's law, performance per watt would grow at roughly the same rate as transistor density, doubling every 1–2 years. According to Dennard scaling transistor dimensions would be scaled by 30% (0.7×) every technology generation, thus reducing their area by 50%. This would reduce the delay by 30% (0.7×) and therefore increase operating frequency by about 40% (1.4×). Finally, to keep electric field constant, voltage would be reduced by 30%, reducing energy by 65% and power (at 1.4× frequency) by 50%.[c] Therefore, in every technology generation transistor density would double, circuit becomes 40% faster, while power consumption (with twice the number of transistors) stays the same.[141] Dennard scaling ended in 2005–2010, due to leakage currents.[17]\n\nThe exponential processor transistor growth predicted by Moore does not always translate into exponentially greater practical CPU performance. Since around 2005–2007, Dennard scaling has ended, so even though Moore's law continued after that, it has not yielded proportional dividends in improved performance.[15][142] The primary reason cited for the breakdown is that at small sizes, current leakage poses greater challenges, and also causes the chip to heat up, which creates a threat of thermal runaway and therefore, further increases energy costs.[15][142][17]\n\nThe breakdown of Dennard scaling prompted a greater focus on multicore processors, but the gains offered by switching to more cores are lower than the gains that would be achieved had Dennard scaling continued.[143][144] In another departure from Dennard scaling, Intel microprocessors adopted a non-planar tri-gate FinFET at 22 nm in 2012 that is faster and consumes less power than a conventional planar transistor.[145] The rate of performance improvement for single-core microprocessors has slowed significantly.[146] Single-core performance was improving by 52% per year in 1986–2003 and 23% per year in 2003–2011, but slowed to just seven percent per year in 2011–2018.[146]\n\nQuality adjusted price of IT equipment – The price of information technology (IT), computers and peripheral equipment, adjusted for quality and inflation, declined 16% per year on average over the five decades from 1959 to 2009.[147][148] The pace accelerated, however, to 23% per year in 1995–1999 triggered by faster IT innovation,[130] and later, slowed to 2% per year in 2010–2013.[147][149]\n\nWhile quality-adjusted microprocessor price improvement continues,[150] the rate of improvement likewise varies, and is not linear on a log scale. Microprocessor price improvement accelerated during the late 1990s, reaching 60% per year (halving every nine months) versus the typical 30% improvement rate (halving every two years) during the years earlier and later.[151][152] Laptop microprocessors in particular improved 25–35% per year in 2004–2010, and slowed to 15–25% per year in 2010–2013.[153]\n\nThe number of transistors per chip cannot explain quality-adjusted microprocessor prices fully.[151][154][155] Moore's 1995 paper does not limit Moore's law to strict linearity or to transistor count, \"The definition of 'Moore's Law' has come to refer to almost anything related to the semiconductor industry that on a semi-log plot approximates a straight line. I hesitate to review its origins and by doing so restrict its definition.\"[129]\n\nHard disk drive areal density – A similar prediction (sometimes called Kryder's law) was made in 2005 for hard disk drive areal density.[156] The prediction was later viewed as over-optimistic. Several decades of rapid progress in areal density slowed around 2010, from 30 to 100% per year to 10–15% per year, because of noise related to smaller grain size of the disk media, thermal stability, and writability using available magnetic fields.[157][158]\n\nFiber-optic capacity – The number of bits per second that can be sent down an optical fiber increases exponentially, faster than Moore's law. Keck's law, in honor of Donald Keck.[159]\n\nNetwork capacity – According to Gerald Butters,[160][161] the former head of Lucent's Optical Networking Group at Bell Labs, there is another version, called Butters' Law of Photonics,[162] a formulation that deliberately parallels Moore's law. Butters' law says that the amount of data coming out of an optical fiber is doubling every nine months.[163] Thus, the cost of transmitting a bit over an optical network decreases by half every nine months. The availability of wavelength-division multiplexing (sometimes called WDM) increased the capacity that could be placed on a single fiber by as much as a factor of 100. Optical networking and dense wavelength-division multiplexing (DWDM) is rapidly bringing down the cost of networking, and further progress seems assured. As a result, the wholesale price of data traffic collapsed in the dot-com bubble. Nielsen's Law says that the bandwidth available to users increases by 50% annually.[164]\n\nPixels per dollar – Similarly, Barry Hendy of Kodak Australia has plotted pixels per dollar as a basic measure of value for a digital camera, demonstrating the historical linearity (on a log scale) of this market and the opportunity to predict the future trend of digital camera price, LCD and LED screens, and resolution.[165][166][167][168]\n\nThe great Moore's law compensator (TGMLC), also known as Wirth's law – generally is referred to as software bloat and is the principle that successive generations of computer software increase in size and complexity, thereby offsetting the performance gains predicted by Moore's law. In a 2008 article in InfoWorld, Randall C. Kennedy,[169] formerly of Intel, introduces this term using successive versions of Microsoft Office between the year 2000 and 2007 as his premise. Despite the gains in computational performance during this time period according to Moore's law, Office 2007 performed the same task at half the speed on a prototypical year 2007 computer as compared to Office 2000 on a year 2000 computer.\n\nLibrary expansion – was calculated in 1945 by Fremont Rider to double in capacity every 16 years, if sufficient space were made available.[170] He advocated replacing bulky, decaying printed works with miniaturized microform analog photographs, which could be duplicated on-demand for library patrons or other institutions. He did not foresee the digital technology that would follow decades later to replace analog microform with digital imaging, storage, and transmission media. Automated, potentially lossless digital technologies allowed vast increases in the rapidity of information growth in an era that now sometimes is called the Information Age.\n\nCarlson curve – is a term coined by The Economist[171] to describe the biotechnological equivalent of Moore's law, and is named after author Rob Carlson.[172] Carlson accurately predicted that the doubling time of DNA sequencing technologies (measured by cost and performance) would be at least as fast as Moore's law.[173] Carlson Curves illustrate the rapid (in some cases hyperexponential) decreases in cost, and increases in performance, of a variety of technologies, including DNA sequencing, DNA synthesis, and a range of physical and computational tools used in protein expression and in determining protein structures.\n\nEroom's law – is a pharmaceutical drug development observation that was deliberately written as Moore's Law spelled backward in order to contrast it with the exponential advancements of other forms of technology (such as transistors) over time. It states that the cost of developing a new drug roughly doubles every nine years.\n\nExperience curve effects says that each doubling of the cumulative production of virtually any product or service is accompanied by an approximate constant percentage reduction in the unit cost. The acknowledged first documented qualitative description of this dates from 1885.[174][175] A power curve was used to describe this phenomenon in a 1936 discussion of the cost of airplanes.[176]\n\nEdholm's law – Phil Edholm observed that the bandwidth of telecommunication networks (including the Internet) is doubling every 18 months.[177] The bandwidths of online communication networks has risen from bits per second to terabits per second. The rapid rise in online bandwidth is largely due to the same MOSFET scaling that enabled Moore's law, as telecommunications networks are built from MOSFETs.[178]\n\nHaitz's law predicts that the brightness of LEDs increases as their manufacturing cost goes down.\n\nSwanson's law is the observation that the price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. At present rates, costs go down 75% about every 10 years.",
        pageTitle: "Moore's law",
    },
    {
        title: "Muphry's law",
        link: "https://en.wikipedia.org/wiki/Muphry%27s_law",
        content:
            'Muphry\'s law is an adage that states: "If you write anything criticizing editing or proofreading, there will be a fault of some kind in what you have written."[1]   The name is a deliberate misspelling of "Murphy\'s law".\n\nNames for variations on the principle have also been coined, usually in the context of online communication, including:\n\nFurther variations state that flaws in a printed ("Clark\'s document law") or published work ("Barker\'s proof") will only be discovered after it is printed and not during proofreading,[2]: 22, 61 [8] and flaws such as spelling errors in a sent email will be discovered by the sender only during rereading from the "Sent" box.\n\nJohn Bangsund of the Society of Editors (Victoria) in Australia identified Muphry\'s law as "the editorial application of the better-known Murphy\'s law",[9][10] and set it down in March 1992 in the Society of Editors Newsletter in his column "John Bangsund\'s Threepenny Planet".[1]\n\n(a) if you write anything criticizing editing or proofreading, there will be a fault of some kind in what you have written; (b) if an author thanks you in a book for your editing or proofreading, there will be mistakes in the book; (c) the stronger the sentiment expressed in (a) and (b), the greater the fault; (d) any book devoted to editing or style will be internally inconsistent.[1]\n\nIn November 2003 the Canberra Editor added the following elaboration:\n\nMuphry\'s Law also dictates that, if a mistake is as plain as the nose on your face, everyone can see it but you. Your readers will always notice errors in a title, in headings, in the first paragraph of anything, and in the top lines of a new page. These are the very places where authors, editors and proofreaders are most likely to make mistakes.[9]\n\nBangsund\'s formulation was not the first to express the general sentiment that editorial criticism or advice usually contains writing errors of its own.  In 1989, Paul Dickson credited editor Joseph A. Umhoefer with the adage, "Articles on writing are themselves badly written", and quoted a correspondent who observed that Umhoefer "was probably the first to phrase it so publicly; however, many others must have thought of it long ago."[2]: 357   An even earlier reference to the idea, though not phrased as an adage, appears in a 1909 book on writing by Ambrose Bierce:\n\nIn neither taste nor precision is any man\'s practice a court of last appeal, for writers all, both great and small, are habitual sinners against the light; and their accuser is cheerfully aware that his own work will supply (as in making this book it has supplied) many \'awful examples\'—his later work less abundantly, he hopes, than his earlier. He nevertheless believes that this does not disqualify him for showing by other instances than his own how not to write. The infallible teacher is still in the forest primeval, throwing seeds to the white blackbirds.\n\nStephen J. Dubner described learning of the existence of Muphry\'s law in the "Freakonomics" section of The New York Times in July 2008. He had accused The Economist of a typo in referring to Cornish pasties being on sale in Mexico, assuming that "pastries" had been intended and being familiar only with the word "pasties" with the meaning of nipple coverings. A reader had alerted him to the existence of the law, and The Economist had responded by sending Dubner a Cornish pasty.[12]\n\nIn 2009, then-British Prime Minister Gordon Brown hand-wrote a letter of condolence to a mother whose son had died in Afghanistan, in which he misspelled the man\'s surname. The Sun (a tabloid newspaper) published an article criticising his lack of care. In this article, the paper misspelled the same name and was forced to publish an apology of its own.[13][14]',
        pageTitle: "Muphry's law",
    },
    {
        title: "Murray's law",
        link: "https://en.wikipedia.org/wiki/Murray%27s_law",
        content:
            "In biophysical fluid dynamics, Murray's law is a potential relationship between radii at junctions in a network of fluid-carrying tubular pipes.  Its simplest version proposes that whenever a branch of radius \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n splits into two branches of radii \n  \n    \n      \n        \n          r\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle r_{1}}\n  \n and \n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle r_{2}}\n  \n, then the three radii should obey the equation \n  \n    \n      \n        \n          r\n          \n            3\n          \n        \n        =\n        \n          r\n          \n            1\n          \n          \n            3\n          \n        \n        +\n        \n          r\n          \n            2\n          \n          \n            3\n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle r^{3}=r_{1}^{3}+r_{2}^{3}{\\text{.}}}\n  \nIf network flow is smooth and leak-free, then systems that obey Murray's law minimize the resistance to flow through the network.  For turbulent networks, the law takes the same form but with a different characteristic exponent α.\n\nMurray's law is observed in the vascular and respiratory systems of animals, xylem in plants, and the respiratory system of insects.  In principle, Murray's law also applies to biomimetic engineering, but human designs rarely exploit the law.\n\nMurray's law is named after Cecil D. Murray, a physiologist at Bryn Mawr College, who first argued that efficient transport might determine the structure of the human vascular system.\n\nMurray's law assumes material is passively transported by the flow of fluid in a network of tubular pipes,[1] and that the network requires energy to maintain both flow and structural integrity.[2]  Variation in the fluid viscosity across scales will affect the Murray's law exponent, but is usually too small to matter.[3]\n\nAt least two different conditions are known in which the cube exponent is optimal.\n\nIn the first, organisms have free (variable) circulatory volume.  Also, maintenance energy is not proportional to the pipe material, but instead the quantity of working fluid.  The latter assumption is justified in metabolically active biological fluids, such as blood.[4]  It is also justified for metabolically inactive fluids, such as air, as long as the energetic \"cost\" of the infrastructure scales with the cross-sectional area of each tube; such is the case for all known biological tubules.[5]\n\nIn the second, organisms have fixed circulatory volume and pressure, but wish to minimize the resistance to flow through the system.  Equivalently, maintenance is negligible and organisms with to maximize the volumetric flow rate.[6]\n\nAlthough most derivations of Murray's law assume a steady state flow field, the same results apply for flow in tubes that have a moderate to small width, relative to the flow wavelength.[7]\n\nMurray's original derivation uses the first set of assumptions described above.  She begins with the Hagen–Poiseuille equation, which states that for fluid of dynamic viscosity μ, flowing laminarly through a cylindrical pipe of radius r and length l, the volumetric flow rate Q associated with a pressure drop Δp is\n  \n    \n      \n        Q\n        =\n        \n          \n            π\n            \n              8\n              μ\n            \n          \n        \n        \n          \n            \n              r\n              \n                4\n              \n            \n            l\n          \n        \n        Δ\n        p\n      \n    \n    {\\displaystyle Q={\\frac {\\pi }{8\\mu }}{\\frac {r^{4}}{l}}\\Delta p}\n  \nand the corresponding power consumed is[1]\n  \n    \n      \n        P\n        =\n        \n          \n            \n              8\n              μ\n            \n            π\n          \n        \n        \n          \n            \n              l\n              \n                Q\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                4\n              \n            \n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle P={\\frac {8\\mu }{\\pi }}{\\frac {lQ^{2}}{r^{4}}}{\\text{.}}}\n  \nSaid pipe contains volume πlr2.  If the maintenance power density is λ, then the total power consumed (from both flow and upkeep) is\n  \n    \n      \n        ∑\n        \n          P\n        \n        =\n        \n          \n            \n              8\n              μ\n            \n            π\n          \n        \n        \n          \n            \n              l\n              \n                Q\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                4\n              \n            \n          \n        \n        +\n        π\n        λ\n        l\n        \n          r\n          \n            2\n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle \\sum {P}={\\frac {8\\mu }{\\pi }}{\\frac {lQ^{2}}{r^{4}}}+\\pi \\lambda lr^{2}{\\text{.}}}\n  \nMinimizing this quantity depends on precisely which variables the organism is free to manipulate, but the minimum invariably occurs when the two terms are proportional to each other.[8]  In that minimal case, the proportionality determines a relationship between Q and r.  Canceling common factors and taking a square root,\n\nThat is, when using as little energy as possible, the mass flowing through the pipe must be proportional to the cube of the pipe's radius.  Since flow is leakless, the total flow rate into a junction must be the total flow rate out:\n  \n    \n      \n        \n          ∑\n          \n            in\n          \n        \n        \n          Q\n        \n        =\n        \n          ∑\n          \n            out\n          \n        \n        \n          Q\n        \n        \n          .\n        \n      \n    \n    {\\displaystyle \\sum _{\\text{in}}{Q}=\\sum _{\\text{out}}{Q}{\\text{.}}}\n  \nSubstituting (1) then gives Murray's law with α=3.[9]\n\nIf the network does not rely on transported material getting \"swept up in the flow\", but instead expects it to passively diffuse, then resistance to transport is minimized when α=2: that is, \n  \n    \n      \n        \n          ∑\n          \n            in\n          \n        \n        \n          \n            r\n            \n              2\n            \n          \n        \n        =\n        \n          ∑\n          \n            out\n          \n        \n        \n          \n            r\n            \n              2\n            \n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle \\sum _{\\text{in}}{r^{2}}=\\sum _{\\text{out}}{r^{2}}{\\text{.}}}\n  \nThe same law would apply to a direct-current electrical grid composed of wires of only one material, but varying diameter.[10]\n\nFor turbulent flow, transport resistance is minimized when α=.mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠7/3⁠; that is:[11][12] \n  \n    \n      \n        \n          ∑\n          \n            in\n          \n        \n        \n          \n            r\n            \n              \n                7\n                3\n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            out\n          \n        \n        \n          \n            r\n            \n              \n                7\n                3\n              \n            \n          \n        \n        \n          .\n        \n      \n    \n    {\\displaystyle \\sum _{\\text{in}}{r^{\\frac {7}{3}}}=\\sum _{\\text{out}}{r^{\\frac {7}{3}}}{\\text{.}}}\n  \nIn general, networks intermediate between diffusion and laminar flow are expected to have characteristic exponents between 2 and 3, at least approximately.[13][14]\n\nMurray's law has been verified in chicks; dog intestines and lungs; cat mesentery; and human intestines and lung capillaries.[15][16]  Mice genetically engineered to lack the blood-vessel-wall protein elastin have smaller and thinner blood vessels, but still obey Murray's law.[17]\n\nIn humans, large vessels, such as the aorta or trachea, do not appear to obey Murray's law, instead obeying a Murray's law with exponent close to 2.[16]  But flow in those vessels is also partially turbulent, and so should exhibit an exponent nearer to ⁠7/3⁠ than to 3.[18]\n\nInsects do not have a fully-fledged circulatory system, instead relying on passive diffusion through the haemocoel.  For those networks, Murray's law predicts constant cross-sectional area, which is observed.[10]\n\nThe same arguments that imply Murray's law also imply that the distribution of tubules should exhibit a specific power law scaling with size.  Plant xylem is known to exhibit that scaling except in scenarios where the passages double as structural supports.[19][20]\n\nThe first phenomenon now recognized as Murray's law is Young's rule for circulatory systems, which states that two identical subcapillaries should combine to form a capillary with radius about 1.26≈3√2 times larger, and dates to the early 19th century.[21]  Bryn Mawr physiologist Cecil D. Murray published the law's modern, general formulation in 1926,[22][21] but it languished in a disciplinary no-man's-land for the next fifty years: too trivial for physicists and too complicated for biologists.[21]  Interest in the law revived in the 1970s.[21]\n\nIn circulatory system governed by Murray's law with α=3, shear stress on vessel walls is roughly constant.  Consequently, variations in shear stress are a sign of deviation from Murray's law; Rodbard and Zamir suggest that such variations stimulate homeostatic growth or contraction.[23]\n\nMurray's law rarely applies to engineered materials, because man-made transport routes attempt to reduce flow resistance by minimizing branching and maximizing diameter.[24]\n\nMaterials that obey Murray's law at the microscale, known as Murray materials, are expected to have favorable flow characteristics, but their construction is difficult, because it requires tight control over pore size typically over a wide range of scales.[14][25]\n\nLim et al propose designing microfluidic \"labs on a chip\" in accord with Murray's law to minimize flow resistance during analysis.  Conventional lithography does not support such construction, because it cannot produce channels of varying depth.[26]\n\nSeeking long-lived lithium battery electrodes, Zheng et al constructed Murray materials out of layers of sintered zinc oxide nanoparticles.  The evaporation rate of the dissolved zinc oxide solvent controlled the size of the pores in each layer; the network was then just layers of ZnO with different pore sizes placed atop each other.[14]\n\nBecause power plant working fluids typically funnel into many small tubules for efficient heat transfer, Murray's law may be appropriate for nuclear reactor design.[27]",
        pageTitle: "Murray's law",
    },
    {
        title: "Murphy's law",
        link: "https://en.wikipedia.org/wiki/Murphy%27s_law",
        content:
            "Murphy's law[a] is an adage or epigram that is typically stated as: \"Anything that can go wrong will go wrong.\".\n\nThough similar statements and concepts have been made over the course of history, the law itself was coined by, and  named after, American aerospace engineer Edward A. Murphy Jr.; its exact origins are debated, but it is generally agreed it originated from Murphy and his team following a mishap during rocket sled tests some time between 1948 and 1949, and was finalized and first popularized by testing project head John Stapp during a later press conference. Murphy's original quote was the precautionary design advice that \"If there are two or more ways to do something and one of those results in a catastrophe, then someone will do it that way.\"[1][2]\n\nThe law entered wider public knowledge in the late 1970s with the publication of Arthur Bloch's 1977 book Murphy's Law, and Other Reasons Why Things Go WRONG, which included other variations and corollaries of the law. Since then, Murphy's law has remained a popular (and occasionally misused) adage, though its accuracy has been disputed by academics.\n\nSimilar \"laws\" include Sod's law, Finagle's law, and Yhprum's law, among others.\n\nThe perceived perversity of the universe has long been a subject of comment, and precursors to the modern version of Murphy's law are abundant. According to Robert A. J. Matthews in a 1997 article in Scientific American,[3] the name \"Murphy's law\" originated in 1949, but the concept itself had already long since been known. As quoted by Richard Rhodes,[4]: 187  Matthews said, \"The familiar version of Murphy's law is not quite 50 years old, but the essential idea behind it has been around for centuries. […] The modern version of Murphy's Law has its roots in U.S. Air Force studies performed in 1949 on the effects of rapid deceleration on pilots.\" Matthews goes on to explain how Edward A. Murphy Jr. was the eponym, but only because his original thought was modified subsequently into the now established form that is not exactly what he himself had said. Research into the origin of Murphy's law has been conducted by members of the American Dialect Society (ADS).\n\nMathematician Augustus De Morgan wrote on June 23, 1866:[5] \"The first experiment already illustrates a truth of the theory, well confirmed by practice, what-ever can happen will happen if we make trials enough.\" In later publications \"whatever can happen will happen\" occasionally is termed \"Murphy's law\", which raises the possibility that \"Murphy\" is simply \"De Morgan\" misremembered.[6]\n\nADS member Stephen Goranson found a version of the law, not yet generalized or bearing that name, in a report by Alfred Holt at an 1877 meeting of an engineering society.\n\nIt is found that anything that can go wrong at sea generally does go wrong sooner or later, so it is not to be wondered that owners prefer the safe to the scientific … Sufficient stress can hardly be laid on the advantages of simplicity. The human factor cannot be safely neglected in planning machinery. If attention is to be obtained, the engine must be such that the engineer will be disposed to attend to it.[7]\n\nADS member Bill Mullins found a slightly broader version of the aphorism in reference to stage magic. The British stage magician Nevil Maskelyne wrote in 1908:\n\nIt is an experience common to all men to find that, on any special occasion, such as the production of a magical effect for the first time in public, everything that can go wrong will go wrong. Whether we must attribute this to the malignity of matter or to the total depravity of inanimate things, whether the exciting cause is hurry, worry, or what not, the fact remains.[8]\n\nIn astronomy, \"Spode's Law\" refers to the phenomenon that the skies are always cloudy at the wrong moment; the law was popularized by amateur astronomer Patrick Moore[9] but dates from the 1930s.[10]\n\nIn 1948, humorist Paul Jennings coined the term resistentialism, a jocular play on resistance and existentialism, to describe \"seemingly spiteful behavior manifested by inanimate objects\",[11] where objects that cause problems (like lost keys or a runaway bouncy ball) are said to exhibit a high degree of malice toward humans.[12][13]\n\nIn 1952, as an epigraph to the mountaineering book The Butcher: The Ascent of Yerupaja, John Sack described the same principle, \"Anything that can possibly go wrong, does\", as an \"ancient mountaineering adage\".[14]\n\nDiffering recollections years later by various participants make it impossible to pinpoint who first coined the saying Murphy's law. The law's name supposedly stems from an attempt to use new measurement devices developed by Edward A. Murphy, a United States Air Force (USAF) captain and aeronautical engineer.[15] The phrase was coined in an adverse reaction to something Murphy said when his devices failed to perform and was eventually cast into its present form prior to a press conference some months later – the first ever (of many) given by John Stapp, a USAF colonel and flight surgeon in the 1950s.[15][16]\n\nFrom 1948 to 1949, Stapp headed research project MX981 at Muroc Army Air Field (later renamed Edwards Air Force Base)[17] for the purpose of testing the human tolerance for g-forces during rapid deceleration. The tests used a rocket sled mounted on a railroad track with a series of hydraulic brakes at the end. Initial tests used a humanoid crash test dummy strapped to a seat on the sled, but subsequent tests were performed by Stapp, at that time a USAF captain. During the tests, questions were raised about the accuracy of the instrumentation used to measure the g-forces Captain Stapp was experiencing. Edward Murphy proposed using electronic strain gauges attached to the restraining clamps of Stapp's harness to measure the force exerted on them by his rapid deceleration. Murphy was engaged in supporting similar research using high speed centrifuges to generate g-forces.\n\nDuring a trial run of this method using a chimpanzee, supposedly around June 1949, Murphy's assistant wired the harness and the rocket sled was launched. The sensors provided a zero reading; however, it became apparent that they had been installed incorrectly, with some sensors wired backwards. It was at this point a frustrated Murphy made his pronouncement, despite being offered the time and chance to calibrate and test the sensor installation prior to the test proper, which he declined somewhat irritably, getting off on the wrong foot with the MX981 team. George E. Nichols, an engineer and quality assurance manager with the Jet Propulsion Laboratory who was present at the time, recalled in an interview that Murphy blamed the failure on his assistant after the failed test, saying, \"If that guy has any way of making a mistake, he will.\"[15][unreliable source?] Nichols' account is that \"Murphy's law\" came about through conversation among the other members of the team; it was condensed to \"If it can happen, it will happen\", and named for Murphy in mockery of what Nichols perceived as arrogance on Murphy's part. Others, including Edward Murphy's surviving son Robert Murphy, deny Nichols' account,[15][unreliable source?] and claim that the phrase did originate with Edward Murphy. According to Robert Murphy's account, his father's statement was along the lines of \"If there's more than one way to do a job, and one of those ways will result in disaster, then he will do it that way.\"\n\nThe phrase first received public attention during a press conference in which Stapp was asked how it was that nobody had been severely injured during the rocket sled tests. Stapp replied that it was because they always took Murphy's law under consideration; he then summarized the law and said that in general, it meant that it was important to consider all the possibilities (possible things that could go wrong) before doing a test and act to counter them. Thus Stapp's usage and Murphy's alleged usage are very different in outlook and attitude. One is sour, the other an affirmation of the predictable being surmountable, usually by sufficient planning and redundancy. Nichols believes Murphy was unwilling to take the responsibility for the device's initial failure (by itself a blip of no large significance) and is to be doubly damned for not allowing the MX981 team time to validate the sensor's operability and for trying to blame an underling in the embarrassing aftermath.\n\nThe name \"Murphy's law\" was not immediately secure. A story by Lee Correy in the February 1955 issue of Astounding Science Fiction referred to \"Reilly's law\", which states that \"in any scientific or engineering endeavor, anything that can go wrong will go wrong\".[18] Atomic Energy Commission Chairman Lewis Strauss was quoted in the Chicago Daily Tribune on February 12, 1955, saying \"I hope it will be known as Strauss' law. It could be stated about like this: If anything bad can happen, it probably will.\"[19]\n\nMartin Caidin a pilot of the Federal Aviation Agency, in his book Operation Nuke (1973) chapter 13: lists the Murphy's Three Laws of Physics as (1.) Whatever can go wrong, will go wrong. (2.) Whatever's wrong is bound to get worse. (3.) When the first two laws have passed, and you're still around panic.\n\nArthur Bloch, in the first volume (1977) of his Murphy's Law, and Other Reasons Why Things Go WRONG series, prints a letter that he received from Nichols, who recalled an event that occurred in 1949 at Edwards Air Force Base that, according to him, is the origination of Murphy's law, and first publicly recounted by Stapp. An excerpt from the letter reads:\n\nThe law's namesake was Capt. Ed Murphy, a development engineer from Wright Field Aircraft Lab. Frustration with a strap transducer which was malfunctioning due to an error in wiring the strain gage bridges caused him to remark – \"If there is any way to do it wrong, he will\" – referring to the technician who had wired the bridges at the Lab. I assigned Murphy's law to the statement and the associated variations.[20]\n\nThe association with the Muroc incident is by no means secure. Despite extensive research, no trace of documentation of the saying as \"Murphy's law\" has been found before 1951. The next citations are not found until 1955, when the May–June issue of Aviation Mechanics Bulletin included the line \"Murphy's law: If an aircraft part can be installed incorrectly, someone will install it that way\",[21] and Lloyd Mallan's book Men, Rockets and Space Rats, referred to: \"Colonel Stapp's favorite takeoff on sober scientific laws—Murphy's law, Stapp calls it—'Everything that can possibly go wrong will go wrong'.\" In 1962, the Mercury Seven attributed Murphy's law to United States Navy training films.[21]\n\nFred R. Shapiro, the editor of the Yale Book of Quotations, has shown that in 1952 the adage was called \"Murphy's law\" in a book by Anne Roe, quoting an unnamed physicist:\n\nhe described [it] as \"Murphy's law or the fourth law of thermodynamics\" (actually there were only three last I heard) which states: \"If anything can go wrong, it will.\"[22]\n\nIn May 1951, Anne Roe gave a transcript of an interview (part of a thematic apperception test, asking impressions on a drawing) with said physicist: \"As for himself he realized that this was the inexorable working of the second law of the thermodynamics which stated Murphy's law 'If anything can go wrong it will'. I always liked 'Murphy's law'. I was told that by an architect.\"[23] ADS member Stephen Goranson, investigating this in 2008 and 2009, found that Anne Roe's papers, held in the American Philosophical Society's archives in Philadelphia, identified the interviewed physicist as Howard Percy \"Bob\" Robertson (1903–1961). Robertson's papers at the Caltech archives include a letter in which Robertson offers Roe an interview within the first three months of 1949, making this apparently predate the Muroc incident said to have occurred in or after June 1949.[15][unreliable source?]\n\nJohn Paul Stapp, Edward A. Murphy, Jr., and George Nichols were jointly awarded an Ig Nobel Prize in 2003 in engineering \" for (probably) giving birth to the name\".[24] Murphy's Law was also the theme of 2024 Ig Nobel Prize ceremony.[25]\n\nAccording to Richard Dawkins, so-called laws like Murphy's law and Sod's law are nonsense because they require inanimate objects to have desires of their own, or else to react according to one's own desires. Dawkins points out that a certain class of events may occur all the time, but are only noticed when they become a nuisance. He gives an example of aircraft noise pollution interfering with filming: there are always aircraft in the sky at any given time, but they are only taken note of when they cause a problem. This is a form of confirmation bias, whereby the investigator seeks out evidence to confirm their already-formed ideas, but does not look for evidence that contradicts them.[26]\n\nSimilarly, David Hand, emeritus professor of mathematics and senior research investigator at Imperial College London, points out that the law of truly large numbers should lead one to expect the kind of events predicted by Murphy's law to occur occasionally. Selection bias will ensure that those ones are remembered and the many times Murphy's law was not true are forgotten.[27]\n\nThere have been persistent references to Murphy's law associating it with the laws of thermodynamics from early on (see the quotation from Anne Roe's book above).[22] In particular, Murphy's law is often cited as a form of the second law of thermodynamics (the law of entropy) because both are predicting a tendency to a more disorganized state.[28] Atanu Chatterjee investigated this idea by formally stating Murphy's law in mathematical terms and found that Murphy's law so stated could be disproved using the principle of least action.[29]\n\nFrom its initial public announcement, Murphy's law quickly spread to various technical cultures connected to aerospace engineering.[30] Before long, variations of the law applied to different topics and subjects had passed into the public imagination, changing over time. Arthur Bloch compiled a number of books of corollaries to Murphy's law and variations thereof, the first being Murphy's Law, and Other Reasons Why Things Go WRONG, which received several follow-ups and reprints.[20]\n\nYhprum's law is an optimistic reversal of Murphy's law, stating that \"anything that can go right will go right\". Its name directly references this, being \"Murphy\" in reverse.\n\nManagement consultant Peter Drucker formulated \"Drucker's law\" in dealing with complexity of management: \"If one thing goes wrong, everything else will, and at the same time.\"[31]\n\n\"Mrs. Murphy's law\" is a corollary of Murphy's law, which states that \"Anything that can go wrong will go wrong while Mr. Murphy is out of town.\"[32][33][34][35]\n\nThe term is sometimes used to describe concise, ironic, humorous rules of thumb that often do not share a relation to the original law or Edward Murphy himself, but still posit him as a relevant expert in the law's subject. Examples of these \"Murphy's laws\" include those for military tactics, technology, romance, social relations, research, and business.[36][37][38]",
        pageTitle: "Murphy's law",
    },
    {
        title: "Sod's law",
        link: "https://en.wikipedia.org/wiki/Sod%27s_law",
        content:
            'Sod\'s law, a British culture axiom, states that "if something can go wrong, it will". The law sometimes has a corollary: that the misfortune will happen at "the worst possible time" (Finagle\'s law). The term is commonly used in the United Kingdom (while in many parts of North America the phrase "Murphy\'s law" is more popular).[1]\n\nThe phrase seems to derive, at least in part, from the colloquialism an "unlucky sod"; a term for someone who has had some bad (unlucky) experience, and is usually used as a sympathetic reference to the person.[2]\n\nA slightly different form of Sod\'s law states that "the degree of failure is in direct proportion to the effort expended and to the need for success."[3]\n\nAn alternative expression, again in British culture, is "hope for the best, expect the worst".[4]\n\nSod\'s law is a more extreme version of Murphy\'s law. While Murphy\'s law says that anything that can go wrong, will go wrong (eventually), Sod\'s law requires that it will always go wrong with the worst possible outcome or at the worst time. Belief in Sod\'s law can be viewed as a combination of the law of truly large numbers and the psychological effect of the law of selection. The former says we should expect things to go wrong now and then, and the latter says the exceptional events where something went wrong stand out in memory, but the great number of mundane events where nothing exceptional happened fall into obscurity.[5] Sod’s law is also explained as a form of the natural human negativity bias, the survival trait of being extra alert to negative events.[6]\n\nSome examples are traffic lights turning red when a driver is in a hurry, or email software crashing at the exact moment the user attempts to send an important message.[5] Sod\'s law has also been applied to individuals, such as the composer Beethoven losing his hearing or Def Leppard drummer Rick Allen losing an arm in a car crash.[5]\n\nOther examples are dropped bread always landing butter side down, or it raining just after one has washed the car and on the weekend one goes to the beach.[6]\n\nA discrediting example is a coin toss resulting in tails the more strongly that one wishes the result to be heads. Richard Dawkins said that this shows the idea of Sod\'s law is "nonsense", as the coin is unaware of the person\'s wish and has no desire to thwart it.[7]',
        pageTitle: "Sod's law",
    },
    {
        title: "Neuhaus's law",
        link: "https://en.wikipedia.org/wiki/Richard_John_Neuhaus",
        content:
            'Richard John Neuhaus (May 14, 1936–January 8, 2009) was a prominent writer and Christian cleric (first in the Lutheran Church–Missouri Synod, then the Evangelical Lutheran Church in America and later the Catholic Church).\n\nBorn in Canada, Neuhaus moved to the United States, where he became a naturalized United States citizen. He was the longtime editor of the Lutheran Forum magazine newsletter and later founder and editor of the monthly journal First Things and the author of numerous books.\n\nA staunch defender of the Catholic Church\'s teachings on abortion and other life issues, he was an unofficial adviser to President George W. Bush on bioethical issues.[1]\n\nBorn in Pembroke, Ontario, on May 14, 1936, Neuhaus was one of eight children of a Lutheran minister and his wife. Although he had dropped out of high school at age 16 to operate a gas station in Texas,[2] he returned to school, graduating from Concordia Lutheran College of Austin, Texas, in 1956. He moved to St. Louis, Missouri, where he earned his Bachelor of Arts and Master of Divinity degrees from Concordia Seminary in 1960.[1]\n\nNeuhaus was first an ordained pastor in the conservative Lutheran Church–Missouri Synod.[3] \nIn 1974, a major schism in the Missouri Synod resulted in many "modernist" churches splitting to form the more progressive Association of Evangelical Lutheran Churches to which Neuhaus eventually affiliated. The AELC merged a decade later in 1988 with the other two more liberal Lutheran denominations in the US, the American Lutheran Church (1960) and the Lutheran Church in America (1962), to finally form the current Evangelical Lutheran Church in America, of which Neuhaus was a member of the clergy.\n\nFrom 1961 to 1978, he served as pastor of St. John the Evangelist Church, a poor, predominantly black and Hispanic congregation in Williamsburg, Brooklyn.[4] From the pulpit he addressed civil rights and social justice concerns and spoke against the Vietnam War. In the late 1960s he gained national prominence when, together with Jesuit priest Daniel Berrigan and Rabbi Abraham Joshua Heschel, he founded Clergy and Laymen Concerned About Vietnam.[1]\n\nHe was active[when?] in the Evangelical Catholic movement in Lutheranism and spent time at Saint Augustine\'s House, the Lutheran Benedictine monastery, in Oxford, Michigan. He was active in liberal politics until the 1973 ruling on abortion in Roe v. Wade by the US Supreme Court, which he opposed. He became a member of the growing neoconservative movement and an outspoken advocate of "democratic capitalism". He also advocated faith-based policy initiatives by the federal government based upon Judeo-Christian values.[1] He originated the "Neuhaus\'s Law",[5] which states, "Where orthodoxy is optional, orthodoxy will sooner or later be proscribed."[5]\n\nHe was a longtime editor of the monthly newsletter published in between quarterly issues of the interdenominational independent journal Lutheran Forum, published by the American Lutheran Publicity Bureau during the 1970s and 1980s. He was a supporter of the movement to reestablish, in Lutheranism, the permanent diaconate (deacon) as a full-fledged office in the threefold ministry of bishop / presbyter (priest) / deacon under the historic episcopacy (office of bishop), following earlier actions of the Catholics in the Second Vatican Council and the churches of the Anglican Communion (including the Episcopal Church in the US).\n\nIn 1981, Neuhaus helped to found the Institute on Religion and Democracy and remained on its board until his death. He wrote its founding document, "Christianity and Democracy". In 1984, he established the Center for Religion and Society as part of the conservative think-tank Rockford Institute in Rockford, Illinois, which publishes Chronicles. In 1989, he and the center were "forcibly evicted" from the institute\'s eastern offices in New York City under disputed circumstances.[citation needed]\n\nIn March 1990, Neuhaus founded the Institute on Religion and Public Life and its journal, First Things, an ecumenical journal "whose purpose is to advance a religiously informed public philosophy for the ordering of society."[6]\n\nIn September 1990, Neuhaus was received into the Catholic Church.[7] A year after becoming a Catholic, he was ordained by Cardinal John O\'Connor as a priest of the Archdiocese of New York. He served as a commentator for the Catholic television network Eternal Word Television (EWTN) during the funeral of Pope John Paul II and the election of Pope Benedict XVI.[8]\n\nNeuhaus continued to edit First Things as a Catholic priest. He was a sought-after public speaker and wrote several books, both scholarly and popular genres. He appeared in the 2010 film, The Human Experience, released after his death, where his voice features in the narration and in the film\'s trailer.\n\nNeuhaus died from complications of cancer in New York City,[9] on January 8, 2009, aged 72.[10]\n\nIn later years, Neuhaus compared pro-life activism to the civil rights movement of the 1960s. During the 2004 presidential campaign, he was a leading advocate for denying communion to Catholic politicians who supported abortion. It was a mistake, he declared, to isolate abortion "from other issues of the sacredness of life."[1]\n\nNeuhaus promoted ecumenical dialogue and social conservatism. Along with Charles Colson, he edited Evangelicals and Catholics Together: Toward a Common Mission (1995).[11] This ecumenical manifesto sparked much debate.[12]\n\nA close yet unofficial adviser of President George W. Bush, he advised Bush on a range of religious and ethical matters, including abortion, stem-cell research, cloning, and the Federal Marriage Amendment.[13] In 2005, under the heading of "Bushism Made Catholic," Neuhaus was named one of the "25 Most Influential Evangelicals in America" by Time magazine.[13] The article noted that in several speeches, Bush cited Neuhaus more than any other living authority. Bush was reported to have said that the Catholic priest helped him articulate religious ideas.[13]\n\nNeuhaus was criticized for his political engagement in "theoconservatism".[14][15] Nonetheless, theologian David Bentley Hart reminded his readers that "words like absolutist are vacuous abstractions when applied to" Neuhaus. Hart praised the editor of First Things for his willingness to publish "views contrary to his own, and he seems quite pleased that it should do so."[16]\n\nNeuhaus controversially defended disgraced Marcial Maciel, founder of the Legionaries of Christ, in the pages of First Things.[17]',
        pageTitle: "Richard John Neuhaus",
    },
    {
        title: "Newton's law of cooling",
        link: "https://en.wikipedia.org/wiki/Newton%27s_law_of_cooling",
        content:
            "In the study of heat transfer, Newton's law of cooling is a physical law which states that the rate of heat loss of a body is directly proportional to the difference in the temperatures between the body and its environment. The law is frequently qualified to include the condition that the temperature difference is small and the nature of heat transfer mechanism remains the same. As such, it is equivalent to a statement that the heat transfer coefficient, which mediates between heat losses and temperature differences, is a constant.\n\nIn heat conduction, Newton's law is generally followed as a consequence of Fourier's law.  The thermal conductivity of most materials is only weakly dependent on temperature, so the constant heat transfer coefficient condition is generally met. In convective heat transfer, Newton's Law is followed for forced air or pumped fluid cooling, where the properties of the fluid do not vary strongly with temperature, but it is only approximately true for buoyancy-driven convection, where the velocity of the flow increases with temperature difference. In the case of heat transfer by thermal radiation, Newton's law of cooling holds only for very small temperature differences.\n\nWhen stated in terms of temperature differences, Newton's law (with several further simplifying assumptions, such as a low Biot number and a temperature-independent heat capacity) results in a simple differential equation expressing temperature-difference as a function of time. The solution to that equation describes an exponential decrease of temperature-difference over time. This characteristic decay of the temperature-difference is also associated with Newton's law of cooling.\n\nIsaac Newton published his work on cooling anonymously in 1701 as \"Scala graduum Caloris. Calorum Descriptiones & signa.\" in Philosophical Transactions.[1] It was the first heat transfer formulation and serves as the formal basis of convective heat transfer.[2]\n\nNewton did not originally state his law in the above form in 1701. Rather, using today's terms, Newton noted after some mathematical manipulation that the rate of temperature change of a body is proportional to the difference in temperatures between the body and its surroundings. This final simplest version of the law, given by Newton himself, was partly due to confusion in Newton's time between the concepts of heat and temperature, which would not be fully disentangled until much later.[3]\n\nIn 2020, Maruyama and Moriya repeated Newton's experiments with modern apparatus, and they applied modern data reduction techniques.[4] In particular, these investigators took account of thermal radiation at high temperatures (as for the molten metals Newton used), and they accounted for buoyancy effects on the air flow. By comparison to Newton's original data, they concluded that his measurements (from 1692 to 1693) had been \"quite accurate\".[4]\n\nConvection cooling is sometimes said to be governed by \"Newton's law of cooling.\"  When the heat transfer coefficient is independent, or relatively independent, of the temperature difference between object and environment, Newton's law is followed. The law holds well for forced air and pumped liquid cooling, where the fluid velocity does not rise with increasing temperature difference. Newton's law is most closely obeyed in purely conduction-type cooling. However, the heat transfer coefficient is a function of the temperature difference in natural convective (buoyancy driven) heat transfer. In that case, Newton's law only approximates the result when the temperature difference is relatively small. Newton himself realized this limitation.\n\nA correction to Newton's law concerning convection for larger temperature differentials by including an exponent, was made in 1817 by Dulong and Petit.[5] (These men are better-known for their formulation of the Dulong–Petit law concerning the molar specific heat capacity of a crystal.)\n\nAnother situation that does not obey Newton's law is radiative heat transfer.  Radiative cooling is better described by the Stefan–Boltzmann law in which the heat transfer rate varies as the difference in the 4th powers of the absolute temperatures of the object and of its environment.\n\nThe statement of Newton's law used in the heat transfer literature puts into mathematics the idea that the rate of heat loss of a body is proportional to the difference in temperatures between the body and its surroundings. For a temperature-independent heat transfer coefficient, the statement is:\n\nq\n        =\n        h\n        \n          (\n          \n            T\n            (\n            t\n            )\n            −\n            \n              T\n              \n                env\n              \n            \n          \n          )\n        \n        =\n        h\n        \n        Δ\n        T\n        (\n        t\n        )\n        ,\n      \n    \n    {\\displaystyle q=h\\left(T(t)-T_{\\text{env}}\\right)=h\\,\\Delta T(t),}\n  \n\nwhere\n\nIn global parameters by integrating on the surface area the heat flux, it can be also stated as:\n\nQ\n              ˙\n            \n          \n        \n        =\n        \n          ∮\n          \n            A\n          \n        \n        h\n        \n          (\n          \n            T\n            (\n            t\n            )\n            −\n            \n              T\n              \n                env\n              \n            \n          \n          )\n        \n        d\n        A\n        =\n        \n          ∮\n          \n            A\n          \n        \n        h\n        \n        Δ\n        T\n        (\n        t\n        )\n        d\n        A\n        ,\n      \n    \n    {\\displaystyle {\\dot {Q}}=\\oint _{A}h\\left(T(t)-T_{\\text{env}}\\right)dA=\\oint _{A}h\\,\\Delta T(t)dA,}\n  \n\nwhere\n\nIf the heat transfer coefficient and the temperature difference are uniform along the heat transfer surface, the above formula simplifies to:\n\nQ\n              ˙\n            \n          \n        \n        =\n        h\n        A\n        \n          (\n          \n            T\n            (\n            t\n            )\n            −\n            \n              T\n              \n                env\n              \n            \n          \n          )\n        \n        =\n        h\n        A\n        \n        Δ\n        T\n        (\n        t\n        )\n      \n    \n    {\\displaystyle {\\dot {Q}}=hA\\left(T(t)-T_{\\text{env}}\\right)=hA\\,\\Delta T(t)}\n  \n.\n\nThe heat transfer coefficient h depends upon physical properties of the fluid and the physical situation in which convection occurs. Therefore, a single usable heat transfer coefficient (one that does not vary significantly across the temperature-difference ranges covered during cooling and heating) must be derived or found experimentally for every system that is to be analyzed.\n\nFormulas and correlations are available in many references to calculate heat transfer coefficients for typical configurations and fluids.  For laminar flows, the heat transfer coefficient is usually smaller than in turbulent flows because turbulent flows have strong mixing within the boundary layer on the heat transfer surface.[6] Note the heat transfer coefficient changes in a system when a transition from laminar to turbulent flow occurs.\n\nThe Biot number, a dimensionless quantity, is defined for a body as\n\nBi\n        \n        =\n        \n          \n            \n              h\n              \n                L\n                \n                  \n                    C\n                  \n                \n              \n            \n            \n              k\n              \n                \n                  b\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\text{Bi}}={\\frac {hL_{\\rm {C}}}{k_{\\rm {b}}}},}\n  \n\nwhere\n\nThe physical significance of Biot number can be understood by imagining the heat flow from a hot metal sphere suddenly immersed in a pool to the surrounding fluid. The heat flow experiences two resistances: the first outside the surface of the sphere, and the second within the solid metal (which is influenced by both the size and composition of the sphere). The ratio of these resistances is the dimensionless Biot number.\n\nIf the thermal resistance at the fluid/sphere interface exceeds that thermal resistance offered by the interior of the metal sphere, the Biot number will be less than one. For systems where it is much less than one, the interior of the sphere may be presumed always to have the same temperature, although this temperature may be changing, as heat passes into the sphere from the surface. The equation to describe this change in (relatively uniform) temperature inside the object, is the simple exponential one described in Newton's law of cooling expressed in terms of temperature difference (see below).\n\nIn contrast, the metal sphere may be large, causing the characteristic length to increase to the point that the Biot number is larger than one. In this case, temperature gradients within the sphere become important, even though the sphere material is a good conductor. Equivalently, if the sphere is made of a thermally insulating (poorly conductive) material, such as wood or styrofoam, the interior resistance to heat flow will exceed that at the fluid/sphere boundary, even with a much smaller sphere. In this case, again, the Biot number will be greater than one.\n\nValues of the Biot number smaller than 0.1 imply that the heat conduction inside the body is much faster than the heat convection away from its surface, and temperature gradients are negligible inside of it.  This can indicate the applicability (or inapplicability) of certain methods of solving transient heat transfer problems.  For example, a Biot number less than 0.1 typically indicates less than 5% error will be present when assuming a lumped-capacitance model of transient heat transfer (also called lumped system analysis).[7] Typically, this type of analysis leads to simple exponential heating or cooling behavior (\"Newtonian\" cooling or heating) since the internal energy of the body is directly proportional to its temperature, which in turn determines the rate of heat transfer into or out of it. This leads to a simple first-order differential equation which describes heat transfer in these systems.\n\nHaving a Biot number smaller than 0.1 labels a substance as \"thermally thin,\" and temperature can be assumed to be constant throughout the material's volume. The opposite is also true: A Biot number greater than 0.1 (a \"thermally thick\" substance) indicates that one cannot make this assumption, and more complicated heat transfer equations for \"transient heat conduction\" will be required to describe the time-varying and non-spatially-uniform temperature field within the material body. Analytic methods for handling these problems, which may exist for simple geometric shapes and uniform material thermal conductivity, are described in the article on the heat equation.\n\nSimple solutions for transient cooling of an object may be obtained when the internal thermal resistance within the object is small in comparison to the resistance to heat transfer away from the object's surface (by external conduction or convection), which is the condition for which the Biot number is less than about 0.1. This condition allows the presumption of a single, approximately uniform temperature inside the body, which varies in time but not with position. (Otherwise the body would have many different temperatures inside it at any one time.) This single temperature will generally change exponentially as time progresses (see below).\n\nThe condition of low Biot number leads to the so-called lumped capacitance model. In this model, the internal energy (the amount of thermal energy in the body) is calculated by assuming a constant heat capacity. In that case, the internal energy of the body is a linear function of the body's single internal temperature.\n\nThe lumped capacitance solution that follows assumes a constant heat transfer coefficient, as would be the case in forced convection. For free convection, the lumped capacitance model can be solved with a heat transfer coefficient that varies with temperature difference.[8]\n\nA body treated as a lumped capacitance object, with a total internal energy of \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n (in joules), is characterized by a single uniform internal temperature, \n  \n    \n      \n        T\n        (\n        t\n        )\n      \n    \n    {\\displaystyle T(t)}\n  \n. The heat capacitance, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, of the body is \n  \n    \n      \n        C\n        =\n        d\n        U\n        \n          /\n        \n        d\n        T\n      \n    \n    {\\displaystyle C=dU/dT}\n  \n (in J/K), for the case of an incompressible material. The internal energy may be written in terms of the temperature of the body, the heat capacitance (taken to be independent of temperature), and a reference temperature at which the internal energy is zero:  \n  \n    \n      \n        U\n        =\n        C\n        (\n        T\n        −\n        \n          T\n          \n            ref\n          \n        \n        )\n      \n    \n    {\\displaystyle U=C(T-T_{\\text{ref}})}\n  \n.\n\nDifferentiating \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n with respect to time gives:\n\n  \n    \n      \n        \n          \n            \n              d\n              U\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        C\n        \n        \n          \n            \n              d\n              T\n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dU}{dt}}=C\\,{\\frac {dT}{dt}}.}\n\nApplying the first law of thermodynamics to the lumped object gives \n  \n    \n      \n        \n          \n            \n              d\n              U\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {dU}{dt}}=-{\\dot {Q}}}\n  \n, where the rate of heat transfer out of the body, \n  \n    \n      \n        \n          \n            \n              Q\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {Q}}}\n  \n, may be expressed by Newton's law of cooling, and where no work transfer occurs for an incompressible material. Thus,\n\n  \n    \n      \n        \n          \n            \n              d\n              T\n              (\n              t\n              )\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              h\n              A\n            \n            C\n          \n        \n        (\n        T\n        (\n        t\n        )\n        −\n        \n          T\n          \n            env\n          \n        \n        )\n        =\n        −\n        \n          \n            1\n            τ\n          \n        \n         \n        Δ\n        T\n        (\n        t\n        )\n        ,\n      \n    \n    {\\displaystyle {\\frac {dT(t)}{dt}}=-{\\frac {hA}{C}}(T(t)-T_{\\text{env}})=-{\\frac {1}{\\tau }}\\ \\Delta T(t),}\n  \n\nwhere the time constant of the system is \n  \n    \n      \n        τ\n        =\n        C\n        \n          /\n        \n        (\n        h\n        A\n        )\n      \n    \n    {\\displaystyle \\tau =C/(hA)}\n  \n. The heat capacitance \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n may be written in terms of the object's specific heat capacity, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n (J/kg-K), and mass, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n (kg). The time constant is then \n  \n    \n      \n        τ\n        =\n        m\n        c\n        \n          /\n        \n        (\n        h\n        A\n        )\n      \n    \n    {\\displaystyle \\tau =mc/(hA)}\n  \n.\n\nWhen the environmental temperature is constant in time, we may define \n  \n    \n      \n        Δ\n        T\n        (\n        t\n        )\n        =\n        T\n        (\n        t\n        )\n        −\n        \n          T\n          \n            env\n          \n        \n      \n    \n    {\\displaystyle \\Delta T(t)=T(t)-T_{\\text{env}}}\n  \n. The equation becomes\n\n  \n    \n      \n        \n          \n            \n              d\n              T\n              (\n              t\n              )\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              d\n              Δ\n              T\n              (\n              t\n              )\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            1\n            τ\n          \n        \n        Δ\n        T\n        (\n        t\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {dT(t)}{dt}}={\\frac {d\\Delta T(t)}{dt}}=-{\\frac {1}{\\tau }}\\Delta T(t).}\n\nThe solution of this differential equation, by integration from the initial condition, is\n\n  \n    \n      \n        Δ\n        T\n        (\n        t\n        )\n        =\n        Δ\n        T\n        (\n        0\n        )\n        \n        \n          e\n          \n            −\n            t\n            \n              /\n            \n            τ\n          \n        \n        .\n      \n    \n    {\\displaystyle \\Delta T(t)=\\Delta T(0)\\,e^{-t/\\tau }.}\n  \n\nwhere \n  \n    \n      \n        Δ\n        T\n        (\n        0\n        )\n      \n    \n    {\\displaystyle \\Delta T(0)}\n  \n is the temperature difference at time 0. Reverting to temperature, the solution is\n\n  \n    \n      \n        T\n        (\n        t\n        )\n        =\n        \n          T\n          \n            env\n          \n        \n        +\n        (\n        T\n        (\n        0\n        )\n        −\n        \n          T\n          \n            env\n          \n        \n        )\n        \n        \n          e\n          \n            −\n            t\n            \n              /\n            \n            τ\n          \n        \n        .\n      \n    \n    {\\displaystyle T(t)=T_{\\text{env}}+(T(0)-T_{\\text{env}})\\,e^{-t/\\tau }.}\n\nThe temperature difference between the body and the environment decays exponentially as a function of time.\n\nBy defining \n  \n    \n      \n        r\n        =\n        1\n        \n          /\n        \n        τ\n        =\n        h\n        A\n        \n          /\n        \n        C\n      \n    \n    {\\displaystyle r=1/\\tau =hA/C}\n  \n, the differential equation becomes\n\nT\n              ˙\n            \n          \n        \n        =\n        r\n        \n          (\n          \n            \n              T\n              \n                env\n              \n            \n            −\n            T\n            (\n            t\n            )\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\dot {T}}=r\\left(T_{\\text{env}}-T(t)\\right),}\n  \n\nwhere\n\nSolving the initial-value problem using separation of variables gives\n\nT\n        (\n        t\n        )\n        =\n        \n          T\n          \n            env\n          \n        \n        +\n        (\n        T\n        (\n        0\n        )\n        −\n        \n          T\n          \n            env\n          \n        \n        )\n        \n          e\n          \n            −\n            r\n            t\n          \n        \n        .\n      \n    \n    {\\displaystyle T(t)=T_{\\text{env}}+(T(0)-T_{\\text{env}})e^{-rt}.}",
        pageTitle: "Newton's law of cooling",
    },
    {
        title: "Newton's laws of motion",
        link: "https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion",
        content:
            "Newton's laws of motion are three physical laws that describe the relationship between the motion of an object and the forces acting on it. These laws, which provide the basis for Newtonian mechanics, can be paraphrased as follows:\n\nThe three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), originally published in 1687.[3] Newton used them to investigate and explain the motion of many physical objects and systems.  In the time since Newton, new insights, especially around the concept of energy, built the field of classical mechanics on his foundations. Limitations to Newton's laws have also been discovered; new theories are necessary when objects move at very high speeds (special relativity), are very massive (general relativity), or are very small (quantum mechanics).\n\nNewton's laws are often stated in terms of point or particle masses, that is, bodies whose volume is negligible. This is a reasonable approximation for real bodies when the motion of internal parts can be neglected, and when the separation between bodies is much larger than the size of each. For instance, the Earth and the Sun can both be approximated as pointlike when considering the orbit of the former around the latter, but the Earth is not pointlike when considering activities on its surface.[note 1]\n\nThe mathematical description of motion, or kinematics, is based on the idea of specifying positions using numerical coordinates. Movement is represented by these numbers changing over time: a body's trajectory is represented by a function that assigns to each value of a time variable the values of all the position coordinates. The simplest case is one-dimensional, that is, when a body is constrained to move only along a straight line. Its position can then be given by a single number, indicating where it is relative to some chosen reference point. For example, a body might be free to slide along a track that runs left to right, and so its location can be specified by its distance from a convenient zero point, or origin, with negative numbers indicating positions to the left and positive numbers indicating positions to the right. If the body's location as a function of time is \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n  \n, then its average velocity over the time interval from \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n to \n  \n    \n      \n        \n          t\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle t_{1}}\n  \n is[6] \n  \n    \n      \n        \n          \n            \n              Δ\n              s\n            \n            \n              Δ\n              t\n            \n          \n        \n        =\n        \n          \n            \n              s\n              (\n              \n                t\n                \n                  1\n                \n              \n              )\n              −\n              s\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n            \n            \n              \n                t\n                \n                  1\n                \n              \n              −\n              \n                t\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\Delta s}{\\Delta t}}={\\frac {s(t_{1})-s(t_{0})}{t_{1}-t_{0}}}.}\n  \nHere, the Greek letter \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n (delta) is used, per tradition, to mean \"change in\". A positive average velocity means that the position coordinate \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n increases over the interval in question, a negative average velocity indicates a net decrease over that interval, and an average velocity of zero means that the body ends the time interval in the same place as it began. Calculus gives the means to define an instantaneous velocity, a measure of a body's speed and direction of movement at a single moment of time, rather than over an interval. One notation for the instantaneous velocity is to replace \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n with the symbol \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n, for example,\n  \n    \n      \n        v\n        =\n        \n          \n            \n              d\n              s\n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle v={\\frac {ds}{dt}}.}\n  \nThis denotes that the instantaneous velocity is the derivative of the position with respect to time. It can roughly be thought of as the ratio between an infinitesimally small change in position \n  \n    \n      \n        d\n        s\n      \n    \n    {\\displaystyle ds}\n  \n to the infinitesimally small time interval \n  \n    \n      \n        d\n        t\n      \n    \n    {\\displaystyle dt}\n  \n over which it occurs.[7] More carefully, the velocity and all other derivatives can be defined using the concept of a limit.[6] A function \n  \n    \n      \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t)}\n  \n has a limit of \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n at a given input value \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n if the difference between \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n and \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n can be made arbitrarily small by choosing an input sufficiently close to \n  \n    \n      \n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t_{0}}\n  \n. One writes, \n  \n    \n      \n        \n          lim\n          \n            t\n            →\n            \n              t\n              \n                0\n              \n            \n          \n        \n        f\n        (\n        t\n        )\n        =\n        L\n        .\n      \n    \n    {\\displaystyle \\lim _{t\\to t_{0}}f(t)=L.}\n  \nInstantaneous velocity can be defined as the limit of the average velocity as the time interval shrinks to zero:\n  \n    \n      \n        \n          \n            \n              d\n              s\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          lim\n          \n            Δ\n            t\n            →\n            0\n          \n        \n        \n          \n            \n              s\n              (\n              t\n              +\n              Δ\n              t\n              )\n              −\n              s\n              (\n              t\n              )\n            \n            \n              Δ\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {ds}{dt}}=\\lim _{\\Delta t\\to 0}{\\frac {s(t+\\Delta t)-s(t)}{\\Delta t}}.}\n  \n Acceleration is to velocity as velocity is to position: it is the derivative of the velocity with respect to time.[note 2] Acceleration can likewise be defined as a limit:\n  \n    \n      \n        a\n        =\n        \n          \n            \n              d\n              v\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          lim\n          \n            Δ\n            t\n            →\n            0\n          \n        \n        \n          \n            \n              v\n              (\n              t\n              +\n              Δ\n              t\n              )\n              −\n              v\n              (\n              t\n              )\n            \n            \n              Δ\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a={\\frac {dv}{dt}}=\\lim _{\\Delta t\\to 0}{\\frac {v(t+\\Delta t)-v(t)}{\\Delta t}}.}\n  \nConsequently, the acceleration is the second derivative of position,[7] often written \n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              s\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}s}{dt^{2}}}}\n  \n.\n\nPosition, when thought of as a displacement from an origin point, is a vector: a quantity with both magnitude and direction.[9]: 1  Velocity and acceleration are vector quantities as well. The mathematical tools of vector algebra provide the means to describe motion in two, three or more dimensions. Vectors are often denoted with an arrow, as in  \n  \n    \n      \n        \n          \n            \n              s\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {s}}}\n  \n, or in bold typeface, such as \n  \n    \n      \n        \n          \n            s\n          \n        \n      \n    \n    {\\displaystyle {\\bf {s}}}\n  \n. Often, vectors are represented visually as arrows, with the direction of the vector being the direction of the arrow, and the magnitude of the vector indicated by the length of the arrow. Numerically, a vector can be represented as a list; for example, a body's velocity vector might be \n  \n    \n      \n        \n          v\n        \n        =\n        (\n        \n          3\n           \n          m\n          \n            /\n          \n          s\n        \n        ,\n        \n          4\n           \n          m\n          \n            /\n          \n          s\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {v} =(\\mathrm {3~m/s} ,\\mathrm {4~m/s} )}\n  \n, indicating that it is moving at 3 metres per second along the horizontal axis and 4 metres per second along the vertical axis. The same motion described in a different coordinate system will be represented by different numbers, and vector algebra can be used to translate between these alternatives.[9]: 4\n\nThe study of mechanics is complicated by the fact that household words like energy are used with a technical meaning.[10][11] Moreover, words which are synonymous in everyday speech are not so in physics: force is not the same as power or pressure, for example, and mass has a different meaning than weight.[12][13]: 150  The physics concept of force makes quantitative the everyday idea of a push or a pull. Forces in Newtonian mechanics are often due to strings and ropes, friction, muscle effort, gravity, and so forth. Like displacement, velocity, and acceleration, force is a vector quantity.\n\nNewton's first law expresses the principle of inertia: the natural behavior of a body is to move in a straight line at constant speed. A body's motion preserves the status quo, but external forces can perturb this.\n\nThe modern understanding of Newton's first law is that no inertial observer is privileged over any other. The concept of an inertial observer makes quantitative the everyday idea of feeling no effects of motion. For example, a person standing on the ground watching a train go past is an inertial observer. If the observer on the ground sees the train moving smoothly in a straight line at a constant speed, then a passenger sitting on the train will also be an inertial observer: the train passenger feels no motion. The principle expressed by Newton's first law is that there is no way to say which inertial observer is \"really\" moving and which is \"really\" standing still. One observer's state of rest is another observer's state of uniform motion in a straight line, and no experiment can deem either point of view to be correct or incorrect. There is no absolute standard of rest.[18][15]: 62–63 [19]: 7–9  Newton himself believed that absolute space and time existed, but that the only measures of space or time accessible to experiment are relative.[20]\n\nBy \"motion\", Newton meant the quantity now called momentum, which depends upon the amount of matter contained in a body, the speed at which that body is moving, and the direction in which it is moving.[21] In modern notation, the momentum of a body is the product of its mass and its velocity:\n\n  \n    \n      \n        \n          p\n        \n        =\n        m\n        \n          v\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} =m\\mathbf {v} \\,,}\n  \n\nwhere all three quantities can change over time.\nIn common cases the mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n does not change with time and the derivative acts only upon the velocity. Then force equals the product of the mass and the time derivative of the velocity, which is the acceleration:[22]\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              d\n              \n                v\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        m\n        \n          a\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {d\\mathbf {v} }{dt}}=m\\mathbf {a} \\,.}\n  \n\nAs the acceleration is the second derivative of position with respect to time, this can also be written\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \n                s\n              \n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {d^{2}\\mathbf {s} }{dt^{2}}}.}\n\nNewton's second law, in modern form, states that the time derivative of the momentum is the force:[23]: 4.1 \n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\frac {d\\mathbf {p} }{dt}}\\,.}\n  \n\nWhen applied to systems of variable mass, the equation above is only valid only for a fixed set of particles. Applying the derivative as in \n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        +\n        \n          v\n        \n        \n          \n            \n              \n                d\n              \n              m\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n         \n         \n        \n          (\n          i\n          n\n          c\n          o\n          r\n          r\n          e\n          c\n          t\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =m{\\frac {\\mathrm {d} \\mathbf {v} }{\\mathrm {d} t}}+\\mathbf {v} {\\frac {\\mathrm {d} m}{\\mathrm {d} t}}\\ \\ \\mathrm {(incorrect)} }\n  \n\ncan lead to incorrect results.[24] For example, the momentum of a water jet system must include the momentum of the ejected water:[25]\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              e\n              x\n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        −\n        \n          \n            v\n          \n          \n            \n              e\n              j\n              e\n              c\n              t\n            \n          \n        \n        \n          \n            \n              \n                d\n              \n              m\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\mathrm {ext} }={\\mathrm {d} \\mathbf {p}  \\over \\mathrm {d} t}-\\mathbf {v} _{\\mathrm {eject} }{\\frac {\\mathrm {d} m}{\\mathrm {d} t}}.}\n\nThe forces acting on a body add as vectors, and so the total force on a body depends upon both the magnitudes and the directions of the individual forces.[23]: 58  When the net force on a body is equal to zero, then by Newton's second law, the body does not accelerate, and it is said to be in mechanical equilibrium. A state of mechanical equilibrium is stable if, when the position of the body is changed slightly, the body remains near that equilibrium. Otherwise, the equilibrium is unstable.[15]: 121 [23]: 174\n\nA common visual representation of forces acting in concert is the free body diagram, which schematically portrays a body of interest and the forces applied to it by outside influences.[26] For example, a free body diagram of a block sitting upon an inclined plane can illustrate the combination of gravitational force, \"normal\" force, friction, and string tension.[note 4]\n\nNewton's second law is sometimes presented as a definition of force, i.e., a force is that which exists when an inertial observer sees a body accelerating. This is sometimes regarded as a potential tautology — acceleration implies force, force implies acceleration. However, Newton's second law not only merely defines the force by the acceleration: forces exist as separate from the acceleration produced by the force in a particular system. The same force that is identified as producing acceleration to an object can then be applied to any other object, and the resulting accelerations (coming from that same force) will always be inversely proportional to the mass of the object.  What Newton's Second Law states is that all the effect of a force onto a system can be reduced to two pieces of information: the magnitude of the force, and it's direction, and then goes on to specify what the effect is.\n\nBeyond that, an equation detailing the force might also be specified, like Newton's law of universal gravitation. By inserting such an expression for \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n into Newton's second law, an equation with predictive power can be written.[note 5] Newton's second law has also been regarded as setting out a research program for physics, establishing that important goals of the subject are to identify the forces present in nature and to catalogue the constituents of matter.[15]: 134 [28]: 12-2\n\nHowever, forces can often be measured directly with no acceleration being involved, such as through weighing scales. By postulating a physical object that can be directly measured independently from acceleration, Newton made a objective physical statement with the second law alone, the predictions of which can be verified even if no force law is given.\n\nOverly brief paraphrases of the third law, like \"action equals reaction\" might have caused confusion among generations of students: the \"action\" and \"reaction\" apply to different bodies. For example, consider a book at rest on a table. The Earth's gravity pulls down upon the book. The \"reaction\" to that \"action\" is not the support force from the table holding up the book, but the gravitational pull of the book acting on the Earth.[note 6]\n\nNewton's third law relates to a more fundamental principle, the conservation of momentum. The latter remains true even in cases where Newton's statement does not, for instance when force fields as well as material bodies carry momentum, and when momentum is defined properly, in quantum mechanics as well.[note 7] In Newtonian mechanics, if two bodies have momenta \n  \n    \n      \n        \n          \n            p\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{1}}\n  \n and \n  \n    \n      \n        \n          \n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{2}}\n  \n respectively, then the total momentum of the pair is \n  \n    \n      \n        \n          p\n        \n        =\n        \n          \n            p\n          \n          \n            1\n          \n        \n        +\n        \n          \n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} =\\mathbf {p} _{1}+\\mathbf {p} _{2}}\n  \n, and the rate of change of \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is \n  \n    \n      \n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              d\n              \n                \n                  p\n                \n                \n                  1\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        +\n        \n          \n            \n              d\n              \n                \n                  p\n                \n                \n                  2\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\mathbf {p} }{dt}}={\\frac {d\\mathbf {p} _{1}}{dt}}+{\\frac {d\\mathbf {p} _{2}}{dt}}.}\n  \n By Newton's second law, the first term is the total force upon the first body, and the second term is the total force upon the second body. If the two bodies are isolated from outside influences, the only force upon the first body can be that from the second, and vice versa. By Newton's third law, these forces have equal magnitude but opposite direction, so they cancel when added, and \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is constant. Alternatively, if \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n is known to be constant, it follows that the forces have equal magnitude and opposite direction.\n\nVarious sources have proposed elevating other ideas used in classical mechanics to the status of Newton's laws. For example, in Newtonian mechanics, the total mass of a body made by bringing together two smaller bodies is the sum of their individual masses. Frank Wilczek has suggested calling attention to this assumption by designating it \"Newton's Zeroth Law\".[37] Another candidate for a \"zeroth law\" is the fact that at any instant, a body reacts to the forces applied to it at that instant.[38] Likewise, the idea that forces add like vectors (or in other words obey the superposition principle), and the idea that forces change the energy of a body, have both been described as a \"fourth law\".[note 8]\n\nMoreover, some texts organize the basic ideas of Newtonian mechanics into different postulates, other than the three laws as commonly phrased, with the goal of being more clear about what is empirically observed and what is true by definition.[19]: 9 [27]\n\nThe study of the behavior of massive bodies using Newton's laws is known as Newtonian mechanics. Some example problems in Newtonian mechanics are particularly noteworthy for conceptual or historical reasons.\n\nIf a body falls from rest near the surface of the Earth, then in the absence of air resistance, it will accelerate at a constant rate. This is known as free fall. The speed attained during free fall is proportional to the elapsed time, and the distance traveled is proportional to the square of the elapsed time.[43] Importantly, the acceleration is the same for all bodies, independently of their mass. This follows from combining Newton's second law of motion with his law of universal gravitation. The latter states that the magnitude of the gravitational force from the Earth upon the body is\n\n  \n    \n      \n        F\n        =\n        \n          \n            \n              G\n              M\n              m\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle F={\\frac {GMm}{r^{2}}},}\n  \n\nwhere \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the mass of the falling body, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the mass of the Earth, \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n is Newton's constant, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n is the distance from the center of the Earth to the body's location, which is very nearly the radius of the Earth. Setting this equal to \n  \n    \n      \n        m\n        a\n      \n    \n    {\\displaystyle ma}\n  \n, the body's mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n cancels from both sides of the equation, leaving an acceleration that depends upon \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n, \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n, and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n can be taken to be constant. This particular value of acceleration is typically denoted \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n:\n\n  \n    \n      \n        g\n        =\n        \n          \n            \n              G\n              M\n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        ≈\n        \n          9.8\n           \n          m\n          \n            /\n          \n          \n            s\n            \n              2\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle g={\\frac {GM}{r^{2}}}\\approx \\mathrm {9.8~m/s^{2}} .}\n\nIf the body is not released from rest but instead launched upwards and/or horizontally with nonzero velocity, then free fall becomes projectile motion.[44] When air resistance can be neglected, projectiles follow parabola-shaped trajectories, because gravity affects the body's vertical motion and not its horizontal. At the peak of the projectile's trajectory, its vertical velocity is zero, but its acceleration is \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n downwards, as it is at all times. Setting the wrong vector equal to zero is a common confusion among physics students.[45]\n\nWhen a body is in uniform circular motion, the force on it changes the direction of its motion but not its speed. For a body moving in a circle of radius \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n at a constant speed \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n, its acceleration has a magnitude\n  \n    \n      \n        a\n        =\n        \n          \n            \n              v\n              \n                2\n              \n            \n            r\n          \n        \n      \n    \n    {\\displaystyle a={\\frac {v^{2}}{r}}}\n  \nand is directed toward the center of the circle.[note 9] The force required to sustain this acceleration, called the centripetal force, is therefore also directed toward the center of the circle and has magnitude \n  \n    \n      \n        m\n        \n          v\n          \n            2\n          \n        \n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle mv^{2}/r}\n  \n. Many orbits, such as that of the Moon around the Earth, can be approximated by uniform circular motion. In such cases, the centripetal force is gravity, and by Newton's law of universal gravitation has magnitude \n  \n    \n      \n        G\n        M\n        m\n        \n          /\n        \n        \n          r\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle GMm/r^{2}}\n  \n, where \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the mass of the larger body being orbited. Therefore, the mass of a body can be calculated from observations of another body orbiting around it.[47]: 130\n\nNewton's cannonball is a thought experiment that interpolates between projectile motion and uniform circular motion. A cannonball that is lobbed weakly off the edge of a tall cliff will hit the ground in the same amount of time as if it were dropped from rest, because the force of gravity only affects the cannonball's momentum in the downward direction, and its effect is not diminished by horizontal movement. If the cannonball is launched with a greater initial horizontal velocity, then it will travel farther before it hits the ground, but it will still hit the ground in the same amount of time. However, if the cannonball is launched with an even larger initial velocity, then the curvature of the Earth becomes significant: the ground itself will curve away from the falling cannonball. A very fast cannonball will fall away from the inertial straight-line trajectory at the same rate that the Earth curves away beneath it; in other words, it will be in orbit (imagining that it is not slowed by air resistance or obstacles).[48]\n\nConsider a body of mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n able to move along the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n axis, and suppose an equilibrium point exists at the position \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n. That is, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n, the net force upon the body is the zero vector, and by Newton's second law, the body will not accelerate. If the force upon the body is proportional to the displacement from the equilibrium point, and directed to the equilibrium point, then the body will perform simple harmonic motion. Writing the force as \n  \n    \n      \n        F\n        =\n        −\n        k\n        x\n      \n    \n    {\\displaystyle F=-kx}\n  \n, Newton's second law becomes\n\n  \n    \n      \n        m\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              x\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        k\n        x\n        \n        .\n      \n    \n    {\\displaystyle m{\\frac {d^{2}x}{dt^{2}}}=-kx\\,.}\n  \n\nThis differential equation has the solution\n\n  \n    \n      \n        x\n        (\n        t\n        )\n        =\n        A\n        cos\n        ⁡\n        ω\n        t\n        +\n        B\n        sin\n        ⁡\n        ω\n        t\n        \n      \n    \n    {\\displaystyle x(t)=A\\cos \\omega t+B\\sin \\omega t\\,}\n  \n\nwhere the frequency \n  \n    \n      \n        ω\n      \n    \n    {\\displaystyle \\omega }\n  \n is equal to \n  \n    \n      \n        \n          \n            k\n            \n              /\n            \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {k/m}}}\n  \n, and the constants \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n can be calculated knowing, for example, the position and velocity the body has at a given time, like \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n.\n\nOne reason that the harmonic oscillator is a conceptually important example is that it is good approximation for many systems near a stable mechanical equilibrium.[note 10] For example, a pendulum has a stable equilibrium in the vertical position: if motionless there, it will remain there, and if pushed slightly, it will swing back and forth. Neglecting air resistance and friction in the pivot, the force upon the pendulum is gravity, and Newton's second law becomes \n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        \n          \n            g\n            L\n          \n        \n        sin\n        ⁡\n        θ\n        ,\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}=-{\\frac {g}{L}}\\sin \\theta ,}\n  \nwhere \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the length of the pendulum and \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is its angle from the vertical. When the angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is small, the sine of \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n is nearly equal to \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n (see small-angle approximation), and so this expression simplifies to the equation for a simple harmonic oscillator with frequency \n  \n    \n      \n        ω\n        =\n        \n          \n            g\n            \n              /\n            \n            L\n          \n        \n      \n    \n    {\\displaystyle \\omega ={\\sqrt {g/L}}}\n  \n.\n\nA harmonic oscillator can be damped, often by friction or viscous drag, in which case energy bleeds out of the oscillator and the amplitude of the oscillations decreases over time. Also, a harmonic oscillator can be driven by an applied force, which can lead to the phenomenon of resonance.[50]\n\nNewtonian physics treats matter as being neither created nor destroyed, though it may be rearranged. It can be the case that an object of interest gains or loses mass because matter is added to or removed from it. In such a situation, Newton's laws can be applied to the individual pieces of matter, keeping track of which pieces belong to the object of interest over time. For instance, if a rocket of mass \n  \n    \n      \n        M\n        (\n        t\n        )\n      \n    \n    {\\displaystyle M(t)}\n  \n, moving at velocity \n  \n    \n      \n        \n          v\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {v} (t)}\n  \n, ejects matter at a velocity \n  \n    \n      \n        \n          u\n        \n      \n    \n    {\\displaystyle \\mathbf {u} }\n  \n relative to the rocket, then[24]\n\n  \n    \n      \n        \n          F\n        \n        =\n        M\n        \n          \n            \n              d\n              \n                v\n              \n            \n            \n              d\n              t\n            \n          \n        \n        −\n        \n          u\n        \n        \n          \n            \n              d\n              M\n            \n            \n              d\n              t\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {F} =M{\\frac {d\\mathbf {v} }{dt}}-\\mathbf {u} {\\frac {dM}{dt}}\\,}\n  \n\nwhere \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n is the net external force (e.g., a planet's gravitational pull).[23]: 139\n\nThe fan and sail example is a situation studied in discussions of Newton's third law.[51] In the situation, a fan is attached to a cart or a sailboat and blows on its sail. From the third law, one would reason that the force of the air pushing in one direction would cancel out the force done by the fan on the sail, leaving the entire apparatus stationary. However, because the system is not entirely enclosed, there are conditions in which the vessel will move; for example, if the sail is built in a manner that redirects the majority of the airflow back towards the fan, the net force will result in the vessel moving forward.[34][52]\n\nThe concept of energy was developed after Newton's time, but it has become an inseparable part of what is considered \"Newtonian\" physics. Energy can broadly be classified into kinetic, due to a body's motion, and potential, due to a body's position relative to others. Thermal energy, the energy carried by heat flow, is a type of kinetic energy not associated with the macroscopic motion of objects but instead with the movements of the atoms and molecules of which they are made. According to the work-energy theorem, when a force acts upon a body while that body moves along the line of the force, the force does work upon the body, and the amount of work done is equal to the change in the body's kinetic energy.[note 11] In many cases of interest, the net work done by a force when a body moves in a closed loop — starting at a point, moving along some trajectory, and returning to the initial point — is zero. If this is the case, then the force can be written in terms of the gradient of a function called a scalar potential:[46]: 303 \n\n  \n    \n      \n        \n          F\n        \n        =\n        −\n        \n          ∇\n        \n        U\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =-\\mathbf {\\nabla } U\\,.}\n  \n\nThis is true for many forces including that of gravity, but not for friction; indeed, almost any problem in a mechanics textbook that does not involve friction can be expressed in this way.[49]: 19  The fact that the force can be written in this way can be understood from the conservation of energy. Without friction to dissipate a body's energy into heat, the body's energy will trade between potential and (non-thermal) kinetic forms while the total amount remains constant. Any gain of kinetic energy, which occurs when the net force on the body accelerates it to a higher speed, must be accompanied by a loss of potential energy. So, the net force upon the body is determined by the manner in which the potential energy decreases.\n\nA rigid body is an object whose size is too large to neglect and which maintains the same shape over time. In Newtonian mechanics, the motion of a rigid body is often understood by separating it into movement of the body's center of mass and movement around the center of mass.\n\nSignificant aspects of the motion of an extended body can be understood by imagining the mass of that body concentrated to a single point, known as the center of mass. The location of a body's center of mass depends upon how that body's material is distributed. For a collection of pointlike objects with masses \n  \n    \n      \n        \n          m\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          m\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle m_{1},\\ldots ,m_{N}}\n  \n at positions \n  \n    \n      \n        \n          \n            r\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            r\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {r} _{1},\\ldots ,\\mathbf {r} _{N}}\n  \n, the center of mass is located at \n  \n    \n      \n        \n          R\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \n              \n                m\n                \n                  i\n                \n              \n              \n                \n                  r\n                \n                \n                  i\n                \n              \n            \n            M\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {R} =\\sum _{i=1}^{N}{\\frac {m_{i}\\mathbf {r} _{i}}{M}},}\n  \n where \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is the total mass of the collection. In the absence of a net external force, the center of mass moves at a constant speed in a straight line. This applies, for example, to a collision between two bodies.[55] If the total external force is not zero, then the center of mass changes velocity as though it were a point body of mass \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. This follows from the fact that the internal forces within the collection, the forces that the objects exert upon each other, occur in balanced pairs by Newton's third law. In a system of two bodies with one much more massive than the other, the center of mass will approximately coincide with the location of the more massive body.[19]: 22–24\n\nWhen Newton's laws are applied to rotating extended bodies, they lead to new quantities that are analogous to those invoked in the original laws. The analogue of mass is the moment of inertia, the counterpart of momentum is angular momentum, and the counterpart of force is torque.\n\nAngular momentum is calculated with respect to a reference point.[56] If the displacement vector from a reference point to a body is \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n and the body has momentum \n  \n    \n      \n        \n          p\n        \n      \n    \n    {\\displaystyle \\mathbf {p} }\n  \n, then the body's angular momentum with respect to that point is, using the vector cross product, \n  \n    \n      \n        \n          L\n        \n        =\n        \n          r\n        \n        ×\n        \n          p\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {L} =\\mathbf {r} \\times \\mathbf {p} .}\n  \n Taking the time derivative of the angular momentum gives \n  \n    \n      \n        \n          \n            \n              d\n              \n                L\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          (\n          \n            \n              \n                d\n                \n                  r\n                \n              \n              \n                d\n                t\n              \n            \n          \n          )\n        \n        ×\n        \n          p\n        \n        +\n        \n          r\n        \n        ×\n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          v\n        \n        ×\n        m\n        \n          v\n        \n        +\n        \n          r\n        \n        ×\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\mathbf {L} }{dt}}=\\left({\\frac {d\\mathbf {r} }{dt}}\\right)\\times \\mathbf {p} +\\mathbf {r} \\times {\\frac {d\\mathbf {p} }{dt}}=\\mathbf {v} \\times m\\mathbf {v} +\\mathbf {r} \\times \\mathbf {F} .}\n  \n The first term vanishes because \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n and \n  \n    \n      \n        m\n        \n          v\n        \n      \n    \n    {\\displaystyle m\\mathbf {v} }\n  \n point in the same direction. The remaining term is the torque, \n  \n    \n      \n        \n          τ\n        \n        =\n        \n          r\n        \n        ×\n        \n          F\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {\\tau } =\\mathbf {r} \\times \\mathbf {F} .}\n  \n When the torque is zero, the angular momentum is constant, just as when the force is zero, the momentum is constant.[19]: 14–15  The torque can vanish even when the force is non-zero, if the body is located at the reference point (\n  \n    \n      \n        \n          r\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {r} =0}\n  \n) or if the force \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n and the displacement vector \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n are directed along the same line.\n\nThe angular momentum of a collection of point masses, and thus of an extended body, is found by adding the contributions from each of the points. This provides a means to characterize a body's rotation about an axis, by adding up the angular momenta of its individual pieces. The result depends on the chosen axis, the shape of the body, and the rate of rotation.[19]: 28\n\nNewton's law of universal gravitation states that any body attracts any other body along the straight line connecting them. The size of the attracting force is proportional to the product of their masses, and inversely proportional to the square of the distance between them. Finding the shape of the orbits that an inverse-square force law will produce is known as the Kepler problem. The Kepler problem can be solved in multiple ways, including by demonstrating that the Laplace–Runge–Lenz vector is constant,[57] or by applying a duality transformation to a 2-dimensional harmonic oscillator.[58] However it is solved, the result is that orbits will be conic sections, that is, ellipses (including circles), parabolas, or hyperbolas. The eccentricity of the orbit, and thus the type of conic section, is determined by the energy and the angular momentum of the orbiting body. Planets do not have sufficient energy to escape the Sun, and so their orbits are ellipses, to a good approximation; because the planets pull on one another, actual orbits are not exactly conic sections.\n\nIf a third mass is added, the Kepler problem becomes the three-body problem, which in general has no exact solution in closed form. That is, there is no way to start from the differential equations implied by Newton's laws and, after a finite sequence of standard mathematical operations, obtain equations that express the three bodies' motions over time.[59][60] Numerical methods can be applied to obtain useful, albeit approximate, results for the three-body problem.[61] The positions and velocities of the bodies can be stored in variables within a computer's memory; Newton's laws are used to calculate how the velocities will change over a short interval of time, and knowing the velocities, the changes of position over that time interval can be computed. This process is looped to calculate, approximately, the bodies' trajectories. Generally speaking, the shorter the time interval, the more accurate the approximation.[62]\n\nNewton's laws of motion allow the possibility of chaos.[63][64] That is, qualitatively speaking, physical systems obeying Newton's laws can exhibit sensitive dependence upon their initial conditions: a slight change of the position or velocity of one part of a system can lead to the whole system behaving in a radically different way within a short time. Noteworthy examples include the three-body problem, the double pendulum, dynamical billiards, and the Fermi–Pasta–Ulam–Tsingou problem.\n\nNewton's laws can be applied to fluids by considering a fluid as composed of infinitesimal pieces, each exerting forces upon neighboring pieces. The Euler momentum equation is an expression of Newton's second law adapted to fluid dynamics.[65][66] A fluid is described by a velocity field, i.e., a function \n  \n    \n      \n        \n          v\n        \n        (\n        \n          x\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {v} (\\mathbf {x} ,t)}\n  \n that assigns a velocity vector to each point in space and time. A small object being carried along by the fluid flow can change velocity for two reasons: first, because the velocity field at its position is changing over time, and second, because it moves to a new location where the velocity field has a different value. Consequently, when Newton's second law is applied to an infinitesimal portion of fluid, the acceleration \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n  \n has two terms, a combination known as a total or material derivative. The mass of an infinitesimal portion depends upon the fluid density, and there is a net force upon it if the fluid pressure varies from one side of it to another. Accordingly, \n  \n    \n      \n        \n          a\n        \n        =\n        \n          F\n        \n        \n          /\n        \n        m\n      \n    \n    {\\displaystyle \\mathbf {a} =\\mathbf {F} /m}\n  \n becomes\n\n  \n    \n      \n        \n          \n            \n              ∂\n              v\n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          ∇\n        \n        ⋅\n        \n          v\n        \n        )\n        \n          v\n        \n        =\n        −\n        \n          \n            1\n            ρ\n          \n        \n        \n          ∇\n        \n        P\n        +\n        \n          f\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\partial v}{\\partial t}}+(\\mathbf {\\nabla } \\cdot \\mathbf {v} )\\mathbf {v} =-{\\frac {1}{\\rho }}\\mathbf {\\nabla } P+\\mathbf {f} ,}\n  \n\nwhere \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n is the density, \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is the pressure, and \n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle \\mathbf {f} }\n  \n stands for an external influence like a gravitational pull. Incorporating the effect of viscosity turns the Euler equation into a Navier–Stokes equation:\n\n  \n    \n      \n        \n          \n            \n              ∂\n              v\n            \n            \n              ∂\n              t\n            \n          \n        \n        +\n        (\n        \n          ∇\n        \n        ⋅\n        \n          v\n        \n        )\n        \n          v\n        \n        =\n        −\n        \n          \n            1\n            ρ\n          \n        \n        \n          ∇\n        \n        P\n        +\n        ν\n        \n          ∇\n          \n            2\n          \n        \n        \n          v\n        \n        +\n        \n          f\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\partial v}{\\partial t}}+(\\mathbf {\\nabla } \\cdot \\mathbf {v} )\\mathbf {v} =-{\\frac {1}{\\rho }}\\mathbf {\\nabla } P+\\nu \\nabla ^{2}\\mathbf {v} +\\mathbf {f} ,}\n  \n\nwhere \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n is the kinematic viscosity.[65]\n\nIt is mathematically possible for a collection of point masses, moving in accord with Newton's laws, to launch some of themselves away so forcefully that they fly off to infinity in a finite time.[67] This unphysical behavior, known as a \"noncollision singularity\",[60] depends upon the masses being pointlike and able to approach one another arbitrarily closely, as well as the lack of a relativistic speed limit in Newtonian physics.[68]\n\nIt is not yet known whether or not the Euler and Navier–Stokes equations exhibit the analogous behavior of initially smooth solutions \"blowing up\" in finite time. The question of existence and smoothness of Navier–Stokes solutions is one of the Millennium Prize Problems.[69]\n\nClassical mechanics can be mathematically formulated in multiple different ways, other than the \"Newtonian\" description (which itself, of course, incorporates contributions from others both before and after Newton). The physical content of these different formulations is the same as the Newtonian, but they provide different insights and facilitate different types of calculations. For example, Lagrangian mechanics helps make apparent the connection between symmetries and conservation laws, and it is useful when calculating the motion of constrained bodies, like a mass restricted to move along a curving track or on the surface of a sphere.[19]: 48  Hamiltonian mechanics is convenient for statistical physics,[70][71]: 57  leads to further insight about symmetry,[19]: 251  and can be developed into sophisticated techniques for perturbation theory.[19]: 284  Due to the breadth of these topics, the discussion here will be confined to concise treatments of how they reformulate Newton's laws of motion.\n\nLagrangian mechanics differs from the Newtonian formulation by considering entire trajectories at once rather than predicting a body's motion at a single instant.[19]: 109  It is traditional in Lagrangian mechanics to denote position with \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n and velocity with \n  \n    \n      \n        \n          \n            \n              q\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {q}}}\n  \n. The simplest example is a massive point particle, the Lagrangian for which can be written as the difference between its kinetic and potential energies:\n\n  \n    \n      \n        L\n        (\n        q\n        ,\n        \n          \n            \n              q\n              ˙\n            \n          \n        \n        )\n        =\n        T\n        −\n        V\n        ,\n      \n    \n    {\\displaystyle L(q,{\\dot {q}})=T-V,}\n  \n\nwhere the kinetic energy is\n\n  \n    \n      \n        T\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle T={\\frac {1}{2}}m{\\dot {q}}^{2}}\n  \n\nand the potential energy is some function of the position, \n  \n    \n      \n        V\n        (\n        q\n        )\n      \n    \n    {\\displaystyle V(q)}\n  \n. The physical path that the particle will take between an initial point \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and a final point \n  \n    \n      \n        \n          q\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle q_{f}}\n  \n is the path for which the integral of the Lagrangian is \"stationary\". That is, the physical path has the property that small perturbations of it will, to a first approximation, not change the integral of the Lagrangian. Calculus of variations provides the mathematical tools for finding this path.[46]: 485  Applying the calculus of variations to the task of finding the path yields the Euler–Lagrange equation for the particle,\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \n          (\n          \n            \n              \n                ∂\n                L\n              \n              \n                ∂\n                \n                  \n                    \n                      q\n                      ˙\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              q\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}\\left({\\frac {\\partial L}{\\partial {\\dot {q}}}}\\right)={\\frac {\\partial L}{\\partial q}}.}\n  \n\nEvaluating the partial derivatives of the Lagrangian gives\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        (\n        m\n        \n          \n            \n              q\n              ˙\n            \n          \n        \n        )\n        =\n        −\n        \n          \n            \n              d\n              V\n            \n            \n              d\n              q\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}(m{\\dot {q}})=-{\\frac {dV}{dq}},}\n  \n\nwhich is a restatement of Newton's second law. The left-hand side is the time derivative of the momentum, and the right-hand side is the force, represented in terms of the potential energy.[9]: 737\n\nLandau and Lifshitz argue that the Lagrangian formulation makes the conceptual content of classical mechanics more clear than starting with Newton's laws.[29] Lagrangian mechanics provides a convenient framework in which to prove Noether's theorem, which relates symmetries and conservation laws.[72] The conservation of momentum can be derived by applying Noether's theorem to a Lagrangian for a multi-particle system, and so, Newton's third law is a theorem rather than an assumption.[19]: 124\n\nIn Hamiltonian mechanics, the dynamics of a system are represented by a function called the Hamiltonian, which in many cases of interest is equal to the total energy of the system.[9]: 742  The Hamiltonian is a function of the positions and the momenta of all the bodies making up the system, and it may also depend explicitly upon time. The time derivatives of the position and momentum variables are given by partial derivatives of the Hamiltonian, via Hamilton's equations.[19]: 203  The simplest example is a point mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n constrained to move in a straight line, under the effect of a potential. Writing \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n for the position coordinate and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n for the body's momentum, the Hamiltonian is\n\n  \n    \n      \n        \n          \n            H\n          \n        \n        (\n        p\n        ,\n        q\n        )\n        =\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        +\n        V\n        (\n        q\n        )\n        .\n      \n    \n    {\\displaystyle {\\mathcal {H}}(p,q)={\\frac {p^{2}}{2m}}+V(q).}\n  \n\nIn this example, Hamilton's equations are\n\n  \n    \n      \n        \n          \n            \n              d\n              q\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {dq}{dt}}={\\frac {\\partial {\\mathcal {H}}}{\\partial p}}}\n  \n\nand\n\n  \n    \n      \n        \n          \n            \n              d\n              p\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              q\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dp}{dt}}=-{\\frac {\\partial {\\mathcal {H}}}{\\partial q}}.}\n  \n\nEvaluating these partial derivatives, the former equation becomes\n\n  \n    \n      \n        \n          \n            \n              d\n              q\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            p\n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dq}{dt}}={\\frac {p}{m}},}\n  \n\nwhich reproduces the familiar statement that a body's momentum is the product of its mass and velocity. The time derivative of the momentum is\n\n  \n    \n      \n        \n          \n            \n              d\n              p\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              d\n              V\n            \n            \n              d\n              q\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dp}{dt}}=-{\\frac {dV}{dq}},}\n  \n\nwhich, upon identifying the negative derivative of the potential with the force, is just Newton's second law once again.[63][9]: 742\n\nAs in the Lagrangian formulation, in Hamiltonian mechanics the conservation of momentum can be derived using Noether's theorem, making Newton's third law an idea that is deduced rather than assumed.[19]: 251\n\nAmong the proposals to reform the standard introductory-physics curriculum is one that teaches the concept of energy before that of force, essentially \"introductory Hamiltonian mechanics\".[73][74]\n\nThe Hamilton–Jacobi equation provides yet another formulation of classical mechanics, one which makes it mathematically analogous to wave optics.[19]: 284 [75] This formulation also uses Hamiltonian functions, but in a different way than the formulation described above. The paths taken by bodies or collections of bodies are deduced from a function \n  \n    \n      \n        S\n        (\n        \n          \n            q\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            q\n          \n          \n            2\n          \n        \n        ,\n        …\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle S(\\mathbf {q} _{1},\\mathbf {q} _{2},\\ldots ,t)}\n  \n of positions \n  \n    \n      \n        \n          \n            q\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {q} _{i}}\n  \n and time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. The Hamiltonian is incorporated into the Hamilton–Jacobi equation, a differential equation for \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n. Bodies move over time in such a way that their trajectories are perpendicular to the surfaces of constant \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, analogously to how a light ray propagates in the direction perpendicular to its wavefront. This is simplest to express for the case of a single point mass, in which \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is a function \n  \n    \n      \n        S\n        (\n        \n          q\n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle S(\\mathbf {q} ,t)}\n  \n, and the point mass moves in the direction along which \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n changes most steeply. In other words, the momentum of the point mass is the gradient of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n:\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          ∇\n        \n        S\n        .\n      \n    \n    {\\displaystyle \\mathbf {v} ={\\frac {1}{m}}\\mathbf {\\nabla } S.}\n  \n\nThe Hamilton–Jacobi equation for a point mass is\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        H\n        \n          (\n          \n            \n              q\n            \n            ,\n            \n              ∇\n            \n            S\n            ,\n            t\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial S}{\\partial t}}=H\\left(\\mathbf {q} ,\\mathbf {\\nabla } S,t\\right).}\n  \n\nThe relation to Newton's laws can be seen by considering a point mass moving in a time-independent potential \n  \n    \n      \n        V\n        (\n        \n          q\n        \n        )\n      \n    \n    {\\displaystyle V(\\mathbf {q} )}\n  \n, in which case the Hamilton–Jacobi equation becomes\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          \n            (\n            \n              \n                ∇\n              \n              S\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        V\n        (\n        \n          q\n        \n        )\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial S}{\\partial t}}={\\frac {1}{2m}}\\left(\\mathbf {\\nabla } S\\right)^{2}+V(\\mathbf {q} ).}\n  \n\nTaking the gradient of both sides, this becomes\n\n  \n    \n      \n        −\n        \n          ∇\n        \n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              t\n            \n          \n        \n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          ∇\n        \n        \n          \n            (\n            \n              \n                ∇\n              \n              S\n            \n            )\n          \n          \n            2\n          \n        \n        +\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle -\\mathbf {\\nabla } {\\frac {\\partial S}{\\partial t}}={\\frac {1}{2m}}\\mathbf {\\nabla } \\left(\\mathbf {\\nabla } S\\right)^{2}+\\mathbf {\\nabla } V.}\n  \n\nInterchanging the order of the partial derivatives on the left-hand side, and using the power and chain rules on the first term on the right-hand side,\n\n  \n    \n      \n        −\n        \n          \n            ∂\n            \n              ∂\n              t\n            \n          \n        \n        \n          ∇\n        \n        S\n        =\n        \n          \n            1\n            m\n          \n        \n        \n          (\n          \n            \n              ∇\n            \n            S\n            ⋅\n            \n              ∇\n            \n          \n          )\n        \n        \n          ∇\n        \n        S\n        +\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial }{\\partial t}}\\mathbf {\\nabla } S={\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\mathbf {\\nabla } S+\\mathbf {\\nabla } V.}\n  \n\nGathering together the terms that depend upon the gradient of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n,\n\n  \n    \n      \n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                1\n                m\n              \n            \n            \n              (\n              \n                \n                  ∇\n                \n                S\n                ⋅\n                \n                  ∇\n                \n              \n              )\n            \n          \n          ]\n        \n        \n          ∇\n        \n        S\n        =\n        −\n        \n          ∇\n        \n        V\n        .\n      \n    \n    {\\displaystyle \\left[{\\frac {\\partial }{\\partial t}}+{\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\right]\\mathbf {\\nabla } S=-\\mathbf {\\nabla } V.}\n  \n\nThis is another re-expression of Newton's second law.[76] The expression in brackets is a total or material derivative as mentioned above,[77] in which the first term indicates how the function being differentiated changes over time at a fixed location, and the second term captures how a moving particle will see different values of that function as it travels from place to place:\n\n  \n    \n      \n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              \n                1\n                m\n              \n            \n            \n              (\n              \n                \n                  ∇\n                \n                S\n                ⋅\n                \n                  ∇\n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                ∂\n                \n                  ∂\n                  t\n                \n              \n            \n            +\n            \n              v\n            \n            ⋅\n            \n              ∇\n            \n          \n          ]\n        \n        =\n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\left[{\\frac {\\partial }{\\partial t}}+{\\frac {1}{m}}\\left(\\mathbf {\\nabla } S\\cdot \\mathbf {\\nabla } \\right)\\right]=\\left[{\\frac {\\partial }{\\partial t}}+\\mathbf {v} \\cdot \\mathbf {\\nabla } \\right]={\\frac {d}{dt}}.}\n\nIn statistical physics, the kinetic theory of gases applies Newton's laws of motion to large numbers (typically on the order of the Avogadro number) of particles. Kinetic theory can explain, for example, the pressure that a gas exerts upon the container holding it as the aggregate of many impacts of atoms, each imparting a tiny amount of momentum.[71]: 62\n\nThe Langevin equation is a special case of Newton's second law, adapted for the case of describing a small object bombarded stochastically by even smaller ones.[78]: 235  It can be written\n  \n    \n      \n        m\n        \n          a\n        \n        =\n        −\n        γ\n        \n          v\n        \n        +\n        \n          ξ\n        \n        \n      \n    \n    {\\displaystyle m\\mathbf {a} =-\\gamma \\mathbf {v} +\\mathbf {\\xi } \\,}\n  \nwhere \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is a drag coefficient and \n  \n    \n      \n        \n          ξ\n        \n      \n    \n    {\\displaystyle \\mathbf {\\xi } }\n  \n is a force that varies randomly from instant to instant, representing the net effect of collisions with the surrounding particles. This is used to model Brownian motion.[79]\n\nNewton's three laws can be applied to phenomena involving electricity and magnetism, though subtleties and caveats exist.\n\nCoulomb's law for the electric force between two stationary, electrically charged bodies has much the same mathematical form as Newton's law of universal gravitation: the force is proportional to the product of the charges, inversely proportional to the square of the distance between them, and directed along the straight line between them. The Coulomb force that a charge \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n exerts upon a charge \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n is equal in magnitude to the force that \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle q_{2}}\n  \n exerts upon \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle q_{1}}\n  \n, and it points in the exact opposite direction. Coulomb's law is thus consistent with Newton's third law.[80]\n\nElectromagnetism treats forces as produced by fields acting upon charges. The Lorentz force law provides an expression for the force upon a charged body that can be plugged into Newton's second law in order to calculate its acceleration.[81]: 85  According to the Lorentz force law, a charged body in an electric field experiences a force in the direction of that field, a force proportional to its charge \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n and to the strength of the electric field. In addition, a moving charged body in a magnetic field experiences a force that is also proportional to its charge, in a direction perpendicular to both the field and the body's direction of motion. Using the vector cross product,\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          E\n        \n        +\n        q\n        \n          v\n        \n        ×\n        \n          B\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =q\\mathbf {E} +q\\mathbf {v} \\times \\mathbf {B} .}\n\nIf the electric field vanishes (\n  \n    \n      \n        \n          E\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbf {E} =0}\n  \n), then the force will be perpendicular to the charge's motion, just as in the case of uniform circular motion studied above, and the charge will circle (or more generally move in a helix) around the magnetic field lines at the cyclotron frequency \n  \n    \n      \n        ω\n        =\n        q\n        B\n        \n          /\n        \n        m\n      \n    \n    {\\displaystyle \\omega =qB/m}\n  \n.[78]: 222  Mass spectrometry works by applying electric and/or magnetic fields to moving charges and measuring the resulting acceleration, which by the Lorentz force law yields the mass-to-charge ratio.[82]\n\nCollections of charged bodies do not always obey Newton's third law: there can be a change of one body's momentum without a compensatory change in the momentum of another. The discrepancy is accounted for by momentum carried by the electromagnetic field itself. The momentum per unit volume of the electromagnetic field is proportional to the Poynting vector.[83]: 184 [84]\n\nThere is subtle conceptual conflict between electromagnetism and Newton's first law: Maxwell's theory of electromagnetism predicts that electromagnetic waves will travel through empty space at a constant, definite speed. Thus, some inertial observers seemingly have a privileged status over the others, namely those who measure the speed of light and find it to be the value predicted by the Maxwell equations. In other words, light provides an absolute standard for speed, yet the principle of inertia holds that there should be no such standard. This tension is resolved in the theory of special relativity, which revises the notions of space and time in such a way that all inertial observers will agree upon the speed of light in vacuum.[note 12]\n\nIn special relativity, the rule that Wilczek called \"Newton's Zeroth Law\" breaks down: the mass of a composite object is not merely the sum of the masses of the individual pieces.[87]: 33  Newton's first law, inertial motion, remains true. A form of Newton's second law, that force is the rate of change of momentum, also holds, as does the conservation of momentum. However, the definition of momentum is modified. Among the consequences of this is the fact that the more quickly a body moves, the harder it is to accelerate, and so, no matter how much force is applied, a body cannot be accelerated to the speed of light. Depending on the problem at hand, momentum in special relativity can be represented as a three-dimensional vector, \n  \n    \n      \n        \n          p\n        \n        =\n        m\n        γ\n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {p} =m\\gamma \\mathbf {v} }\n  \n, where \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the body's rest mass and \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is the Lorentz factor, which depends upon the body's speed. Alternatively, momentum and force can be represented as four-vectors.[88]: 107\n\nNewton's third law must be modified in special relativity. The third law refers to the forces between two bodies at the same moment in time, and a key feature of special relativity is that simultaneity is relative. Events that happen at the same time relative to one observer can happen at different times relative to another. So, in a given observer's frame of reference, action and reaction may not be exactly opposite, and the total momentum of interacting bodies may not be conserved. The conservation of momentum is restored by including the momentum stored in the field that describes the bodies' interaction.[89][90]\n\nNewtonian mechanics is a good approximation to special relativity when the speeds involved are small compared to that of light.[91]: 131\n\nGeneral relativity is a theory of gravity that advances beyond that of Newton. In general relativity, the gravitational force of Newtonian mechanics is reimagined as curvature of spacetime. A curved path like an orbit, attributed to a gravitational force in Newtonian mechanics, is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: \"Spacetime tells matter how to move; matter tells spacetime how to curve.\"[92][93] Wheeler himself thought of this reciprocal relationship as a modern, generalized form of Newton's third law.[92] The relation between matter distribution and spacetime curvature is given by the Einstein field equations, which require tensor calculus to express.[87]: 43 [94]\n\nThe Newtonian theory of gravity is a good approximation to the predictions of general relativity when gravitational effects are weak and objects are moving slowly compared to the speed of light.[85]: 327 [95]\n\nQuantum mechanics is a theory of physics originally developed in order to understand microscopic phenomena: behavior at the scale of molecules, atoms or subatomic particles. Generally and loosely speaking, the smaller a system is, the more an adequate mathematical model will require understanding quantum effects. The conceptual underpinning of quantum physics is very different from that of classical physics. Instead of thinking about quantities like position, momentum, and energy as properties that an object has, one considers what result might appear when a measurement of a chosen type is performed. Quantum mechanics allows the physicist to calculate the probability that a chosen measurement will elicit a particular result.[96][97] The expectation value for a measurement is the average of the possible results it might yield, weighted by their probabilities of occurrence.[98]\n\nThe Ehrenfest theorem provides a connection between quantum expectation values and Newton's second law, a connection that is necessarily inexact, as quantum physics is fundamentally different from classical. In quantum physics, position and momentum are represented by mathematical entities known as Hermitian operators, and the Born rule is used to calculate the expectation values of a position measurement or a momentum measurement. These expectation values will generally change over time; that is, depending on the time at which (for example) a position measurement is performed, the probabilities for its different possible outcomes will vary. The Ehrenfest theorem says, roughly speaking, that the equations describing how these expectation values change over time have a form reminiscent of Newton's second law. However, the more pronounced quantum effects are in a given situation, the more difficult it is to derive meaningful conclusions from this resemblance.[note 13]\n\nThe concepts invoked in Newton's laws of motion — mass, velocity, momentum, force — have predecessors in earlier work, and the content of Newtonian physics was further developed after Newton's time. Newton combined knowledge of celestial motions with the study of events on Earth and showed that one theory of mechanics could encompass both.[note 14]\n\nAs noted by scholar I. Bernard Cohen, Newton's work was more than a mere synthesis of previous results, as he selected certain ideas and further transformed them, with each in a new form that was useful to him, while at the same time proving false of certain basic or fundamental principles of scientists such as Galileo Galilei, Johannes Kepler, René Descartes, and Nicolaus Copernicus.[103] He approached natural philosophy with mathematics in a completely novel way, in that instead of a preconceived natural philosophy, his style was to begin with a mathematical construct, and build on from there, comparing it to the real world to show that his system accurately accounted for it.[104]\n\nThe subject of physics is often traced back to Aristotle, but the history of the concepts involved is obscured by multiple factors. An exact correspondence between Aristotelian and modern concepts is not simple to establish: Aristotle did not clearly distinguish what we would call speed and force, used the same term for density and viscosity, and conceived of motion as always through a medium, rather than through space. In addition, some concepts often termed \"Aristotelian\" might better be attributed to his followers and commentators upon him.[105] These commentators found that Aristotelian physics had difficulty explaining projectile motion.[note 15] Aristotle divided motion into two types: \"natural\" and \"violent\". The \"natural\" motion of terrestrial solid matter was to fall downwards, whereas a \"violent\" motion could push a body sideways. Moreover, in Aristotelian physics, a \"violent\" motion requires an immediate cause; separated from the cause of its \"violent\" motion, a body would revert to its \"natural\" behavior. Yet, a javelin continues moving after it leaves the thrower's hand. Aristotle concluded that the air around the javelin must be imparted with the ability to move the javelin forward.\n\nJohn Philoponus, a Byzantine Greek thinker active during the sixth century, found this absurd: the same medium, air, was somehow responsible both for sustaining motion and for impeding it. If Aristotle's idea were true, Philoponus said, armies would launch weapons by blowing upon them with bellows. Philoponus argued that setting a body into motion imparted a quality, impetus, that would be contained within the body itself. As long as its impetus was sustained, the body would continue to move.[107]: 47  In the following centuries, versions of impetus theory were advanced by individuals including Nur ad-Din al-Bitruji, Avicenna, Abu'l-Barakāt al-Baghdādī, John Buridan, and Albert of Saxony. In retrospect, the idea of impetus can be seen as a forerunner of the modern concept of momentum.[note 16] The intuition that objects move according to some kind of impetus persists in many students of introductory physics.[109]\n\nThe French philosopher René Descartes introduced the concept of inertia by way of his \"laws of nature\" in The World (Traité du monde et de la lumière) written 1629–33. However, The World purported a heliocentric worldview, and in 1633 this view had given rise a great conflict between Galileo Galilei and the Roman Catholic Inquisition. Descartes knew about this controversy and did not wish to get involved. The World was not published until 1664, ten years after his death.[110]\n\nThe modern concept of inertia is credited to Galileo. Based on his experiments, Galileo concluded that the \"natural\" behavior of a moving body was to keep moving, until something else interfered with it. In Two New Sciences (1638) Galileo wrote:[111][112].mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nImagine any particle projected along a horizontal plane without friction; then we know, from what has been more fully explained in the preceding pages, that this particle will move along this same plane with a motion which is uniform and perpetual, provided the plane has no limits.\n\nGalileo recognized that in projectile motion, the Earth's gravity affects vertical but not horizontal motion.[113] However, Galileo's idea of inertia was not exactly the one that would be codified into Newton's first law. Galileo thought that a body moving a long distance inertially would follow the curve of the Earth. This idea was corrected by Isaac Beeckman, Descartes, and Pierre Gassendi, who recognized that inertial motion should be motion in a straight line.[114] Descartes published his laws of nature (laws of motion) with this correction in Principles of Philosophy (Principia Philosophiae) in 1644, with the heliocentric part toned down.[115][110]\n\nFirst Law of Nature: Each thing when left to itself continues in the same state; so any moving body goes on moving until something stops it.\n\nSecond Law of Nature: Each moving thing if left to itself moves in a straight line; so any body moving in a circle always tends to move away from the centre of the circle.\n\nAccording to American philosopher Richard J. Blackwell, Dutch scientist Christiaan Huygens had worked out his own, concise version of the law in 1656.[116] It was not published until 1703, eight years after his death, in the opening paragraph of De Motu Corporum ex Percussione.\n\nHypothesis I: Any body already in motion will continue to move perpetually with the same speed and in a straight line unless it is impeded.\n\nAccording to Huygens, this law was already known by Galileo and Descartes among others.[116]\n\nChristiaan Huygens, in his Horologium Oscillatorium (1673), put forth the hypothesis that \"By the action of gravity, whatever its sources, it happens that bodies are moved by a motion composed both of a uniform motion in one direction or another and of a motion downward due to gravity.\" Newton's second law generalized this hypothesis from gravity to all forces.[117]\n\nOne important characteristic of Newtonian physics is that forces can act at a distance without requiring physical contact.[note 17] For example, the Sun and the Earth pull on each other gravitationally, despite being separated by millions of kilometres. This contrasts with the idea, championed by Descartes among others, that the Sun's gravity held planets in orbit by swirling them in a vortex of transparent matter, aether.[124] Newton considered aetherial explanations of force but ultimately rejected them.[122] The study of magnetism by William Gilbert and others created a precedent for thinking of immaterial forces,[122] and unable to find a quantitatively satisfactory explanation of his law of gravity in terms of an aetherial model, Newton eventually declared, \"I feign no hypotheses\": whether or not a model like Descartes's vortices could be found to underlie the Principia's theories of motion and gravity, the first grounds for judging them must be the successful predictions they made.[125] And indeed, since Newton's time every attempt at such a model has failed.\n\nJohannes Kepler suggested that gravitational attractions were reciprocal — that, for example, the Moon pulls on the Earth while the Earth pulls on the Moon — but he did not argue that such pairs are equal and opposite.[126] In his Principles of Philosophy (1644), Descartes introduced the idea that during a collision between bodies, a \"quantity of motion\" remains unchanged. Descartes defined this quantity somewhat imprecisely by adding up the products of the speed and \"size\" of each body, where \"size\" for him incorporated both volume and surface area.[127] Moreover, Descartes thought of the universe as a plenum, that is, filled with matter, so all motion required a body to displace a medium as it moved.\n\nDuring the 1650s, Huygens studied collisions between hard spheres and deduced a principle that is now identified as the conservation of momentum.[128][129] Christopher Wren would later deduce the same rules for elastic collisions that Huygens had, and John Wallis would apply momentum conservation to study inelastic collisions. Newton cited the work of Huygens, Wren, and Wallis to support the validity of his third law.[130]\n\nNewton arrived at his set of three laws incrementally. In a 1684 manuscript written to Huygens, he listed four laws: the principle of inertia, the change of motion by force, a statement about relative motion that would today be called Galilean invariance, and the rule that interactions between bodies do not change the motion of their center of mass. In a later manuscript, Newton added a law of action and reaction, while saying that this law and the law regarding the center of mass implied one another. Newton probably settled on the presentation in the Principia, with three primary laws and then other statements reduced to corollaries, during 1685.[131]\n\nNewton expressed his second law by saying that the force on a body is proportional to its change of motion, or momentum. By the time he wrote the Principia, he had already developed calculus (which he called \"the science of fluxions\"), but in the Principia he made no explicit use of it, perhaps because he believed geometrical arguments in the tradition of Euclid to be more rigorous.[133]: 15 [134] Consequently, the Principia does not express acceleration as the second derivative of position, and so it does not give the second law as \n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  \n. This form of the second law was written (for the special case of constant force) at least as early as 1716, by Jakob Hermann; Leonhard Euler would employ it as a basic premise in the 1740s.[135] Euler pioneered the study of rigid bodies[136] and established the basic theory of fluid dynamics.[137] Pierre-Simon Laplace's five-volume Traité de mécanique céleste (1798–1825) forsook geometry and developed mechanics purely through algebraic expressions, while resolving questions that the Principia had left open, like a full theory of the tides.[138]\n\nThe concept of energy became a key part of Newtonian mechanics in the post-Newton period. Huygens' solution of the collision of hard spheres showed that in that case, not only is momentum conserved, but kinetic energy is as well (or, rather, a quantity that in retrospect we can identify as one-half the total kinetic energy). The question of what is conserved during all other processes, like inelastic collisions and motion slowed by friction, was not resolved until the 19th century. Debates on this topic overlapped with philosophical disputes between the metaphysical views of Newton and Leibniz, and variants of the term \"force\" were sometimes used to denote what we would call types of energy. For example, in 1742, Émilie du Châtelet wrote, \"Dead force consists of a simple tendency to motion: such is that of a spring ready to relax; living force is that which a body has when it is in actual motion.\" In modern terminology, \"dead force\" and \"living force\" correspond to potential energy and kinetic energy respectively.[139] Conservation of energy was not established as a universal principle until it was understood that the energy of mechanical work can be dissipated into heat.[140][141] With the concept of energy given a solid grounding, Newton's laws could then be derived within formulations of classical mechanics that put energy first, as in the Lagrangian and Hamiltonian formulations described above.\n\nModern presentations of Newton's laws use the mathematics of vectors, a topic that was not developed until the late 19th and early 20th centuries. Vector algebra, pioneered by Josiah Willard Gibbs and Oliver Heaviside, stemmed from and largely supplanted the earlier system of quaternions invented by William Rowan Hamilton.[142][143]",
        pageTitle: "Newton's laws of motion",
    },
    {
        title: "scientific laws",
        link: "https://en.wikipedia.org/wiki/List_of_laws_in_Science",
        content:
            "Scientific laws or laws of science are statements, based on repeated experiments or observations, that describe or predict a range of natural phenomena.[1] The term law has diverse usage in many cases (approximate, accurate, broad, or narrow) across all fields of natural science (physics, chemistry, astronomy, geoscience, biology). Laws are developed from data and can be further developed through mathematics; in all cases they are directly or indirectly based on empirical evidence. It is generally understood that they implicitly reflect, though they do not explicitly assert, causal relationships fundamental to reality, and are discovered rather than invented.[2]\n\nScientific laws summarize the results of experiments or observations, usually within a certain range of application. In general, the accuracy of a law does not change when a new theory of the relevant phenomenon is worked out, but rather the scope of the law's application, since the mathematics or statement representing the law does not change. As with other kinds of scientific knowledge, scientific laws do not express absolute certainty, as mathematical laws do. A scientific law may be contradicted, restricted, or extended by future observations.\n\nA law can often be formulated as one or several statements or equations, so that it can predict the outcome of an experiment.  Laws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws, since they have not been verified to the same degree, although they may lead to the formulation of laws.  Laws are narrower in scope than scientific theories, which may entail one or several laws.[3] Science distinguishes a law or theory from facts.[4] Calling a law a fact is ambiguous, an overstatement, or an equivocation.[5] The nature of scientific laws has been much discussed in philosophy, but in essence scientific laws are simply empirical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nSocial sciences such as economics have also attempted to formulate scientific laws, though these generally have much less predictive power.\n\nA scientific law always applies to a physical system under repeated conditions, and it implies that there is a causal relationship involving the elements of the system. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.[6]\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, the applicability of a law is limited to circumstances resembling those already observed, and the law may be found to be false when extrapolated. Ohm's law only applies to linear networks; Newton's law of universal gravitation only applies in weak gravitational fields; the early laws of aerodynamics, such as Bernoulli's principle, do not apply in the case of compressible flow such as occurs in transonic and supersonic flight; Hooke's law only applies to strain below the elastic limit; Boyle's law applies with perfect accuracy only to the ideal gas, etc. These laws remain useful, but only under the specified conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as \n  \n    \n      \n        Δ\n        E\n        =\n        0\n      \n    \n    {\\displaystyle \\Delta E=0}\n  \n, where \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as \n  \n    \n      \n        \n          d\n        \n        U\n        =\n        δ\n        Q\n        −\n        δ\n        W\n        \n      \n    \n    {\\displaystyle \\mathrm {d} U=\\delta Q-\\delta W\\,}\n  \n, and Newton's second law can be written as \n  \n    \n      \n        \n          F\n          =\n          \n            \n              \n                d\n                p\n              \n              \n                d\n                t\n              \n            \n          \n          .\n        \n      \n    \n    {\\displaystyle \\textstyle F={\\frac {dp}{dt}}.}\n  \n While these scientific laws explain what our senses perceive, they are still empirical (acquired by observation or scientific experiment) and so are not like mathematical theorems which can be proved purely by mathematics.\n\nLike theories and hypotheses, laws make predictions; specifically, they predict that new observations will conform to the given law. Laws can be falsified if they are found in contradiction with new data.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to quantum electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nLaws are constantly being tested experimentally to increasing degrees of precision, which is one of the main goals of science. The fact that laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed. Well-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations, to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. This, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nScientific laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. A scientific law is \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present\".[7] The production of a summary description of our environment in the form of such laws is a fundamental aim of science.\n\nSeveral general properties of scientific laws, particularly when referring to laws in physics, have been identified. Scientific laws are:\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws.[11] For example, Zipf's law is a law in the social sciences which is based on mathematical statistics. In these cases, laws may describe general trends or expected behaviors rather than being absolutes.\n\nIn natural science, impossibility assertions come to be widely accepted as overwhelmingly probable rather than considered proved to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that something is impossible. While an impossibility assertion in natural science can never be absolutely proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined.\n\nSome examples of widely accepted impossibilities in physics are perpetual motion machines, which violate the law of conservation of energy, exceeding the speed of light, which violates the implications of special relativity, the uncertainty principle of quantum mechanics, which asserts the impossibility of simultaneously knowing both the position and the momentum of a particle, and Bell's theorem: no physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics.\n\nSome laws reflect mathematical symmetries found in nature (e.g. the Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, and Lorentz transformations reflect rotational symmetry of spacetime). Many fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different from any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. Special relativity uses rapidity to express motion according to the symmetries of hyperbolic rotation, a transformation mixing space and time. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nConservation laws are fundamental laws that follow from the homogeneity of space, time and phase, in other words symmetry.\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇⋅) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point; hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see the main article for details). In the table below, the fluxes flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nClassical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from the following principle:\n\nwhere \n  \n    \n      \n        \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {S}}}\n  \n is the action; the integral of the Lagrangian\n\nof the physical system between two times t1 and t2. The kinetic energy of the system is T (a function of the rate of change of the configuration of the system), and potential energy is V (a function of the configuration and its rate of change). The configuration of a system which has N degrees of freedom is defined by generalized coordinates q = (q1, q2, ... qN).\n\nThere are generalized momenta conjugate to these coordinates, p = (p1, p2, ..., pN), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(t), parameterized by time (see also parametric equation for this concept).\n\nThe action is a functional rather than a function, since it depends on the Lagrangian, and the Lagrangian depends on the path q(t), so the action depends on the entire \"shape\" of the path for all times (in the time interval from t1 to t2). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the entire continuum of Lagrangian values corresponding to some path, not just one value of the Lagrangian, is required (in other words it is not as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).[12]\n\nNotice L is not the total energy E of the system due to the difference, rather than the sum:\n\nThe following[13][14] general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations. Newton's is commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nS\n          \n        \n        =\n        \n          ∫\n          \n            \n              t\n              \n                1\n              \n            \n          \n          \n            \n              t\n              \n                2\n              \n            \n          \n        \n        L\n        \n        \n          d\n        \n        t\n        \n        \n      \n    \n    {\\displaystyle {\\mathcal {S}}=\\int _{t_{1}}^{t_{2}}L\\,\\mathrm {d} t\\,\\!}\n\nUsing the definition of generalized momentum, there is the symmetry:\n\nThe Hamiltonian as a function of generalized coordinates and momenta has the general form:\n\nThey are low-limit solutions to relativity. Alternative formulations of Newtonian mechanics are Lagrangian and Hamiltonian mechanics.\n\nThe laws can be summarized by two equations (since the 1st is a special case of the 2nd, zero resultant acceleration):\n\nwhere p = momentum of body, Fij = force on body i by body j, Fji = force on body j by body i.\n\nFor a dynamical system the two equations (effectively) combine into one:\n\nin which FE = resultant external force (due to any agent not part of system). Body i does not exert a force on itself.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his Philosophiae Naturalis Principia Mathematica, and in Albert Einstein's theory of relativity.\n\nThe two postulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of relative motion.\n\nThey can be stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant and has the same value in all inertial frames\".\n\nThe said postulates lead to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light c.\n\nThe magnitudes of 4-vectors are invariants – not \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if A is the four-momentum, the magnitude can derive the famous invariant equation for mass–energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass–energy equivalence E = mc2 is a special case.\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass–energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous gravitomagnetic field. They are well established by the theory, and experimental tests form ongoing research.[15]\n\nwhere Λ = cosmological constant, Rμν = Ricci curvature tensor, Tμν = stress–energy tensor, gμν = metric tensor\n\nwhere Γ is a Christoffel symbol of the second kind, containing the metric.\n\nIf g the gravitational field and H the gravitomagnetic field, the solutions in these limits are:\n\nwhere ρ is the mass density and J is the mass current density or mass flux.\n\nwhere m is the rest mass of the particlce and γ is the Lorentz factor.\n\nKepler's laws, though originally discovered from planetary observations (also due to Tycho Brahe), are true for any central forces.[16]\n\nFor a non uniform mass distribution of local mass density ρ (r) of body of Volume V, this becomes:\n\nis the eccentricity of the elliptic orbit, of  semi-major axis a and semi-minor axis b, and ℓ is the semi-latus rectum. This equation in itself is nothing physically fundamental; simply the polar equation of an ellipse in which the pole (origin of polar coordinate system) is positioned at a focus of the ellipse, where the orbited star is.\n\nwhere L is the orbital angular momentum of the particle (i.e. planet) of mass m about the focus of orbit,\n\nwhere M is the mass of the central body (i.e. star).\n\nSecond law of thermodynamics: There are many statements of this law, perhaps the simplest is \"the entropy of isolated systems never decreases\",\n\nmeaning reversible changes have zero entropy change, irreversible process are positive, and impossible process are negative.\n\nMaxwell's equations give the time-evolution of the electric and magnetic fields due to electric charge and current distributions. Given the fields, the Lorentz force law is the equation of motion for charges in the fields.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's equations. Coulomb's law can be found from Gauss's law (electrostatic form) and the Biot–Savart law can be deduced from Ampere's law (magnetostatic form). Lenz's law and Faraday's law can be incorporated into the Maxwell–Faraday equation. Nonetheless, they are still very effective for simple calculations.\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\nIn physical optics, laws are based on physical properties of materials.\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them. These postulates can be summarized as follows:\n\nThese postulates in turn imply many other phenomena, e.g., uncertainty principles and the Pauli exclusion principle.\n\nSchrödinger equation (general form): Describes the time dependence of a quantum mechanical system.\n\nThe Hamiltonian (in quantum mechanics) H is a self-adjoint operator acting on the state space, \n  \n    \n      \n        \n          |\n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n  \n (see Dirac notation) is the instantaneous quantum state vector at time t, position r, i is the unit imaginary number, ħ = h/2π is the reduced Planck constant.\n\nPlanck–Einstein law: the energy of photons is proportional to the frequency of the light (the constant is the Planck constant, h).\n\nDe Broglie wavelength: this laid the foundations of wave–particle duality, and was the key concept in the Schrödinger equation,\n\nHeisenberg uncertainty principle: Uncertainty in position multiplied by uncertainty in momentum is at least half of the reduced Planck constant, similarly for time and energy;\n\nThe uncertainty principle can be generalized to any pair of observables – see main article.\n\nwhere ri is the position of particle i, and s is the spin of the particle. There is no way to keep track of particles physically, labels are only used mathematically to prevent confusion.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass.  Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers; although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nThe law of definite composition and the law of multiple proportions are the first two of the three laws of stoichiometry, the proportions by which the chemical elements combine to form chemical compounds. The third law of stoichiometry is the law of reciprocal proportions, which provides the basis for establishing equivalent weights for each chemical element. Elemental equivalent weights can then be used to derive atomic weights for each element.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\nWhether or not Natural Selection is a “law of nature” is controversial among biologists.[17][18]  Henry Byerly, an American philosopher known for his work on evolutionary theory, discussed the problem of interpreting a principle of natural selection as a law.  He suggested a formulation of natural selection as a framework principle that can contribute to a better understanding of evolutionary theory.[18]  His approach was to express relative fitness, the propensity of a genotype to increase in proportionate representation in a competitive environment, as a function of adaptedness (adaptive design) of the organism.\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, and Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\nThe observation and detection of underlying regularities in nature date from prehistoric times – the recognition of cause-and-effect relationships implicitly recognises the existence of  laws of nature. The recognition of such regularities as independent scientific laws per se, though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as physical phenomena—to the actions of gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nIn Europe, systematic theorizing about nature (physis) began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.\n\nThe formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative,[19] the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\nFor the Romans ... the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's Natural Questions, and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.[20]\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and the development of advanced forms of mathematics.  During this period, natural philosophers such as Isaac Newton (1642–1727) were influenced by a religious view – stemming from medieval concepts of divine law – which held that God had instituted  absolute, universal and immutable physical laws.[21][22]  In chapter 7 of The World, René Descartes (1596–1650) described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\"[23]   The modern scientific method which took shape at this time (with Francis Bacon (1561–1626) and Galileo (1564–1642)) contributed to a trend of separating science from theology, with minimal speculation about metaphysics and ethics. (Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period by scholars such as Grotius (1583–1645), Spinoza (1632–1677), and Hobbes (1588–1679).)\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from physis, the Greek word (translated into Latin as natura) for nature.[24]",
        pageTitle: "Scientific law",
    },
    {
        title: "Nielsen's law",
        link: "https://en.wikipedia.org/wiki/Nielsen%27s_law",
        content:
            "Jakob Nielsen (born 5 October 1957) is a Danish web usability consultant, human–computer interaction researcher, and co-founder of Nielsen Norman Group.[2][3] He was named the “guru of Web page usability” in 1998 by The New York Times and the “king of usability” by Internet Magazine.[4][5][1]\n\nJakob Nielsen was born 5 October 1957 in Copenhagen, Denmark.[6][7] He holds a PhD in 1988 in human–computer interaction from the Technical University of Denmark from DAIMI.[8][6]\n\nNielsen's affiliations include Bellcore, teaching at the Technical University of Denmark, and the IBM User Interface Institute at the Thomas J. Watson Research Center.[9][10][when?] From 1994 to 1998, he was a distinguished engineer Sun Microsystems.[4]\n\nAfter his regular articles on his website about usability research attracted media attention, he co-founded usability consulting company Nielsen Norman Group (NN/g) of Fremont, California in 1998 with fellow usability expert Donald Norman.[6][11][12] The company's vision is to help designers and other companies move toward more human-centered products and internet interactions, as experts and pioneers in the field of usability.[11]\n\nNielsen serves on the editorial board of Morgan Kaufmann Publishers' book series in Interactive Technologies.[citation needed] Nielsen writes a fortnightly newsletter, Alertbox, on web design matters and has published several books on the subject of web design.[6][10]\n\nNielsen founded the usability engineering movement for efficient and affordable improvements of user interfaces and he has invented several usability methods, including heuristic evaluation. He holds more than a thousand United States patents,[13][14] mainly on ways of improving usability for technology.\n\nIn the early 1990s, Nielsen popularized the principle that five test users per usability test session is enough, allowing numerous tests at various stages of the development process.[15] His argument is that \"elaborate usability tests are a waste of resources.\" Once it is found that a few people are totally confused by a home page, little is gained by watching more people suffer through the same flawed design.[15]\n\nUsers will anticipate what an experience will be like, based on their mental models of prior experiences on websites.[16][17] When making changes to a design of a website, try to minimize changes in order to maintain an ease of use.[17]\n\nNielsen's list of ten heuristics is probably the most-used usability framework for user interface design. An early version of the heuristics appeared in two papers by Nielsen and Rolf Molich  published in 1989-1990.[18][19] Nielsen published an updated set in 1994,[20] and the final set still in use today was published in 2005:[21]\n\nIn his book Usability Engineering (1993), Nielsen also defined the five quality components of his \"Usability Goals\":[22]\n\nNielsen has been quoted in the computing and the mainstream press for his criticism of Microsoft's Windows 8 (2012) user interface.[23][24][25] Tom Hobbs, creative director of the design firm Teague, criticized what he perceived to be some of Nielsen's points on the matter, and Nielsen responded with some clarifications.[26] The subsequent short and troubled history of Windows 8, released on 26 October 2012, seems to have confirmed Nielsen's criticism: the sales of Windows-based systems plummeted after the introduction of Windows 8;[27] Microsoft released a new version, Windows 8.1, on 18 October 2013, to fix the numerous problems identified in Windows 8, and later released Windows 10, a complete overhaul, in July 2015.\n\nAs Nielsen's newsletter and website grew, and with his use of \"acronomic platitudes\"[28] to describe his concepts, some critics like Philip Greenspun argued that Nielsen's work was more about marketing himself than any particular research.[10]\n\nIn 1990, when the Nielsen heuristic evaluation guidelines were created,[19] user interface was less complicated than it is in present-day.[29][30] There has never been any research-based validation of Nielsen's heuristics.[30] Researchers at the University of Calgary published an article in 2008, questioning if the Nielsen heuristics were an oversimplification.[31]\n\nNielsen has been criticized by some visual designers and graphic designers for failing to balance the importance of other user experience considerations such as typography, readability, visual cues for hierarchy and importance, and eye appeal.[32][33]\n\nNielsen's 2012 guidelines, \"Repurposing vs Optimized Design\"  that web sites made for mobile devices be designed separately from their desktop-oriented counterparts has come under fire from Webmonkey's Scott Gilbertson,[34] as well as Josh Clark writing in .net magazine,[35] and Opera's Bruce Lawson, writing in Smashing Magazine,[36] and other technologists and web designers who advocate responsive web design.[37][38] In an interview with .net magazine, Nielsen explained that he wrote his guidelines from a usability perspective, not from the viewpoint of implementation.[39]\n\nNielsen has been accused of taking a \"puritanical\" approach to usability, and not being able to keep up his usability evaluations in step of technological changes.[10]\n\nIn 2010, Nielsen was listed by Bloomberg Businessweek among 28 \"World's Most Influential Designers\".[40]\n\nIn recognition of Nielsen's contributions to usability studies, in 2013 SIGCHI awarded him the Lifetime Practice Award.[41]",
        pageTitle: "Jakob Nielsen (usability consultant)",
    },
    {
        title: "Niven's laws",
        link: "https://en.wikipedia.org/wiki/Niven%27s_laws",
        content:
            'Niven\'s laws were named after science fiction author Larry Niven, who has periodically published them as "how the Universe works" as far as he can tell. These were most recently rewritten on January 29, 2002 (and published in Analog magazine in the November 2002 issue). Among the rules are:\n\nA different law is given this name in Niven\'s essay "The Theory and Practice of Time Travel":[1]\n\nHans Moravec glosses this version of Niven\'s Law as follows:[2]\n\nRyan North examines this law in Dinosaur Comics #1818.[3]\n\nThis proposition is also extensively examined in James P. Hogan\'s Thrice Upon a Time.\n\nNiven\'s Law is also a term given to the converse of Clarke\'s third law, so Niven\'s Law reads: "Any sufficiently advanced magic is indistinguishable from technology."  However, it has also been credited[by whom?] as being from Terry Pratchett.[citation needed]  Keystone Folklore identifies it as a "fan-composed corollary slogan" of Arthur C. Clarke fans.[4]  Gregory Benford in his January 30, 2013 "Variations on Clarke\'s Third Law" identifies it as a corollary to Clarke’s third law,[5]\n\nBoth Clarke\'s Third Law and Niven\'s Law are referenced in part 2 of the serial Battlefield from season 26 of Doctor Who, first aired September 13, 1989.  In this episode, the Doctor and his companion Ace have entered a trans-dimensional spaceship.  While discussing the ship itself, the Doctor asks his companion if she knows Clarke\'s Law, which she then recites: "Any advanced form of technology is indistinguishable from magic."  The Doctor replies that the reverse is true and Ace voices this, working through the inverse, "any advanced form of magic is indistinguishable from technology."\n\nNiven\'s Laws is also the title of a 1984 collection of Niven\'s short stories.\n\nIncluded in the 1989 collection N-Space are six laws titled Niven\'s Laws for Writers. They are:\n\nIn the acknowledgments of his 2003 novel Conquistador, S.M. Stirling wrote:\n\nDrawn from Known Space: The Future Worlds of Larry Niven\n\nIn November 2002 the above list was published to Analog Magazine but with slightly different numbering and new commentary.  "The world\'s dullest subjects" entry was removed, and a new final entry "Never let a waiter escape." was added to the end.[7]',
        pageTitle: "Niven's laws",
    },
    {
        title: "Ohm's law",
        link: "https://en.wikipedia.org/wiki/Ohm%27s_law",
        content:
            "Ohm's law states that the electric current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality, the resistance,[1] one arrives at the three mathematical equations used to describe this relationship:[2]\n\nV\n        =\n        I\n        R\n        \n        \n          or\n        \n        \n        I\n        =\n        \n          \n            V\n            R\n          \n        \n        \n        \n          or\n        \n        \n        R\n        =\n        \n          \n            V\n            I\n          \n        \n      \n    \n    {\\displaystyle V=IR\\quad {\\text{or}}\\quad I={\\frac {V}{R}}\\quad {\\text{or}}\\quad R={\\frac {V}{I}}}\n\nwhere I is the current through the conductor, V is the voltage measured across the conductor and R is the resistance of the conductor. More specifically, Ohm's law states that the R in this relation is constant, independent of the current.[3] If the resistance is not constant, the previous equation cannot be called Ohm's law, but it can still be used as a definition of static/DC resistance.[4] Ohm's law is an empirical relation which accurately describes the conductivity of the vast majority of electrically conductive materials over many orders of magnitude of current. However some materials do not obey Ohm's law; these are called non-ohmic.\n\nThe law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. Ohm explained his experimental results by a slightly more complex equation than the modern form above (see § History below).\n\nIn physics, the term Ohm's law is also used to refer to various generalizations of the law; for example the vector form of the law used in electromagnetics and material science:\n\nJ\n        \n        =\n        σ\n        \n          E\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {J} =\\sigma \\mathbf {E} ,}\n\nwhere J is the current density at a given location in a resistive material, E is the electric field at that location, and σ (sigma) is a material-dependent parameter called the conductivity, defined as the inverse of resistivity ρ (rho). This reformulation of Ohm's law is due to Gustav Kirchhoff.[5]\n\nIn January 1781, before Georg Ohm's work, Henry Cavendish experimented with Leyden jars and glass tubes of varying diameter and length filled with salt solution. He measured the current by noting how strong a shock he felt as he completed the circuit with his body. Cavendish wrote that the \"velocity\" (current) varied directly as the \"degree of electrification\" (voltage). He did not communicate his results to other scientists at the time,[6] and his results were unknown until James Clerk Maxwell published them in 1879.[7]\n\nFrancis Ronalds delineated \"intensity\" (voltage) and \"quantity\" (current) for the dry pile—a high voltage source—in 1814 using a gold-leaf electrometer. He found for a dry pile that the relationship between the two parameters was not proportional under certain meteorological conditions.[8][9]\n\nOhm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book Die galvanische Kette, mathematisch bearbeitet (\"The galvanic circuit investigated mathematically\").[10] He drew considerable inspiration from Joseph Fourier's work on heat conduction in the theoretical explanation of his work. For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant voltage. He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature. He then added test wires of varying length, diameter, and material to complete the circuit. He found that his data could be modeled through the equation\n\n  \n    \n      \n        x\n        =\n        \n          \n            a\n            \n              b\n              +\n              ℓ\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle x={\\frac {a}{b+\\ell }},}\n  \n\nwhere x was the reading from the galvanometer, ℓ was the length of the test conductor, a depended on the thermocouple junction temperature, and b was a constant of the entire setup. From this, Ohm determined his law of proportionality and published his results.\n\nIn modern notation we would write,\n\n  \n    \n      \n        I\n        =\n        \n          \n            \n              E\n            \n            \n              r\n              +\n              R\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I={\\frac {\\mathcal {E}}{r+R}},}\n  \n\nwhere \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is the open-circuit emf of the thermocouple, \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n is the internal resistance of the thermocouple and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n is the resistance of the test wire. In terms of the length of the wire this becomes,\n\n  \n    \n      \n        I\n        =\n        \n          \n            \n              E\n            \n            \n              r\n              +\n              \n                \n                  R\n                \n              \n              ℓ\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I={\\frac {\\mathcal {E}}{r+{\\mathcal {R}}\\ell }},}\n  \n\nwhere \n  \n    \n      \n        \n          \n            R\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}}\n  \n is the resistance of the test wire per unit length. Thus, Ohm's coefficients are,\n\n  \n    \n      \n        a\n        =\n        \n          \n            \n              E\n            \n            \n              R\n            \n          \n        \n        ,\n        \n        b\n        =\n        \n          \n            \n              r\n            \n            \n              R\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle a={\\frac {\\mathcal {E}}{\\mathcal {R}}},\\quad b={\\frac {\\mathcal {r}}{\\mathcal {R}}}.}\n\nOhm's law was probably the most important of the early quantitative descriptions of the physics of electricity. We consider it almost obvious today. When Ohm first published his work, this was not the case; critics reacted to his treatment of the subject with hostility. They called his work a \"web of naked fancies\"[11] and the Minister of Education proclaimed that \"a professor who preached such heresies was unworthy to teach science.\"[12] The prevailing scientific philosophy in Germany at the time asserted that experiments need not be performed to develop an understanding of nature because nature is so well ordered, and that scientific truths may be deduced through reasoning alone.[13] Also, Ohm's brother Martin, a mathematician, was battling the German educational system. These factors hindered the acceptance of Ohm's work, and his work did not become widely accepted until the 1840s. However, Ohm received recognition for his contributions to science well before he died.\n\nIn the 1850s, Ohm's law was widely known and considered proved. Alternatives such as \"Barlow's law\", were discredited, in terms of real applications to telegraph system design, as discussed by Samuel F. B. Morse in 1855.[14]\n\nThe electron was discovered in 1897 by J. J. Thomson, and it was quickly realized that it was the particle (charge carrier) that carried electric currents in electric circuits. In 1900, the first (classical) model of electrical conduction, the Drude model, was proposed by Paul Drude, which finally gave a scientific explanation for Ohm's law. In this model, a solid conductor consists of a stationary lattice of atoms (ions), with conduction electrons moving randomly in it. A voltage across a conductor causes an electric field, which accelerates the electrons in the direction of the electric field, causing a drift of electrons which is the electric current. However the electrons collide with atoms which causes them to scatter and randomizes their motion, thus converting kinetic energy to heat (thermal energy). Using statistical distributions, it can be shown that the average drift velocity of the electrons, and thus the current, is proportional to the electric field, and thus the voltage, over a wide range of voltages.\n\nThe development of quantum mechanics in the 1920s modified this picture somewhat, but in modern theories the average drift velocity of electrons can still be shown to be proportional to the electric field, thus deriving Ohm's law. In 1927 Arnold Sommerfeld applied the quantum Fermi-Dirac distribution of electron energies to the Drude model, resulting in the free electron model. A year later, Felix Bloch showed that electrons move in waves (Bloch electrons) through a solid crystal lattice, so scattering off the lattice atoms as postulated in the Drude model is not a major process; the electrons scatter off impurity atoms and defects in the material. The final successor, the modern quantum band theory of solids, showed that the electrons in a solid cannot take on any energy as assumed in the Drude model but are restricted to energy bands, with gaps between them of energies that electrons are forbidden to have. The size of the band gap is a characteristic of a particular substance which has a great deal to do with its electrical resistivity, explaining why some substances are electrical conductors, some semiconductors, and some insulators.\n\nWhile the old term for electrical conductance, the mho (the inverse of the resistance unit ohm), is still used, a new name, the siemens, was adopted in 1971, honoring Ernst Werner von Siemens. The siemens is preferred in formal papers.\n\nIn the 1920s, it was discovered that the current through a practical resistor actually has statistical fluctuations, which depend on temperature, even when voltage and resistance are exactly constant; this fluctuation, now known as Johnson–Nyquist noise, is due to the discrete nature of charge. This thermal effect implies that measurements of current and voltage that are taken over sufficiently short periods of time will yield ratios of V/I that fluctuate from the value of R implied by the time average or ensemble average of the measured current; Ohm's law remains correct for the average current, in the case of ordinary resistive materials.\n\nOhm's work long preceded Maxwell's equations and any understanding of frequency-dependent effects in AC circuits. Modern developments in electromagnetic theory and circuit theory do not contradict Ohm's law when they are evaluated within the appropriate limits.\n\nOhm's law is an empirical law, a generalization from many experiments that have shown that current is approximately proportional to electric field for most materials. It is less fundamental than Maxwell's equations and is not always obeyed. Any given material will break down under a strong-enough electric field, and some materials of interest in electrical engineering are \"non-ohmic\" under weak fields.[15][16]\n\nOhm's law has been observed on a wide range of length scales. In the early 20th century, it was thought that Ohm's law would fail at the atomic scale, but experiments have not borne out this expectation. As of 2012, researchers have demonstrated that Ohm's law works for silicon wires as small as four atoms wide and one atom high.[17]\n\nThe dependence of the current density on the applied electric field is essentially quantum mechanical in nature; (see Classical and quantum conductivity.) A qualitative description leading to Ohm's law can be based upon classical mechanics using the Drude model developed by Paul Drude in 1900.[18][19]\n\nThe Drude model treats electrons (or other charge carriers) like pinballs bouncing among the ions that make up the structure of the material. Electrons will be accelerated in the opposite direction to the electric field by the average electric field at their location. With each collision, though, the electron is deflected in a random direction with a velocity that is much larger than the velocity gained by the electric field. The net result is that electrons take a zigzag path due to the collisions, but generally drift in a direction opposing the electric field.\n\nThe drift velocity then determines the electric current density and its relationship to E and is independent of the collisions. Drude calculated the average drift velocity from p = −eEτ where p is the average momentum, −e is the charge of the electron and τ is the average time between the collisions. Since both the momentum and the current density are proportional to the drift velocity, the current density becomes proportional to the applied electric field; this leads to Ohm's law.\n\nA hydraulic analogy is sometimes used to describe Ohm's law. Water pressure, measured by pascals (or PSI), is the analog of voltage because establishing a water pressure difference between two points along a (horizontal) pipe causes water to flow. The water volume flow rate, as in liters per second, is the analog of current, as in coulombs per second. Finally, flow restrictors—such as apertures placed in pipes between points where the water pressure is measured—are the analog of resistors. We say that the rate of water flow through an aperture restrictor is proportional to the difference in water pressure across the restrictor. Similarly, the rate of flow of electrical charge, that is, the electric current, through an electrical resistor is proportional to the difference in voltage measured across the resistor. More generally, the hydraulic head may be taken as the analog of voltage, and Ohm's law is then analogous to Darcy's law which relates hydraulic head to the volume flow rate via the hydraulic conductivity.\n\nFlow and pressure variables can be calculated in fluid flow network with the use of the hydraulic ohm analogy.[20][21] The method can be applied to both steady and transient flow situations. In the linear laminar flow region, Poiseuille's law describes the hydraulic resistance of a pipe, but in the turbulent flow region the pressure–flow relations become nonlinear.\n\nThe hydraulic analogy to Ohm's law has been used, for example, to approximate blood flow through the circulatory system.[22]\n\nIn circuit analysis, three equivalent expressions of Ohm's law are used interchangeably:\n\nI\n        =\n        \n          \n            V\n            R\n          \n        \n        \n        \n          or\n        \n        \n        V\n        =\n        I\n        R\n        \n        \n          or\n        \n        \n        R\n        =\n        \n          \n            V\n            I\n          \n        \n        .\n      \n    \n    {\\displaystyle I={\\frac {V}{R}}\\quad {\\text{or}}\\quad V=IR\\quad {\\text{or}}\\quad R={\\frac {V}{I}}.}\n\nEach equation is quoted by some sources as the defining relationship of Ohm's law,[2][23][24]\nor all three are quoted,[25] or derived from a proportional form,[26]\nor even just the two that do not correspond to Ohm's original statement may sometimes be given.[27][28]\n\nThe interchangeability of the equation may be represented by a triangle, where V (voltage) is placed on the top section, the I (current) is placed to the left section, and the R (resistance) is placed to the right. The divider between the top and bottom sections indicates division (hence the division bar).\n\nResistors are circuit elements that impede the passage of electric charge in agreement with Ohm's law, and are designed to have a specific resistance value R. In schematic diagrams, a resistor is shown as a long rectangle or zig-zag symbol. An element (resistor or conductor) that behaves according to Ohm's law over some operating range is referred to as an ohmic device (or an ohmic resistor) because Ohm's law and a single value for the resistance suffice to describe the behavior of the device over that range.\n\nOhm's law holds for circuits containing only resistive elements (no capacitances or inductances) for all forms of driving voltage or current, regardless of whether the driving voltage or current is constant (DC) or time-varying such as AC. At any instant of time Ohm's law is valid for such circuits.\n\nResistors which are in series or in parallel may be grouped together into a single \"equivalent resistance\" in order to apply Ohm's law in analyzing the circuit.\n\nWhen reactive elements such as capacitors, inductors, or transmission lines are involved in a circuit to which AC or time-varying voltage or current is applied, the relationship between voltage and current becomes the solution to a differential equation, so Ohm's law (as defined above) does not directly apply since that form contains only resistances having value R, not complex impedances which may contain capacitance (C) or inductance (L).\n\nEquations for time-invariant AC circuits take the same form as Ohm's law. However, the variables are generalized to complex numbers and the current and voltage waveforms are complex exponentials.[29]\n\nIn this approach, a voltage or current waveform takes the form Aest, where t is time, s is a complex parameter, and A is a complex scalar. In any linear time-invariant system, all of the currents and voltages can be expressed with the same s parameter as the input to the system, allowing the time-varying complex exponential term to be canceled out and the system described algebraically in terms of the complex scalars in the current and voltage waveforms.\n\nThe complex generalization of resistance is impedance, usually denoted Z; it can be shown that for an inductor,\n\n  \n    \n      \n        Z\n        =\n        s\n        L\n      \n    \n    {\\displaystyle Z=sL}\n  \n\nand for a capacitor,\n\n  \n    \n      \n        Z\n        =\n        \n          \n            1\n            \n              s\n              C\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle Z={\\frac {1}{sC}}.}\n\nWe can now write,\n\n  \n    \n      \n        V\n        =\n        Z\n        \n        I\n      \n    \n    {\\displaystyle V=Z\\,I}\n  \n\nwhere V and I are the complex scalars in the voltage and current respectively and Z is the complex impedance.\n\nThis form of Ohm's law, with Z taking the place of R, generalizes the simpler form. When Z is complex, only the real part is responsible for dissipating heat.\n\nIn a general AC circuit, Z varies strongly with the frequency parameter s, and so also will the relationship between voltage and current.\n\nFor the common case of a steady sinusoid, the s parameter is taken to be \n  \n    \n      \n        j\n        ω\n      \n    \n    {\\displaystyle j\\omega }\n  \n, corresponding to a complex sinusoid \n  \n    \n      \n        A\n        \n          e\n          \n            \n              \n                 \n              \n            \n            j\n            ω\n            t\n          \n        \n      \n    \n    {\\displaystyle Ae^{{\\mbox{ }}j\\omega t}}\n  \n. The real parts of such complex current and voltage waveforms describe the actual sinusoidal currents and voltages in a circuit, which can be in different phases due to the different complex scalars.\n\nOhm's law is one of the basic equations used in the analysis of electrical circuits. It applies to both metal conductors and circuit components (resistors) specifically made for this behaviour. Both are ubiquitous in electrical engineering. Materials and components that obey Ohm's law are described as \"ohmic\"[30] which means they produce the same value for resistance (R = V/I) regardless of the value of V or I which is applied and whether the applied voltage or current is DC (direct current) of either positive or negative polarity or AC (alternating current).\n\nIn a true ohmic device, the same value of resistance will be calculated from R = V/I regardless of the value of the applied voltage V. That is, the ratio of V/I is constant, and when current is plotted as a function of voltage the curve is linear (a straight line). If voltage is forced to some value V, then that voltage V divided by measured current I will equal R. Or if the current is forced to some value I, then the measured voltage V divided by that current I is also R. Since the plot of I versus V is a straight line, then it is also true that for any set of two different voltages V1 and V2 applied across a given device of resistance R, producing currents I1 = V1/R and I2 = V2/R, that the ratio (V1 − V2)/(I1 − I2) is also a constant equal to R. The operator \"delta\" (Δ) is used to represent a difference in a quantity, so we can write ΔV = V1 − V2 and ΔI = I1 − I2. Summarizing, for any truly ohmic device having resistance R, V/I = ΔV/ΔI = R for any applied voltage or current or for the difference between any set of applied voltages or currents.\n\nThere are, however, components of electrical circuits which do not obey Ohm's law; that is, their relationship between current and voltage (their I–V curve) is nonlinear (or non-ohmic). An example is the p–n junction diode (curve at right). As seen in the figure, the current does not increase linearly with applied voltage for a diode. One can determine a value of current (I) for a given value of applied voltage (V) from the curve, but not from Ohm's law, since the value of \"resistance\" is not constant as a function of applied voltage. Further, the current only increases significantly if the applied voltage is positive, not negative. The ratio V/I for some point along the nonlinear curve is sometimes called the static, or chordal, or DC, resistance,[31][32] but as seen in the figure the value of total V over total I varies depending on the particular point along the nonlinear curve which is chosen. This means the \"DC resistance\" V/I at some point on the curve is not the same as what would be determined by applying an AC signal having peak amplitude ΔV volts or ΔI amps centered at that same point along the curve and measuring ΔV/ΔI. However, in some diode applications, the AC signal applied to the device is small and it is possible to analyze the circuit in terms of the dynamic, small-signal, or incremental resistance, defined as the one over the slope of the V–I curve at the average value (DC operating point) of the voltage (that is, one over the derivative of current with respect to voltage). For sufficiently small signals, the dynamic resistance allows the Ohm's law small signal resistance to be calculated as approximately one over the slope of a line drawn tangentially to the V–I curve at the DC operating point.[33]\n\nOhm's law has sometimes been stated as, \"for a conductor in a given state, the electromotive force is proportional to the current produced. \"That is, that the resistance, the ratio of the applied electromotive force (or voltage) to the current, \"does not vary with the current strength.\"The qualifier \"in a given state\" is usually interpreted as meaning \"at a constant temperature,\" since the resistivity of materials is usually temperature dependent. Because the conduction of current is related to Joule heating of the conducting body, according to Joule's first law, the temperature of a conducting body may change when it carries a current. The dependence of resistance on temperature therefore makes resistance depend upon the current in a typical experimental setup, making the law in this form difficult to directly verify. Maxwell and others worked out several methods to test the law experimentally in 1876, controlling for heating effects.[34] Usually, the measurements of a sample resistance are carried out at low currents to prevent Joule heating. However, even a small current causes heating(cooling) at the first(second) sample contact due to the Peltier effect. The temperatures at the sample contacts become different, their difference is linear in current. The voltage drop across the circuit includes additionally the Seebeck thermoelectromotive force which again is again linear in current. As a result, there exists a thermal correction to the sample resistance even at negligibly small current.[35] The magnitude of the correction could be comparable with the sample resistance.[36]\n\nOhm's principle predicts the flow of electrical charge (i.e. current) in electrical conductors when subjected to the influence of voltage differences; Jean-Baptiste-Joseph Fourier's principle predicts the flow of heat in heat conductors when subjected to the influence of temperature differences.\n\nThe same equation describes both phenomena, the equation's variables taking on different meanings in the two cases. Specifically, solving a heat conduction (Fourier) problem with temperature (the driving \"force\") and flux of heat (the rate of flow of the driven \"quantity\", i.e. heat energy) variables also solves an analogous electrical conduction (Ohm) problem having electric potential (the driving \"force\") and electric current (the rate of flow of the driven \"quantity\", i.e. charge) variables.[37]\n\nThe basis of Fourier's work was his clear conception and definition of thermal conductivity. He assumed that, all else being the same, the flux of heat is strictly proportional to the gradient of temperature. Although undoubtedly true for small temperature gradients, strictly proportional behavior will be lost when real materials (e.g. ones having a thermal conductivity that is a function of temperature) are subjected to large temperature gradients.\n\nA similar assumption is made in the statement of Ohm's law: other things being alike, the strength of the current at each point is proportional to the gradient of electric potential. The accuracy of the assumption that flow is proportional to the gradient is more readily tested, using modern measurement methods, for the electrical case than for the heat case.\n\nOhm's law, in the form above, is an extremely useful equation in the field of electrical/electronic engineering because it describes how voltage, current and resistance are interrelated on a \"macroscopic\" level, that is, commonly, as circuit elements in an electrical circuit. Physicists who study the electrical properties of matter at the microscopic level use a closely related and more general vector equation, sometimes also referred to as Ohm's law, having variables that are closely related to the V, I, and R scalar variables of Ohm's law, but which are each functions of position within the conductor. Physicists often use this continuum form of Ohm's Law:[38]\n\nE\n        \n        =\n        ρ\n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {E} =\\rho \\mathbf {J} }\n\nwhere E is the electric field vector with units of volts per meter (analogous to V of Ohm's law which has units of volts), J is the current density vector with units of amperes per unit area (analogous to I of Ohm's law which has units of amperes), and ρ \"rho\" is the resistivity with units of ohm·meters (analogous to R of Ohm's law which has units of ohms). The above equation is also written[39] as J = σE where σ \"sigma\" is the conductivity which is the reciprocal of ρ.\n\nThe voltage between two points is defined as:[40]\n\n  \n    \n      \n        \n          Δ\n          V\n        \n        =\n        −\n        ∫\n        \n          \n            E\n          \n          ⋅\n          d\n          \n            ℓ\n          \n        \n      \n    \n    {\\displaystyle {\\Delta V}=-\\int {\\mathbf {E} \\cdot d{\\boldsymbol {\\ell }}}}\n  \n\nwith \n  \n    \n      \n        d\n        \n          ℓ\n        \n      \n    \n    {\\displaystyle d{\\boldsymbol {\\ell }}}\n  \n the element of path along the integration of electric field vector E. If the applied E field is uniform and oriented along the length of the conductor as shown in the figure, then defining the voltage V in the usual convention of being opposite in direction to the field (see figure), and with the understanding that the voltage V is measured differentially across the length of the conductor allowing us to drop the Δ symbol, the above vector equation reduces to the scalar equation:\n\nV\n        =\n        \n          E\n        \n        \n          ℓ\n        \n         \n         \n        \n          or\n        \n         \n         \n        E\n        =\n        \n          \n            V\n            ℓ\n          \n        \n        .\n      \n    \n    {\\displaystyle V={E}{\\ell }\\ \\ {\\text{or}}\\ \\ E={\\frac {V}{\\ell }}.}\n\nSince the E field is uniform in the direction of wire length, for a conductor having uniformly consistent resistivity ρ, the current density J will also be uniform in any cross-sectional area and oriented in the direction of wire length, so we may write:[41]\n\n  \n    \n      \n        J\n        =\n        \n          \n            I\n            a\n          \n        \n        .\n      \n    \n    {\\displaystyle J={\\frac {I}{a}}.}\n\nSubstituting the above 2 results (for E and J respectively) into the continuum form shown at the beginning of this section:\n\n  \n    \n      \n        \n          \n            V\n            ℓ\n          \n        \n        =\n        \n          \n            I\n            a\n          \n        \n        ρ\n        \n        \n          or\n        \n        \n        V\n        =\n        I\n        ρ\n        \n          \n            ℓ\n            a\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {V}{\\ell }}={\\frac {I}{a}}\\rho \\qquad {\\text{or}}\\qquad V=I\\rho {\\frac {\\ell }{a}}.}\n\nThe electrical resistance of a uniform conductor is given in terms of resistivity by:[41]\n\n  \n    \n      \n        \n          R\n        \n        =\n        ρ\n        \n          \n            ℓ\n            a\n          \n        \n      \n    \n    {\\displaystyle {R}=\\rho {\\frac {\\ell }{a}}}\n  \n\nwhere ℓ is the length of the conductor in SI units of meters, a is the cross-sectional area (for a round wire a = πr2 if r is radius) in units of meters squared, and ρ is the resistivity in units of ohm·meters.\n\nAfter substitution of R from the above equation into the equation preceding it, the continuum form of Ohm's law for a uniform field (and uniform current density) oriented along the length of the conductor reduces to the more familiar form:\n\n  \n    \n      \n        V\n        =\n        I\n        R\n        .\n      \n    \n    {\\displaystyle V=IR.}\n\nA perfect crystal lattice, with low enough thermal motion and no deviations from periodic structure, would have no resistivity,[42] but a real metal has crystallographic defects, impurities, multiple isotopes, and thermal motion of the atoms. Electrons scatter from all of these, resulting in resistance to their flow.\n\nThe more complex generalized forms of Ohm's law are important to condensed matter physics, which studies the properties of matter and, in particular, its electronic structure. In broad terms, they fall under the topic of constitutive equations and the theory of transport coefficients.\n\nIf an external B-field is present and the conductor is not at rest but moving at velocity v, then an extra term must be added to account for the current induced by the Lorentz force on the charge carriers.\n\n  \n    \n      \n        \n          J\n        \n        =\n        σ\n        (\n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {J} =\\sigma (\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} )}\n\nIn the rest frame of the moving conductor this term drops out because v = 0. There is no contradiction because the electric field in the rest frame differs from the E-field in the lab frame: E′  = E + v × B.\nElectric and magnetic fields are relative, see Lorentz transformation.\n\nIf the current J is alternating because the applied voltage or E-field varies in time, then reactance must be added to resistance to account for self-inductance, see electrical impedance. The reactance may be strong if the frequency is high or the conductor is coiled.\n\nIn a conductive fluid, such as a plasma, there is a similar effect. Consider a fluid moving with the velocity \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n in a magnetic field \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n. The relative motion induces an electric field \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n  \n which exerts electric force on the charged particles giving rise to an electric current \n  \n    \n      \n        \n          J\n        \n      \n    \n    {\\displaystyle \\mathbf {J} }\n  \n. The equation of motion for the electron gas, with a number density \n  \n    \n      \n        \n          n\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle n_{e}}\n  \n, is written as\n\n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n        \n          n\n          \n            e\n          \n        \n        \n          \n            \n              d\n              \n                \n                  v\n                \n                \n                  e\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          n\n          \n            e\n          \n        \n        e\n        \n          E\n        \n        +\n        \n          n\n          \n            e\n          \n        \n        \n          m\n          \n            e\n          \n        \n        ν\n        (\n        \n          \n            v\n          \n          \n            i\n          \n        \n        −\n        \n          \n            v\n          \n          \n            e\n          \n        \n        )\n        −\n        e\n        \n          n\n          \n            e\n          \n        \n        \n          \n            v\n          \n          \n            e\n          \n        \n        ×\n        \n          B\n        \n        ,\n      \n    \n    {\\displaystyle m_{e}n_{e}{d\\mathbf {v} _{e} \\over dt}=-n_{e}e\\mathbf {E} +n_{e}m_{e}\\nu (\\mathbf {v} _{i}-\\mathbf {v} _{e})-en_{e}\\mathbf {v} _{e}\\times \\mathbf {B} ,}\n\nwhere \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n, \n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle m_{e}}\n  \n and \n  \n    \n      \n        \n          \n            v\n          \n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} _{e}}\n  \n are the charge, mass and velocity of the electrons, respectively. Also, \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n is the frequency of collisions of the electrons with ions which have a velocity field \n  \n    \n      \n        \n          \n            v\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} _{i}}\n  \n. Since, the electron has a very small mass compared with that of ions, we can ignore the left hand side of the above equation to write\n\n  \n    \n      \n        σ\n        (\n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n        =\n        \n          J\n        \n        ,\n      \n    \n    {\\displaystyle \\sigma (\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} )=\\mathbf {J} ,}\n\nwhere we have used the definition of the current density, and also put \n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                n\n                \n                  e\n                \n              \n              \n                e\n                \n                  2\n                \n              \n            \n            \n              ν\n              \n                m\n                \n                  e\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={n_{e}e^{2} \\over \\nu m_{e}}}\n  \n which is the electrical conductivity. This equation can also be equivalently written as\n\n  \n    \n      \n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        =\n        ρ\n        \n          J\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} =\\rho \\mathbf {J} ,}\n  \n\nwhere \n  \n    \n      \n        ρ\n        =\n        \n          σ\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\rho =\\sigma ^{-1}}\n  \n is the electrical resistivity. It is also common to write \n  \n    \n      \n        η\n      \n    \n    {\\displaystyle \\eta }\n  \n instead of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n which can be confusing since it is the same notation used for the magnetic diffusivity defined as \n  \n    \n      \n        η\n        =\n        1\n        \n          /\n        \n        \n          μ\n          \n            0\n          \n        \n        σ\n      \n    \n    {\\displaystyle \\eta =1/\\mu _{0}\\sigma }\n  \n.",
        pageTitle: "Ohm's law",
    },
    {
        title: "Ohm's acoustic law",
        link: "https://en.wikipedia.org/wiki/Ohm%27s_acoustic_law",
        content:
            "Ohm's acoustic law, sometimes called the acoustic phase law or simply Ohm's law, states that a musical sound is perceived by the ear as a set of a number of constituent pure harmonic tones.[1][2]\n\nThe law was proposed by physicist Georg Ohm in 1843.[3] Hermann von Helmholtz elaborated the law into what is often today known as Ohm's acoustic law, by adding that the quality of a tone depends solely on the number and relative strength of its partial simple tones, and not on their relative phases.[4][5] Helmholtz championed the law in opposition to contrary evidence expounded by August Seebeck.[6]\n\nThe law has also been interpreted as \"a pitch corresponding to a certain frequency can only be heard if the acoustical wave contains power at that frequency.\"[7]\n\nThese laws are true to the extent that the ear is sensitive to the frequency and amplitude of the acoustic waves, and further, is able to resolve the differences in their frequency. In modern times, the sensitivity of human hearing to the phase of tone components has been extensively investigated.[8] Controversy has led to this characterization:[9]\n\nFor years musicians have been told that the ear is able to separate any complex signal into a series of sinusoidal signals – that it acts as a Fourier analyzer.  This quarter-truth, known as Ohm's Other Law, has served to increase the distrust with which perceptive musicians regard scientists, since it is readily apparent to them that the ear acts in this way only under very restricted conditions.",
        pageTitle: "Ohm's acoustic law",
    },
    {
        title: "Okrent's law",
        link: "https://en.wikipedia.org/wiki/Okrent%27s_law",
        content:
            'Daniel Okrent (born April 2, 1948) is an American writer and editor. He is best known for having served as the first public editor of The New York Times newspaper, inventing Rotisserie League Baseball,[1] and for writing several books (such as Last Call: The Rise and Fall of Prohibition, which served as a major source for the 2011 Ken Burns/Lynn Novick miniseries Prohibition). In November 2011, Last Call won the Albert J. Beveridge prize, awarded by the American Historical Association to the year\'s best book of American history. His most recent book, published May 2019, is The Guarded Gate: Bigotry, Eugenics, and the Law That Kept Two Generations of Jews, Italians, and Other European Immigrants Out of America.[2]\n\nBorn to a Jewish family[3] in Detroit, Michigan, Okrent graduated from Cass Technical High School in Detroit[4] in 1965 and from the University of Michigan, where he worked on the university\'s student newspaper The Michigan Daily.[citation needed]\n\nMost of his career has been spent as an editor, at such places as Alfred A. Knopf; Harcourt, Brace, Jovanovich; Esquire Magazine; New England Monthly; Life Magazine; and Time, Inc.\n\nHis book Great Fortune: The Epic of Rockefeller Center (Viking, 2003) was a finalist for the Pulitzer Prize for History.\n\nIn October 2003, Okrent was named public editor for The New York Times following the Jayson Blair scandal. He held this position until May 2005.\n\nOkrent and Peter Gethers, having acquired the theatrical rights to the site and name of the web series Old Jews Telling Jokes, co-wrote and co-produced a revue of that name.[5] It opened at the Westside Theatre in Manhattan on May 20, 2012.\n\nFrom 2003-2008, he was chairman of the Smithsonian’s National Portrait Gallery. He has been awarded honorary degrees by the University of Michigan and the Massachusetts College of Liberal Arts.\n\nSince 2017, Okrent has been listed on the Advisory Board of the Secular Coalition for America.[6]\n\nOkrent formulated what has become known as "Okrent\'s law" in an interview comment he made about his new job. It states: "The pursuit of balance can create imbalance because sometimes something is true", referring to the phenomenon of the press providing legitimacy to unsupported fringe viewpoints in an effort to appear even-handed.[7][8][9][10]\n\nOkrent invented Rotisserie League Baseball, the best-known form of fantasy baseball, in 1979. The name comes from the fact that he proposed the idea to his friends while dining at La Rôtisserie Française restaurant on New York City\'s East 52nd Street. Okrent\'s team in the Rotisserie League was called the "Okrent Fenokees", a pun on the Okefenokee Swamp. He was one of the first two people inducted into the Fantasy Sports Hall of Fame.[11] Okrent was still playing Rotisserie as of 2009 under the team name Dan Druffs. Despite having been credited with inventing fantasy baseball he has never been able to win a Rotisserie League. His exploits of inventing Rotisserie League Baseball were chronicled in Silly Little Game, part of the ESPN 30 for 30 documentary series, in 2010.[12]\n\nOkrent is also credited with inventing the baseball stat, WHIP.[13] At the time he referred to it as IPRAT, signifying "Innings Pitched Ratio".\n\nIn May 1981, Okrent wrote and Sports Illustrated published "He Does It by the Numbers".[14] This profile of the then-unknown Bill James launched James\'s career as baseball\'s foremost analyst.[15]\n\nIn 1994, Okrent was filmed for his in-depth knowledge of baseball history for the Ken Burns documentary Baseball.[16] During the nine-part series, a red-sweater-wearing Okrent delivered a detailed analysis of the cultural aspects of the national pastime, including a comparison of the dramatic Game 6 of the 1975 World Series between the Boston Red Sox and Cincinnati Reds to the conflict and character development in Russian novels.\n\nIn the late 1990s, as editor of new media at Time Inc., Okrent wrote about the future of magazine publishing.[17] He believed that the advancement of digital technologies would make it easier for people to read newspapers, magazines and books online.[18] In late 1999, Okrent made a prediction about the future of print media in the Hearst New Media Lecture at the Graduate School of Journalism of Columbia University.[17] He told his audience: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nI believe they, and all forms of print, are dead. Finished. Over. Perhaps not in my professional lifetime, but certainly in that of the youngest people in this room. Remove the question mark from the title of this talk. The Death of Print, full stop.[19]\n\nOkrent has participated in LearnedLeague under the name "OkrentD".[20][21]',
        pageTitle: "Daniel Okrent",
    },
    {
        title: "Okun's law",
        link: "https://en.wikipedia.org/wiki/Okun%27s_law",
        content:
            "In economics, Okun's law  is an empirically observed relationship between unemployment and losses in a country's production. It is named after Arthur Melvin Okun, who first proposed the relationship in 1962.[1] The \"gap version\" states that for every 1% increase in the unemployment rate, a country's GDP will be roughly an additional 2% lower than its potential GDP.  The \"difference version\"[2] describes the relationship between quarterly changes in unemployment and quarterly changes in real GDP. The stability and usefulness of the law has been disputed.[3]\n\nOkun's law is an empirical relationship. In Okun's original statement of his law, a 2% increase in output corresponds to a 1% decline in the rate of cyclical unemployment; a 0.5% increase in labor force participation; a 0.5% increase in hours worked per employee; and a 1% increase in output per hours worked (labor productivity).[4]\n\nOkun's law states that a one-point increase in the cyclical unemployment rate is associated with two percentage points of negative growth in real GDP. The relationship varies depending on the country and time period under consideration.\n\nThe relationship has been tested by regressing GDP or GNP growth on change in the unemployment rate. Martin Prachowny estimated about a 3% decrease in output for every 1% increase in the unemployment rate.[5] However, he argued that the majority of this change in output is actually due to changes in factors other than unemployment, such as capacity utilization and hours worked. Holding these other factors constant reduces the association between unemployment and GDP to around 0.7% for every 1% change in the unemployment rate. The magnitude of the decrease seems to be declining over time in the United States. According to Andrew Abel and Ben Bernanke, estimates based on data from more recent years give about a 2% decrease in output for every 1% increase in unemployment.[6]\n\nThere are several reasons why GDP may increase or decrease more rapidly than unemployment decreases or increases:\n\nOne implication of Okun's law is that an increase in labor productivity or an increase in the size of the labor force can mean that real net output grows without net unemployment rates falling (the phenomenon of \"jobless growth\")\n\nThe gap version of Okun's law may be written (Abel & Bernanke 2005) as:\n\nIn the United States since 1955 or so, the value of c has typically been around 2 or 3, as explained above.\n\nThe gap version of Okun's law, as shown above, is difficult to use in practice because \n  \n    \n      \n        \n          \n            Y\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {Y}}}\n  \n and \n  \n    \n      \n        \n          \n            u\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {u}}}\n  \n\ncan only be estimated, not measured. A more commonly used form of Okun's law, known as the difference or growth rate form of\nOkun's law, relates changes in output to changes in unemployment:\n\nAt the present time in the United States, k is about 3% and c is about 2, so the equation may be written\n\nThe graph at the top of this article illustrates the growth rate form of Okun's law, measured quarterly rather than annually.\n\nPutting both numerators over a common denominator, we obtain\n\nMultiplying the left hand side by \n  \n    \n      \n        \n          \n            \n              \n                \n                  Y\n                  ¯\n                \n              \n              +\n              Δ\n              \n                \n                  Y\n                  ¯\n                \n              \n            \n            Y\n          \n        \n      \n    \n    {\\displaystyle {\\frac {{\\overline {Y}}+\\Delta {\\overline {Y}}}{Y}}}\n  \n, which is approximately equal to 1, we obtain\n\nWe assume that \n  \n    \n      \n        Δ\n        \n          \n            u\n            ¯\n          \n        \n      \n    \n    {\\displaystyle \\Delta {\\overline {u}}}\n  \n, the change in the natural rate of unemployment, is approximately equal to 0. We also assume that \n  \n    \n      \n        \n          \n            \n              Δ\n              \n                \n                  Y\n                  ¯\n                \n              \n            \n            \n              Y\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\Delta {\\overline {Y}}}{\\overline {Y}}}}\n  \n, the growth rate of full-employment output, is approximately equal to its average value, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n. So we finally obtain\n\nThrough comparisons between actual data and theoretical forecasting, Okun's law proves to be an invaluable[clarification needed] tool in predicting trends between unemployment and real GDP. However, the accuracy of the data theoretically proved through Okun's law compared to real world numbers proves to be generally inaccurate. This is due to the variances in Okun's coefficient. Many, including the Reserve Bank of Australia, conclude that information proved by Okun's law to be acceptable to a certain degree.[7] Also, some findings[which?] have concluded that Okun's law tends to have higher rates of accuracy for short-run predictions, rather than long-run predictions. Forecasters[who?] have concluded this to be true due to unforeseen market conditions that may affect Okun's coefficient.\n\nAs such, Okun's law is generally acceptable by forecasters as a tool for short-run trend analysis between unemployment and real GDP, rather than being used for long run analysis as well as accurate numerical calculations.\n\nThe San Francisco Federal Reserve Bank determined through the use of empirical data from past recessions in the 1970s, 1990s, and 2000s that Okun’s law was a useful theory. All recessions showed two common main trends: a counterclockwise loop[clarification needed] for both real-time and revised data. The recoveries of the 1990s and 2000s did have smaller and tighter loops.[8]",
        pageTitle: "Okun's law",
    },
    {
        title: "Ostrom's law",
        link: "https://en.wikipedia.org/wiki/Ostrom%27s_law",
        content:
            'Elinor Claire "Lin" Ostrom (née Awan; August 7, 1933 – June 12, 2012) was an American political scientist and political economist[1][2][3] whose work was associated with New Institutional Economics and the resurgence of political economy.[4] In 2009, she was awarded the Nobel Memorial Prize in Economic Sciences for her "analysis of economic governance, especially the commons", which she shared with Oliver E. Williamson; she was the first woman to win the prize.[5]\n\nTrained in political science at UCLA, Ostrom was a faculty member at Indiana University Bloomington for 47 years. Beginning in the 1960s, Ostrom was involved in resource management policy and created a research center, the Workshop in Political Theory and Policy Analysis, which attracted scientists from different disciplines from around the world. Working and teaching at her center was created on the principle of a workshop, rather than a university with lectures and a strict hierarchy. Late in her career, she held an affiliation with Arizona State University.\n\nOstrom studied the interaction of people and ecosystems for many years and showed that the use of exhaustible resources by groups of people (communities, cooperatives, trusts, trade unions) can be rational and prevent depletion of the resource without either state intervention or markets with private property.[6]\n\nElinor Claire Awan was born in Los Angeles, California as the only child of Leah Hopkins, a musician, and Adrian Awan, a set designer.[7][8] Her parents separated early in her life, and Elinor lived with her mother most of the time.[9] She attended a Protestant church with her mother and often spent weekends with her father\'s Jewish family.[7][10] Growing up in the post-Depression era to divorced artisans, Ostrom described herself as a "poor kid."[9][11] Her major recreational activity was swimming, where she eventually joined a swimming team and swam competitively until she started teaching swimming to earn funds to help put herself through college.[12]\n\nOstrom grew up across the street from Beverly Hills High School, which she attended, graduating in 1951.[13] She regarded this as fortunate, for the school had a very high rate of college admission. During Ostrom\'s junior year, she was encouraged to join the debate team. Learning debate tactics had an important impact on her ways of thinking. It allowed her to realize there are two sides to public policy and it is imperative to have quality arguments for both sides.[12] As a high school student, Elinor Ostrom had been discouraged from studying trigonometry, as girls without top marks in algebra and geometry were not allowed to take the subject. No one in her immediate family had any college experience, but seeing that 90% of students in her high school attended college, she saw it as the "normal" thing to do.[12] Her mother did not wish for her to attend college, seeing no reason for it.[13]\n\nShe attended UCLA, receiving a B.A. with honors in political science at UCLA in 1954.[14] By attending multiple summer sessions and extra classes throughout semesters, she was able to graduate in three years. She worked at the library, dime store and bookstore in order to pay her fees which were $50 per semester.[12] After graduation, she had trouble finding a job because employers presumed that she was only looking for jobs as a teacher or secretary. She began a job as an export clerk after taking a correspondence course for shorthand, which she later found to be helpful when taking notes in face-to-face interviews on research projects. After a year, she obtained a position as assistant personnel manager in a business firm that had never before hired a woman in anything but a secretarial position. This job inspired her to think about attending graduate-level courses and eventually applying for a research assistantship and admission to a Ph.D. program.[12]\n\nLacking any math from her undergraduate education and trigonometry from high school, she was consequently rejected for an economics Ph.D. program at UCLA.[15] She was admitted to UCLA\'s graduate program in political science, where she was awarded an M.A. in 1962 and a Ph.D. in 1965.[14] The teams of graduate students she was involved with were analyzing the political economic effects of a group of groundwater basins in Southern California. Specifically, Ostrom was assigned to look at the West Basin. She found it is very difficult to manage a common-pool resource when it is used between individuals.[12] The locals were pumping too much groundwater and salt water seeped into the basin. Ostrom was impressed with how people from conflicting and overlapping jurisdictions who depended on that source found incentives to settle contradictions and solve the problem. She made the study of this collaboration the topic of her dissertation, laying the foundation for the study of "shared resources".\n\nOstrom was informed by fieldwork, both her own and that of others. During her PhD at the University of California, Los Angeles, she spent years studying the water wars and pumping races going on in the 1950s in her own dry backyard. In contrast to the prevailing rational-economic predictions of Malthusianism and the tragedy of the commons, she showed cases where humans were not trapped and helpless amid diminishing supplies. In her book Governing the Commons, she draws on studies of irrigation systems in Spain and Nepal, mountain villages in Switzerland and Japan, and fisheries in Maine and Indonesia.[16]\n\nIn 1961, Vincent Ostrom, Charles Tiebout, and Robert Warren published "The Organization of Government in Metropolitan Areas," which would go on to be an influential article and introduced themes that would be central to the Ostroms\' work.[13][17] However, the article aggravated a conflict with UCLA\'s Bureau of Governmental Research because, counter to the Bureau\'s interests, it advised against centralization of metropolitan areas in favor of polycentrism. This conflict prompted the Ostroms to leave UCLA.[13] They moved to Bloomington, Indiana, in 1965, when Vincent accepted a political science professorship at Indiana University.[18] She joined the faculty as a visiting assistant professor. The first course she taught was an evening class on American government.[7][19]\n\nOstrom is probably best known for revisiting the so-called “tragedy of the commons" – a conjecture proposed by biologist Garrett Hardin in 1968.[20][21]\n\n"In an article by the same name published in the journal Science, Hardin theorized that if each herdsman sharing a piece of common grazing land made the individually rational economic decision of increasing the number of cattle he keeps on the land, the collective effect would deplete or destroy the commons. In other words, multiple individuals—acting independently and rationally consulting their own self-interest—will ultimately deplete a shared limited resource, even when it is clear that it is not in anyone’s long-term interest for this to happen. Ostrom believes that the “tragedy” in such situations isn’t inevitable, as Hardin thought. Instead, if the herders decide to cooperate with one another, monitoring each other’s use of the land and enforcing rules for managing it, they can avoid the tragedy."[20]\n\nGarrett Hardin believes that the most important aspect that we need to realize today is the need to abandon the principle of shared resources in reproduction. A possible alternative to the tragedy of the commons (shared needs) was described in Elinor Ostrom\'s book Governing the Commons. Based on her fieldwork, the book demonstrates that there are practical algorithms for the collective use of a limited common resource, which solve the many issues with both government/regulation driven solutions and market-based ones.\n\nIn 1973, Ostrom and her husband founded the Workshop in Political Theory and Policy Analysis at Indiana University.[22] Examining the use of collective action, trust, and cooperation in the management of common pool resources (CPR), her institutional approach to public policy, known as the Institutional analysis and development framework (IAD), has been considered sufficiently distinct to be thought of as a separate school of public choice theory.[23] She authored many books in the fields of organizational theory, political science, and public administration. Elinor Ostrom was a dedicated scholar until the very end of her life. Indeed, on the day before she died, she sent e-mail messages to at least two different sets of coauthors about papers that she was writing with them. She was the chief scientific advisor for the International Council for Science (ICSU) Planet Under Pressure meeting in London in March, and Johan Rockström of the Stockholm Resilience Centre wrote that\n\n"Lin, up until the very end, was heavily involved in our preparations for the Nobel laureate dialogues on global sustainability we will be hosting in Rio 17th and 18th of June during the UN Rio+20 Earth Summit. In the end, she decided she could not come in person, but was contributing sharp, enthusiastically charged, inputs, in the way only she could."[24][25]\n\nIt was long unanimously held among economists that natural resources that were collectively used by their users would be over-exploited and destroyed in the long-term. Elinor Ostrom disproved this idea by conducting field studies on how people in small, local communities manage shared natural resources, such as pastures, fishing waters and forests. She showed that when natural resources are jointly used by their users, in time, rules are established for how these are to be cared for and they become used in a way that is both economically and ecologically sustainable.[26]\n\nOstrom was appointed Professor of Political Science in 1974. She was the head of the department from 1980 to 1984, and then held the Arthur F. Bentley Chair of Political Science[27] She was appointed Distinguished Professor in 2010 and held a partial appointment in the School of Public and Environmental Affairs.\n\nShe was senior research director of the Vincent and Elinor Ostrom Workshop in Political Theory and Policy Analysis, Distinguished Professor and Arthur F. Bentley Professor of Political Science in the College of Arts and Sciences, and professor in the School of Public and Environmental Affairs.[28] The Workshop in Political Theory and Policy Analysis was meant to utilize diverse scholars throughout economics, political science, and other fields to collaborate and attempt to understand how institutional arrangements in a diverse set of ecological and social economic political settings affected behavior and outcomes. The goal was not to fly around the world collecting data, rather it is to create a network of scholars who live in particular areas of the world and had strong interests in forest conditions and forest policy conducted the studies.[29]\n\nOstrom\'s innovative and ground-breaking research was supported by National Science Foundation, the Andrew Mellon Foundation, the Hynde and Harry Bradley Foundation, the MacArthur Foundation, the Ford Foundation, the Food and Agriculture Organization of the United Nations, U.S.A.I.D., the U.S. Geological Survey, the U.S. Department of Justice, and the National Institute of Mental Health.[30]\n\nOstrom has been involved in international activities throughout her long and productive career. She had experience in Kenya, Nepal and Nigeria, and also made research trips to Australia, Bolivia, India, Indonesia, Mexico, Philippines, Poland and Zimbabwe. During workshops and research grants, she and her husband supported many international students, and visited researchers and policymakers. They did not have children of their own and used personal funds and efforts to receive grants to help others. In a 2010 interview, Ostrom noted that because they had no family to support, “I was not ever concerned about salary, so that’s never been an issue for me. For some colleagues who have big families, and all the rest, it’s a major issue.” [30]\n\nOstrom was a founding member and first president of the IASC (International Association for the Study of the Commons).[31] She was a lead researcher for the Sustainable Agriculture and Natural Resource Management Collaborative Research Support Program (SANREM CRSP), managed by Virginia Tech and funded by USAID.[32] Beginning in 2008, she and her husband Vincent Ostrom advised the journal Transnational Corporations Review.[33]\n\nOstrom\'s early work emphasized the role of public choice on decisions influencing the production of public goods and services.[34] Among her better known works in this area is her study on the polycentricity of police functions in Indianapolis.[35] Caring for the commons had to be a multiple task, organised from the ground up and shaped to cultural norms. It had to be discussed face to face, and based on trust. Dr. Ostrom, besides poring over satellite data and quizzing lobstermen herself, enjoyed employing game theory to try to predict the behaviour of people faced with limited resources. In her Workshop in Political Theory and Policy Analysis at Indiana University—set up with her husband Vincent, a political scientist, in 1973—her students were given shares in a national common. When they discussed what they should do before they did it, their rate of return from their "investments" more than doubled. Her later, and more famous, work focused on how humans interact with ecosystems to maintain long-term sustainable resource yields. Common pool resources include many forests, fisheries, oil fields, grazing lands, and irrigation systems. She conducted her field studies on the management of pasture by locals in Africa and irrigation systems management in villages of western Nepal (e.g., Dang Deukhuri). Her work has considered how societies have developed diverse institutional arrangements for managing natural resources and avoiding ecosystem collapse in many cases, even though some arrangements have failed to prevent resource exhaustion. Her work emphasized the multifaceted nature of human–ecosystem interaction and argues against any singular "panacea" for individual social-ecological system problems.[36]\n\nIn Governing the Commons, Ostrom summarized eight design principles that were present in the sustainable common pool resource institutions she studied:[37][38]\n\nIndividuals or households who have rights to withdraw resource units from the CPR must be clearly defined, as must the boundaries of the CPR itself.\n\n2. Congruence between appropriation and provision rules and local conditions\n\nAppropriation rules restricting time, place, technology, and/or quantity of resource units are related to local labor, material, and/or money.\n\nMost individuals affected by the operational rules can participate in modifying the operational rules.\n\nMonitors, who actively audit CPR conditions and appropriator behavior, are accountable to the appropriators or are the appropriators.\n\nAppropriators who violate operational rules are likely to be assessed graduated sanctions (depending on the seriousness and context of the offense) by other appropriators, by officials accountable to these appropriators, or by both.\n\nAppropriators and their officials have rapid access to low-cost local arenas to resolve conflicts among appropriators or between appropriators and officials.\n\nThe rights of appropriators to devise their own institutions are not challenged by external governmental authorities.\n\nAppropriation, provision, monitoring, enforcement, conflict resolution, and governance activities are organized in multiple layers of nested enterprises.\n\nThese principles have since been slightly modified and expanded to include a number of additional variables believed to affect the success of self-organized governance systems, including effective communication, internal trust and reciprocity, and the nature of the resource system as a whole.[39]\n\nOstrom and her many co-researchers have developed a comprehensive "Social-Ecological Systems (SES) framework", within which much of the still-evolving theory of common-pool resources and collective self-governance is now located.[40]\n\nAccording to the Norwegian Institute for Urban and Regional Research, "Ostrom cautioned against single governmental units at global level to solve the collective action problem of coordinating work against environmental destruction. Partly, this is due to their complexity, and partly to the diversity of actors involved. Her proposal was that of a polycentric approach, where key management decisions should be made as close to the scene of events and the actors involved as possible." Ostrom helped disprove the idea held by economists that natural resources would be over-used and destroyed in the long run. Elinor Ostrom disproved this idea by conducting field studies on how people in small, local communities manage shared natural resources, such as pastures, fishing waters in Maine and Indonesia, and forests in Nepal. She showed that when natural resources are jointly managed by their users, in time, rules are established for how these are to be cared for and used in a way that is both economically and ecologically sustainable.[41]\n\nOstrom\'s law is an adage that represents how Elinor Ostrom\'s works in economics challenge previous theoretical frameworks and assumptions about property, especially the commons. Ostrom\'s detailed analyses of functional examples of the commons create an alternative view of the arrangement of resources that are both practically and theoretically possible. This eponymous law is stated succinctly by Lee Anne Fennell as:\n\nA resource arrangement that works in practice can work in theory.[42]\n\nAfter college, Ostrom married a classmate, Charles Scott, and worked at General Radio in Cambridge, Massachusetts, while Scott attended Harvard Law School.[7] They divorced several years later when Ostrom began contemplating a Ph.D.[7][43]\n\nHer postgraduate seminar was led by Vincent Ostrom, an associate professor of political science, 14 years her senior, whom she married in 1963. This marked the beginning of a lifelong partnership named "love and contestation," as Ostrom put it in her dedication to her seminal 1990 book, Governing the Commons: The Evolution of Institutions for Collective Action.[20]\n\nOstrom was a member of the United States National Academy of Sciences,[19] a member of the American Philosophical Society,[44] and president of the American Political Science Association and the Public Choice Society. In 1999, she became the first woman to receive the prestigious Johan Skytte Prize in Political Science.[45]\n\nOstrom was awarded the Frank E. Seidman Distinguished Award for Political Economy in 1998. Her presented paper, on "The Comparative Study of Public Economies",[46] was followed by a discussion among Kenneth Arrow, Thomas Schelling, and Amartya Sen. She was awarded the John J. Carty Award from the National Academy of Sciences in 2004,[47] and, in 2005, received the James Madison Award by the American Political Science Association. In 2008, she became the first woman to receive the William H. Riker Prize in political science; and, the following year, she received the Tisch Civic Engagement Research Prize from the Jonathan M. Tisch College of Citizenship and Public Service at Tufts University. In 2010, the Utne Reader magazine included Ostrom as one of the "25 Visionaries Who Are Changing Your World".[48] She was named one of Time magazine\'s "100 Most Influential People in the World" in 2012.\n\nThe International Institute of Social Studies (ISS) awarded its Honorary Fellowship to her in 2002.\n\nIn 2008 she was awarded an honorary degree, doctor honoris causa, at the Norwegian University of Science and Technology.[49]\n\nIn July 2019, Indiana University Bloomington announced that as part of their Bridging the Visibility Gap initiative, a statue of Ostrom would be placed outside of the building which houses the university\'s political science department.[50]\n\nIn 2009, Ostrom became the first woman to receive the Nobel Memorial Prize in Economic Sciences. The Royal Swedish Academy of Sciences cited Ostrom "for her analysis of economic governance", saying her work had demonstrated how common property could be successfully managed by groups using it. Ostrom and Oliver E. Williamson shared the 10-million Swedish kronor (€990,000; $1.44 million) prize for their separate work in economic governance.[51] As she had done with previous monetary prizes, Ostrom donated her award to the Workshop she helped to found.[9][52]\n\nThe Royal Swedish Academy of Sciences said Ostrom\'s "research brought this topic from the fringe to the forefront of scientific attention...by showing how common resources—forests, fisheries, oil fields or grazing lands—can be managed successfully by the people who use them rather than by governments or private companies". Ostrom\'s work in this regard challenged conventional wisdom, showing that common resources can be successfully managed without government regulation or privatization.[53]\n\nIn awarding Ostrom the Nobel Prize for the Analysis of Economic Governance, the Royal Swedish Academy of Sciences noted that her work "teaches us novel lessons about the deep mechanisms that sustain cooperation in human societies." Even if Ostrom\'s selection (along with co-recipient Oliver Williamson of the University of California, Berkeley) seemed odd to some, others saw it as an appropriate reaction to free-market inefficiencies highlighted by the 2008 financial crisis.[20]\n\nOstrom was diagnosed with pancreatic cancer in October 2011.[54][55] During the final year of her life, she continued to write and lecture, giving the Hayek Lecture at the Institute of Economic Affairs just eleven weeks before her death.[9] She died at 6:40 a.m. Tuesday, June 12, 2012, at IU Health Bloomington Hospital at the age of 78.[28] On the day of her death, she published her last article, "Green from the Grassroots," in Project Syndicate.[56][57] Indiana University president Michael McRobbie wrote: "Indiana University has lost an irreplaceable and magnificent treasure with the passing of Elinor Ostrom".[58] Her Indiana colleague Michael McGinnis commented after her death that Ostrom donated her share of the $1.4 million Nobel award money to the Workshop—the biggest, by far, of several academic prizes with monetary awards that the Ostroms had given to the center over the years.[24] Her husband Vincent died 17 days later from complications related to cancer. He was 92.[59]',
        pageTitle: "Elinor Ostrom",
    },
    {
        title: "property law",
        link: "https://en.wikipedia.org/wiki/Property_law",
        content:
            'Property law is the area of law that governs the various forms of ownership in real property (land) and personal property. Property refers to legally protected claims to resources, such as land and personal property, including intellectual property.[1] Property can be exchanged through contract law, and if property is violated, one could sue under tort law to protect it.[1]\n\nThe concept, idea or philosophy of property underlies all property law. In some jurisdictions, historically all property was owned by the monarch and it devolved through feudal land tenure or other feudal systems of loyalty and fealty.\n\nThe word property, in everyday usage, refers to an object (or objects) owned by a person—a car, a book, or a cellphone—and the relationship the person has to it.[2] In law, the concept acquires a more nuanced rendering. Factors to consider include the nature of the object, the relationship between the person and the object, the relationship between a number of people in relation to the object, and how the object is regarded within the prevailing political system. Most broadly and concisely, property in the legal sense refers to the rights of people in or over certain objects or things.[3]\n\nNon-legally recognized or documented property rights are known as informal property rights.  These informal property rights are non-codified or documented, but recognized among local residents to varying degrees.\n\nIn capitalist societies with market economies, much of property is owned privately by persons or associations and not the government. Five general justifications have been given on private property rights:[1]\n\nArguments in favor of limiting private property rights have also been raised:[4][1]\n\nIn his Second Treatise on Government, English philosopher John Locke asserted the right of an individual to own one part of the world, when, according to the Bible, God gave the world to all humanity in common.[5] He claimed that although persons belong to God, they own the fruits of their labor. When a person works, that labor enters into the object. Thus, the object becomes the property of that person. However, Locke conditioned property on the Lockean proviso, that is, "there is enough, and as good, left in common for others".\n\nU.S. Supreme Court Justice James Wilson undertook a survey of the philosophical grounds of American property law in 1790 and 1791. He proceeds from two premises: "Every crime includes an injury: every injury includes a violation of a right." (Lectures III, ii.) The government\'s role in protecting property depends upon an idea of right. Wilson believes that "man has a natural right to his property, to his character, to liberty, and to safety."[6] He also indicates that "the primary and principal object in the institution of government... was... to acquire a new security for the possession or the recovery of those rights".[7]\n\nWilson states that: "Property is the right or lawful power, which a person has to a thing." He then divides the right into three degrees: possession, the lowest; possession and use; and, possession, use, and disposition – the highest. Further, he states: "Useful and skillful industry is the soul of an active life. But industry should have her just reward. That reward is property, for of useful and active industry, property is the natural result." From this simple reasoning he is able to present the conclusion that exclusive, as opposed to communal property, is to be preferred. Wilson does, however, give a survey of communal property arrangements in history, not only in colonial Virginia but also ancient Sparta.\n\nThere are two main views on the right to property, the traditional view and the bundle of rights view.[8] The traditionalists believe that there is a core, inherent meaning in the concept of property, while the bundle of rights view states that the property owner only has bundle of permissible uses over the property.[1] The two views exist on a spectrum and the difference may be a matter of focus and emphasis.[1]\n\nWilliam Blackstone, in his Commentaries on the Laws of England, wrote that the essential core of property is the right to exclude.[9] That is, the owner of property must be able to exclude others from the thing in question, even though the right to exclude is subject to limitations.[10] By implication, the owner can use the thing, unless another restriction, such as zoning law, prevents it.[1] Other traditionalists argue that three main rights define property: the right to exclusion, use and transfer.[11]\n\nAn alternative view of property, favored by legal realists, is that property simply denotes a bundle of rights defined by law and social policy.[1] Which rights are included in the bundle known as property rights, and which bundles are preferred to which others, is simply a matter of policy.[1] Therefore, a government can prevent the building of a factory on a piece of land, through zoning law or criminal law, without damaging the concept of property.[1] The "bundle of rights" view was prominent in academia in the 20th century and remains influential today in American law.[1]\n\nDifferent parties may claim a competing interest in the same property by mistake or by fraud, with the claims being inconsistent of each other. For example, the party creating or transferring an interest may have a valid title, but may intentionally or negligently create several interests wholly or partially inconsistent with each other. A court resolves the dispute by adjudicating the priorities of the interests.\n\nProperty rights are rights over things enforceable against all other persons. By contrast, contractual rights are rights enforceable against particular persons. Property rights may, however, arise from a contract; the two systems of rights overlap. In relation to the sale of land, for example, two sets of legal relationships exist alongside one another: the contractual right to sue for damages, and the property right exercisable over the land. More minor property rights may be created by contract, as in the case of easements, covenants, and equitable servitudes.[12]\n\nA separate distinction is evident where the rights granted are insufficiently substantial to confer on the nonowner a definable interest or right in the thing. The clearest example of these rights is the license. In general, even if licenses are created by a binding contract, they do not give rise to property interests.\n\nProperty rights are also distinguished from personal rights. Practically all contemporary societies acknowledge this basic ontological and ethical distinction. In the past, groups lacking political power have often been disqualified from the benefits of property. In an extreme form, this has meant that people have become "objects" of property—legally "things" or chattels (see slavery.) More commonly, marginalized groups have been denied legal rights to own property. These include Jews in England and married women in Western societies until the late 19th century.\n\nThe dividing line between personal rights and property rights is not always easy to draw. For instance, is one\'s reputation property that can be commercially exploited by affording property rights to it? The question of the proprietary character of personal rights is particularly relevant in the case of rights over human tissue, organs and other body parts.\n\nIn the United States, a "quasi-property" interest has been explicitly declared in the dead body. Also in the United States, it has been recognised that people have an alienable proprietary "right of publicity" over their "persona". The patent/patenting of biotechnological processes and products based on human genetic material may be characterised as creating property in human life.\n\nA particularly difficult question is whether people have rights to intellectual property developed by others from their body parts. In the pioneering case on this issue, the Supreme Court of California held in Moore v. Regents of the University of California (1990) that individuals do not have such a property right.\n\nProperty law is characterised by a great deal of historical continuity and technical terminology. The basic distinction in common law systems is between real property (land) and personal property (chattels).\n\nBefore the mid-19th century, the principles governing the transfer of real property and personal property on an intestacy were quite different. Though this dichotomy does not have the same significance anymore, the distinction is still fundamental because of the essential differences between the two categories. An obvious example is the fact that land is immovable, and thus the rules that govern its use must differ. A further reason for the distinction is that legislation is often drafted employing the traditional terminology.\n\nThe division of land and chattels has been criticised as being not satisfactory as a basis for categorising the principles of property law since it concentrates attention not on the proprietary interests themselves but on the objects of those interests.[13] Moreover, in the case of fixtures, chattels which are affixed to or placed on land may become part of the land.\n\nAlthough a tenancy involves rights to real property, a leasehold estate is typically considered personal property, being derived from contract law. In the civil law system, the distinction is between movable and immovable property, with movable property roughly corresponding to personal property, while immovable property corresponding to real estate or real property, and the associated rights, and obligations thereon.\n\nThe concept of possession developed from a legal system whose principal concern was to avoid civil disorder. The general principle is that a person in possession of land or goods, even as a wrongdoer, is entitled to take action against anyone interfering with the possession unless the person interfering is able to demonstrate a superior right to do so.\n\nIn England, the Torts (Interference with Goods) Act 1977 has significantly amended the law relating to wrongful interference with goods and abolished some longstanding remedies and doctrines.[14]\n\nThe term "transfer of property" means an act by which a living person, company, or state conveys property, in present or in future, to one or more other living persons, to himself and one or more other living persons, to the state, or to a private company. The transfer of property can be consensual or non-consensual, and to transfer property is to perform such an act.\n\nThe most common method of acquiring an interest in property is as the result of a consensual transaction with the previous owner, for example, a sale, a gift, or through inheritance. In law, an inheritor is a person who is entitled to receive a share of the heritor\'s (the person who died) property, subject to the rules of inheritance in the jurisdiction of which the heritor was a citizen or where the heritor died or owned property at the time of death. Dispositions by will may also be regarded as consensual transactions, since the effect of a will is to provide for the distribution of the deceased person\'s property to nominated beneficiaries. A person may also obtain an interest in property under a trust established for his or her benefit by the owner of the property.\n\nIt is also possible for property to pass from one person to another independently of the consent of the property owner. For example, this occurs when a person dies intestate, goes bankrupt, or has the property taken in execution of a court judgment.\n\nThere are cases when a person is legally capable of owning property, but is not capable of maintaining and dealing with it (such as paying property taxes). This is the case for young children and mentally handicapped individuals. The state deems them incompetent in their capacity to deal with property. Thus, they must be appointed a legal guardian to deal with the property on the incompetent individual\'s behalf. In cases where the individual cannot find a legal guardian to deal with the property, the property is put up for sale and the incompetent individual is involuntarily deprived of such property.\n\nTax sales are another process by which individuals can be forcibly deprived of their private property. A tax sale is the forced sale of property by the state due to unpaid taxes on that property. The property is typically auctioned off as a tax sale by the local government to payoff the delinquent taxes on that property. One could make the argument that, given the presence of property taxes, an individual never truly owns a piece of property; they rent it from the government.\n\nProperty can also pass from one person to the state independently of the consent of the property owner through the state\'s power of eminent domain. Eminent domain refers to the ability of the state to buyout private property from individuals at their will in order to use the property for public use. Eminent domain requires the state to "justly compensate" the property owner for the acquisition of their land. The practice dates back to at least the 17th century.[15] Common examples include buying land from individuals in order for the state to build public roads, transportation systems, governmental buildings, and to construct certain public goods. The state also uses its eminent domain power for large urban renewal projects by which it will buy out large portions of typically poor housing areas in order to rebuild it.\n\nEminent domain also consists of enabling the state to condemn certain real estate construction and development rights for various reasons. One must meet location specific regulatory standards and building codes in order to construct on property. The general rule for stairs (in the US) is 7-11 (a 7-inch rise and 11 inch run). More exactly, no more than 7 3/4 inches for the riser (vertical) and a minimum of 10 inches for the tread (horizontal or step). Failure to meet these regulatory standards can result in an inability to receive state building permits, state destruction of property, legal fines, and increased liability.\n\nKELO V. NEW LONDON (04-108) 545 U.S. 469 (2005) was a pivotal case that increased the scope of the eminent domain power of the state. The U.S. supreme court ruled that private property could be condemned by the state and transferred to a private company.[16]\n\nIn property law, economics and finance, the term "legal successor" may refer to a legally established successor of property rights (inheritance, interest) or in terms of liabilities (debt).\n\nIn the case of bankruptcy of a lender, the legal successor in interest has the right to collect the debt.[17]\n\nHistorically, leases served many purposes, and the regulation varied according to intended purposes and the economic conditions of the time. Leaseholds, for example, were mainly granted for agriculture until the late eighteenth century and early nineteenth century, when the growth of cities made the leasehold an important form of landholding in urban areas.\n\nThe modern law of landlord and tenant in common law jurisdictions retains the influence of the common law and, particularly, the laissez-faire philosophy that dominated the law of contract and the law of property in the 19th century. With the growth of consumerism, the law of consumer protection recognised that common law principles assuming equal bargaining power between parties may cause unfairness. Consequently, reformers have emphasised the need to assess residential tenancy laws in terms of protection they provide to tenants. Legislation to protect tenants is now common.\n\nProperty can mostly be owned by any single human. However, many jurisdictions have some stipulations that limit property-owning capacity. The two main limiting factors include citizenship and competency of maintaining property.\n\nIn many countries, non-citizens cannot own property or are limited greatly in their capacity to own property. The United States allows foreign entities to buy and own property. But the United States does have stipulations surrounding tribal land owned by the indigenous Native Americans.\n\nIncompetent individuals also cannot own property, at least without a legal guardian. Incompetent individuals consist largely of children and the cognitively impaired. They are legally recognized and allowed to own property, but they cannot deal with it without the consent of their legal guardians. Children do not have the capacity to pay property taxes.\n\nAll western legal systems allow for a number of different forms of group ownership of property. Group ownership in property law is referred to as co-tenancy, or concurrent ownership. Two or more owners of a property are referred to as co-owners.\n\nIn U.S. common law, property can be owned by many different people and parties. Property can be shared by an infinitely divisible number of people. There are three types of concurrent estates, or ways people can jointly own property: joint tenancy, tenancy in common, or tenancy by entirety.\n\nIn joint tenancy, each owner of the property has an undivided interest in it along with full and complete ownership. Each owner in joint tenancy has the full right to occupy and use all of it. If one owner dies in joint tenancy, then the other owner takes control of the deceased owner\'s interest.[18]\n\nIn tenancy in common, the shares of ownership can be equal or unequal in size. One person may own a larger share of the property than another. Even if owners own an unequal amount of shares, all owners still have the right to use all of the property. If one owner dies, their share of the property is transferred to the designated individual in their will contract.\n\nIn tenancy by the entirety, each owner of the property has an undivided interest in it along with full and complete ownership. Each spouse has the full right to occupy and use all of the property. It is only available to married couples. A spouse cannot transfer their interest in the property without the consent of the other spouse. If the couple divorces and goes to court, a judge is granted wide discretion on how to divide the share interests of the property in common-law jurisdictions.\n\nCorporations are legal non-human entities that are entitled to property rights just as an individual human is. A corporation has legal power to use and possess property just as a fictitious legal human would. However, a corporation isn\'t a single human, it is the collective will of a group of people who provide a service or build a good. With many agent in play, there are many different and opposing interests in play with respect to ownership. The majority of property is now owned by corporations. They were created under general incorporation statutes that allow such fictitious legal persons to have property rights.[21]\n\nThe community, or the state, can have many different roles concerning property: facilitator, protector, and owner. In capitalist market economies, the state largely serves as a mediator that facilitates and enforces private property laws.\n\nCommunist ideals oppose private property laws. Communism / Marxism advocates for full state / public ownership of property. "Private property has made us so stupid and one-sided that an object is only ours when we have it – when it exists for us as capital, or when it is directly possessed, eaten, drunk, worn, inhabited, etc., – in short, when it is used by us" (Marx).[22]  However, it is important to note that many Marxist–Leninist societies such as China and the dissolved Soviet Union have forms of private property laws.\n\nIn the United States, "the federal government owns roughly 640 million acres, about 28% of the 2.27 billion acres of land in the United States. Four major federal land management agencies administer 606.5 million acres of this land (as of September 30, 2018). They are the Bureau of Land Management (BLM), Fish and Wildlife Service (FWS), and National Park Service (NPS) in the Department of the Interior (DOI) and the Forest Service (FS) in the Department of Agriculture. A fifth agency, the Department of Defense (excluding the U.S. Army Corps of Engineers), administers 8.8 million acres in the United States (as of September 30, 2017), consisting of military bases, training ranges, and more. Together, the five agencies manage about 615.3 million acres, or 27% of the U.S. land base. Many other agencies administer the remaining federal acreage."[23]',
        pageTitle: "Property law",
    },
    {
        title: "O'Sullivan's first law",
        link: "https://en.wikipedia.org/wiki/O%27Sullivan%27s_first_law",
        content:
            "John O'Sullivan, CBE (born 25 April 1942) is a British conservative political commentator and journalist. From 1987 to 1988, he was a senior policy writer and speechwriter in 10 Downing Street for Margaret Thatcher when she was British prime minister and remained close to her up to her death.[1][2]\n\nO'Sullivan served as vice president and executive editor of Radio Free Europe/Radio Liberty from 2008 to 2012.[3] He was editor of the Australian monthly magazine Quadrant from 2015 to 2017.[4][5]\n\nSince 2017, he has been president of the Danube Institute,[6] a Fidesz government-financed[7][8] think tank based in Budapest, Hungary, and a member of the board of advisors for the Global Panel Foundation, an NGO that works behind the scenes in crisis areas around the world.[9]\n\nA former editor of National Review from 1988 to 1997, O'Sullivan has been an editor-at-large there since then.[10]\n\nBorn in Liverpool, O'Sullivan was educated at St Mary's College, Crosby, and received his higher education at the University of London.[11] He stood unsuccessfully as a Conservative candidate for the constituency of Gateshead West in the 1970 British general election.\n\nIn 2014 he moved to Budapest, to set up the Danube Institute.[11] He is the Director of 21st Century Initiatives and Senior Fellow at the National Review Institute in Washington, D.C..\n\nO'Sullivan is a former editor (1988–1997) and current editor-at-large of the opinion magazine National Review[12] and a former senior fellow at the Hudson Institute.[13] He had previously been the editor-in-chief of United Press International, editor-in-chief of the international affairs magazine, The National Interest, and a special adviser to British prime minister Margaret Thatcher.[14] He was made a Commander of the Order of the British Empire (CBE) in the 1991 New Year's Honours List.\n\nIn 1998 O'Sullivan was a leading member of the journalistic team that founded the National Post, a right-leaning national newspaper in Canada.[15]\n\nO'Sullivan is the founder and co-chairman of the New Atlantic Initiative, an international organisation dedicated to reinvigorating and expanding the Atlantic community of democracies. The organisation was created at the Congress of Prague in May 1996 by Václav Havel and Margaret Thatcher.\n\nIn 2013, O'Sullivan became first the director and then president of the Danube Institute, a Budapest-based think tank, for which he is paid an annual salary of 150,000 Euros, indirectly financed by the Hungarian government. The Danube Institute exists to provide a centre of intellectual debate for conservatives and classical liberals and their democratic opponents in Central Europe. Based in Budapest and Washington, D. C., it seeks to engage with centre-right institutions, scholars, political parties and individuals of achievement across the region to discuss problems of mutual interest.\n\nConcurrently, in February 2015 O'Sullivan also became the editor of the Australian monthly magazine Quadrant.[4] In January 2017 he stepped down as editor and become the international editor.\n\nO'Sullivan has published articles in Encounter, Commentary, The New York Times, The Washington Post, Policy Review, The Times Literary Supplement, The American Spectator, The Spectator, The American Conservative, Quadrant, The Hibernian, the Hungarian Review[16] and other journals, and is the author of The President, the Pope, and the Prime Minister (Washington, D.C.: Regnery, 2006).[17][18]\n\nPhilosopher Roger Scruton praises O'Sullivan's book, which \"forcefully\" argues \"that the simultaneous presence in the highest offices of Reagan, Thatcher and Pope John Paul II was the cause of the Soviet collapse. And my own experience confirms this.\"[19]\n\nHe also lectures on British and American politics and is the Bruges Group's representative in Washington DC.\n\nHe is known for O'Sullivan's first law, or O'Sullivan's law, stating: \"All organizations that are not actually right-wing will over time become left-wing.\"[20] The law is sometimes (mistakenly) referred to as  Robert Conquest's second law of politics.[21][22]\n\nIn an article, O'Sullivan wrote: \"After all, radical Islamists have three advantages on their side: demography (the populations of Islamic nations are increasing while the West suffers a 'birth dearth'); rapidly growing Islamic diasporas in the West, fueled by illegal immigration; and official Western policies of multiculturalism (which not only encourage immigrants to retain their original cultural identity but even promote the 'de-assimilation' of previously assimilated minorities in the West)...the decline of Christian belief and social influence; and the habit of respecting other cultures as unities while treating the West as a kind of multi-cultural supermarket in which Western civilization is merely one rather dusty shelf. To these trends politicians add appeasement, both diplomatic (of neighboring North Africa) and electoral (of local Muslim constituencies)\".[23]\n\nOn July 18, 2005, O'Sullivan wrote an article titled, \"The Islamic Republic of Holland. How One Nation Deals with a Revolutionary Problem\".[24]\n\nIn a 2017 review, O'Sullivan says \"The new policy [encouraging migration] accelerated the transformation of Britain into a multicultural society with racial and religious tensions; terrorist murders, bombings, and beheadings; physical attacks on gays in East London; the extraordinary epidemic of the rape and sexual grooming of underage girls...hostile demonstrations against British soldiers returning from Afghanistan; an estimated (by the British Medical Association) 74,000 cases of female genital mutilation by 2006; the occasional honor killing; and excellent restaurants\".[25]\n\nO'Sullivan currently resides in Budapest with his wife Melissa.\n\nMedia related to John O'Sullivan at Wikimedia Commons",
        pageTitle: "John O'Sullivan (columnist)",
    },
    {
        title: "Parkinson's law",
        link: "https://en.wikipedia.org/wiki/Parkinson%27s_law",
        content:
            'Parkinson\'s law can refer to either of two observations, published in 1955 by the naval historian C. Northcote Parkinson as an essay in The Economist:[1]\n\nThe first paragraph of the essay mentioned the first meaning above as a "commonplace observation", and the rest of the essay was devoted to the latter observation, terming it "Parkinson\'s Law".\n\nThe first-referenced meaning of the law – "Work expands to fill the available time" – has sprouted several corollaries, the best known being the Stock-Sanford corollary to Parkinson\'s law:\n\nIf you wait until the last minute, it only takes a minute to do.[2]\n\nIn ten hours a day you have time to fall twice as far behind your commitments as in five hours a day.[3]\n\nas well as corollaries relating to computers, such as:\n\nData expands to fill the space available for storage.[4]\n\nThis was the main focus of the essay by Cyril Northcote Parkinson, published in The Economist in 1955,[1][5] and reprinted with other similar essays in the successful 1958 book Parkinson\'s Law: The Pursuit of Progress.[6] The book was translated into many languages. It was highly popular in the Soviet Union and its sphere of influence.[7] In 1986, Alessandro Natta complained about the swelling bureaucracy in Italy. Mikhail Gorbachev responded that "Parkinson\'s law works everywhere."[8]\n\nParkinson derived the dictum from his extensive experience in the British Civil Service. He gave, as examples, the growth in the size of the British Admiralty and Colonial Office even though the numbers of their ships and colonies were declining.\n\nMuch of the essay is dedicated to a summary of purportedly scientific observations supporting the law, such as the increase in the number of employees at the Colonial Office while the British Empire declined (he showed that it had its greatest number of staff when it was folded into the Foreign Office due to a lack of colonies to administer). He explained this growth using two forces: (1) "An official wants to multiply subordinates, not rivals", and (2) "Officials make work for each other." He noted that the number employed in a bureaucracy rose by 5–7% per year "irrespective of any variation in the amount of work (if any) to be done".\n\nParkinson presented the growth as a mathematical equation describing the rate at which bureaucracies expand over time, with the formula \n  \n    \n      \n        x\n        =\n        (\n        2\n        \n          k\n          \n            m\n          \n        \n        +\n        P\n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle x=(2k^{m}+P)/n}\n  \n, in which k was the number of officials wanting subordinates, m was the hours they spent writing minutes to each other.\n\nObserving that the promotion of employees necessitated the hiring of subordinates, and that time used answering minutes requires more work; Parkinson states: "In any public administrative department not actually at war the staff increase may be expected to follow this formula" (for a given year) [1]\n\nx\n        =\n        \n          \n            \n              2\n              \n                k\n                \n                  m\n                \n              \n              +\n              P\n            \n            n\n          \n        \n      \n    \n    {\\displaystyle x={\\frac {2k^{m}+P}{n}}}\n\nIn a different essay included in the book, Parkinson proposed a rule about the efficiency of administrative councils. He defined a "coefficient of inefficiency" with the number of members as the main determining variable. This is a semi-humorous attempt to define the size at which a committee or other decision-making body becomes completely inefficient.\n\nIn .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}Parkinson\'s Law: The Pursuit of Progress, London: John Murray, 1958 a chapter is devoted to the basic question of what he called comitology: how committees, government cabinets, and other such bodies are created and eventually grow irrelevant (or are initially designed as such). (The word comitology has recently been independently invented by the European Union for a different non-humorous meaning.)[9][10]\n\nEmpirical evidence is drawn from historical and contemporary government cabinets. Most often, the minimal size of a state\'s most powerful and prestigious body is five members. From English history, Parkinson notes a number of bodies that lost power as they grew:\n\nA detailed mathematical expression is proposed by Parkinson for the coefficient of inefficiency, featuring many possible influences. In 2008, an attempt was made to empirically verify the proposed model.[11] Parkinson\'s conjecture that membership exceeding a number "between 19.9 and 22.4" makes a committee manifestly inefficient seems well justified by the evidence proposed[citation needed]. Less certain is the optimal number of members, which must lie between three (a logical minimum) and 20. (Within a group of 20, individual discussions may occur, diluting the power of the leader.) That it may be eight seems arguable but is not supported by observation: no contemporary government in Parkinson\'s data set had eight members, and only king Charles I of England had a Committee of State of that size.',
        pageTitle: "Parkinson's law",
    },
    {
        title: "Parkinson's law of triviality",
        link: "https://en.wikipedia.org/wiki/Parkinson%27s_law_of_triviality",
        content:
            'The law of triviality is C. Northcote Parkinson\'s 1957 argument that people within an organization commonly give disproportionate weight to trivial issues.[1] Parkinson provides the example of a fictional committee whose job was to approve the plans for a nuclear power plant spending the majority of its time on discussions about relatively minor but easy-to-grasp issues, such as what materials to use for the staff bicycle shed, while neglecting the proposed design of the plant itself, which is far more important and a far more difficult and complex task.\n\nThe law has been applied to software development and other activities.[2] The terms bicycle-shed effect, bike-shed effect, and bike-shedding were coined based on Parkinson\'s example; it was popularized in the Berkeley Software Distribution community by the Danish software developer Poul-Henning Kamp in 1999[3] and, due to that, has since become popular within the field of software development generally.\n\nThe concept was first presented as a corollary of his broader "Parkinson\'s law" spoof of management. He dramatizes this "law of triviality" with the example of a committee\'s deliberations on an atomic reactor, contrasting it to deliberations on a bicycle shed. As he put it: "The time spent on any item of the agenda will be in inverse proportion to the sum [of money] involved." A reactor is so vastly expensive and complicated that an average person cannot understand it (see ambiguity aversion), so one assumes that those who work on it understand it. However, everyone can visualize a cheap, simple bicycle shed, so planning one can result in endless discussions because everyone involved wants to implement their own proposal and demonstrate personal contribution.[4]\n\nAfter a suggestion of building something new for the community, like a bike shed, problems arise when everyone involved argues about the details. This is a metaphor indicating that it is not necessary to argue about every little feature based simply on having the knowledge to do so. Some people have commented that the amount of noise generated by a change is inversely proportional to the complexity of the change.[3]\n\nBehavioral research has produced evidence which confirms theories proposed by the law of triviality.   People tend to spend more time on small decisions than they should, and less time on big decisions than they should. A simple explanation is that during the process of making a decision, one has to assess whether enough information has been collected to make the decision. If people make mistakes about whether they have enough information, then they will tend to feel overwhelmed by large and complex matters and stop collecting information too early to adequately inform their big decisions. The reason is that big decisions require collecting information for a long time and working hard to understand its complex ramifications. This leaves more of an opportunity to make a mistake (and stop) before getting enough information. Conversely, for small decisions, where people should devote little attention and act without hesitation, they may inefficiently continue to ponder for too long, partly because they are better able to understand the subject.[5]\n\nThere are several other principles, well known in specific problem domains, which express a similar sentiment.\n\nSayre\'s law is a more general principle, which holds (among other formulations) that "In any dispute, the intensity of feeling is inversely proportional to the value of the issues at stake"; many formulations of the principle focus on academia.\n\nWadler\'s law, named for computer scientist Philip Wadler,[6] is a principle which asserts that the bulk of discussion on programming-language design centers on syntax (which, for purposes of the argument, is considered a solved problem), as opposed to semantics.[7]',
        pageTitle: "Law of triviality",
    },
    {
        title: "Planck's law",
        link: "https://en.wikipedia.org/wiki/Planck%27s_law",
        content:
            "In physics, Planck's law (also Planck radiation law[1]: 1305 ) describes the spectral density of electromagnetic radiation emitted by a black body in thermal equilibrium at a given temperature T, when there is no net flow of matter or energy between the body and its environment.[2]\n\nAt the end of the 19th century, physicists were unable to explain why the observed spectrum of black-body radiation, which by then had been accurately measured, diverged significantly at higher frequencies from that predicted by existing theories. In 1900, German physicist Max Planck heuristically derived a formula for the observed spectrum by assuming that a hypothetical electrically charged oscillator in a cavity that contained black-body radiation could only change its energy in a minimal increment, E, that was proportional to the frequency of its associated electromagnetic wave. While Planck originally regarded the hypothesis of dividing energy into increments as a mathematical artifice, introduced merely to get the correct answer, other physicists including Albert Einstein built on his work, and Planck's insight is now recognized to be of fundamental importance to quantum theory.\n\nEvery physical body spontaneously and continuously emits electromagnetic radiation and the spectral radiance of a body, Bν, describes the spectral emissive power per unit area, per unit solid angle and per unit frequency for particular radiation frequencies. The relationship given by Planck's radiation law, given below, shows that with increasing temperature, the total radiated energy of a body increases and the peak of the emitted spectrum shifts to shorter wavelengths.[3] According to Planck's distribution law, the spectral energy density (energy per unit volume per unit frequency) at given temperature is given by:[4][5]\n  \n    \n      \n        \n          u\n          \n            ν\n          \n        \n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              8\n              π\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n        \n          \n            1\n            \n              exp\n              ⁡\n              \n                (\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          \n                            B\n                          \n                        \n                      \n                      T\n                    \n                  \n                \n                )\n              \n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle u_{\\nu }(\\nu ,T)={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}{\\frac {1}{\\exp \\left({\\frac {h\\nu }{k_{\\mathrm {B} }T}}\\right)-1}}}\n  \nalternatively, the law can be expressed for the spectral radiance of a body for frequency ν at absolute temperature T \ngiven as:[6][7][8]\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              exp\n              ⁡\n              \n                (\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          \n                            B\n                          \n                        \n                      \n                      T\n                    \n                  \n                \n                )\n              \n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle B_{\\nu }(\\nu ,T)={\\frac {2h\\nu ^{3}}{c^{2}}}{\\frac {1}{\\exp \\left({\\frac {h\\nu }{k_{\\mathrm {B} }T}}\\right)-1}}}\n  \nwhere kB is the Boltzmann constant, h is the Planck constant, and c is the speed of light in the medium, whether material or vacuum. The cgs units of spectral radiance Bν are erg·s−1·sr−1·cm−2·Hz−1. The terms B and u are related to each other by a factor of .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}⁠4π/c⁠ since B is independent of direction and radiation travels at speed c.\nThe spectral radiance can also be expressed per unit wavelength λ instead of per unit frequency. In addition, the law may be expressed in other terms, such as the number of photons emitted at a certain wavelength, or the energy density in a volume of radiation.\n\nIn the limit of low frequencies (i.e. long wavelengths), Planck's law tends to the Rayleigh–Jeans law, while in the limit of high frequencies (i.e. small wavelengths) it tends to the Wien approximation.\n\nMax Planck developed the law in 1900 with only empirically determined constants, and later showed that, expressed as an energy distribution, it is the unique stable distribution for radiation in thermodynamic equilibrium.[2] As an energy distribution, it is one of a family of thermal equilibrium distributions which include the Bose–Einstein distribution, the Fermi–Dirac distribution and the Maxwell–Boltzmann distribution.\n\nA black-body is an idealised object which absorbs and emits all radiation frequencies. Near thermodynamic equilibrium, the emitted radiation is closely described by Planck's law and because of its dependence on temperature, Planck radiation is said to be thermal radiation, such that the higher the temperature of a body the more radiation it emits at every wavelength.\n\nPlanck radiation has a maximum intensity at a wavelength that depends on the temperature of the body.  For example, at room temperature (~300 K), a body emits thermal radiation that is mostly infrared and invisible. At higher temperatures the amount of infrared radiation increases and can be felt as heat, and more visible radiation is emitted so the body glows visibly red. At higher temperatures, the body is bright yellow or blue-white and emits significant amounts of short wavelength radiation, including ultraviolet and even x-rays.  The surface of the Sun (~6000 K) emits large amounts of both infrared and ultraviolet radiation; its emission is peaked in the visible spectrum. This shift due to temperature is called Wien's displacement law.\n\nPlanck radiation is the greatest amount of radiation that any body at thermal equilibrium can emit from its surface, whatever its chemical composition or surface structure.[9] The passage of radiation across an interface between media can be characterized by the emissivity of the interface (the ratio of the actual radiance to the theoretical Planck radiance), usually denoted by the symbol ε. It is in general dependent on chemical composition and physical structure, on temperature, on the wavelength, on the angle of passage, and on the polarization.[10] The emissivity of a natural interface is always between ε = 0 and 1.\n\nA body that interfaces with another medium which both has ε = 1 and absorbs all the radiation incident upon it is said to be a black body. The surface of a black body can be modelled by a small hole in the wall of a large enclosure which is maintained at a uniform temperature with opaque walls that, at every wavelength, are not perfectly reflective. At equilibrium, the radiation inside this enclosure is described by Planck's law, as is the radiation leaving the small hole.\n\nJust as the Maxwell–Boltzmann distribution is the unique maximum entropy energy distribution for a gas of material particles at thermal equilibrium, so is Planck's distribution for a gas of photons.[11][12] By contrast to a material gas where the masses and number of particles play a role, the spectral radiance, pressure and energy density of a photon gas at thermal equilibrium are entirely determined by the temperature.\n\nIf the photon gas is not Planckian, the second law of thermodynamics guarantees that interactions (between photons and other particles or even, at sufficiently high temperatures, between the photons themselves) will cause the photon energy distribution to change and approach the Planck distribution.  In such an approach to thermodynamic equilibrium, photons are created or annihilated in the right numbers and with the right energies to fill the cavity with a Planck distribution until they reach the equilibrium temperature. It is as if the gas is a mixture of sub-gases, one for every band of wavelengths, and each sub-gas eventually attains the common temperature.\n\nThe quantity Bν(ν, T) is the spectral radiance as a function of temperature and frequency.  It has units of W·m−2·sr−1·Hz−1 in the SI system.  An infinitesimal amount of power Bν(ν, T) cos θ dA dΩ dν is radiated in the direction described by the angle θ from the surface normal from infinitesimal surface area dA into infinitesimal solid angle dΩ in an infinitesimal frequency band of width dν centered on frequency ν.  The total power radiated into any solid angle is the integral of Bν(ν, T) over those three quantities, and is given by the Stefan–Boltzmann law.  The spectral radiance of Planckian radiation from a black body has the same value for every direction and angle of polarization, and so the black body is said to be a Lambertian radiator.\n\nPlanck's law can be encountered in several forms depending on the conventions and preferences of different scientific fields. The various forms of the law for spectral radiance are summarized in the table below. Forms on the left are most often encountered in experimental fields, while those on the right are most often encountered in theoretical fields.\n\nIn the fractional bandwidth formulation, \n  \n    \n      \n        x\n        =\n        \n          \n            \n              h\n              ν\n            \n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n          \n        \n        =\n        \n          \n            \n              h\n              c\n            \n            \n              λ\n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n          \n        \n      \n    \n    {\\textstyle x={\\frac {h\\nu }{k_{\\mathrm {B} }T}}={\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}\n  \n, and the integration is with respect to \n  \n    \n      \n        \n          d\n        \n        (\n        ln\n        ⁡\n        x\n        )\n        =\n        \n          d\n        \n        (\n        ln\n        ⁡\n        ν\n        )\n        =\n        \n          \n            \n              \n                d\n              \n              ν\n            \n            ν\n          \n        \n        =\n        −\n        \n          \n            \n              \n                d\n              \n              λ\n            \n            λ\n          \n        \n        =\n        −\n        \n          d\n        \n        (\n        ln\n        ⁡\n        λ\n        )\n      \n    \n    {\\textstyle \\mathrm {d} (\\ln x)=\\mathrm {d} (\\ln \\nu )={\\frac {\\mathrm {d} \\nu }{\\nu }}=-{\\frac {\\mathrm {d} \\lambda }{\\lambda }}=-\\mathrm {d} (\\ln \\lambda )}\n  \n.\n\nPlanck's law can also be written in terms of the spectral energy density (u) by multiplying B by ⁠4π/c⁠:[17]\n  \n    \n      \n        \n          u\n          \n            i\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              4\n              π\n            \n            c\n          \n        \n        \n          B\n          \n            i\n          \n        \n        (\n        T\n        )\n        .\n      \n    \n    {\\displaystyle u_{i}(T)={\\frac {4\\pi }{c}}B_{i}(T).}\n\nThese distributions represent the spectral radiance of blackbodies—the power emitted from the emitting surface, per unit projected area of emitting surface, per unit solid angle, per spectral unit (frequency, wavelength, wavenumber or their angular equivalents, or fractional frequency or wavelength). Since the radiance is isotropic (i.e. independent of direction), the power emitted at an angle to the normal is proportional to the projected area, and therefore to the cosine of that angle as per Lambert's cosine law, and is unpolarized.\n\nDifferent spectral variables require different corresponding forms of expression of the law. In general, one may not convert between the various forms of Planck's law simply by substituting one variable for another, because this would not take into account that the different forms have different units. Wavelength and frequency units are reciprocal.\n\nCorresponding forms of expression are related because they express one and the same physical fact: for a particular physical spectral increment, a corresponding particular physical energy increment is radiated.\n\nThis is so whether it is expressed in terms of an increment of frequency, dν, or, correspondingly, of wavelength, dλ, or of fractional bandwidth, dν/ν or dλ/λ. Introduction of a minus sign can indicate that an increment of frequency corresponds with decrement of wavelength.\n\nIn order to convert the corresponding forms so that they express the same quantity in the same units we multiply by the spectral increment. Then, for a particular spectral increment, the particular physical energy increment may be written\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        λ\n        ,\n        T\n        )\n        \n        d\n        λ\n        =\n        −\n        \n          B\n          \n            ν\n          \n        \n        (\n        ν\n        (\n        λ\n        )\n        ,\n        T\n        )\n        \n        d\n        ν\n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(\\lambda ,T)\\,d\\lambda =-B_{\\nu }(\\nu (\\lambda ),T)\\,d\\nu ,}\n  \n\nwhich leads to\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        λ\n        ,\n        T\n        )\n        =\n        −\n        \n          \n            \n              d\n              ν\n            \n            \n              d\n              λ\n            \n          \n        \n        \n          B\n          \n            ν\n          \n        \n        (\n        ν\n        (\n        λ\n        )\n        ,\n        T\n        )\n        .\n      \n    \n    {\\displaystyle B_{\\lambda }(\\lambda ,T)=-{\\frac {d\\nu }{d\\lambda }}B_{\\nu }(\\nu (\\lambda ),T).}\n\nAlso, ν(λ) = ⁠c/λ⁠, so that ⁠dν/dλ⁠ = − ⁠c/λ2⁠.  Substitution gives the correspondence between the frequency and wavelength forms, with their different dimensions and units.[15][18]\nConsequently,\n\n  \n    \n      \n        \n          \n            \n              \n                B\n                \n                  λ\n                \n              \n              (\n              T\n              )\n            \n            \n              \n                B\n                \n                  ν\n                \n              \n              (\n              T\n              )\n            \n          \n        \n        =\n        \n          \n            c\n            \n              λ\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              ν\n              \n                2\n              \n            \n            c\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {B_{\\lambda }(T)}{B_{\\nu }(T)}}={\\frac {c}{\\lambda ^{2}}}={\\frac {\\nu ^{2}}{c}}.}\n\nEvidently, the location of the peak of the spectral distribution for Planck's law depends on the choice of spectral variable. Nevertheless, in a manner of speaking, this formula means that the shape of the spectral distribution is independent of temperature, according to Wien's displacement law, as detailed below in § Properties §§ Percentiles.\n\nThe fractional bandwidth form is related to the other forms by[16]\n\nIn the above variants of Planck's law, the wavelength and wavenumber variants use the terms 2hc2 and ⁠hc/kB⁠ which comprise physical constants only. Consequently, these terms can be considered as physical constants themselves,[19] and are therefore referred to as the first radiation constant c1L and the second radiation constant c2 with\n\nUsing the radiation constants, the wavelength variant of Planck's law can be simplified to\n\n  \n    \n      \n        L\n        (\n        λ\n        ,\n        T\n        )\n        =\n        \n          \n            \n              c\n              \n                1\n                L\n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              exp\n              ⁡\n              \n                (\n                \n                  \n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \n                      λ\n                      T\n                    \n                  \n                \n                )\n              \n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle L(\\lambda ,T)={\\frac {c_{1L}}{\\lambda ^{5}}}{\\frac {1}{\\exp \\left({\\frac {c_{2}}{\\lambda T}}\\right)-1}}}\n  \n\nand the wavenumber variant can be simplified correspondingly.\n\nL is used here instead of B because it is the SI symbol for spectral radiance. The L in c1L refers to that. This reference is necessary because Planck's law can be reformulated to give spectral radiant exitance M(λ, T) rather than spectral radiance L(λ, T), in which case c1 replaces c1L, with\n\nso that Planck's law for spectral radiant exitance can be written as\n\n  \n    \n      \n        M\n        (\n        λ\n        ,\n        T\n        )\n        =\n        \n          \n            \n              c\n              \n                1\n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              exp\n              ⁡\n              \n                (\n                \n                  \n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \n                      λ\n                      T\n                    \n                  \n                \n                )\n              \n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle M(\\lambda ,T)={\\frac {c_{1}}{\\lambda ^{5}}}{\\frac {1}{\\exp \\left({\\frac {c_{2}}{\\lambda T}}\\right)-1}}}\n\nAs measuring techniques have improved, the General Conference on Weights and Measures has revised its estimate of c2; see Planckian locus § International Temperature Scale for details.\n\nPlanck's law describes the unique and characteristic spectral distribution for electromagnetic radiation in thermodynamic equilibrium, when there is no net flow of matter or energy.[2] Its physics is most easily understood by considering the radiation in a cavity with rigid opaque walls. Motion of the walls can affect the radiation. If the walls are not opaque, then the thermodynamic equilibrium is not isolated. It is of interest to explain how the thermodynamic equilibrium is attained. There are two main cases: (a) when the approach to thermodynamic equilibrium is in the presence of matter, when the walls of the cavity are imperfectly reflective for every wavelength or when the walls are perfectly reflective while the cavity contains a small black body (this was the main case considered by Planck); or (b) when the approach to equilibrium is in the absence of matter, when the walls are perfectly reflective for all wavelengths and the cavity contains no matter. For matter not enclosed in such a cavity, thermal radiation can be approximately explained by appropriate use of Planck's law.\n\nClassical physics led, via the equipartition theorem, to the ultraviolet catastrophe, a prediction that the total blackbody radiation intensity was infinite. If supplemented by the classically unjustifiable assumption that for some reason the radiation is finite, classical thermodynamics provides an account of some aspects of the Planck distribution, such as the Stefan–Boltzmann law, and the Wien displacement law. For the case of the presence of matter, quantum mechanics provides a good account, as found below in the section headed Einstein coefficients. This was the case considered by Einstein, and is nowadays used for quantum optics.[20][21] For the case of the absence of matter, quantum field theory is necessary, because non-relativistic quantum mechanics with fixed particle numbers does not provide a sufficient account.\n\nQuantum theoretical explanation of Planck's law views the radiation as a gas of massless, uncharged, bosonic particles, namely photons, in thermodynamic equilibrium.  Photons are viewed as the carriers of the electromagnetic interaction between electrically charged elementary particles. Photon numbers are not conserved.  Photons are created or annihilated in the right numbers and with the right energies to fill the cavity with photons described by the Planck distribution.  For a photon gas in thermodynamic equilibrium, the internal energy density is entirely determined by the temperature; moreover, the pressure is entirely determined by the internal energy density. This is unlike the case of thermodynamic equilibrium for material gases, for which the internal energy is determined not only by the temperature, but also, independently, by the respective numbers of the different molecules, and independently again, by the specific characteristics of the different molecules. For different material gases at given temperature, the pressure and internal energy density can vary independently, because different molecules can carry independently different excitation energies.\n\nPlanck's law arises as a limit of the Bose–Einstein distribution, the energy distribution describing non-interactive bosons in thermodynamic equilibrium. In the case of massless bosons such as photons and gluons, the chemical potential is zero and the Bose–Einstein distribution reduces to the Planck distribution.  There is another fundamental equilibrium energy distribution: the Fermi–Dirac distribution, which describes fermions, such as electrons, in thermal equilibrium. The two distributions differ because multiple bosons can occupy the same quantum state, while multiple fermions cannot. At low densities, the number of available quantum states per particle is large, and this difference becomes irrelevant. In the low density limit, the Bose–Einstein and the Fermi–Dirac distribution each reduce to the Maxwell–Boltzmann distribution.\n\nKirchhoff's law of thermal radiation is a succinct and brief account of a complicated physical situation. The following is an introductory sketch of that situation, and is very far from being a rigorous physical argument. The purpose here is only to summarize the main physical factors in the situation, and the main conclusions.\n\nThere is a difference between conductive heat transfer and radiative heat transfer. Radiative heat transfer can be filtered to pass only a definite band of radiative frequencies.\n\nIt is generally known that the hotter a body becomes, the more heat it radiates at every frequency.\n\nIn a cavity in an opaque body with rigid walls that are not perfectly reflective at any frequency, in thermodynamic equilibrium, there is only one temperature, and it must be shared in common by the radiation of every frequency.\n\nOne may imagine two such cavities, each in its own isolated radiative and thermodynamic equilibrium. One may imagine an optical device that allows radiative heat transfer between the two cavities, filtered to pass only a definite band of radiative frequencies. If the values of the spectral radiances of the radiations in the cavities differ in that frequency band, heat may be expected to pass from the hotter to the colder. One might propose to use such a filtered transfer of heat in such a band to drive a heat engine. If the two bodies are at the same temperature, the second law of thermodynamics does not allow the heat engine to work. It may be inferred that for a temperature common to the two bodies, the values of the spectral radiances in the pass-band must also be common. This must hold for every frequency band.[22][23][24] This became clear to Balfour Stewart and later to Kirchhoff. Balfour Stewart found experimentally that of all surfaces, one of lamp-black emitted the greatest amount of thermal radiation for every quality of radiation, judged by various filters.\n\nThinking theoretically, Kirchhoff went a little further and pointed out that this implied that the spectral radiance, as a function of radiative frequency, of any such cavity in thermodynamic equilibrium must be a unique universal function of temperature. He postulated an ideal black body that interfaced with its surroundings in just such a way as to absorb all the radiation that falls on it. By the Helmholtz reciprocity principle, radiation from the interior of such a body would pass unimpeded directly to its surroundings without reflection at the interface. In thermodynamic equilibrium, the thermal radiation emitted from such a body would have that unique universal spectral radiance as a function of temperature. This insight is the root of Kirchhoff's law of thermal radiation.\n\nOne may imagine a small homogeneous spherical material body labeled X at a temperature TX, lying in a radiation field within a large cavity with walls of material labeled Y at a temperature TY. The body X emits its own thermal radiation. At a particular frequency ν, the radiation emitted from a particular cross-section through the centre of X in one sense in a direction normal to that cross-section may be denoted Iν,X(TX), characteristically for the material of X. At that frequency ν, the radiative power from the walls into that cross-section in the opposite sense in that direction may be denoted Iν,Y(TY), for the wall temperature TY. For the material of X, defining the absorptivity αν,X,Y(TX, TY) as the fraction of that incident radiation absorbed by X, that incident energy is absorbed at a rate αν,X,Y(TX, TY) Iν,Y(TY).\n\nThe rate q(ν,TX,TY) of accumulation of energy in one sense into the cross-section of the body can then be expressed\n\n  \n    \n      \n        q\n        (\n        ν\n        ,\n        \n          T\n          \n            X\n          \n        \n        ,\n        \n          T\n          \n            Y\n          \n        \n        )\n        =\n        \n          α\n          \n            ν\n            ,\n            X\n            ,\n            Y\n          \n        \n        (\n        \n          T\n          \n            X\n          \n        \n        ,\n        \n          T\n          \n            Y\n          \n        \n        )\n        \n          I\n          \n            ν\n            ,\n            Y\n          \n        \n        (\n        \n          T\n          \n            Y\n          \n        \n        )\n        −\n        \n          I\n          \n            ν\n            ,\n            X\n          \n        \n        (\n        \n          T\n          \n            X\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle q(\\nu ,T_{X},T_{Y})=\\alpha _{\\nu ,X,Y}(T_{X},T_{Y})I_{\\nu ,Y}(T_{Y})-I_{\\nu ,X}(T_{X}).}\n\nKirchhoff's seminal insight, mentioned just above, was that, at thermodynamic equilibrium at temperature T, there exists a unique universal radiative distribution, nowadays denoted Bν(T), that is independent of the chemical characteristics of the materials X and Y, that leads to a very valuable understanding of the radiative exchange equilibrium of any body at all, as follows.\n\nWhen there is thermodynamic equilibrium at temperature T, the cavity radiation from the walls has that unique universal value, so that Iν,Y(TY) = Bν(T). Further, one may define the emissivity εν,X(TX) of the material of the body X just so that at thermodynamic equilibrium at temperature TX = T, one has Iν,X(TX) = Iν,X(T) = εν,X(T) Bν(T).\n\nWhen thermal equilibrium prevails at temperature T = TX = TY, the rate of accumulation of energy vanishes so that q(ν,TX,TY) = 0. It follows that in thermodynamic equilibrium, when T = TX = TY,\n\n  \n    \n      \n        0\n        =\n        \n          α\n          \n            ν\n            ,\n            X\n            ,\n            Y\n          \n        \n        (\n        T\n        ,\n        T\n        )\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        −\n        \n          ϵ\n          \n            ν\n            ,\n            X\n          \n        \n        (\n        T\n        )\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        .\n      \n    \n    {\\displaystyle 0=\\alpha _{\\nu ,X,Y}(T,T)B_{\\nu }(T)-\\epsilon _{\\nu ,X}(T)B_{\\nu }(T).}\n\nKirchhoff pointed out that it follows that in thermodynamic equilibrium, when T = TX = TY,\n\n  \n    \n      \n        \n          α\n          \n            ν\n            ,\n            X\n            ,\n            Y\n          \n        \n        (\n        T\n        ,\n        T\n        )\n        =\n        \n          ϵ\n          \n            ν\n            ,\n            X\n          \n        \n        (\n        T\n        )\n        .\n      \n    \n    {\\displaystyle \\alpha _{\\nu ,X,Y}(T,T)=\\epsilon _{\\nu ,X}(T).}\n\nIntroducing the special notation αν,X(T) for the absorptivity of material X at thermodynamic equilibrium at temperature T (justified by a discovery of Einstein, as indicated below), one further has the equality\n\n  \n    \n      \n        \n          α\n          \n            ν\n            ,\n            X\n          \n        \n        (\n        T\n        )\n        =\n        \n          ϵ\n          \n            ν\n            ,\n            X\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle \\alpha _{\\nu ,X}(T)=\\epsilon _{\\nu ,X}(T)}\n  \n\nat thermodynamic equilibrium.\n\nThe equality of absorptivity and emissivity here demonstrated is specific for thermodynamic equilibrium at temperature T and is in general not to be expected to hold when conditions of thermodynamic equilibrium do not hold. The emissivity and absorptivity are each separately properties of the molecules of the material but they depend differently upon the distributions of states of molecular excitation on the occasion, because of a phenomenon known as \"stimulated emission\", that was discovered by Einstein. On occasions when the material is in thermodynamic equilibrium or in a state known as local thermodynamic equilibrium, the emissivity and absorptivity become equal. Very strong incident radiation or other factors can disrupt thermodynamic equilibrium or local thermodynamic equilibrium. Local thermodynamic equilibrium in a gas means that molecular collisions far outweigh light emission and absorption in determining the distributions of states of molecular excitation.\n\nKirchhoff pointed out that he did not know the precise character of  Bν(T), but he thought it important that it should be found out. Four decades after Kirchhoff's insight of the general principles of its existence and character, Planck's contribution was to determine the precise mathematical expression of that equilibrium distribution Bν(T).\n\nIn physics, one considers an ideal black body, here labeled B, defined as one that completely absorbs all of the electromagnetic radiation falling upon it at every frequency ν (hence the term \"black\"). According to Kirchhoff's law of thermal radiation, this entails that, for every frequency ν, at thermodynamic equilibrium at temperature T, one has αν,B(T) = εν,B(T) = 1, so that the thermal radiation from a black body is always equal to the full amount specified by Planck's law. No physical body can emit thermal radiation that exceeds that of a black body, since if it were in equilibrium with a radiation field, it would be emitting more energy than was incident upon it.\n\nThough perfectly black materials do not exist, in practice a black surface can be accurately approximated.[2] As to its material interior, a body of condensed matter, liquid, solid, or plasma, with a definite interface with its surroundings, is completely black to radiation if it is completely opaque. That means that it absorbs all of the radiation that penetrates the interface of the body with its surroundings, and enters the body. This is not too difficult to achieve in practice. On the other hand, a perfectly black interface is not found in nature. A perfectly black interface reflects no radiation, but transmits all that falls on it, from either side. The best practical way to make an effectively black interface is to simulate an 'interface' by a small hole in the wall of a large cavity in a completely opaque rigid body of material that does not reflect perfectly at any frequency, with its walls at a controlled temperature. Beyond these requirements, the component material of the walls is unrestricted. Radiation entering the hole has almost no possibility of escaping the cavity without being absorbed by multiple impacts with its walls.[25]\n\nAs explained by Planck,[26] a radiating body has an interior consisting of matter, and an interface with its contiguous neighbouring material medium, which is usually the medium from within which the radiation from the surface of the body is observed. The interface is not composed of physical matter but is a theoretical conception, a mathematical two-dimensional surface, a joint property of the two contiguous media, strictly speaking belonging to neither separately. Such an interface can neither absorb nor emit, because it is not composed of physical matter; but it is the site of reflection and transmission of radiation, because it is a surface of discontinuity of optical properties. The reflection and transmission of radiation at the interface obey the Stokes–Helmholtz reciprocity principle.\n\nAt any point in the interior of a black body located inside a cavity in thermodynamic equilibrium at temperature T the radiation is homogeneous, isotropic and unpolarized. A black body absorbs all and reflects none of the electromagnetic radiation incident upon it. According to the Helmholtz reciprocity principle, radiation from the interior of a black body is not reflected at its surface, but is fully transmitted to its exterior. Because of the isotropy of the radiation in the body's interior, the spectral radiance of radiation transmitted from its interior to its exterior through its surface is independent of direction.[27]\n\nThis is expressed by saying that radiation from the surface of a black body in thermodynamic equilibrium obeys Lambert's cosine law.[28][29] This means that the spectral flux dΦ(dA, θ, dΩ, dν) from a given infinitesimal element of area dA of the actual emitting surface of the black body, detected from a given direction that makes an angle θ with the normal to the actual emitting surface at dA, into an element of solid angle of detection dΩ centred on the direction indicated by θ, in an element of frequency bandwidth dν, can be represented as[30]\n\n  \n    \n      \n        \n          \n            \n              d\n              Φ\n              (\n              d\n              A\n              ,\n              θ\n              ,\n              d\n              Ω\n              ,\n              d\n              ν\n              )\n            \n            \n              d\n              Ω\n            \n          \n        \n        =\n        \n          L\n          \n            0\n          \n        \n        (\n        d\n        A\n        ,\n        d\n        ν\n        )\n        \n        d\n        A\n        \n        d\n        ν\n        \n        cos\n        ⁡\n        θ\n      \n    \n    {\\displaystyle {\\frac {d\\Phi (dA,\\theta ,d\\Omega ,d\\nu )}{d\\Omega }}=L^{0}(dA,d\\nu )\\,dA\\,d\\nu \\,\\cos \\theta }\n  \n\nwhere L0(dA, dν) denotes the flux, per unit area per unit frequency per unit solid angle, that area dA would show if it were measured in its normal direction θ = 0.\n\nThe factor cos θ is present because the area to which the spectral radiance refers directly is the projection, of the actual emitting surface area, onto a plane perpendicular to the direction indicated by θ . This is the reason for the name cosine law.\n\nTaking into account the independence of direction of the spectral radiance of radiation from the surface of a black body in thermodynamic equilibrium, one has L0(dA, dν) = Bν(T) and so\n\n  \n    \n      \n        \n          \n            \n              d\n              Φ\n              (\n              d\n              A\n              ,\n              θ\n              ,\n              d\n              Ω\n              ,\n              d\n              ν\n              )\n            \n            \n              d\n              Ω\n            \n          \n        \n        =\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        \n        d\n        A\n        \n        d\n        ν\n        \n        cos\n        ⁡\n        θ\n        .\n      \n    \n    {\\displaystyle {\\frac {d\\Phi (dA,\\theta ,d\\Omega ,d\\nu )}{d\\Omega }}=B_{\\nu }(T)\\,dA\\,d\\nu \\,\\cos \\theta .}\n\nThus Lambert's cosine law expresses the independence of direction of the spectral radiance Bν (T) of the surface of a black body in thermodynamic equilibrium.\n\nThe total power emitted per unit area at the surface of a black body (P) may be found by integrating the black body spectral flux found from Lambert's law over all frequencies, and over the solid angles corresponding to a hemisphere (h) above the surface.\n\n  \n    \n      \n        P\n        =\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        d\n        ν\n        \n          ∫\n          \n            h\n          \n        \n        d\n        Ω\n        \n        \n          B\n          \n            ν\n          \n        \n        cos\n        ⁡\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle P=\\int _{0}^{\\infty }d\\nu \\int _{h}d\\Omega \\,B_{\\nu }\\cos(\\theta )}\n\nThe infinitesimal solid angle can be expressed in spherical polar coordinates:\n\n  \n    \n      \n        d\n        Ω\n        =\n        sin\n        ⁡\n        (\n        θ\n        )\n        \n        d\n        θ\n        \n        d\n        ϕ\n        .\n      \n    \n    {\\displaystyle d\\Omega =\\sin(\\theta )\\,d\\theta \\,d\\phi .}\n  \n\nSo that:\n\n  \n    \n      \n        P\n        =\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        d\n        ν\n        \n          ∫\n          \n            0\n          \n          \n            \n              \n                π\n                2\n              \n            \n          \n        \n        d\n        θ\n        \n          ∫\n          \n            0\n          \n          \n            2\n            π\n          \n        \n        d\n        ϕ\n        \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        cos\n        ⁡\n        (\n        θ\n        )\n        sin\n        ⁡\n        (\n        θ\n        )\n        =\n        σ\n        \n          T\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle P=\\int _{0}^{\\infty }d\\nu \\int _{0}^{\\tfrac {\\pi }{2}}d\\theta \\int _{0}^{2\\pi }d\\phi \\,B_{\\nu }(T)\\cos(\\theta )\\sin(\\theta )=\\sigma T^{4}}\n  \n\nwhere \n  \n    \n      \n        σ\n        =\n        \n          \n            \n              2\n              \n                k\n                \n                  \n                    B\n                  \n                \n                \n                  4\n                \n              \n              \n                π\n                \n                  5\n                \n              \n            \n            \n              15\n              \n                c\n                \n                  2\n                \n              \n              \n                h\n                \n                  3\n                \n              \n            \n          \n        \n        ≈\n        5.670400\n        ×\n        \n          10\n          \n            −\n            8\n          \n        \n        \n        \n          J\n          \n          \n            s\n            \n              −\n              1\n            \n          \n          \n            m\n            \n              −\n              2\n            \n          \n          \n            K\n            \n              −\n              4\n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\frac {2k_{\\mathrm {B} }^{4}\\pi ^{5}}{15c^{2}h^{3}}}\\approx 5.670400\\times 10^{-8}\\,\\mathrm {J\\,s^{-1}m^{-2}K^{-4}} }\n  \n is known as the Stefan–Boltzmann constant.[31]\n\nThe equation of radiative transfer describes the way in which radiation is affected as it travels through a material medium. For the special case in which the material medium is in thermodynamic equilibrium in the neighborhood of a point in the medium, Planck's law is of special importance.\n\nFor simplicity, we can consider the linear steady state, without scattering. The equation of radiative transfer states that for a beam of light going through a small distance ds, energy is conserved: The change in the (spectral) radiance of that beam (Iν) is equal to the amount removed by the material medium plus the amount gained from the material medium. If the radiation field is in equilibrium with the material medium, these two contributions will be equal. The material medium will have a certain emission coefficient and absorption coefficient.\n\nThe absorption coefficient α is the fractional change in the intensity of the light beam as it travels the distance ds, and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density ρ of the material, we may define a \"mass absorption coefficient\" κν = ⁠α/ρ⁠ which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance ds will then be[7]\n\n  \n    \n      \n        d\n        \n          I\n          \n            ν\n          \n        \n        =\n        −\n        \n          κ\n          \n            ν\n          \n        \n        ρ\n        \n          I\n          \n            ν\n          \n        \n        \n        d\n        s\n      \n    \n    {\\displaystyle dI_{\\nu }=-\\kappa _{\\nu }\\rho I_{\\nu }\\,ds}\n\nThe \"mass emission coefficient\" jν is equal to the radiance per unit volume of a small volume element divided by its mass (since, as for the mass absorption coefficient, the emission is proportional to the emitting mass) and has units of power⋅solid angle−1⋅frequency−1⋅density−1. Like the mass absorption coefficient, it too is a property of the material itself. The change in a light beam as it traverses a small distance ds will then be[32]\n\n  \n    \n      \n        d\n        \n          I\n          \n            ν\n          \n        \n        =\n        \n          j\n          \n            ν\n          \n        \n        ρ\n        \n        d\n        s\n      \n    \n    {\\displaystyle dI_{\\nu }=j_{\\nu }\\rho \\,ds}\n\nThe equation of radiative transfer will then be the sum of these two contributions:[33]\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                I\n                \n                  ν\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        =\n        \n          j\n          \n            ν\n          \n        \n        ρ\n        −\n        \n          κ\n          \n            ν\n          \n        \n        ρ\n        \n          I\n          \n            ν\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dI_{\\nu }}{ds}}=j_{\\nu }\\rho -\\kappa _{\\nu }\\rho I_{\\nu }.}\n\nIf the radiation field is in equilibrium with the material medium, then the radiation will be homogeneous (independent of position) so that dIν = 0 and:\n\n  \n    \n      \n        \n          κ\n          \n            ν\n          \n        \n        \n          B\n          \n            ν\n          \n        \n        =\n        \n          j\n          \n            ν\n          \n        \n      \n    \n    {\\displaystyle \\kappa _{\\nu }B_{\\nu }=j_{\\nu }}\n  \n\nwhich is another statement of Kirchhoff's law, relating two material properties of the medium, and which yields the radiative transfer equation at a point around which the medium is in thermodynamic equilibrium:\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                I\n                \n                  ν\n                \n              \n            \n            \n              d\n              s\n            \n          \n        \n        =\n        \n          κ\n          \n            ν\n          \n        \n        ρ\n        (\n        \n          B\n          \n            ν\n          \n        \n        −\n        \n          I\n          \n            ν\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {dI_{\\nu }}{ds}}=\\kappa _{\\nu }\\rho (B_{\\nu }-I_{\\nu }).}\n\nThe principle of detailed balance states that, at thermodynamic equilibrium, each elementary process is in equilibrium with its reverse process.\n\nIn 1916, Albert Einstein applied this principle on an atomic level to the case of an atom radiating and absorbing radiation due to transitions between two particular energy levels,[34] giving a deeper insight into the equation of radiative transfer and Kirchhoff's law for this type of radiation. If level 1 is the lower energy level with energy E1, and level 2 is the upper energy level with energy E2, then the frequency ν of the radiation radiated or absorbed will be determined by Bohr's frequency condition:[35][36]\n\n  \n    \n      \n        \n          E\n          \n            2\n          \n        \n        −\n        \n          E\n          \n            1\n          \n        \n        =\n        h\n        ν\n        .\n      \n    \n    {\\displaystyle E_{2}-E_{1}=h\\nu .}\n\nIf n1 and n2 are the number densities of the atom in states 1 and 2 respectively, then the rate of change of these densities in time will be due to three processes:\n\nwhere uν is the spectral energy density of the radiation field. The three parameters A21, B21 and B12, known as the Einstein coefficients, are associated with the photon frequency ν produced by the transition between two energy levels (states). As a result, each line in a spectrum has its own set of associated coefficients. When the atoms and the radiation field are in equilibrium, the radiance will be given by Planck's law and, by the principle of detailed balance, the sum of these rates must be zero:\n\n  \n    \n      \n        0\n        =\n        \n          A\n          \n            21\n          \n        \n        \n          n\n          \n            2\n          \n        \n        +\n        \n          B\n          \n            21\n          \n        \n        \n          n\n          \n            2\n          \n        \n        \n          \n            \n              4\n              π\n            \n            c\n          \n        \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        −\n        \n          B\n          \n            12\n          \n        \n        \n          n\n          \n            1\n          \n        \n        \n          \n            \n              4\n              π\n            \n            c\n          \n        \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle 0=A_{21}n_{2}+B_{21}n_{2}{\\frac {4\\pi }{c}}B_{\\nu }(T)-B_{12}n_{1}{\\frac {4\\pi }{c}}B_{\\nu }(T)}\n\nSince the atoms are also in equilibrium, the populations of the two levels are related by the Boltzmann factor:\n\n  \n    \n      \n        \n          \n            \n              n\n              \n                2\n              \n            \n            \n              n\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              g\n              \n                2\n              \n            \n            \n              g\n              \n                1\n              \n            \n          \n        \n        \n          e\n          \n            −\n            h\n            ν\n            \n              /\n            \n            \n              k\n              \n                \n                  B\n                \n              \n            \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\frac {n_{2}}{n_{1}}}={\\frac {g_{2}}{g_{1}}}e^{-h\\nu /k_{\\mathrm {B} }T}}\n  \n\nwhere g1 and g2 are the multiplicities of the respective energy levels. Combining the above two equations with the requirement that they be valid at any temperature yields two relationships between the Einstein coefficients:\n\n  \n    \n      \n        \n          \n            \n              A\n              \n                21\n              \n            \n            \n              B\n              \n                21\n              \n            \n          \n        \n        =\n        \n          \n            \n              8\n              π\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {A_{21}}{B_{21}}}={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}}\n  \n\n\n  \n    \n      \n        \n          \n            \n              B\n              \n                21\n              \n            \n            \n              B\n              \n                12\n              \n            \n          \n        \n        =\n        \n          \n            \n              g\n              \n                1\n              \n            \n            \n              g\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {B_{21}}{B_{12}}}={\\frac {g_{1}}{g_{2}}}}\n  \n\nso that knowledge of one coefficient will yield the other two.\n\nFor the case of isotropic absorption and emission, the emission coefficient (jν) and absorption coefficient (κν) defined in the radiative transfer section above, can be expressed in terms of the Einstein coefficients. The relationships between the Einstein coefficients will yield the expression of Kirchhoff's law expressed in the Radiative transfer section above, namely that\n  \n    \n      \n        \n          j\n          \n            ν\n          \n        \n        =\n        \n          κ\n          \n            ν\n          \n        \n        \n          B\n          \n            ν\n          \n        \n        .\n      \n    \n    {\\displaystyle j_{\\nu }=\\kappa _{\\nu }B_{\\nu }.}\n\nThese coefficients apply to both atoms and molecules.\n\nThe distributions Bν, Bω, Bν̃ and Bk peak at a photon energy of[37]\n  \n    \n      \n        E\n        =\n        \n          [\n          \n            3\n            +\n            W\n            \n              (\n              \n                −\n                3\n                \n                  e\n                  \n                    −\n                    3\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ≈\n        2.821\n         \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ,\n      \n    \n    {\\displaystyle E=\\left[3+W\\left(-3e^{-3}\\right)\\right]k_{\\mathrm {B} }T\\approx 2.821\\ k_{\\mathrm {B} }T,}\n  \nwhere W is the Lambert W function and e is Euler's number.\n\nHowever, the distribution Bλ peaks at a different energy[37]\n  \n    \n      \n        E\n        =\n        \n          [\n          \n            5\n            +\n            W\n            \n              (\n              \n                −\n                5\n                \n                  e\n                  \n                    −\n                    5\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ≈\n        4.965\n         \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ,\n      \n    \n    {\\displaystyle E=\\left[5+W\\left(-5e^{-5}\\right)\\right]k_{\\mathrm {B} }T\\approx 4.965\\ k_{\\mathrm {B} }T,}\n  \nThe reason for this is that, as mentioned above, one cannot go from (for example) Bν to Bλ simply by substituting ν by λ. In addition, one must also multiply by \n  \n    \n      \n        \n          |\n          \n            \n              d\n              ν\n            \n            \n              /\n            \n            \n              d\n              λ\n            \n          \n          |\n        \n        =\n        c\n        \n          /\n        \n        \n          \n            λ\n            \n              2\n            \n          \n        \n      \n    \n    {\\textstyle \\left|{d\\nu }/{d\\lambda }\\right|=c/{\\lambda ^{2}}}\n  \n, which shifts the peak of the distribution to higher energies. These peaks are the mode energy of a photon, when binned using equal-size bins of frequency or wavelength, respectively. Dividing hc (14387.770 μm·K) by these energy expression gives the wavelength of the peak.\n\nwith \n  \n    \n      \n        x\n        =\n        3\n        +\n        W\n        (\n        −\n        3\n        \n          e\n          \n            −\n            3\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle x=3+W(-3e^{-3}),}\n  \n and\n  \n    \n      \n        \n          \n            \n              \n                \n                  B\n                  \n                    λ\n                    ,\n                    \n                      max\n                    \n                  \n                \n                (\n                T\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      2\n                      \n                        k\n                        \n                          \n                            B\n                          \n                        \n                        \n                          5\n                        \n                      \n                      \n                        T\n                        \n                          5\n                        \n                      \n                      \n                        x\n                        \n                          5\n                        \n                      \n                    \n                    \n                      \n                        h\n                        \n                          4\n                        \n                      \n                      \n                        c\n                        \n                          3\n                        \n                      \n                    \n                  \n                \n                \n                  \n                    1\n                    \n                      \n                        e\n                        \n                          x\n                        \n                      \n                      −\n                      1\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                ≈\n                4.096\n                ×\n                \n                  10\n                  \n                    −\n                    6\n                  \n                \n                \n                  \n                    W\n                    \n                      \n                        \n                          m\n                        \n                        \n                          2\n                        \n                      \n                      ⋅\n                      \n                        sr\n                      \n                    \n                  \n                \n                ×\n                 \n                (\n                T\n                \n                  /\n                \n                \n                  K\n                \n                \n                  )\n                  \n                    5\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}B_{\\lambda ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{5}T^{5}x^{5}}{h^{4}c^{3}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 4.096\\times 10^{-6}{\\frac {\\text{W}}{{\\text{m}}^{2}\\cdot {\\text{sr}}}}\\times ~(T/{\\text{K}})^{5}\\end{aligned}}}\n  \nwith \n  \n    \n      \n        x\n        =\n        5\n        +\n        W\n        (\n        −\n        5\n        \n          e\n          \n            −\n            5\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle x=5+W(-5e^{-5}).}\n\nMeanwhile, the average energy of a photon from a blackbody is\n  \n    \n      \n        E\n        =\n        \n          [\n          \n            \n              \n                π\n                \n                  4\n                \n              \n              \n                30\n                 \n                ζ\n                (\n                3\n                )\n              \n            \n          \n          ]\n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ≈\n        2.701\n         \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n        ,\n      \n    \n    {\\displaystyle E=\\left[{\\frac {\\pi ^{4}}{30\\ \\zeta (3)}}\\right]k_{\\mathrm {B} }T\\approx 2.701\\ k_{\\mathrm {B} }T,}\n  \nwhere \n  \n    \n      \n        ζ\n      \n    \n    {\\displaystyle \\zeta }\n  \n is the Riemann zeta function.\n\nIn the limit of low frequencies (i.e. long wavelengths), Planck's law becomes the Rayleigh–Jeans law[38][39][40]\n\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        ≈\n        \n          \n            \n              2\n              \n                ν\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle B_{\\nu }(T)\\approx {\\frac {2\\nu ^{2}}{c^{2}}}k_{\\mathrm {B} }T}\n  \n\nor\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        ≈\n        \n          \n            \n              2\n              c\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n        T\n      \n    \n    {\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2c}{\\lambda ^{4}}}k_{\\mathrm {B} }T}\n\nThe radiance increases as the square of the frequency, illustrating the ultraviolet catastrophe. In the limit of high frequencies (i.e. small wavelengths) Planck's law tends to the Wien approximation:[40][41][42]\n\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        ≈\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  h\n                  ν\n                \n                \n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  T\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle B_{\\nu }(T)\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}e^{-{\\frac {h\\nu }{k_{\\mathrm {B} }T}}}}\n  \n or \n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        ≈\n        \n          \n            \n              2\n              h\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  h\n                  c\n                \n                \n                  λ\n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  T\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2hc^{2}}{\\lambda ^{5}}}e^{-{\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}.}\n\nWien's displacement law in its stronger form states that the shape of Planck's law is independent of temperature. It is therefore possible to list the percentile points of the total radiation as well as the peaks for wavelength and frequency, in a form which gives the wavelength λ when divided by temperature T.[43] The second column of the following table lists the corresponding values of λT, that is, those values of x for which the wavelength λ is ⁠x/T⁠ micrometers at the radiance percentile point given by the corresponding entry in the first column.\n\nThat is, 0.01% of the radiation is at a wavelength below ⁠910/T⁠ μm, 20% below ⁠2676/T⁠ μm, etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions ⁠1/λ5⁠, ν3 and ⁠ν2/λ2⁠, respectively, divided by exp(⁠hν/kBT⁠) − 1 attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.\n\nWhich peak to use depends on the application. The conventional choice is the wavelength peak at 25.0% given by Wien's displacement law in its weak form. For some purposes the median or 50% point dividing the total radiation into two-halves may be more suitable. The latter is closer to the frequency peak than to the wavelength peak because the radiance drops exponentially at short wavelengths and only polynomially at long. The neutral peak occurs at a shorter wavelength than the median for the same reason.\n\nSolar radiation can be compared to black-body radiation at about 5778 K (but see graph). The table on the right shows how the radiation of a black body at this temperature is partitioned, and also how sunlight is partitioned for comparison. Also for comparison a planet modeled as a black body is shown, radiating at a nominal 288 K (15 °C) as a representative value of the Earth's highly variable temperature. Its wavelengths are more than twenty times that of the Sun, tabulated in the third column in micrometers (thousands of nanometers).\n\nThat is, only 1% of the Sun's radiation is at wavelengths shorter than 296 nm, and only 1% at longer than 3728 nm. Expressed in micrometers this puts 98% of the Sun's radiation in the range from 0.296 to 3.728 μm. The corresponding 98% of energy radiated from a 288 K planet is from 5.03 to 79.5 μm, well above the range of solar radiation (or below if expressed in terms of frequencies ν = ⁠c/λ⁠ instead of wavelengths λ).\n\nA consequence of this more-than-order-of-magnitude difference in wavelength between solar and planetary radiation is that filters designed to pass one and block the other are easy to construct. For example, windows fabricated of ordinary glass or transparent plastic pass at least 80% of the incoming 5778 K solar radiation, which is below 1.2 μm in wavelength, while blocking over 99% of the outgoing 288 K thermal radiation from 5 μm upwards, wavelengths at which most kinds of glass and plastic of construction-grade thickness are effectively opaque.\n\nThe Sun's radiation is that arriving at the top of the atmosphere (TOA). As can be read from the table, radiation below 400 nm, or ultraviolet, is about 8%, while that above 700 nm, or infrared, starts at about the 48% point and so accounts for 52% of the total. Hence only 40% of the TOA insolation is visible to the human eye. The atmosphere shifts these percentages substantially in favor of visible light as it absorbs most of the ultraviolet and significant amounts of infrared.\n\nConsider a cube of side L with conducting walls filled with electromagnetic radiation in thermal equilibrium at temperature T. If there is a small hole in one of the walls, the radiation emitted from the hole will be characteristic of a perfect black body. We will first calculate the spectral energy density within the cavity and then determine the spectral radiance of the emitted radiation.\n\nAt the walls of the cube, the parallel component of the electric field and the orthogonal component of the magnetic field must vanish. Analogous to the wave function of a particle in a box, one finds that the fields are superpositions of periodic functions. The three wavelengths λ1, λ2, and λ3, in the three directions orthogonal to the walls can be:\n  \n    \n      \n        \n          λ\n          \n            i\n          \n        \n        =\n        \n          \n            \n              2\n              L\n            \n            \n              n\n              \n                i\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\lambda _{i}={\\frac {2L}{n_{i}}},}\n  \nwhere the ni are positive integers. For each set of integers ni there are two linearly independent solutions (known as modes). The two modes for each set of these ni correspond to the two polarization states of the photon which has a spin of 1.  According to quantum theory, the total energy of a mode is given by:\n\nThe number r can be interpreted as the number of photons in the mode.  For r = 0 the energy of the mode is not zero. This vacuum energy of the electromagnetic field is responsible for the Casimir effect. In the following we will calculate the internal energy of the box at absolute temperature T.\n\nAccording to statistical mechanics, the equilibrium probability distribution over the energy levels of a particular mode is given by:\n  \n    \n      \n        \n          P\n          \n            r\n          \n        \n        =\n        \n          \n            \n              e\n              \n                −\n                β\n                E\n                \n                  (\n                  r\n                  )\n                \n              \n            \n            \n              Z\n              \n                (\n                β\n                )\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P_{r}={\\frac {e^{-\\beta E\\left(r\\right)}}{Z\\left(\\beta \\right)}}.}\n  \nwhere we use the reciprocal temperature\n  \n    \n      \n        β\n         \n        \n          \n            \n              \n                =\n              \n              \n                \n                  d\n                  e\n                  f\n                \n              \n            \n          \n        \n         \n        \n          \n            1\n            \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\beta \\ {\\stackrel {\\mathrm {def} }{=}}\\ {\\frac {1}{k_{\\mathrm {B} }T}}.}\n  \nThe denominator Z(β), is the partition function of a single mode. It makes Pr properly normalized, and can be evaluated as\n  \n    \n      \n        Z\n        \n          (\n          β\n          )\n        \n        =\n        \n          ∑\n          \n            r\n            =\n            0\n          \n          \n            ∞\n          \n        \n        \n          e\n          \n            −\n            β\n            E\n            \n              (\n              r\n              )\n            \n          \n        \n        =\n        \n          \n            \n              e\n              \n                −\n                β\n                ε\n                \n                  /\n                \n                2\n              \n            \n            \n              1\n              −\n              \n                e\n                \n                  −\n                  β\n                  ε\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle Z\\left(\\beta \\right)=\\sum _{r=0}^{\\infty }e^{-\\beta E\\left(r\\right)}={\\frac {e^{-\\beta \\varepsilon /2}}{1-e^{-\\beta \\varepsilon }}},}\n  \nwith\n\nbeing the energy of a single photon. The average energy in a mode can be obtained from the partition function:\n  \n    \n      \n        \n          ⟨\n          E\n          ⟩\n        \n        =\n        −\n        \n          \n            \n              d\n              log\n              ⁡\n              \n                (\n                Z\n                )\n              \n            \n            \n              d\n              β\n            \n          \n        \n        =\n        \n          \n            ε\n            2\n          \n        \n        +\n        \n          \n            ε\n            \n              \n                e\n                \n                  β\n                  ε\n                \n              \n              −\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\left\\langle E\\right\\rangle =-{\\frac {d\\log \\left(Z\\right)}{d\\beta }}={\\frac {\\varepsilon }{2}}+{\\frac {\\varepsilon }{e^{\\beta \\varepsilon }-1}}.}\n  \nThis formula, apart from the first vacuum energy term, is a special case of the general formula for particles obeying Bose–Einstein statistics. Since there is no restriction on the total number of photons, the chemical potential is zero.\n\nIf we measure the energy relative to the ground state, the total energy in the box follows by summing ⟨E⟩ − ⁠ε/2⁠ over all allowed single photon states. This can be done exactly in the thermodynamic limit as L approaches infinity. In this limit, ε becomes continuous and we can then integrate ⟨E⟩ − ⁠ε/2⁠ over this parameter. To calculate the energy in the box in this way, we need to evaluate how many photon states there are in a given energy range. If we write the total number of single photon states with energies between ε and ε + dε as g(ε) dε, where g(ε) is the density of states (which is evaluated below), then the total energy is given by\n\nTo calculate the density of states we rewrite equation (2) as follows:\n  \n    \n      \n        ε\n         \n        \n          =\n        \n         \n        \n          \n            \n              h\n              c\n            \n            \n              2\n              L\n            \n          \n        \n        n\n        ,\n      \n    \n    {\\displaystyle \\varepsilon \\ {=}\\ {\\frac {hc}{2L}}n,}\n  \nwhere n is the norm of the vector n = (n1, n2, n3).\n\nFor every vector n with integer components larger than or equal to zero, there are two photon states. This means that the number of photon states in a certain region of n-space is twice the volume of that region. An energy range of dε corresponds to shell of thickness dn = ⁠2L/hc⁠ dε in n-space. Because the components of n have to be positive, this shell spans an octant of a sphere. The number of photon states g(ε) dε, in an energy range dε, is thus given by:\n  \n    \n      \n        g\n        (\n        ε\n        )\n        \n        d\n        ε\n        =\n        2\n        \n          \n            1\n            8\n          \n        \n        4\n        π\n        \n          n\n          \n            2\n          \n        \n        \n        d\n        n\n        =\n        \n          \n            \n              8\n              π\n              \n                L\n                \n                  3\n                \n              \n            \n            \n              \n                h\n                \n                  3\n                \n              \n              \n                c\n                \n                  3\n                \n              \n            \n          \n        \n        \n          ε\n          \n            2\n          \n        \n        \n        d\n        ε\n        .\n      \n    \n    {\\displaystyle g(\\varepsilon )\\,d\\varepsilon =2{\\frac {1}{8}}4\\pi n^{2}\\,dn={\\frac {8\\pi L^{3}}{h^{3}c^{3}}}\\varepsilon ^{2}\\,d\\varepsilon .}\n  \nInserting this in Eq. (3) and dividing by volume V = L3 gives the total energy density\n  \n    \n      \n        \n          \n            U\n            V\n          \n        \n        =\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        \n          u\n          \n            ν\n          \n        \n        (\n        T\n        )\n        \n        d\n        ν\n        ,\n      \n    \n    {\\displaystyle {\\frac {U}{V}}=\\int _{0}^{\\infty }u_{\\nu }(T)\\,d\\nu ,}\n  \nwhere the frequency-dependent spectral energy density uν(T) is given by\n  \n    \n      \n        \n          u\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              8\n              π\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  h\n                  ν\n                  \n                    /\n                  \n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle u_{\\nu }(T)={8\\pi h\\nu ^{3} \\over c^{3}}{1 \\over e^{h\\nu /k_{\\mathrm {B} }T}-1}.}\n  \nSince the radiation is the same in all directions, and propagates at the speed of light, the spectral radiance of radiation exiting the small hole is\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              \n                u\n                \n                  ν\n                \n              \n              (\n              T\n              )\n              c\n            \n            \n              4\n              π\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\nu }(T)={\\frac {u_{\\nu }(T)c}{4\\pi }},}\n  \nwhich yields Planck's law\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n         \n        \n          \n            1\n            \n              \n                e\n                \n                  h\n                  ν\n                  \n                    /\n                  \n                  \n                    k\n                    \n                      \n                        B\n                      \n                    \n                  \n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\nu }(T)={\\frac {2h\\nu ^{3}}{c^{2}}}~{\\frac {1}{e^{h\\nu /k_{\\mathrm {B} }T}-1}}.}\n  \nOther forms of the law can be obtained by change of variables in the total energy integral. The above derivation is based on Brehm & Mullin 1989.\n\nFor the non-degenerate case, A and B coefficients can be calculated using dipole approximation in time dependent perturbation theory in quantum mechanics. Calculation of A also requires second quantization since semi-classical theory cannot explain spontaneous emission which does not go to zero as the perturbing field goes to zero. Hence, the calculated transition rates are (in SI units):[45][46][47]\n\nNote that the rate of transition formula depends on dipole moment operator. For higher order approximations, it involves quadrupole moment and other similar terms. The A and B coefficients (which correspond to angular frequency energy distribution) are hence:\n\nwhere \n  \n    \n      \n        \n          ω\n          \n            a\n            b\n          \n        \n        =\n        \n          \n            \n              \n                E\n                \n                  a\n                \n              \n              −\n              \n                E\n                \n                  b\n                \n              \n            \n            ℏ\n          \n        \n      \n    \n    {\\displaystyle \\omega _{ab}={\\frac {E_{a}-E_{b}}{\\hbar }}}\n  \n and A and B coefficients satisfy the given ratios for non degenerate case:\n\nAnother useful ratio is from the Maxwell-Boltzmann distribution, which says that the number of particles in an energy level \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is proportional to the exponent \n  \n    \n      \n        β\n        E\n      \n    \n    {\\displaystyle \\beta E}\n  \n. Mathematically:\n\nwhere \n  \n    \n      \n        \n          N\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle N_{a}}\n  \n and \n  \n    \n      \n        \n          N\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle N_{b}}\n  \n are number of occupied energy levels of \n  \n    \n      \n        \n          E\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle E_{a}}\n  \n and \n  \n    \n      \n        \n          E\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle E_{b}}\n  \n respectively, where \n  \n    \n      \n        \n          E\n          \n            b\n          \n        \n        >\n        \n          E\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle E_{b}>E_{a}}\n  \n. Then, using: \n  \n    \n      \n        \n          \n            \n              d\n              \n                N\n                \n                  b\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        −\n        \n          A\n          \n            b\n            a\n          \n        \n        \n          N\n          \n            b\n          \n        \n        −\n        \n          N\n          \n            b\n          \n        \n        u\n        (\n        \n          ω\n          \n            b\n            a\n          \n        \n        )\n        \n          B\n          \n            b\n            a\n          \n        \n        +\n        \n          N\n          \n            a\n          \n        \n        u\n        (\n        \n          ω\n          \n            b\n            a\n          \n        \n        )\n        \n          B\n          \n            a\n            b\n          \n        \n        =\n        −\n        \n          N\n          \n            b\n          \n        \n        \n          w\n          \n            b\n            →\n            a\n          \n          \n            s\n            .\n            e\n            m\n            i\n          \n        \n        −\n        \n          N\n          \n            b\n          \n        \n        \n          w\n          \n            b\n            →\n            a\n          \n          \n            e\n            m\n            i\n          \n        \n        +\n        \n          N\n          \n            a\n          \n        \n        \n          w\n          \n            a\n            →\n            b\n          \n          \n            a\n            b\n            s\n          \n        \n      \n    \n    {\\displaystyle {\\frac {dN_{b}}{dt}}=-A_{ba}N_{b}-N_{b}u(\\omega _{ba})B_{ba}+N_{a}u(\\omega _{ba})B_{ab}=-N_{b}w_{b\\rightarrow a}^{s.emi}-N_{b}w_{b\\rightarrow a}^{emi}+N_{a}w_{a\\rightarrow b}^{abs}}\n\nSolving for \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n for equilibrium condition \n  \n    \n      \n        \n          \n            \n              d\n              \n                N\n                \n                  b\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {dN_{b}}{dt}}=0}\n  \n, and using the derived ratios, we get Planck's Law:\n\nIn 1858, Balfour Stewart described his experiments on the thermal radiative emissive and absorptive powers of polished plates of various substances, compared with the powers of lamp-black surfaces, at the same temperature.[9] Stewart chose lamp-black surfaces as his reference because of various previous experimental findings, especially those of Pierre Prevost and of John Leslie. He wrote \"Lamp-black, which absorbs all the rays that fall upon it, and therefore possesses the greatest possible absorbing power, will possess also the greatest possible radiating power.\"\n\nStewart measured radiated power with a thermo-pile and sensitive galvanometer read with a microscope. He was concerned with selective thermal radiation, which he investigated with plates of substances that radiated and absorbed selectively for different qualities of radiation rather than maximally for all qualities of radiation. He discussed the experiments in terms of rays which could be reflected and refracted, and which obeyed the Helmholtz reciprocity principle (though he did not use an eponym for it). He did not in this paper mention that the qualities of the rays might be described by their wavelengths, nor did he use spectrally resolving apparatus such as prisms or diffraction gratings. His work was quantitative within these constraints. He made his measurements in a room temperature environment, and quickly so as to catch his bodies in a condition near the thermal equilibrium in which they had been prepared by heating to equilibrium with boiling water. His measurements confirmed that substances that emit and absorb selectively respect the principle of selective equality of emission and absorption at thermal equilibrium.\n\nStewart offered a theoretical proof that this should be the case separately for every selected quality of thermal radiation, but his mathematics was not rigorously valid. According to historian D. M. Siegel: \"He was not a practitioner of the more sophisticated techniques of nineteenth-century mathematical physics; he did not even make use of the functional notation in dealing with spectral distributions.\"[48] He made no mention of thermodynamics in this paper, though he did refer to conservation of vis viva. He proposed that his measurements implied that radiation was both absorbed and emitted by particles of matter throughout depths of the media in which it propagated. He applied the Helmholtz reciprocity principle to account for the material interface processes as distinct from the processes in the interior material. He concluded that his experiments showed that, in the interior of an enclosure in thermal equilibrium, the radiant heat, reflected and emitted combined, leaving any part of the surface, regardless of its substance, was the same as would have left that same portion of the surface if it had been composed of lamp-black. He did not mention the possibility of ideally perfectly reflective walls; in particular he noted that highly polished real physical metals absorb very slightly.\n\nIn 1859, not knowing of Stewart's work, Gustav Robert Kirchhoff reported the coincidence of the wavelengths of spectrally resolved lines of absorption and of emission of visible light. Importantly for thermal physics, he also observed that bright lines or dark lines were apparent depending on the temperature difference between emitter and absorber.[49]\n\nKirchhoff then went on to consider bodies that emit and absorb heat radiation, in an opaque enclosure or cavity, in equilibrium at temperature T.\n\nHere is used a notation different from Kirchhoff's. Here, the emitting power E(T, i) denotes a dimensioned quantity, the total radiation emitted by a body labeled by index i at temperature T. The total absorption ratio a(T, i) of that body is dimensionless, the ratio of absorbed to incident radiation in the cavity at temperature T . (In contrast with Balfour Stewart's, Kirchhoff's definition of his absorption ratio did not refer in particular to a lamp-black surface as the source of the incident radiation.) Thus the ratio ⁠E(T, i)/a(T, i)⁠ of emitting power to absorption ratio is a dimensioned quantity, with the dimensions of emitting power, because a(T, i) is dimensionless. Also here the wavelength-specific emitting power of the body at temperature T is denoted by E(λ, T, i) and the wavelength-specific absorption ratio by a(λ, T, i) . Again, the ratio ⁠E(λ, T, i)/a(λ, T, i)⁠ of emitting power to absorption ratio is a dimensioned quantity, with the dimensions of emitting power.\n\nIn a second report made in 1859, Kirchhoff announced a new general principle or law for which he offered a theoretical and mathematical proof, though he did not offer quantitative measurements of radiation powers.[50] His theoretical proof was and still is considered by some writers to be invalid.[48][51] His principle, however, has endured: it was that for heat rays of the same wavelength, in equilibrium at a given temperature, the wavelength-specific ratio of emitting power to absorption ratio has one and the same common value for all bodies that emit and absorb at that wavelength. In symbols, the law stated that the wavelength-specific ratio ⁠E(λ, T, i)/a(λ, T, i)⁠ has one and the same value for all bodies, that is for all values of index i. In this report there was no mention of black bodies.\n\nIn 1860, still not knowing of Stewart's measurements for selected qualities of radiation, Kirchhoff pointed out that it was long established experimentally that for total heat radiation, of unselected quality, emitted and absorbed by a body in equilibrium, the dimensioned total radiation ratio ⁠E(T, i)/a(T, i)⁠, has one and the same value common to all bodies, that is, for every value of the material index i.[52] Again without measurements of radiative powers or other new experimental data, Kirchhoff then offered a fresh theoretical proof of his new principle of the universality of the value of the wavelength-specific ratio ⁠E(λ, T, i)/a(λ, T, i)⁠ at thermal equilibrium. His fresh theoretical proof was and still is considered by some writers to be invalid.[48][51]\n\nBut more importantly, it relied on a new theoretical postulate of \"perfectly black bodies\", which is the reason why one speaks of Kirchhoff's law. Such black bodies showed complete absorption in their infinitely thin most superficial surface. They correspond to Balfour Stewart's reference bodies, with internal radiation, coated with lamp-black. They were not the more realistic perfectly black bodies later considered by Planck. Planck's black bodies radiated and absorbed only by the material in their interiors; their interfaces with contiguous media were only mathematical surfaces, capable neither of absorption nor emission, but only of reflecting and transmitting with refraction.[53]\n\nKirchhoff's proof considered an arbitrary non-ideal body labeled i as well as various perfect black bodies labeled BB. It required that the bodies be kept in a cavity in thermal equilibrium at temperature T . His proof intended to show that the ratio ⁠E(λ, T, i)/a(λ, T, i)⁠ was independent of the nature i of the non-ideal body, however partly transparent or partly reflective it was.\n\nHis proof first argued that for wavelength λ and at temperature T, at thermal equilibrium, all perfectly black bodies of the same size and shape have the one and the same common value of emissive power E(λ, T, BB), with the dimensions of power. His proof noted that the dimensionless wavelength-specific absorption ratio a(λ, T, BB) of a perfectly black body is by definition exactly 1. Then for a perfectly black body, the wavelength-specific ratio of emissive power to absorption ratio ⁠E(λ, T, BB)/a(λ, T, BB)⁠ is again just E(λ, T, BB), with the dimensions of power. Kirchhoff considered, successively, thermal equilibrium with the arbitrary non-ideal body, and with a perfectly black body of the same size and shape, in place in his cavity in equilibrium at temperature T . He argued that the flows of heat radiation must be the same in each case. Thus he argued that at thermal equilibrium the ratio ⁠E(λ, T, i)/a(λ, T, i)⁠ was equal to E(λ, T, BB), which may now be denoted Bλ (λ, T), a continuous function, dependent only on λ at fixed temperature T, and an increasing function of T at fixed wavelength λ, at low temperatures vanishing for visible but not for longer wavelengths, with positive values for visible wavelengths at higher temperatures, which does not depend on the nature i of the arbitrary non-ideal body. (Geometrical factors, taken into detailed account by Kirchhoff, have been ignored in the foregoing.)\n\nThus Kirchhoff's law of thermal radiation can be stated: For any material at all, radiating and absorbing in thermodynamic equilibrium at any given temperature T, for every wavelength λ, the ratio of emissive power to absorptive ratio has one universal value, which is characteristic of a perfect black body, and is an emissive power which we here represent by Bλ (λ, T). (For our notation Bλ (λ, T), Kirchhoff's original notation was simply e.)[7][52][54][55][56][57]\n\nKirchhoff announced that the determination of the function Bλ (λ, T) was a problem of the highest importance, though he recognized that there would be experimental difficulties to be overcome. He supposed that like other functions that do not depend on the properties of individual bodies, it would be a simple function. That function Bλ (λ, T) has occasionally been called 'Kirchhoff's (emission, universal) function',[58][59][60][61] though its precise mathematical form would not be known for another forty years, till it was discovered by Planck in 1900. The theoretical proof for Kirchhoff's universality principle was worked on and debated by various physicists over the same time, and later.[51] Kirchhoff stated later in 1860 that his theoretical proof was better than Balfour Stewart's, and in some respects it was so.[48] Kirchhoff's 1860 paper did not mention the second law of thermodynamics, and of course did not mention the concept of entropy which had not at that time been established. In a more considered account in a book in 1862, Kirchhoff mentioned the connection of his law with \"Carnot's principle\", which is a form of the second law.[62]\n\nAccording to Helge Kragh, \"Quantum theory owes its origin to the study of thermal radiation, in particular to the \"blackbody\" radiation that Robert Kirchhoff had first defined in 1859–1860.\"[63]\n\nIn 1860, Kirchhoff predicted experimental difficulties for the empirical determination of the function that described the dependence of the black-body spectrum as a function only of temperature and wavelength. And so it turned out. It took some forty years of development of improved methods of measurement of electromagnetic radiation to get a reliable result.[64]\n\nIn 1865, John Tyndall described radiation from electrically heated filaments and from carbon arcs as visible and invisible.[65] Tyndall spectrally decomposed the radiation by use of a rock salt prism, which passed heat as well as visible rays, and measured the radiation intensity by means of a thermopile.[66][67]\n\nIn 1880, André-Prosper-Paul Crova published a diagram of the three-dimensional appearance of the graph of the strength of thermal radiation as a function of wavelength and temperature.[68] He determined the spectral variable by use of prisms. He analyzed the surface through what he called \"isothermal\" curves, sections for a single temperature, with a spectral variable on the abscissa and a power variable on the ordinate. He put smooth curves through his experimental data points. They had one peak at a spectral value characteristic for the temperature, and fell either side of it towards the horizontal axis.[69][70] Such spectral sections are widely shown even today.\n\nIn a series of papers from 1881 to 1886, Langley reported measurements of the spectrum of heat radiation, using diffraction gratings and prisms, and the most sensitive detectors that he could make. He reported that there was a peak intensity that increased with temperature, that the shape of the spectrum was not symmetrical about the peak, that there was a strong fall-off of intensity when the wavelength was shorter than an approximate cut-off value for each temperature, that the approximate cut-off wavelength decreased with increasing temperature, and that the wavelength of the peak intensity decreased with temperature, so that the intensity increased strongly with temperature for short wavelengths that were longer than the approximate cut-off for the temperature.[71]\n\nHaving read Langley, in 1888, Russian physicist V.A. Michelson published a consideration of the idea that the unknown Kirchhoff radiation function could be explained physically and stated mathematically in terms of \"complete irregularity of the vibrations of ... atoms\".[72][73] At this time, Planck was not studying radiation closely, and believed in neither atoms nor statistical physics.[74] Michelson produced a formula for the spectrum for temperature:\n\n  \n    \n      \n        \n          I\n          \n            λ\n          \n        \n        =\n        \n          B\n          \n            1\n          \n        \n        \n          θ\n          \n            \n              3\n              2\n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                c\n                \n                  \n                    λ\n                    \n                      2\n                    \n                  \n                  θ\n                \n              \n            \n          \n          )\n        \n        \n          λ\n          \n            −\n            6\n          \n        \n        ,\n      \n    \n    {\\displaystyle I_{\\lambda }=B_{1}\\theta ^{\\frac {3}{2}}\\exp \\left(-{\\frac {c}{\\lambda ^{2}\\theta }}\\right)\\lambda ^{-6},}\n  \n\nwhere Iλ denotes specific radiative intensity at wavelength λ and temperature θ, and where B1 and c are empirical constants.\n\nIn 1898, Otto Lummer and Ferdinand Kurlbaum published an account of their cavity radiation source.[75] Their design has been used largely unchanged for radiation measurements to the present day. It was a platinum box, divided by diaphragms, with its interior blackened with iron oxide. It was an important ingredient for the progressively improved measurements that led to the discovery of Planck's law.[76] A version described in 1901 had its interior blackened with a mixture of chromium, nickel, and cobalt oxides.[77]\n\nThe importance of the Lummer and Kurlbaum cavity radiation source was that it was an experimentally accessible source of black-body radiation, as distinct from radiation from a simply exposed incandescent solid body, which had been the nearest available experimental approximation to black-body radiation over a suitable range of temperatures. The simply exposed incandescent solid bodies, that had been used before, emitted radiation with departures from the black-body spectrum that made it impossible to find the true black-body spectrum from experiments.[78][79]\n\nPlanck first turned his attention to the problem of black-body radiation in 1897.[80]\nTheoretical and empirical progress enabled Lummer and Pringsheim to write in 1899 that available experimental evidence was approximately consistent with the specific intensity law Cλ−5e.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}−c⁄λT where C and c denote empirically measurable constants, and where λ and T denote wavelength and temperature respectively.[81][82] For theoretical reasons, Planck at that time accepted this formulation, which has an effective cut-off of short wavelengths.[83][84][85]\n\nGustav Kirchhoff was Max Planck's teacher and surmised that there was a universal law for blackbody radiation and this was called \"Kirchhoff's challenge\".[86] Planck, a theorist, believed that Wilhelm Wien had discovered this law and Planck expanded on Wien's work presenting it in 1899 to the meeting of the German Physical Society. Experimentalists Otto Lummer, Ferdinand Kurlbaum, Ernst Pringsheim Sr., and Heinrich Rubens did experiments that appeared to support Wien's law especially at higher frequency short wavelengths which Planck so wholly endorsed at the German Physical Society that it began to be called the Wien-Planck Law.[87] However, by September 1900, the experimentalists had proven beyond a doubt that the Wien-Planck law failed at the longer wavelengths. They would present their data on October 19.  Planck was informed by his friend Rubens and quickly created a formula within a few days.[88] In June of that same year, Lord Rayleigh had created a formula that would work for short lower frequency wavelengths based on the widely accepted theory of equipartition.[89] So Planck submitted a formula combining both Rayleigh's Law (or a similar equipartition theory) and Wien's law which would be weighted to one or the other law depending on wavelength to match the experimental data. However, although this equation worked, Planck himself said unless he could explain the formula derived from a \"lucky intuition\" into one of \"true meaning\" in physics, it did not have true significance.[90] Planck explained that thereafter followed the hardest work of his life. Planck did not believe in atoms, nor did he think the second law of thermodynamics should be statistical because probability does not provide an absolute answer, and Boltzmann's entropy law rested on the hypothesis of atoms and was statistical. But Planck was unable to find a way to reconcile his Blackbody equation with continuous laws such as Maxwell's wave equations. So in what Planck called \"an act of desperation\",[91] he turned to Boltzmann's atomic law of entropy as it was the only one that made his equation work. Therefore, he used the Boltzmann constant k and his new constant h to explain the blackbody radiation law which became widely known through his published paper.[92][93]\n\nMax Planck produced his law on 19 October 1900[94][95] as an improvement upon the Wien approximation, published in 1896 by Wilhelm Wien, which fit the experimental data at short wavelengths (high frequencies) but deviated from it at long wavelengths (low frequencies).[41] In June 1900, based on heuristic theoretical considerations, Rayleigh had suggested a formula[96] that he proposed might be checked experimentally. The suggestion was that the Stewart–Kirchhoff universal function might be of the form c1Tλ−4exp(–⁠c2/λT⁠) . This was not the celebrated Rayleigh–Jeans formula 8πkBTλ−4, which did not emerge until 1905,[38] though it did reduce to the latter for long wavelengths, which are the relevant ones here. According to Klein,[80] one may speculate that it is likely that Planck had seen this suggestion though he did not mention it in his papers of 1900 and 1901. Planck would have been aware of various other proposed formulas which had been offered.[64][97] On 7 October 1900, Rubens told Planck that in the complementary domain (long wavelength, low frequency), and only there, Rayleigh's 1900 formula fitted the observed data well.[97]\n\nFor long wavelengths, Rayleigh's 1900 heuristic formula approximately meant that energy was proportional to temperature, Uλ = const. T.[80][97][98] It is known that ⁠dS/dUλ⁠ = ⁠1/T⁠ and this leads to ⁠dS/dUλ⁠ = ⁠const./Uλ⁠ and thence to ⁠d2S/dUλ2⁠ = −⁠const./Uλ2⁠ for long wavelengths. But for short wavelengths, the Wien formula leads to ⁠1/T⁠ = − const. ln Uλ + const. and thence to ⁠d2S/dUλ2⁠ = − ⁠const./Uλ⁠ for short wavelengths. Planck perhaps patched together these two heuristic formulas, for long and for short wavelengths,[97][99] to produce a formula[94]\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              S\n            \n            \n              d\n              \n                U\n                \n                  λ\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            α\n            \n              \n                U\n                \n                  λ\n                \n              \n              (\n              β\n              +\n              \n                U\n                \n                  λ\n                \n              \n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d^{2}S}{dU_{\\lambda }^{2}}}={\\frac {\\alpha }{U_{\\lambda }(\\beta +U_{\\lambda })}}.}\n\nThis led Planck to the formula\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              C\n              \n                λ\n                \n                  −\n                  5\n                \n              \n            \n            \n              \n                e\n                \n                  \n                    c\n                    \n                      λ\n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {C\\lambda ^{-5}}{e^{\\frac {c}{\\lambda T}}-1}},}\n  \n\nwhere Planck used the symbols C and c to denote empirical fitting constants.\n\nPlanck sent this result to Rubens, who compared it with his and Kurlbaum's observational data and found that it fitted for all wavelengths remarkably well. On 19 October 1900, Rubens and Kurlbaum briefly reported the fit to the data,[100] and Planck added a short presentation to give a theoretical sketch to account for his formula.[94] Within a week, Rubens and Kurlbaum gave a fuller report of their measurements confirming Planck's law. Their technique for spectral resolution of the longer wavelength radiation was called the residual ray method. The rays were repeatedly reflected from polished crystal surfaces, and the rays that made it all the way through the process were 'residual', and were of wavelengths preferentially reflected by crystals of suitably specific materials.[101][102][103]\n\nOnce Planck had discovered the empirically fitting function, he constructed a physical derivation of this law. His thinking revolved around entropy rather than being directly about temperature. Planck considered a cavity with perfectly reflective walls; inside the cavity, there are finitely many distinct but identically constituted resonant oscillatory bodies of definite magnitude, with several such oscillators at each of finitely many characteristic frequencies. These hypothetical oscillators were for Planck purely imaginary theoretical investigative probes, and he said of them that such oscillators do not need to \"really exist somewhere in nature, provided their existence and their properties are consistent with the laws of thermodynamics and electrodynamics.\".[104] Planck did not attribute any definite physical significance to his hypothesis of resonant oscillators but rather proposed it as a mathematical device that enabled him to derive a single expression for the black body spectrum that matched the empirical data at all wavelengths.[105] He tentatively mentioned the possible connection of such oscillators with atoms. In a sense, the oscillators corresponded to Planck's speck of carbon; the size of the speck could be small regardless of the size of the cavity, provided the speck effectively transduced energy between radiative wavelength modes.[97]\n\nPartly following a heuristic method of calculation pioneered by Boltzmann for gas molecules, Planck considered the possible ways of distributing electromagnetic energy over the different modes of his hypothetical charged material oscillators. This acceptance of the probabilistic approach, following Boltzmann, for Planck was a radical change from his former position, which till then had deliberately opposed such thinking proposed by Boltzmann.[106] In Planck's words, \"I considered the [quantum hypothesis] a purely formal assumption, and I did not give it much thought except for this: that I had obtained a positive result under any circumstances and at whatever cost.\"[107] Heuristically, Boltzmann had distributed the energy in arbitrary merely mathematical quanta  ϵ, which he had proceeded to make tend to zero in magnitude, because the finite magnitude ϵ had served only to allow definite counting for the sake of mathematical calculation of probabilities, and had no physical significance. Referring to a new universal constant of nature, h,[108] Planck supposed that, in the several oscillators of each of the finitely many characteristic frequencies, the total energy was distributed to each in an integer multiple of a definite physical unit of energy, ϵ, characteristic of the respective characteristic frequency.[95][109][110][111] His new universal constant of nature, h, is now known as the Planck constant.\n\nPlanck explained further[95] that the respective definite unit, ϵ, of energy should be proportional to the respective characteristic oscillation frequency ν of the hypothetical oscillator, and in 1901 he expressed this with the constant of proportionality h:[112][113]\n\n  \n    \n      \n        ϵ\n        =\n        h\n        ν\n        .\n      \n    \n    {\\displaystyle \\epsilon =h\\nu .}\n\nPlanck did not propose that light propagating in free space is quantized.[114][115][116] The idea of quantization of the free electromagnetic field was developed later, and eventually incorporated into what we now know as quantum field theory.[117]\n\nIn 1906, Planck acknowledged that his imaginary resonators, having linear dynamics, did not provide a physical explanation for energy transduction between frequencies.[118][119] Present-day physics explains the transduction between frequencies in the presence of atoms by their quantum excitability, following Einstein. Planck believed that in a cavity with perfectly reflecting walls and with no matter present, the electromagnetic field cannot exchange energy between frequency components.[120] This is because of the linearity of Maxwell's equations.[121] Present-day quantum field theory predicts that, in the absence of matter, the electromagnetic field obeys nonlinear equations and in that sense does self-interact.[122][123] Such interaction in the absence of matter has not yet been directly measured because it would require very high intensities and very sensitive and low-noise detectors, which are still in the process of being constructed.[122][124] Planck believed that a field with no interactions neither obeys nor violates the classical principle of equipartition of energy,[125][126] and instead remains exactly as it was when introduced, rather than evolving into a black body field.[127] Thus, the linearity of his mechanical assumptions precluded Planck from having a mechanical explanation of the maximization of the entropy of the thermodynamic equilibrium thermal radiation field. This is why he had to resort to Boltzmann's probabilistic arguments.[128][129]\n\nPlanck's law may be regarded as fulfilling the prediction of Gustav Kirchhoff that his law of thermal radiation was of the highest importance. In his mature presentation of his own law, Planck offered a thorough and detailed theoretical proof for Kirchhoff's law,[130] theoretical proof of which until then had been sometimes debated, partly because it was said to rely on unphysical theoretical objects, such as Kirchhoff's perfectly absorbing infinitely thin black surface.[131]\n\nIt was not until five years after Planck made his heuristic assumption of abstract elements of energy or of action that Albert Einstein conceived of really existing quanta of light in 1905[132] as a revolutionary explanation of black-body radiation, of photoluminescence, of the photoelectric effect, and of the ionization of gases by ultraviolet light. In 1905, \"Einstein believed that Planck's theory could not be made to agree with the idea of light quanta, a mistake he corrected in 1906.\"[133] Contrary to Planck's beliefs of the time, Einstein proposed a model and formula whereby light was emitted, absorbed, and propagated in free space in energy quanta localized in points of space.[132] As an introduction to his reasoning, Einstein recapitulated Planck's model of hypothetical resonant material electric oscillators as sources and sinks of radiation, but then he offered a new argument, disconnected from that model, but partly based on a thermodynamic argument of Wien, in which Planck's formula ϵ = hν played no role.[134] Einstein gave the energy content of such quanta in the form ⁠Rβν/N⁠. Thus Einstein was contradicting the undulatory theory of light held by Planck. In 1910, criticizing a manuscript sent to him by Planck, knowing that Planck was a steady supporter of Einstein's theory of special relativity, Einstein wrote to Planck: \"To me it seems absurd to have energy continuously distributed in space without assuming an aether.\"[135]\n\nAccording to Thomas Kuhn, it was not till 1908 that Planck more or less accepted part of Einstein's arguments for physical as distinct from abstract mathematical discreteness in thermal radiation physics. Still in 1908, considering Einstein's proposal of quantal propagation, Planck opined that such a revolutionary step was perhaps unnecessary.[136] Until then, Planck had been consistent in thinking that discreteness of action quanta was to be found neither in his resonant oscillators nor in the propagation of thermal radiation. Kuhn wrote that, in Planck's earlier papers and in his 1906 monograph,[137] there is no \"mention of discontinuity, [nor] of talk of a restriction on oscillator energy, [nor of] any formula like U = nhν.\" Kuhn pointed out that his study of Planck's papers of 1900 and 1901, and of his monograph of 1906,[137] had led him to \"heretical\" conclusions, contrary to the widespread assumptions of others who saw Planck's writing only from the perspective of later, anachronistic, viewpoints.[138] Kuhn's conclusions, finding a period till 1908, when Planck consistently held his 'first theory', have been accepted by other historians.[139]\n\nIn the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein's proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal.[121][140] This has at times been called Planck's \"second theory\".[141]\n\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his 'third theory', that both emission and absorption of light were quantal.[142]\n\nThe colourful term \"ultraviolet catastrophe\" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation.[143][144] But this had not been part of Planck's thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of \"catastrophe\".[83][84][85][80][145] It was first noted by Lord Rayleigh in 1900,[96][146][147] and then in 1901[148] by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called 'photons', and by Rayleigh[39] and by Jeans.[38][149][150][151]\n\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν.[34][35][36][152][153][154] In contrast to Planck's and Einstein's formulas, Bohr's formula referred explicitly and categorically to energy levels of atoms. Bohr's formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck's model, the frequency \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n has no immediate relation to frequencies that might describe those quantum states themselves.\n\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck's law.[155] The actual word 'photon' was invented still later, by G.N. Lewis in 1926,[156] who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word 'photon' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein's 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\n\nUltimately, Planck's law of black-body radiation contributed to Einstein's concept of quanta of light carrying linear momentum,[34][132] which became the fundamental basis for the development of quantum mechanics.\n\nThe above-mentioned linearity of Planck's mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg's original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg's theory accounted for Bohr's above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states.[157][158] At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg's paper and recognized the matrix character of Heisenberg's theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg's original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics.[159][160][161] Heisenberg's explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck's oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\n\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = ⁠h/2π⁠, and ω = 2πν denotes angular frequency,[162][163][164][165][166] and less often the equivalent formula E = hν.[165][166][167][168][169] This statement about a really existing and propagating light quantum, based on Einstein's, has a physical meaning different from that of Planck's above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\n\nAn article by Helge Kragh published in Physics World gives an account of this history.[111]",
        pageTitle: "Planck's law",
    },
    {
        title: "Plateau's laws",
        link: "https://en.wikipedia.org/wiki/Plateau%27s_laws",
        content:
            "Plateau's laws describe the structure of soap films. These laws were formulated in the 19th century by the Belgian physicist Joseph Plateau from his experimental observations. Many patterns in nature are based on foams obeying these laws.[1]\n\nPlateau's laws describe the shape and configuration of soap films as follows:[2]\n\nConfigurations other than those of Plateau's laws are unstable, and the film will quickly tend to rearrange itself to conform to these laws.[3]\n\nThat these laws hold for minimal surfaces was proved mathematically by Jean Taylor using geometric measure theory.[4][5]",
        pageTitle: "Plateau's laws",
    },
    {
        title: "Poe's law",
        link: "https://en.wikipedia.org/wiki/Poe%27s_law",
        content:
            "Poe's law is an adage of Internet culture which says that, without a clear indicator of the author's intent, any parodic or sarcastic expression of extreme views can be mistaken by some readers for a sincere expression of those views.[1][2][3]\n\nPoe's law is based on a comment written by Nathan Poe in 2005 on christianforums.com, an Internet forum on Christianity. The message was posted during a debate on creationism, where a previous poster had remarked to another user: \"Good thing you included the winky. Otherwise people might think you are serious\".[4]\n\nWithout a winking smiley or other blatant display of humor, it is utterly impossible to parody a Creationist in such a way that someone won't mistake for the genuine article.\n\nThe original statement of Poe's law referred specifically to creationism, but it has since been generalized to apply to any kind of fundamentalism or extremism.[3]\n\nIts original conceptualization held that online parodies or sarcasm on religious views are indistinguishable from sincere expressions of religious views.[5] In part, Poe was simply reiterating common advice about the need to clearly mark online sarcasm or parody, otherwise it would be interpreted as the real thing[5] or used by online trolls,[6] extremists, and fundamentalists as sincere expressions of their authors, particularly if they match their own views.[7] Some abuse the law by publishing extremism or defamation without a smiley or other indication of satire, and if there is too much criticism towards it, reply that it was only an irony. As early as 1983, Jerry Schwarz, in a post on Usenet, wrote:\n\nWithout the voice inflection and body language of personal communication these are easily misinterpreted. A sideways smile, :-), has become widely accepted on the net as an indication that \"I'm only kidding\". If you submit a satiric item without this symbol, no matter how obvious the satire is to you, do not be surprised if people take it seriously.[8]\n\nIn 2017, Wired published an article calling Poe's Law \"2017's Most Important Internet Phenomenon\", and wrote that \"Poe's Law applies to more and more internet interactions.\" The article gave examples of cases such as on 4chan forums with the usage of the OK gesture as a white power symbol and the Trump administration where there were deliberate ambiguities over whether something was serious or intended as a parody, where people were using Poe's law as \"a refuge\" to camouflage beliefs that would otherwise be considered unacceptable.[9] Some treat Poe's law as part of contemporary kitsch culture;[7] another view maintains that Poe's law could lead to nihilism, a situation where nothing matters and everything is a joke.[6]",
        pageTitle: "Poe's law",
    },
    {
        title: "Poisson's law of large numbers",
        link: "https://en.wikipedia.org/wiki/Poisson%27s_law_of_large_numbers",
        content:
            'In probability theory, the Law of Large Numbers (LLN) is a mathematical law that states that the average of the results obtained from a large number of independent random samples converges to the true value, if it exists.[1] More formally, the Law of Large Numbers states that given a sample of independent and identically distributed values, the sample mean converges to the true mean.\n\nThe Law of Large Numbers is important because it guarantees stable long-term results for the averages of some random events.[1][2] For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. Importantly, the law applies (as the name indicates) only when a large number of observations are considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be "balanced" by the others (see the gambler\'s fallacy).\n\nThe Law of Large Numbers only applies to the average of the results obtained from repeated trials and claims that this average converges to the expected value; it does not claim that the sum of n results gets close to the expected value times n as n increases.\n\nThroughout its history, many mathematicians have refined this law. Today, the Law of Large Numbers is used in many fields including statistics, probability theory, economics, and insurance.[3]\n\nFor example, a single roll of a six-sided die produces one of the numbers 1, 2, 3, 4, 5, or 6, each with equal probability.  Therefore, the expected value of the roll is:\n\n1\n              +\n              2\n              +\n              3\n              +\n              4\n              +\n              5\n              +\n              6\n            \n            6\n          \n        \n        =\n        3.5\n      \n    \n    {\\displaystyle {\\frac {1+2+3+4+5+6}{6}}=3.5}\n\nAccording to the law of large numbers, if a large number of six-sided dice are rolled, the average of their values (sometimes called the sample mean) will approach 3.5, with the precision increasing as more dice are rolled.\n\nIt follows from the law of large numbers that the empirical probability of success in a series of Bernoulli trials will converge to the theoretical probability. For a Bernoulli random variable, the expected value is the theoretical probability of success, and the average of n such variables (assuming they are independent and identically distributed (i.i.d.)) is precisely the relative frequency.\n\nFor example, a fair coin toss is a Bernoulli trial. When a fair coin is flipped once, the theoretical probability that the outcome will be heads is equal to .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄2. Therefore, according to the law of large numbers, the proportion of heads in a "large" number of coin flips "should be" roughly 1⁄2. In particular, the proportion of heads after n flips will almost surely converge to 1⁄2 as n approaches infinity.\n\nAlthough the proportion of heads (and tails) approaches 1⁄2, almost surely the absolute difference in the number of heads and tails will become large as the number of flips becomes large. That is, the probability that the absolute difference is a small number approaches zero as the number of flips becomes large. Also, almost surely the ratio of the absolute difference to the number of flips will approach zero. Intuitively, the expected difference grows, but at a slower rate than the number of flips.\n\nAnother good example of the Law of Large Numbers is the Monte Carlo method. These methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The larger the number of repetitions, the better the approximation tends to be. The reason that this method is important is mainly that, sometimes, it is difficult or impossible to use other approaches.[4]\n\nThe average of the results obtained from a large number of trials may fail to converge in some cases. For instance, the average of n results taken from the Cauchy distribution or some Pareto distributions (α<1) will not converge as n becomes larger; the reason is heavy tails.[5] The Cauchy distribution and the Pareto distribution represent two cases: the Cauchy distribution does not have an expectation,[6] whereas the expectation of the Pareto distribution (α<1) is infinite.[7] One way to generate the Cauchy-distributed example is where the random numbers equal the tangent of an angle uniformly distributed between −90° and +90°.[8] The median is zero, but the expected value does not exist, and indeed the average of n such variables have the same distribution as one such variable. It does not converge in probability toward zero (or any other value) as n goes to infinity.\n\nAnd if the trials embed a selection bias, typical in human economic/rational behaviour, the law of large numbers does not help in solving the bias. Even if the number of trials is increased the selection bias remains.\n\nThe Italian mathematician Gerolamo Cardano (1501–1576) stated without proof that the accuracies of empirical statistics tend to improve with the number of trials.[9][3] This was then formalized as a law of large numbers. A special form of the Law of Large Numbers (for a binary random variable) was first proved by Jacob Bernoulli.[10][3] It took him over 20 years to develop a sufficiently rigorous mathematical proof which was published in his Ars Conjectandi (The Art of Conjecturing) in 1713. He named this his "Golden Theorem" but it became generally known as "Bernoulli\'s theorem". This should not be confused with Bernoulli\'s principle, named after Jacob Bernoulli\'s nephew Daniel Bernoulli. In 1837, S. D. Poisson further described it under the name "la loi des grands nombres" ("the law of large numbers").[11][12][3] Thereafter, it was known under both names, but the "law of large numbers" is most frequently used.\n\nAfter Bernoulli and Poisson published their efforts, other mathematicians also contributed to refinement of the law, including Chebyshev,[13] Markov, Borel, Cantelli, Kolmogorov and Khinchin.[3] Markov showed that the law can apply to a random variable that does not have a finite variance under some other weaker assumption, and Khinchin showed in 1929 that if the series consists of independent identically distributed random variables, it suffices that the expected value exists for the weak law of large numbers to be true.[14][15] These further studies have given rise to two prominent forms of the Law of Large Numbers. One is called the "weak" law and the other the "strong" law, in reference to two different modes of convergence of the cumulative sample means to the expected value; in particular, as explained below, the strong form implies the weak.[14]\n\nThere are two different versions of the Law of Large Numbers that are described below. They are called the strong law of large numbers and the weak law of large numbers.[16][1] Stated for the case where X1, X2, ... is an infinite sequence of independent and identically distributed (i.i.d.) Lebesgue integrable random variables with expected value E(X1) = E(X2) = ... = μ, both versions of the law state that the sample average\n\nX\n              ¯\n            \n          \n          \n            n\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\overline {X}}_{n}={\\frac {1}{n}}(X_{1}+\\cdots +X_{n})}\n\n(Lebesgue integrability of Xj means that the expected value E(Xj) exists according to Lebesgue integration and is finite. It does not mean that the associated probability measure is absolutely continuous with respect to Lebesgue measure.)\n\nIntroductory probability texts often additionally assume identical finite variance \n  \n    \n      \n        Var\n        ⁡\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        =\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {Var} (X_{i})=\\sigma ^{2}}\n  \n (for all \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n) and no correlation between random variables.  In that case, the variance of the average of n random variables is\n\nVar\n        ⁡\n        (\n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        )\n        =\n        Var\n        ⁡\n        (\n        \n          \n            \n              1\n              n\n            \n          \n        \n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        )\n        =\n        \n          \n            1\n            \n              n\n              \n                2\n              \n            \n          \n        \n        Var\n        ⁡\n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            \n              n\n              \n                σ\n                \n                  2\n                \n              \n            \n            \n              n\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {Var} ({\\overline {X}}_{n})=\\operatorname {Var} ({\\tfrac {1}{n}}(X_{1}+\\cdots +X_{n}))={\\frac {1}{n^{2}}}\\operatorname {Var} (X_{1}+\\cdots +X_{n})={\\frac {n\\sigma ^{2}}{n^{2}}}={\\frac {\\sigma ^{2}}{n}}.}\n\nwhich can be used to shorten and simplify the proofs.  This assumption of finite variance is not necessary. Large or infinite variance will make the convergence slower, but the Law of Large Numbers holds anyway.[17]\n\nMutual independence of the random variables can be replaced by pairwise independence[18] or exchangeability[19] in both versions of the law.\n\nThe difference between the strong and the weak version is concerned with the mode of convergence being asserted. For interpretation of these modes, see Convergence of random variables.\n\nThe weak law of large numbers (also called Khinchin\'s law) states that given a collection of independent and identically distributed (iid) samples from a random variable with finite mean, the sample mean converges in probability to the expected value[20]\n\nlim\n          \n            n\n            →\n            ∞\n          \n        \n        Pr\n        \n        \n          (\n          \n            \n            \n              |\n            \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            −\n            μ\n            \n              |\n            \n            <\n            ε\n            \n          \n          )\n        \n        =\n        1.\n      \n    \n    {\\displaystyle \\lim _{n\\to \\infty }\\Pr \\!\\left(\\,|{\\overline {X}}_{n}-\\mu |<\\varepsilon \\,\\right)=1.}\n\nInterpreting this result, the weak law states that for any nonzero margin specified (ε), no matter how small, with a sufficiently large sample there will be a very high probability that the average of the observations will be close to the expected value; that is, within the margin.\n\nAs mentioned earlier, the weak law applies in the case of i.i.d. random variables, but it also applies in some other cases. For example, the variance may be different for each random variable in the series, keeping the expected value constant. If the variances are bounded, then the law applies, as shown by Chebyshev as early as 1867. (If the expected values change during the series, then we can simply apply the law to the average deviation from the respective expected values. The law then states that this converges in probability to zero.) In fact, Chebyshev\'s proof works so long as the variance of the average of the first n values goes to zero as n goes to infinity.[15] As an example, assume that each random variable in the series follows a Gaussian distribution (normal distribution) with mean zero, but with variance equal to \n  \n    \n      \n        2\n        n\n        \n          /\n        \n        log\n        ⁡\n        (\n        n\n        +\n        1\n        )\n      \n    \n    {\\displaystyle 2n/\\log(n+1)}\n  \n, which is not bounded. At each stage, the average will be normally distributed (as the average of a set of normally distributed variables). The variance of the sum is equal to the sum of the variances, which is asymptotic to \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n        \n          /\n        \n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle n^{2}/\\log n}\n  \n. The variance of the average is therefore asymptotic to \n  \n    \n      \n        1\n        \n          /\n        \n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle 1/\\log n}\n  \n and goes to zero.\n\nThere are also examples of the weak law applying even though the expected value does not exist.\n\nThe strong law of large numbers (also called Kolmogorov\'s law) states that the sample average converges almost surely to the expected value[21]\n\nPr\n        \n        \n          (\n          \n            \n              lim\n              \n                n\n                →\n                ∞\n              \n            \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            =\n            μ\n          \n          )\n        \n        =\n        1.\n      \n    \n    {\\displaystyle \\Pr \\!\\left(\\lim _{n\\to \\infty }{\\overline {X}}_{n}=\\mu \\right)=1.}\n\nWhat this means is that, as the number of trials n goes to infinity, the probability that the average of the observations converges to the expected value, is equal to one.  The modern proof of the strong law is more complex than that of the weak law, and relies on passing to an appropriate sub-sequence.[17]\n\nThe strong law of large numbers can itself be seen as a special case of the pointwise ergodic theorem. This view justifies the intuitive interpretation of the expected value (for Lebesgue integration only) of a random variable when sampled repeatedly as the "long-term average".\n\nLaw 3 is called the strong law because random variables which converge strongly (almost surely) are guaranteed to converge weakly (in probability). However the weak law is known to hold in certain conditions where the strong law does not hold and then the convergence is only weak (in probability). See differences between the weak law and the strong law.\n\nThe strong law applies to independent identically distributed random variables having an expected value (like the weak law). This was proved by Kolmogorov in 1930. It can also apply in other cases. Kolmogorov also showed, in 1933, that if the variables are independent and identically distributed, then for the average to converge almost surely on something (this can be considered another statement of the strong law), it is necessary that they have an expected value (and then of course the average will converge almost surely on that).[22]\n\nIf the commands are independent but not identically distributed, then\n\nprovided that each Xk has a finite second moment and\n\n∑\n          \n            k\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            1\n            \n              k\n              \n                2\n              \n            \n          \n        \n        Var\n        ⁡\n        [\n        \n          X\n          \n            k\n          \n        \n        ]\n        <\n        ∞\n        .\n      \n    \n    {\\displaystyle \\sum _{k=1}^{\\infty }{\\frac {1}{k^{2}}}\\operatorname {Var} [X_{k}]<\\infty .}\n\nThis statement is known as Kolmogorov\'s strong law, see e.g. Sen & Singer (1993, Theorem 2.3.10).\n\nThe weak law states that for a specified large n, the average \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {X}}_{n}}\n  \n is likely to be near μ.[23] Thus, it leaves open the possibility that \n  \n    \n      \n        \n          |\n        \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        −\n        μ\n        \n          |\n        \n        >\n        ε\n      \n    \n    {\\displaystyle |{\\overline {X}}_{n}-\\mu |>\\varepsilon }\n  \n happens an infinite number of times, although at infrequent intervals. (Not necessarily \n  \n    \n      \n        \n          |\n        \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        −\n        μ\n        \n          |\n        \n        ≠\n        0\n      \n    \n    {\\displaystyle |{\\overline {X}}_{n}-\\mu |\\neq 0}\n  \n for all n).\n\nThe strong law shows that this almost surely will not occur. It does not imply that with probability 1, we have that for any ε > 0 the inequality \n  \n    \n      \n        \n          |\n        \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        −\n        μ\n        \n          |\n        \n        <\n        ε\n      \n    \n    {\\displaystyle |{\\overline {X}}_{n}-\\mu |<\\varepsilon }\n  \n holds for all large enough n, since the convergence is not necessarily uniform on the set where it holds.[24]\n\nThe strong law does not hold in the following cases, but the weak law does.[25][26]\n\nThere are extensions of the law of large numbers to collections of estimators, where the convergence is uniform over the collection; thus the name uniform law of large numbers.\n\nSuppose f(x,θ) is some function defined for θ ∈ Θ, and continuous in θ. Then for any fixed θ, the sequence {f(X1,θ), f(X2,θ), ...} will be a sequence of independent and identically distributed random variables, such that the sample mean of this sequence converges in probability to E[f(X,θ)]. This is the pointwise (in θ) convergence.\n\nA particular example of a uniform law of large numbers states the conditions under which the convergence happens uniformly in θ. If[29][30]\n\nsup\n          \n            θ\n            ∈\n            Θ\n          \n        \n        \n          ‖\n          \n            \n              \n                1\n                n\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            f\n            (\n            \n              X\n              \n                i\n              \n            \n            ,\n            θ\n            )\n            −\n            E\n            ⁡\n            [\n            f\n            (\n            X\n            ,\n            θ\n            )\n            ]\n          \n          ‖\n        \n        \n          \n            →\n            \n              P\n            \n          \n        \n         \n        0.\n      \n    \n    {\\displaystyle \\sup _{\\theta \\in \\Theta }\\left\\|{\\frac {1}{n}}\\sum _{i=1}^{n}f(X_{i},\\theta )-\\operatorname {E} [f(X,\\theta )]\\right\\|{\\overset {\\mathrm {P} }{\\rightarrow }}\\ 0.}\n\nThis result is useful to derive consistency of a large class of estimators (see Extremum estimator).\n\nBorel\'s law of large numbers, named after Émile Borel, states that if an experiment is repeated a large number of times, independently under identical conditions, then the proportion of times that any specified event is expected to occur approximately equals the probability of the event\'s occurrence on any particular trial; the larger the number of repetitions, the better the approximation tends to be. More precisely, if E denotes the event in question, p its probability of occurrence, and Nn(E) the number of times E occurs in the first n trials, then with probability one,[31]\n\n  \n    \n      \n        \n          \n            \n              \n                N\n                \n                  n\n                \n              \n              (\n              E\n              )\n            \n            n\n          \n        \n        →\n        p\n        \n           as \n        \n        n\n        →\n        ∞\n        .\n      \n    \n    {\\displaystyle {\\frac {N_{n}(E)}{n}}\\to p{\\text{ as }}n\\to \\infty .}\n\nThis theorem makes rigorous the intuitive notion of probability as the expected long-run relative frequency of an event\'s occurrence.  It is a special case of any of several more general laws of large numbers in probability theory.\n\nChebyshev\'s inequality. Let X be a random variable with finite expected value μ and finite non-zero variance σ2. Then for any real number k > 0,\n\nPr\n        (\n        \n          |\n        \n        X\n        −\n        μ\n        \n          |\n        \n        ≥\n        k\n        σ\n        )\n        ≤\n        \n          \n            1\n            \n              k\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\Pr(|X-\\mu |\\geq k\\sigma )\\leq {\\frac {1}{k^{2}}}.}\n\nGiven X1, X2, ... an infinite sequence of i.i.d. random variables with finite expected value \n  \n    \n      \n        E\n        (\n        \n          X\n          \n            1\n          \n        \n        )\n        =\n        E\n        (\n        \n          X\n          \n            2\n          \n        \n        )\n        =\n        ⋯\n        =\n        μ\n        <\n        ∞\n      \n    \n    {\\displaystyle E(X_{1})=E(X_{2})=\\cdots =\\mu <\\infty }\n  \n, we are interested in the convergence of the sample average\n\nX\n              ¯\n            \n          \n          \n            n\n          \n        \n        =\n        \n          \n            \n              1\n              n\n            \n          \n        \n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\overline {X}}_{n}={\\tfrac {1}{n}}(X_{1}+\\cdots +X_{n}).}\n\nThis proof uses the assumption of finite variance \n  \n    \n      \n        Var\n        ⁡\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        =\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {Var} (X_{i})=\\sigma ^{2}}\n  \n (for all \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n). The independence of the random variables implies no correlation between them, and we have that\n\nVar\n        ⁡\n        (\n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        )\n        =\n        Var\n        ⁡\n        (\n        \n          \n            \n              1\n              n\n            \n          \n        \n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        )\n        =\n        \n          \n            1\n            \n              n\n              \n                2\n              \n            \n          \n        \n        Var\n        ⁡\n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            \n              n\n              \n                σ\n                \n                  2\n                \n              \n            \n            \n              n\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {Var} ({\\overline {X}}_{n})=\\operatorname {Var} ({\\tfrac {1}{n}}(X_{1}+\\cdots +X_{n}))={\\frac {1}{n^{2}}}\\operatorname {Var} (X_{1}+\\cdots +X_{n})={\\frac {n\\sigma ^{2}}{n^{2}}}={\\frac {\\sigma ^{2}}{n}}.}\n\nThe common mean μ of the sequence is the mean of the sample average:\n\nE\n        (\n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n        )\n        =\n        μ\n        .\n      \n    \n    {\\displaystyle E({\\overline {X}}_{n})=\\mu .}\n\nUsing Chebyshev\'s inequality on \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {X}}_{n}}\n  \n results in\n\nP\n        ⁡\n        (\n        \n          |\n          \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            −\n            μ\n          \n          |\n        \n        ≥\n        ε\n        )\n        ≤\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            \n              n\n              \n                ε\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {P} (\\left|{\\overline {X}}_{n}-\\mu \\right|\\geq \\varepsilon )\\leq {\\frac {\\sigma ^{2}}{n\\varepsilon ^{2}}}.}\n\nP\n        ⁡\n        (\n        \n          |\n          \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            −\n            μ\n          \n          |\n        \n        <\n        ε\n        )\n        =\n        1\n        −\n        P\n        ⁡\n        (\n        \n          |\n          \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            −\n            μ\n          \n          |\n        \n        ≥\n        ε\n        )\n        ≥\n        1\n        −\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            \n              n\n              \n                ε\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {P} (\\left|{\\overline {X}}_{n}-\\mu \\right|<\\varepsilon )=1-\\operatorname {P} (\\left|{\\overline {X}}_{n}-\\mu \\right|\\geq \\varepsilon )\\geq 1-{\\frac {\\sigma ^{2}}{n\\varepsilon ^{2}}}.}\n\nAs n approaches infinity, the expression approaches 1. And by definition of convergence in probability, we have obtained\n\nBy Taylor\'s theorem for complex functions, the characteristic function of any random variable, X, with finite mean μ, can be written as\n\nφ\n          \n            X\n          \n        \n        (\n        t\n        )\n        =\n        1\n        +\n        i\n        t\n        μ\n        +\n        o\n        (\n        t\n        )\n        ,\n        \n        t\n        →\n        0.\n      \n    \n    {\\displaystyle \\varphi _{X}(t)=1+it\\mu +o(t),\\quad t\\rightarrow 0.}\n\nAll X1, X2, ... have the same characteristic function, so we will simply denote this φX.\n\nAmong the basic properties of characteristic functions there are\n\nφ\n          \n            \n              \n                1\n                n\n              \n            \n            X\n          \n        \n        (\n        t\n        )\n        =\n        \n          φ\n          \n            X\n          \n        \n        (\n        \n          \n            \n              t\n              n\n            \n          \n        \n        )\n        \n        \n          and\n        \n        \n        \n          φ\n          \n            X\n            +\n            Y\n          \n        \n        (\n        t\n        )\n        =\n        \n          φ\n          \n            X\n          \n        \n        (\n        t\n        )\n        \n          φ\n          \n            Y\n          \n        \n        (\n        t\n        )\n        \n      \n    \n    {\\displaystyle \\varphi _{{\\frac {1}{n}}X}(t)=\\varphi _{X}({\\tfrac {t}{n}})\\quad {\\text{and}}\\quad \\varphi _{X+Y}(t)=\\varphi _{X}(t)\\varphi _{Y}(t)\\quad }\n  \n if X and Y are independent.\n\nThese rules can be used to calculate the characteristic function of \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {X}}_{n}}\n  \n in terms of φX:\n\nφ\n          \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n          \n        \n        (\n        t\n        )\n        =\n        \n          \n            [\n            \n              \n                φ\n                \n                  X\n                \n              \n              \n                (\n                \n                  \n                    t\n                    n\n                  \n                \n                )\n              \n            \n            ]\n          \n          \n            n\n          \n        \n        =\n        \n          \n            [\n            \n              1\n              +\n              i\n              μ\n              \n                \n                  t\n                  n\n                \n              \n              +\n              o\n              \n                (\n                \n                  \n                    t\n                    n\n                  \n                \n                )\n              \n            \n            ]\n          \n          \n            n\n          \n        \n        \n        →\n        \n        \n          e\n          \n            i\n            t\n            μ\n          \n        \n        ,\n        \n        \n          as\n        \n        \n        n\n        →\n        ∞\n        .\n      \n    \n    {\\displaystyle \\varphi _{{\\overline {X}}_{n}}(t)=\\left[\\varphi _{X}\\left({t \\over n}\\right)\\right]^{n}=\\left[1+i\\mu {t \\over n}+o\\left({t \\over n}\\right)\\right]^{n}\\,\\rightarrow \\,e^{it\\mu },\\quad {\\text{as}}\\quad n\\to \\infty .}\n\nThe limit eitμ is the characteristic function of the constant random variable μ, and hence by the Lévy continuity theorem, \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {X}}_{n}}\n  \n converges in distribution to μ:\n\nX\n              ¯\n            \n          \n          \n            n\n          \n        \n        \n        \n          \n            →\n            \n              D\n            \n          \n        \n        \n        μ\n        \n        \n          for\n        \n        \n        n\n        →\n        ∞\n        .\n      \n    \n    {\\displaystyle {\\overline {X}}_{n}\\,{\\overset {\\mathcal {D}}{\\rightarrow }}\\,\\mu \\qquad {\\text{for}}\\qquad n\\to \\infty .}\n\nμ is a constant, which implies that convergence in distribution to μ and convergence in probability to μ are equivalent (see Convergence of random variables.) Therefore,\n\nThis shows that the sample mean converges in probability to the derivative of the characteristic function at the origin, as long as the latter exists.\n\nWe give a relatively simple proof of the strong law under the assumptions that the \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  \n are iid, \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n        \n        ]\n        =:\n        μ\n        <\n        ∞\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}]=:\\mu <\\infty }\n  \n, \n  \n    \n      \n        Var\n        ⁡\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        =\n        \n          σ\n          \n            2\n          \n        \n        <\n        ∞\n      \n    \n    {\\displaystyle \\operatorname {Var} (X_{i})=\\sigma ^{2}<\\infty }\n  \n, and \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            4\n          \n        \n        ]\n        =:\n        τ\n        <\n        ∞\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{4}]=:\\tau <\\infty }\n  \n.\n\nLet us first note that without loss of generality we can assume that \n  \n    \n      \n        μ\n        =\n        0\n      \n    \n    {\\displaystyle \\mu =0}\n  \n by centering. In this case, the strong law says that\n\nPr\n        \n        \n          (\n          \n            \n              lim\n              \n                n\n                →\n                ∞\n              \n            \n            \n              \n                \n                  X\n                  ¯\n                \n              \n              \n                n\n              \n            \n            =\n            0\n          \n          )\n        \n        =\n        1\n        ,\n      \n    \n    {\\displaystyle \\Pr \\!\\left(\\lim _{n\\to \\infty }{\\overline {X}}_{n}=0\\right)=1,}\n  \n\nor\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            ω\n            :\n            \n              lim\n              \n                n\n                →\n                ∞\n              \n            \n            \n              \n                \n                  \n                    S\n                    \n                      n\n                    \n                  \n                  (\n                  ω\n                  )\n                \n                n\n              \n            \n            =\n            0\n          \n          )\n        \n        =\n        1.\n      \n    \n    {\\displaystyle \\Pr \\left(\\omega :\\lim _{n\\to \\infty }{\\frac {S_{n}(\\omega )}{n}}=0\\right)=1.}\n  \n\nIt is equivalent to show that\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            ω\n            :\n            \n              lim\n              \n                n\n                →\n                ∞\n              \n            \n            \n              \n                \n                  \n                    S\n                    \n                      n\n                    \n                  \n                  (\n                  ω\n                  )\n                \n                n\n              \n            \n            ≠\n            0\n          \n          )\n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\Pr \\left(\\omega :\\lim _{n\\to \\infty }{\\frac {S_{n}(\\omega )}{n}}\\neq 0\\right)=0,}\n  \n\nNote that\n\n  \n    \n      \n        \n          lim\n          \n            n\n            →\n            ∞\n          \n        \n        \n          \n            \n              \n                S\n                \n                  n\n                \n              \n              (\n              ω\n              )\n            \n            n\n          \n        \n        ≠\n        0\n        \n        ⟺\n        \n        ∃\n        ϵ\n        >\n        0\n        ,\n        \n          |\n          \n            \n              \n                \n                  S\n                  \n                    n\n                  \n                \n                (\n                ω\n                )\n              \n              n\n            \n          \n          |\n        \n        ≥\n        ϵ\n         \n        \n          \n            infinitely often\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\lim _{n\\to \\infty }{\\frac {S_{n}(\\omega )}{n}}\\neq 0\\iff \\exists \\epsilon >0,\\left|{\\frac {S_{n}(\\omega )}{n}}\\right|\\geq \\epsilon \\ {\\mbox{infinitely often}},}\n  \n\nand thus to prove the strong law we need to show that for every \n  \n    \n      \n        ϵ\n        >\n        0\n      \n    \n    {\\displaystyle \\epsilon >0}\n  \n, we have\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            ω\n            :\n            \n              |\n            \n            \n              S\n              \n                n\n              \n            \n            (\n            ω\n            )\n            \n              |\n            \n            ≥\n            n\n            ϵ\n            \n              \n                 infinitely often\n              \n            \n          \n          )\n        \n        =\n        0.\n      \n    \n    {\\displaystyle \\Pr \\left(\\omega :|S_{n}(\\omega )|\\geq n\\epsilon {\\mbox{ infinitely often}}\\right)=0.}\n  \n\nDefine the events \n  \n    \n      \n        \n          A\n          \n            n\n          \n        \n        =\n        {\n        ω\n        :\n        \n          |\n        \n        \n          S\n          \n            n\n          \n        \n        \n          |\n        \n        ≥\n        n\n        ϵ\n        }\n      \n    \n    {\\displaystyle A_{n}=\\{\\omega :|S_{n}|\\geq n\\epsilon \\}}\n  \n, and if we can show that \n\n  \n    \n      \n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        Pr\n        (\n        \n          A\n          \n            n\n          \n        \n        )\n        <\n        ∞\n        ,\n      \n    \n    {\\displaystyle \\sum _{n=1}^{\\infty }\\Pr(A_{n})<\\infty ,}\n  \n\nthen the Borel-Cantelli Lemma implies the result.  So let us estimate \n  \n    \n      \n        Pr\n        (\n        \n          A\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Pr(A_{n})}\n  \n.\n\nWe compute \n\n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          S\n          \n            n\n          \n          \n            4\n          \n        \n        ]\n        =\n        \n          \n            E\n          \n        \n        \n          [\n          \n            \n              (\n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  X\n                  \n                    i\n                  \n                \n              \n              )\n            \n            \n              4\n            \n          \n          ]\n        \n        =\n        \n          \n            E\n          \n        \n        \n          [\n          \n            \n              ∑\n              \n                1\n                ≤\n                i\n                ,\n                j\n                ,\n                k\n                ,\n                l\n                ≤\n                n\n              \n            \n            \n              X\n              \n                i\n              \n            \n            \n              X\n              \n                j\n              \n            \n            \n              X\n              \n                k\n              \n            \n            \n              X\n              \n                l\n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle {\\mathbb {E} }[S_{n}^{4}]={\\mathbb {E} }\\left[\\left(\\sum _{i=1}^{n}X_{i}\\right)^{4}\\right]={\\mathbb {E} }\\left[\\sum _{1\\leq i,j,k,l\\leq n}X_{i}X_{j}X_{k}X_{l}\\right].}\n  \n\nWe first claim that every term of the form \n  \n    \n      \n        \n          X\n          \n            i\n          \n          \n            3\n          \n        \n        \n          X\n          \n            j\n          \n        \n        ,\n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        \n          X\n          \n            j\n          \n        \n        \n          X\n          \n            k\n          \n        \n        ,\n        \n          X\n          \n            i\n          \n        \n        \n          X\n          \n            j\n          \n        \n        \n          X\n          \n            k\n          \n        \n        \n          X\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle X_{i}^{3}X_{j},X_{i}^{2}X_{j}X_{k},X_{i}X_{j}X_{k}X_{l}}\n  \n where all subscripts are distinct, must have zero expectation.  This is because \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            3\n          \n        \n        \n          X\n          \n            j\n          \n        \n        ]\n        =\n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            3\n          \n        \n        ]\n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            j\n          \n        \n        ]\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{3}X_{j}]={\\mathbb {E} }[X_{i}^{3}]{\\mathbb {E} }[X_{j}]}\n  \n by independence, and the last term is zero—and similarly for the other terms.  Therefore the only terms in the sum with nonzero expectation are \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            4\n          \n        \n        ]\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{4}]}\n  \n and \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        \n          X\n          \n            j\n          \n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{2}X_{j}^{2}]}\n  \n.  Since the \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  \n are identically distributed, all of these are the same, and moreover \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        \n          X\n          \n            j\n          \n          \n            2\n          \n        \n        ]\n        =\n        (\n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        ]\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{2}X_{j}^{2}]=({\\mathbb {E} }[X_{i}^{2}])^{2}}\n  \n.\n\nThere are \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n terms of the form \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            4\n          \n        \n        ]\n      \n    \n    {\\displaystyle {\\mathbb {E} }[X_{i}^{4}]}\n  \n and \n  \n    \n      \n        3\n        n\n        (\n        n\n        −\n        1\n        )\n      \n    \n    {\\displaystyle 3n(n-1)}\n  \n terms of the form \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        [\n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        ]\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle ({\\mathbb {E} }[X_{i}^{2}])^{2}}\n  \n, and so\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          S\n          \n            n\n          \n          \n            4\n          \n        \n        ]\n        =\n        n\n        τ\n        +\n        3\n        n\n        (\n        n\n        −\n        1\n        )\n        \n          σ\n          \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\mathbb {E} }[S_{n}^{4}]=n\\tau +3n(n-1)\\sigma ^{4}.}\n  \n\nNote that the right-hand side is a quadratic polynomial in \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, and as such there exists a \n  \n    \n      \n        C\n        >\n        0\n      \n    \n    {\\displaystyle C>0}\n  \n such that \n  \n    \n      \n        \n          \n            E\n          \n        \n        [\n        \n          S\n          \n            n\n          \n          \n            4\n          \n        \n        ]\n        ≤\n        C\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathbb {E} }[S_{n}^{4}]\\leq Cn^{2}}\n  \n for \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n sufficiently large.  By Markov,\n\n  \n    \n      \n        Pr\n        (\n        \n          |\n        \n        \n          S\n          \n            n\n          \n        \n        \n          |\n        \n        ≥\n        n\n        ϵ\n        )\n        ≤\n        \n          \n            1\n            \n              (\n              n\n              ϵ\n              \n                )\n                \n                  4\n                \n              \n            \n          \n        \n        \n          \n            E\n          \n        \n        [\n        \n          S\n          \n            n\n          \n          \n            4\n          \n        \n        ]\n        ≤\n        \n          \n            C\n            \n              \n                ϵ\n                \n                  4\n                \n              \n              \n                n\n                \n                  2\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\Pr(|S_{n}|\\geq n\\epsilon )\\leq {\\frac {1}{(n\\epsilon )^{4}}}{\\mathbb {E} }[S_{n}^{4}]\\leq {\\frac {C}{\\epsilon ^{4}n^{2}}},}\n  \n\nfor \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n sufficiently large, and therefore this series is summable.  Since this holds for any \n  \n    \n      \n        ϵ\n        >\n        0\n      \n    \n    {\\displaystyle \\epsilon >0}\n  \n, we have established the Strong Law of Large Numbers.[32] The proof can be strengthened immensely by dropping all finiteness assumptions on the second and fourth moments. It can also be extended for example to discuss partial sums of distributions without any finite moments. Such proofs use more intricate arguments to prove the same Borel-Cantelli predicate, a strategy attributed to Kolmogorov to conceptually bring the limit inside the probability parentheses. [33]\n\nThe law of large numbers provides an expectation of an unknown distribution from a realization of the sequence, but also any feature of the probability distribution.[1] By applying Borel\'s law of large numbers, one could easily obtain the probability mass function. For each event in the objective probability mass function, one could approximate the probability of the event\'s occurrence with the proportion of times that any specified event occurs. The larger the number of repetitions, the better the approximation. As for the continuous case: \n  \n    \n      \n        C\n        =\n        (\n        a\n        −\n        h\n        ,\n        a\n        +\n        h\n        ]\n      \n    \n    {\\displaystyle C=(a-h,a+h]}\n  \n, for small positive h. Thus, for large n:\n\nN\n                \n                  n\n                \n              \n              (\n              C\n              )\n            \n            n\n          \n        \n        ≈\n        p\n        =\n        P\n        (\n        X\n        ∈\n        C\n        )\n        =\n        \n          ∫\n          \n            a\n            −\n            h\n          \n          \n            a\n            +\n            h\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        ≈\n        2\n        h\n        f\n        (\n        a\n        )\n      \n    \n    {\\displaystyle {\\frac {N_{n}(C)}{n}}\\thickapprox p=P(X\\in C)=\\int _{a-h}^{a+h}f(x)\\,dx\\thickapprox 2hf(a)}\n\nWith this method, one can cover the whole x-axis with a grid (with grid size 2h) and obtain a bar graph which is called a histogram.\n\nOne application of the Law of Large Numbers is an important method of approximation known as the Monte Carlo method,[3] which uses a random sampling of numbers to approximate numerical results. The algorithm to compute an integral of f(x) on an interval [a,b] is as follows:[3]\n\nWe can find the integral of \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        c\n        o\n        \n          s\n          \n            2\n          \n        \n        (\n        x\n        )\n        \n          \n            \n              x\n              \n                3\n              \n            \n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle f(x)=cos^{2}(x){\\sqrt {x^{3}+1}}}\n  \n on [-1,2]. Using traditional methods to compute this integral is very difficult, so the Monte Carlo method can be used here.[3] Using the above algorithm, we get\n\n∫\n          \n            −\n            1\n          \n          \n            2\n          \n        \n        f\n        (\n        x\n        )\n        \n          d\n          x\n        \n      \n    \n    {\\displaystyle \\int _{-1}^{2}f(x){dx}}\n  \n  = 0.905 when n=25\n\n∫\n          \n            −\n            1\n          \n          \n            2\n          \n        \n        f\n        (\n        x\n        )\n        \n          d\n          x\n        \n      \n    \n    {\\displaystyle \\int _{-1}^{2}f(x){dx}}\n  \n  = 1.028 when n=250\n\nWe observe that as n increases, the numerical value also increases. When we get the actual results for the integral we get\n\n∫\n          \n            −\n            1\n          \n          \n            2\n          \n        \n        f\n        (\n        x\n        )\n        \n          d\n          x\n        \n      \n    \n    {\\displaystyle \\int _{-1}^{2}f(x){dx}}\n  \n = 1.000194\n\nWhen the LLN was used, the approximation of the integral was closer to its true value, and thus more accurate.[3]\n\nAnother example is the integration of f(x) = \n  \n    \n      \n        \n          \n            \n              \n                e\n                \n                  x\n                \n              \n              −\n              1\n            \n            \n              e\n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {e^{x}-1}{e-1}}}\n  \n on [0,1].[34] Using the Monte Carlo method and the LLN, we can see that as the number of samples increases, the numerical value gets closer to 0.4180233.[34]',
        pageTitle: "Law of large numbers",
    },
    {
        title: "Postel's law",
        link: "https://en.wikipedia.org/wiki/Postel%27s_law",
        content:
            'In computing, the robustness principle is a design guideline for software that states: "be conservative in what you do, be liberal in what you accept from others". It is often reworded as: "be conservative in what you send, be liberal in what you accept". The principle is also known as Postel\'s law, after Jon Postel, who used the wording in an early specification of TCP.[1]\n\nIn other words, programs that send messages to other machines (or to other programs on the same machine) should conform completely to the specifications, but programs that receive messages should accept non-conformant input as long as the meaning is clear.\n\nAmong programmers, to produce compatible functions, the principle is also known in the form: be contravariant in the input type and covariant in the output type.\n\nRFC 1122 (1989) expanded on Postel\'s principle by recommending that programmers "assume that the network is filled with malevolent entities that will send in packets designed to have the worst possible effect".[2] Protocols should allow for the addition of new codes for existing fields in future versions of protocols by accepting messages with unknown codes (possibly logging them). Programmers should avoid sending messages with "legal but obscure protocol features" that might expose deficiencies in receivers, and design their code "not just to survive other misbehaving hosts, but also to cooperate to limit the amount of disruption such hosts can cause to the shared communication facility".[3]\n\nIn 2001, Marshall Rose characterized several deployment problems when applying Postel\'s principle in the design of a new application protocol.[4] For example, a defective implementation that sends non-conforming messages might be used only with implementations that tolerate those deviations from the specification until, possibly several years later, it is connected with a less tolerant application that rejects its messages. In such a situation, identifying the problem is often difficult, and deploying a solution can be costly. Rose therefore recommended "explicit consistency checks in a protocol ... even if they impose implementation overhead".\n\nIn 2018, a paper on privacy-enhancing technologies by Florentin Rochet and Olivier Pereira showed how to exploit Postel\'s robustness principle inside the Tor routing protocol to compromise the anonymity of onion services and Tor clients.[5]\n\nIn 2023, Martin Thomson and David Schinazi argued that Postel\'s robustness principle actually leads to a lack of robustness, including security:[6].mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nA flaw can become entrenched as a de facto standard. Any implementation of the protocol is required to replicate the aberrant behavior, or it is not interoperable. This is both a consequence of tolerating the unexpected and a product of a natural reluctance to avoid fatal error conditions. Ensuring interoperability in this environment is often referred to as aiming to be "bug-for-bug compatible".',
        pageTitle: "Robustness principle",
    },
    {
        title: "Pournelle's iron law of bureaucracy",
        link: "https://en.wikipedia.org/wiki/Pournelle%27s_iron_law_of_bureaucracy",
        content:
            'Jerry Eugene Pournelle (/pʊərˈnɛl/; August 7, 1933 – September 8, 2017) was an American scientist in the area of operations research and human factors research, a science fiction writer, essayist, journalist, and one of the first bloggers.[1] In the 1960s and early 1970s, he worked in the aerospace industry, but eventually focused on his writing career. In an obituary in Gizmodo, he was described as "a tireless ambassador for the future."[2]\n\nPournelle\'s hard science fiction writing received multiple awards. In addition to his solo writing, he wrote several novels with collaborators including Larry Niven. Pournelle served a term as President of the Science Fiction and Fantasy Writers of America.[3]\n\nPournelle\'s journalism focused primarily on the computer industry, astronomy, and space exploration. From the 1970s until the early 1990s, he contributed to the computer magazine Byte, writing from the viewpoint of an intelligent user, with the oft-cited credo, "We do this stuff so you won\'t have to."[4] He created one of the first blogs, entitled "Chaos Manor", which included commentary about politics, computer technology, space technology, and science fiction.\n\nPournelle held paleoconservative political views,[5] which were sometimes expressed in his fiction. He was one of the founders of the Citizens\' Advisory Council on National Space Policy, which developed some of the Reagan Administration\'s space initiatives, including the earliest versions of what would become the Strategic Defense Initiative.\n\nPournelle was born in Shreveport, Louisiana, the seat of Caddo Parish in northwestern Louisiana, and later lived with his family in Capleville, Tennessee, an unincorporated area near Memphis.[6] Percival Pournelle, his father, was a radio advertising executive and general manager of several radio stations. Ruth Pournelle, his mother, was a teacher, although during World War II, she worked in a munitions factory.[7]\n\nHe attended first grade at St. Anne\'s Elementary School, in Memphis, which had two grades to a classroom. Beginning with third grade, he attended Coleville Consolidated Elementary School, in Colevile, which had about 25 pupils per grade and four rooms and four teachers for 8 grades[8][9] Pournelle attended high school at Christian Brothers College in Memphis, run by the De La Salle Christian Brothers; despite its name, it was a high school at the time.[10]\n\nHe served in the United States Army during the Korean War. In 1953–54, after his military service, Pournelle attended the University of Iowa in Iowa City.[11] Subsequently, he studied at the University of Washington, where he received a B.S. in psychology on June 11, 1955; an M.S. in psychology (experimental statistics) on March 21, 1958; and a Ph.D. in political science in March 1964.[12]\n\nHis master\'s thesis is titled "Behavioural observations of the effects of personality needs and leadership in small discussion groups", and is dated 1957.[13] Pournelle\'s Ph.D. dissertation is titled "The American political continuum; an examination of the validity of the left-right model as an instrument for studying contemporary American political \'isms\'".[14][15]\n\nPournelle married Roberta Jane Isdell in 1959; the couple had five children.[16] His wife, and son, naval officer Phillip, and daughter, archaeologist Jennifer, have also written science fiction in collaboration with their father.[17][18][19]\n\nIn 2008, Pournelle battled a brain tumor, which appeared to respond favorably to radiation treatment.[20] An August 28, 2008 report on his weblog claimed he was now cancer-free. Pournelle suffered a stroke on December 16, 2014, for which he was hospitalized for a time. By June 2015, he was writing again, though impairment from the stroke had slowed his typing.[21][22] Pournelle died in his sleep of heart failure at his home in Studio City, California, on September 8, 2017.[23][24][7]\n\nPournelle was raised a Unitarian. He converted to Roman Catholicism while attending Christian Brothers College.\n\nPournelle was introduced to Malthusian principles upon reading the book Road to Survival[25] by the ecologist (and ornithologist) William Vogt, who depicted an Earth denuded of species other than humans, all of them headed for squalor. Concerned about the Malthusian dangers of human overpopulation, and considering the Catholic Church\'s position on contraception to be untenable, he left the Catholic Church while an undergraduate at the University of Iowa. Pournelle eventually returned to religion, and for a number of years was a high church Anglican, in part because Anglican theology was virtually identical to Catholic theology, with the exception that the Anglicans accepted as moral the use of birth control.[25]\n\nPournelle eventually returned to the Catholic Church, as his other beliefs were consistent with the Catholic communion, although he did not agree with the Church\'s position on birth control.[25] Despite his estrangement from the Catholic Church, he opposed having the government require that Catholic institutions provide access to birth control or abortion.[25]  He wrote that Sunday attendance at St. Francis de Sales Catholic Church, in Sherman Oaks, Los Angeles, was part of his family\'s routine.[4] Upon his death, his family arranged a memorial mass at the church, on 16 September 2017.[26]\n\nPournelle was an intellectual protégé of Russell Kirk and Stefan T. Possony.[27] Pournelle wrote numerous publications with Possony, including The Strategy of Technology (1970).  The Strategy has been used as a textbook at the United States Military Academy (West Point), the United States Air Force Academy (Colorado Springs), the Air War College, and the National War College.[28]\n\nHe told fellow author Robert Heinlein, Pournelle recalled, "that once I got into advance plans at Boeing I probably wrote more science fiction than he did, and I didn\'t have to put characters in mine".[7] In the late 1950s, while conducting operations research at the company, he envisioned a weapon consisting of massive tungsten rods dropped from high above the Earth. These super-dense, super-fast kinetic energy projectiles delivered enormous destructive force to the target without contaminating the environs with radioactive isotopes, as would occur with a nuclear bomb. Pournelle named his superweapon “Project Thor”. Others called it "Rods from God".[29] Pournelle headed the Human Factors Laboratory at Boeing, where his group did pioneering work on astronaut heat tolerance in extreme environments. His group also did experimental work that resulted in certification of the passenger oxygen system for the Boeing 707 airplane. He later worked as a Systems Analyst in a design and analysis group at the company, where he did strategic analysis of proposed new weapons systems.\n\nIn 1964, Pournelle joined the Aerospace Corporation in San Bernardino, California where he was Editor of Project 75, a major study of all ballistic missile technology for the purpose of making recommendations to the US Air Force on investment in technologies required to build the missile force to be deployed in 1975.[30] After Project 75 was completed Pournelle became manager of several advanced concept studies.\n\nAt North American Rockwell’s Space Division, Pournelle was associate director of operations research, where he took part in the Apollo program and general operations.[31][32][33][34]\n\nHe was founding President of the Pepperdine Research Institute. In 1989, Pournelle, Max Hunter, and retired Army Lieutenant General Daniel O. Graham made a presentation to then Vice President Dan Quayle promoting development of the DC-X rocket.[35]\n\nPournelle was among those who in 1968 signed a pro-Vietnam War advertisement in Galaxy Science Fiction.[36] During the 1970s and 1980s, he also published articles on military tactics and war gaming in the military simulations industry in Avalon Hill\'s magazine The General. That led him into correspondences with some of the early figures in Dungeons & Dragons and other fantasy role-playing games.[37]\n\nTwo of his collaborations with Larry Niven reached the top rankings in the New York Times Best Seller List. In 1977, Lucifer\'s Hammer reached number two.[citation needed] Footfall — wherein Heinlein was a thinly veiled minor character — reached the number one spot in 1986.[7]\n\nPournelle served as President of the Science Fiction and Fantasy Writers of America in 1973.[3]\n\nIn 1994, Pournelle\'s friendly relationship with Newt Gingrich led to Gingrich securing a government job for Pournelle\'s son, Richard.[38] At the time, Pournelle and Gingrich were reported to be collaborating on "a science fiction political thriller."[38] Pournelle\'s relationship with Gingrich was long established even then, as Pournelle had written the preface to Gingrich\'s book, Window of Opportunity (1985).[39]\n\nYears after Byte shuttered, Pournelle wrote his Chaos Manor column online. He reprised it at Byte.com, which he helped launch with journalist Gina Smith, John C. Dvorak, and others. However, after a shakeup, he announced that rather than stay at United Business Media, he would follow Smith, Dvorak, and 14 other news journalists to start an independent tech and politics site called anewdomain.net. As an active director of that site and others it launched, Pournelle wrote, edited, and worked with young writers and journalists on the craft of writing about science and tech.\n\nBeginning during his tenure at Boeing Company, Pournelle submitted science fiction short stories to John W. Campbell, the editor of Astounding Science Fiction (later called Analog Science Fiction and Fact), but Campbell did not accept any of Pournelle\'s submissions until shortly before Campbell\'s death in 1971, when he accepted for publication Pournelle\'s novelette "Peace with Honor."[40][41]  From the beginning, Pournelle\'s work has engaged strong military themes. Several books are centered on a fictional mercenary infantry force known as Falkenberg\'s Legion. There are strong parallels between these stories and the Childe Cycle mercenary stories by Gordon R. Dickson, as well as Heinlein\'s Starship Troopers, although Pournelle\'s work takes far fewer technological leaps than either of these.\n\nPournelle was one of the few close friends of H. Beam Piper and was granted by Piper the rights to produce stories set in Piper\'s Terro-Human Future History. This right has been recognized by the Piper estate.[citation needed] Pournelle worked for some years on a sequel to Space Viking but abandoned this in the early 1990s, however John F. Carr and Mike Robertson completed this sequel, entitled The Last Space Viking, and it was published in 2011.[42]\n\nIn 2013, Variety reported that motion picture rights to Pournelle\'s novel Janissaries had been acquired by the newly formed Goddard Film Group, headed by Gary Goddard.[43] The IMDb website reported that the film was in development, and that husband-and-wife writing team, Judith and Garfield Reeves-Stevens, had written the screenplay.[44]\n\nPournelle began fiction writing non-SF work under a pseudonym in 1965. His early SF was published under the name "Wade Curtis", in Analog and other magazines. Some works were also published under the name "J.E. Pournelle".\n\nIn the mid-1970s, Pournelle began a fruitful collaboration with Larry Niven; he has also collaborated on novels with Roland J. Green, Michael F. Flynn, and Steven Barnes, and collaborated as an editor on an anthology series The Endless Frontier with John F. Carr.\n\nIn 2010, his daughter Jennifer R. Pournelle (writing as J.R. Pournelle), an archaeology professor, e-published a novel Outies, an authorized sequel to the Mote in God\'s Eye series.[45][46]\n\nPournelle began using a computer to write in 1977 on the advice of his "mad" friend Dan MacLean.[47] He wrote the "Computing at Chaos Manor" column in Byte, describing experiences with computer hardware and software, some purchased and some sent by vendors for review, at his home office. Because Pournelle was then, according to the magazine, "virtually Byte\'s only writer who was a mere user—he didn\'t create compilers and computers, he merely used them", it began as "The User\'s Column" in July 1980. Subtitled "Omikron TRS-80 Boards, NEWDOS+, and Sundry Other Matters", an Editor\'s Note accompanied the article:[48][49]\n\nThe other day we were sitting around the Byte offices listening to software and hardware explosions going off around us in the microcomputer world. We wondered, "Who could cover some of the latest developments for us in a funny, frank (and sometimes irascible) style?" The phone rang. It was Jerry Pournelle with an idea for a funny, frank (and sometimes irascible) series of articles to be presented in Byte on a semi-regular (i.e.: every 2 to 3 months) basis, which would cover the wild microcomputer goings-on at the Pournelle House ("Chaos Manor") in Southern California. We said yes. Herewith the first installment ...\n\nThis will be a column by and for computer users, and with rare exceptions I won\'t discuss anything I haven\'t installed and implemented here in Chaos Manor. At Chaos Manor we have computer users ranging in sophistication from my 9-year-old through a college-undergraduate assistant and up to myself. (Not that I\'m the last word in sophistication, but I do sit here and pound this machine a lot; if I can\'t get something to work, it takes an expert.) Fair warning, then: the very nature of this column limits its scope. I can\'t talk about anything I can\'t run on my machines, nor am I likely to discuss things I have no use for.\n\nAmong recurring characters were Pournelle\'s family members, friends, and many computers.[47] He introduced to readers "my friend Ezekiel, who happens to be a Cromemco Z-2 with iCom 8-inch soft-sectored floppy disk drives"; he also owned a TRS-80 Model I, and the first subject discussed in the column was an add-on that permitted it to use the same data and CP/M applications as the Cromemco.[49] The next column appeared in December 1980 with the subtitle "BASIC, Computer Languages, and Computer Adventures";[50] Ezekiel II, a Compupro S-100 CP/M system, debuted in March 1983.[51] Other computers received nicknames, such as Zorro, Pournelle\'s "colorful" Zenith Z-100, and Lucy Van Pelt, a "fussbudget" IBM PC;[52] he referred to generic PC compatibles as "PClones". Pournelle often denounced companies that announced vaporware, sarcastically writing that they would arrive "Real Soon Now"[53] (later abbreviated to just "RSN"), and those that used software copy protection.[47] As part of a redesign in June 1984, the magazine renamed the popular column to "Computing at Chaos Manor", and the accompanying letter column became "Chaos Manor Mail".[48]\n\nPournelle preferred 5 1/4-inch floppy disks to 8-inch disks as late as 1982,[54] and still used "Zeke" to write as late as 1987, but admitted that he would soon have to use PCs because tools like Borland Sidekick were unavailable. He hesitated, Pournelle said, because Niven would buy two exact copies of his writing computer and software.[55] He announced in February 1989 that the Smithsonian had asked for "Zeke" as part of a history of computing.[56] A memorable column in August 1989 was "The Great Power Spike", which gives a digital necropsy of his electronic equipment after high voltage transmission wires dropped onto the power line for his neighborhood.[57][58]\n\nAfter the print version of Byte ended publication in the United States, Pournelle continued publishing the column for the online version and international print editions of Byte.  In July 2006, Pournelle and Byte declined to renew their contract[citation needed] and Pournelle moved the column to his own web site, Chaos Manor Reviews.[59]\n\nPournelle claimed to be the first author to have written a published book contribution using a word processor on a personal computer, in 1977.[60][61]\n\nIn the 1980s, Pournelle was an editor and columnist for Survive, a survivalist magazine.[62] He wrote the monthly column "The Micro Revolution" for Popular Computing from April 1984 until the magazine\'s closure in December 1985. The column focused on the ways microcomputers were reshaping society.[63][64]\n\nIn 2011, Pournelle joined journalist Gina Smith, pundit John C. Dvorak, political cartoonist Ted Rall, and several other Byte.com staff reporters to launch an independent tech and political news site, aNewDomain.net[65]  Pournelle served as director of aNewDomain until his death.[66]\n\nAfter 1998, Pournelle maintained a website with a daily online journal, "View from Chaos Manor," a blog dating from before the use of that term.[67] It is a collection of his "Views" and "Mail" from a large variety of readers. This is a continuation of his 1980s blog-like online journal on GEnie.  He said he resists using the term "blog" because he considered the word ugly, and because he maintained that his "View" is primarily a vehicle for writing rather than a collection of links. In his book Dave Barry in Cyberspace, humorist Dave Barry has fun with Pournelle\'s guru column in Byte magazine.\n\nPournelle, in collaboration with his wife, Roberta (who was an expert on reading education) wrote the commercial education software program called Reading: The Learning Connection.[68][69]\n\nPournelle served as campaign research director for the mayoral campaign of 1969 for Los Angeles Mayor Sam Yorty (Democrat), working under campaign director Henry Salvatori.[70] The election took place on May 27, 1969.[71] Pournelle was later named Executive Assistant to the Mayor in charge of research in September 1969, but resigned from the position after two weeks.[72] After leaving Yorty\'s office, in 1970 he was a consultant to the Professional Educators of Los Angeles (PELA), a group opposed to the unionization of school teachers in LA.[73]\n\nHe is sometimes quoted as describing his politics as "somewhere to the right of Genghis Khan."[74] Pournelle resisted others classifying him into any particular political group, but acknowledged the approximate accuracy of the term paleoconservatism as applying to him. He distinguished his conservativism from the alternative neoconservatism, noting that he had been drummed out of the Conservative movement by "the egregious Frum", referring to prominent neoconservative, David Frum.[75] Notably, Pournelle opposed the Gulf War and the Iraq War, maintaining that the money would be better spent developing energy technologies for the United States. According to a Wall Street Journal article, "Pournelle estimates that for what the Iraq war has cost so far, the United States could have paid for a network of nuclear power stations sufficient to achieve energy independence, and bankrupt the Arabs for once and for all."[76]\n\nPournelle created the Pournelle chart in his doctoral dissertation, a 2-dimensional coordinate system used to distinguish political ideologies. It is a cartesian diagram in which the X-axis gauges opinion toward state and centralized government (farthest right being state worship, farthest left being the idea of a state as the "ultimate evil"), and the Y-axis measures the belief that all problems in society have rational solutions (top being complete confidence in rational planning, bottom being complete lack of confidence in rational planning).[77]\n\nIn a 1997 article, Norman Spinrad wrote that Pournelle had written the SDI portion of Ronald Reagan\'s State of the Union Address, as part of a plan to use SDI to get more money for space exploration using the larger defense budget.[78] Pournelle wrote in response that while the Citizens\' Advisory Council on National Space Policy "wrote parts of Reagan\'s 1983 SDI speech, and provided much of the background for the policy, we certainly did not write the speech…  We were not trying to boost space, we were trying to win the Cold War". The Council\'s first report in 1980[79] became the transition team policy paper on space for the incoming Reagan administration. The third report was quoted in the Reagan "Star Wars" speech.[citation needed]\n\nJames Wheatfield wrote that "Pournelle delights in setting up complex background situations and plots, leading the reader step by step towards a solution which is the very opposite of politically correct and… defying a dissenting reader to find where in this logical chain he or she would have acted differently."[80]\n\nPournelle suggested several "laws".  He used the term "Pournelle\'s law" for the expression "One user, one CPU". He later amended this to "One user, at least one CPU" in a column in InfoWorld.[81] He also used the term "Pournelle\'s law" for "Silicon is cheaper than iron." That is, a computer is cheaper to upgrade than replace. A second aspect of this law was Pournelle\'s prediction that hard disk drives would eventually be replaced by solid-state memory,[82] although he admitted that bubble memory had failed to do so as he had expected.[55] He has also used "Pournelle\'s law" to apply to the importance of checking cable connections when diagnosing computer problems: "You\'ll find by and large, the trouble is a cable."[83] Another Pournelle\'s Law is "If you don’t know what you’re doing, deal with those who do".[47]\n\nAnother "law" of his is "Pournelle\'s iron law of bureaucracy":\n\nIn any bureaucracy, the people devoted to the benefit of the bureaucracy itself always get in control and those dedicated to the goals that the bureaucracy is supposed to accomplish have less and less influence, and sometimes are eliminated entirely.[84]\n\nPournelle\'s Iron Law of Bureaucracy states that in any bureaucratic organization there will be two kinds of people: First, there will be those who are devoted to the goals of the organization. Examples are dedicated classroom teachers in an educational bureaucracy, many of the engineers and launch technicians and scientists at NASA, even some agricultural scientists and advisors in the former Soviet Union collective farming administration.  Secondly, there will be those dedicated to the organization itself. Examples are many of the administrators in the education system, many professors of education, many teachers union officials, much of the NASA headquarters staff, etc.  The Iron Law states that in every case the second group will gain and keep control of the organization. It will write the rules, and control promotions within the organization.[85]\n\nThis is related to the iron law of oligarchy and to the Self-licking ice cream cone.[citation needed] His blog, "The View from Chaos Manor", often references apparent examples of the law.[86] Some of Pournelle\'s standard themes that recur in the stories are: welfare states become self-perpetuating, building a technological society requires a strong defense and the rule of law, and "those who forget history are condemned to repeat it".\n\nPournelle never won a Hugo Award. He said, "Money will get you through times of no Hugos better than Hugos will get you through times of no money."[87] The Mote in God\'s Eye and Inferno were both nominated for a Nebula Award for Best Novel in 1975.[88]',
        pageTitle: "Jerry Pournelle",
    },
    {
        title: "Price's law",
        link: "https://en.wikipedia.org/wiki/Price%27s_law",
        content:
            "Price's law or Price's square root law is a bibliometric hypothesis proposed by Derek J. de Solla Price suggesting that in any scientific field, half of the published research comes from the square root of the total number of authors in that field.\n\nThe law specifically states that if n represents the total number of authors in a scientific domain, then √n authors will be responsible for producing approximately 50% of the total publications in that field. For example, if 100 papers are written by 25 authors, then \n  \n    \n      \n        \n          \n            25\n          \n        \n        =\n        5\n      \n    \n    {\\displaystyle {\\sqrt {25}}=5}\n  \n out of the 25 authors will have contributed 50 papers.\n\nDerek J. de Solla Price introduced this concept in his 1963 book \"Little Science, Big Science\" as part of his broader research on scientific productivity and information dynamics.[1] The law was intended to describe the uneven distribution of scientific output across researchers.\n\nSubsequent research has largely contradicted Price's original hypothesis. Multiple studies across various scientific disciplines have found that the actual distribution of publications is more skewed than Price's law predicted. Most empirical analyses suggest that a much smaller proportion of researchers produce a significantly larger percentage of publications. The related Lotka's law,[2] for example, is a better fit.[3][4]\n\nDespite its empirical limitations, Price's law remains important in various fields,[5][6] for example to understand scientific productivity patterns, analyze or research output distributions, or highlight the concentration of scientific work among a small number of researchers",
        pageTitle: "Price's law",
    },
    {
        title: "Putt's law",
        link: "https://en.wikipedia.org/wiki/Putt%27s_Law_and_the_Successful_Technocrat",
        content:
            "Putt's Law and the Successful Technocrat is a book, credited to the pseudonym Archibald Putt, published in 1981. An updated edition, subtitled How to Win in the Information Age, was published by Wiley-IEEE Press in 2006.  The book is based upon a series of articles published in Research/Development Magazine in 1976 and 1977.\n\nIt proposes Putt's Law and Putt's Corollary[1] which are principles of negative selection similar to the Dilbert principle proposed by Scott Adams in 1995. Putt's law is sometimes grouped together with the Peter principle, Parkinson's Law and Stephen Potter's Gamesmanship series as \"P-literature\".[2]\n\nThis article about a book on management is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Putt's Law and the Successful Technocrat",
    },
    {
        title: "Raoult's law",
        link: "https://en.wikipedia.org/wiki/Raoult%27s_law",
        content:
            "Raoult's law (/ˈrɑːuːlz/ law) is a relation of physical chemistry, with implications in thermodynamics. Proposed by French chemist François-Marie Raoult in 1887,[1][2] it states that the partial pressure of each component of an ideal mixture of liquids is equal to the vapor pressure of the pure component (liquid or solid) multiplied by its mole fraction in the mixture. In consequence, the relative lowering of vapor pressure of a dilute solution of nonvolatile solute is equal to the mole fraction of solute in the solution.\n\nMathematically, Raoult's law for a single component in an ideal solution is stated as\n\nwhere \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  \n is the partial pressure of the component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in the gaseous mixture above the solution, \n  \n    \n      \n        \n          p\n          \n            i\n          \n          \n            ⋆\n          \n        \n      \n    \n    {\\displaystyle p_{i}^{\\star }}\n  \n is the equilibrium vapor pressure of the pure component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n is the mole fraction of the component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in the liquid or solid solution.[3]\n\nWhere two volatile liquids A and B are mixed with each other to form a solution, the vapor phase consists of both components of the solution. Once the components in the solution have reached equilibrium, the total vapor pressure of the solution can be determined by combining Raoult's law with Dalton's law of partial pressures to give\n\nIn other words, the vapor pressure of the solution is the mole-weighted mean of the individual vapour pressures:\n\nIf a non-volatile solute B (it has zero vapor pressure, so does not evaporate) is dissolved into a solvent A to form an ideal solution, the vapor pressure of the solution will be lower than that of the solvent. In an ideal solution of a nonvolatile solute, the decrease in vapor pressure is directly proportional to the mole fraction of solute:\n\nIf the solute associates or dissociates in the solution (such as an electrolyte/salt), the expression of the law includes the van 't Hoff factor as a correction factor. That is, the mole fraction must be calculated using the actual number of particles in solution.[4]\n\nRaoult's law is a phenomenological relation that assumes ideal behavior based on the simple microscopic assumption that intermolecular forces between unlike molecules are equal to those between similar molecules, and that their molar volumes are the same: the conditions of an ideal solution. This is analogous to the ideal gas law, which is a limiting law valid when the interactive forces between molecules approach zero, for example as the concentration approaches zero. Raoult's law is instead valid if the physical properties of the components are identical. The more similar the components are, the more their behavior approaches that described by Raoult's law. For example, if the two components differ only in isotopic content, then Raoult's law is essentially exact.\n\nComparing measured vapor pressures to predicted values from Raoult's law provides information about the true relative strength of intermolecular forces. If the vapor pressure is less than predicted (a negative deviation), fewer molecules of each component than expected have left the solution in the presence of the other component, indicating that the forces between unlike molecules are stronger. The converse is true for positive deviations.\n\nFor a solution of two liquids A and B, Raoult's law predicts that if no other gases are present, then the total vapor pressure \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n above the solution is equal to the weighted sum of the \"pure\" vapor pressures \n  \n    \n      \n        \n          p\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{A}}}\n  \n and \n  \n    \n      \n        \n          p\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{B}}}\n  \n of the two components.  Thus the total pressure above the solution of A and B would be\n\nSince the sum of the mole fractions is equal to one,\n\nThis is a linear function of the mole fraction \n  \n    \n      \n        \n          x\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{B}}}\n  \n, as shown in the graph.\n\nRaoult's law was first observed empirically and led François-Marie Raoult[1][2] to postulate that the vapor pressure above an ideal mixture of liquids is equal to the sum of the vapor pressures of each component multiplied by its mole fraction.[5]: 325  Taking compliance with Raoult's Law as a defining characteristic of ideality in a  solution, it is possible to deduce that the chemical potential of each component of the liquid is given by\n\nwhere \n  \n    \n      \n        \n          μ\n          \n            i\n          \n          \n            ⋆\n          \n        \n      \n    \n    {\\displaystyle \\mu _{i}^{\\star }}\n  \n is the chemical potential in the pure state and \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n is the mole fraction of component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in the ideal solution.  From this equation, other thermodynamic properties of an ideal solution may be determined.  If the assumption that the vapor follows the ideal gas law is added, Raoult's law may be derived as follows.\n\nIf the system is ideal, then, at equilibrium, the chemical potential of each component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n must be the same in the liquid and gas states. That is,\n\nSubstituting the formula for chemical potential gives\n\nas the gas-phase mole fraction depends on its fugacity, \n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f_{i}}\n  \n, as a fraction of the pressure in the reference state, \n  \n    \n      \n        \n          p\n          \n            ⊖\n          \n        \n      \n    \n    {\\displaystyle p^{\\ominus }}\n  \n.\n\nThe corresponding equation when the system consists purely of component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in equilibrium with its vapor is\n\nSubtracting these equations and re-arranging leads to the result[5]: 326\n\nFor the ideal gas, pressure and fugacity are equal, so introducing simple pressures to this result yields Raoult's law:\n\nAn ideal solution would follow Raoult's law, but most solutions deviate from ideality. Interactions between gas molecules are typically quite small, especially if the vapor pressures are low. However, the interactions in a liquid are very strong. For a solution to be ideal, the interactions between unlike molecules must be of the same magnitude as those between like molecules.[6] This approximation is only true when the different species are almost chemically identical. One can see that from considering the Gibbs free energy change of mixing:\n\nThis is always negative, so mixing is spontaneous. However, the expression is, apart from a factor \n  \n    \n      \n        −\n        T\n      \n    \n    {\\displaystyle -T}\n  \n, equal to the entropy of mixing. This leaves no room at all for an enthalpy effect and implies that \n  \n    \n      \n        \n          Δ\n          \n            mix\n          \n        \n        H\n      \n    \n    {\\displaystyle \\Delta _{\\text{mix}}H}\n  \n must be equal to zero, and this can only be true if the interactions between the molecules are indifferent.\n\nIt can be shown using the Gibbs–Duhem equation that if Raoult's law holds over the entire concentration range \n  \n    \n      \n        x\n        ∈\n        [\n        0\n        ,\n         \n        1\n        ]\n      \n    \n    {\\displaystyle x\\in [0,\\ 1]}\n  \n in a binary solution then, for the second component, the same must also hold.\n\nIf deviations from the ideal are not too large, Raoult's law is still valid in a narrow concentration range when approaching \n  \n    \n      \n        x\n        →\n        1\n      \n    \n    {\\displaystyle x\\to 1}\n  \n for the majority phase (the solvent). The solute also shows a linear limiting law, but with a different coefficient. This relationship is known as Henry's law.\n\nThe presence of these limited linear regimes has been experimentally verified in a great number of cases, though large deviations occur in a variety of cases.  Consequently, both its pedagogical value and utility have been questioned at the introductory college level.[7] In a perfectly ideal system, where ideal liquid and ideal vapor are assumed, a very useful equation emerges if Raoult's law is combined with Dalton's Law:\n\nwhere \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n is the mole fraction of component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in the solution, and \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n is its mole fraction in the gas phase. This equation shows that, for an ideal solution where each pure component has a different vapor pressure, the gas phase is enriched in the component with the higher vapor pressure when pure, and the solution is enriched in the component with the lower pure vapor pressure. This phenomenon is the basis for distillation.\n\nIn elementary applications, Raoult's law is generally valid when the liquid phase is either nearly pure or a mixture of similar substances.[8] Raoult's law may be adapted to non-ideal solutions by incorporating two factors that account for the interactions between molecules of different substances.  The first factor is a correction for gas non-ideality, or deviations from the ideal-gas law. It is called the fugacity coefficient (\n  \n    \n      \n        \n          ϕ\n          \n            p\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\phi _{p,i}}\n  \n). The second, the activity coefficient \n  \n    \n      \n        \n          γ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{i}}\n  \n, is a correction for interactions in the liquid phase between the different molecules.[5]: 326\n\nThis modified or extended Raoult's law is then written as[9]\n\nIn many pairs of liquids, there is no uniformity of attractive forces, i.e., the adhesive (between dissimilar molecules) and cohesive  forces (between similar molecules)   are not uniform between the two liquids. Therefore, they deviate from Raoult's law, which applies only to ideal solutions.\n\nNotably, when the concentration of A is small, its vapor pressure instead follows Henry's law, and likewise for substance B when its concentration is small.\n\nWhen the adhesion is stronger than the cohesion, fewer liquid particles turn into vapor thereby lowering the vapor pressure and leading to negative deviation in the graph.\n\nFor example, the system of chloroform (CHCl3) and acetone (CH3COCH3) has a negative deviation[10] from Raoult's law, indicating an attractive interaction between the two components that have been described as a hydrogen bond.[11] The system HCl–water has a large enough negative deviation to form a minimum in the vapor pressure curve known as a (negative) azeotrope, corresponding to a mixture that evaporates without change of composition.[12] When these two components are mixed, the reaction is exothermic as ion-dipole intermolecular forces of attraction are formed between the resulting ions (H3O+ and Cl–) and the polar water molecules so that ΔHmix is negative.\n\nWhen the adhesion is weaker than cohesion, which is quite common, the liquid particles escape the solution more easily that increases the vapor pressure and leads to a positive deviation.\n\nIf the deviation is large, then the vapor pressure curve shows a maximum at a particular composition and forms a positive azeotrope (low-boiling mixture). Some mixtures in which this happens are (1) ethanol and water, (2) benzene and methanol, (3) carbon disulfide and acetone, (4) chloroform and ethanol, and (5) glycine and water. When these pairs of components are mixed, the process is endothermic as weaker intermolecular interactions are formed so that ΔmixH is positive.\n\nIt is possible to have mixed deviations, which are positive for one component and negative for the other, and which switch between positive and negative while moving from \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n to \n  \n    \n      \n        x\n        =\n        1\n      \n    \n    {\\displaystyle x=1}\n  \n. These are not merely theoretically possible, as actual examples of mixed deviation exist.[13] The possible physical deviations are not entirely arbitrary however, as they are constrained by the Duhem–Margules equation: for example, if one component has positive deviation over the entire range then the other component cannot have negative deviation over the entire range.[13]",
        pageTitle: "Raoult's law",
    },
    {
        title: "Rayleigh–Jeans law",
        link: "https://en.wikipedia.org/wiki/Rayleigh%E2%80%93Jeans_law",
        content:
            "In physics, the Rayleigh–Jeans law is an approximation to the spectral radiance of electromagnetic radiation as a function of wavelength from a black body at a given temperature through classical arguments. For wavelength λ, it is\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              c\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {2ck_{\\text{B}}T}{\\lambda ^{4}}},}\n  \n\nwhere \n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n      \n    \n    {\\displaystyle B_{\\lambda }}\n  \n is the spectral radiance (the power emitted per unit emitting area, per steradian, per unit wavelength), \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the speed of light, \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{\\text{B}}}\n  \n is the Boltzmann constant, and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the temperature in kelvins. For frequency \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n, the expression is instead\n\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              \n                ν\n                \n                  2\n                \n              \n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\nu }(T)={\\frac {2\\nu ^{2}k_{\\text{B}}T}{c^{2}}}.}\n\nThe Rayleigh–Jeans law agrees with experimental results at large wavelengths (low frequencies) but strongly disagrees at short wavelengths (high frequencies). This inconsistency between observations and the predictions of classical physics is commonly known as the ultraviolet catastrophe.[1][2] Planck's law, which gives the correct radiation at all frequencies, has the Rayleigh–Jeans law as its low-frequency limit.\n\nIn 1900, the British physicist Lord Rayleigh derived the λ−4 dependence of the Rayleigh–Jeans law based on classical physical arguments, relying upon the equipartition theorem. This law predicted an energy output that diverges towards infinity as wavelength approaches zero (as frequency tends to infinity).  Measurements of the spectral emission of actual black bodies revealed that the emission agreed with Rayleigh's calculation at low frequencies but diverged at high frequencies, reaching a maximum and then falling with frequency, so the total energy emitted is finite. Rayleigh recognized the unphysical behavior of his formula at high frequencies and introduced an ad hoc cutoff to correct it, but experimentalists found that his cutoff did not agree with data.[1][3] Hendrik Lorentz also presented a derivation of the wavelength dependence in 1903. More complete derivations, which included the proportionality constant, were presented in 1905 by Rayleigh and Sir James Jeans and independently by Albert Einstein.[3] Rayleigh believed that this discrepancy could be resolved by the equipartition theorem failing to be valid for high-frequency vibrations, while Jeans argued that the underlying cause was matter and luminiferous aether not being in thermal equilibrium.[3]\n\nRayleigh published his first derivation of the frequency dependence in June 1900. Planck discovered the curve now known as Planck's law in October of that year and presented it in December.[3] Planck's original intent was to find a satisfactory derivation of Wien's expression for the blackbody radiation curve, which accurately described the data at high frequencies. Planck found Wien's original derivation inadequate and devised his own. Then, after learning that the most recent experimental results disagreed with his predictions for low frequencies, Planck revised his calculation, obtaining what is now called Planck's law.[4]\n\nIn 1900 Max Planck empirically obtained an expression for black-body radiation expressed in terms of wavelength λ = c/ν (Planck's law):\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      c\n                    \n                    \n                      λ\n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {2hc^{2}}{\\lambda ^{5}}}{\\frac {1}{e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}-1}},}\n  \n\nwhere h is the Planck constant, and kB is the Boltzmann constant. Planck's law does not suffer from an ultraviolet catastrophe and agrees well with the experimental data, but its full significance (which ultimately led to quantum theory) was only appreciated several years later. Since\n\n  \n    \n      \n        \n          e\n          \n            x\n          \n        \n        =\n        1\n        +\n        x\n        +\n        \n          \n            \n              x\n              \n                2\n              \n            \n            \n              2\n              !\n            \n          \n        \n        +\n        \n          \n            \n              x\n              \n                3\n              \n            \n            \n              3\n              !\n            \n          \n        \n        +\n        ⋯\n        ,\n      \n    \n    {\\displaystyle e^{x}=1+x+{\\frac {x^{2}}{2!}}+{\\frac {x^{3}}{3!}}+\\cdots ,}\n  \n\nthen in the limit of high temperatures or long wavelengths, the term in the exponential becomes small, and the exponential is well approximated with the Taylor polynomial's first-order term:\n\n  \n    \n      \n        \n          e\n          \n            \n              \n                h\n                c\n              \n              \n                λ\n                \n                  k\n                  \n                    B\n                  \n                \n                T\n              \n            \n          \n        \n        ≈\n        1\n        +\n        \n          \n            \n              h\n              c\n            \n            \n              λ\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}\\approx 1+{\\frac {hc}{\\lambda k_{\\text{B}}T}}.}\n\nSo\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      c\n                    \n                    \n                      λ\n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            1\n            \n              \n                h\n                c\n              \n              \n                λ\n                \n                  k\n                  \n                    B\n                  \n                \n                T\n              \n            \n          \n        \n        =\n        \n          \n            \n              λ\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              h\n              c\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {1}{e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}-1}}\\approx {\\frac {1}{\\frac {hc}{\\lambda k_{\\text{B}}T}}}={\\frac {\\lambda k_{\\text{B}}T}{hc}}.}\n\nThis results in Planck's blackbody formula reducing to\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              c\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {2ck_{\\text{B}}T}{\\lambda ^{4}}},}\n  \n\nwhich is identical to the classically derived Rayleigh–Jeans expression.\n\nThe same argument can be applied to the blackbody radiation expressed in terms of frequency ν = c/λ. In the limit of small frequencies, that is \n  \n    \n      \n        h\n        ν\n        ≪\n        \n          k\n          \n            B\n          \n        \n        T\n      \n    \n    {\\displaystyle h\\nu \\ll k_{\\text{B}}T}\n  \n,\n\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        ⋅\n        \n          \n            \n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              h\n              ν\n            \n          \n        \n        =\n        \n          \n            \n              2\n              \n                ν\n                \n                  2\n                \n              \n              \n                k\n                \n                  \n                    B\n                  \n                \n              \n              T\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\nu }(T)={\\frac {2h\\nu ^{3}}{c^{2}}}{\\frac {1}{e^{\\frac {h\\nu }{k_{\\text{B}}T}}-1}}\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}\\cdot {\\frac {k_{\\text{B}}T}{h\\nu }}={\\frac {2\\nu ^{2}k_{\\mathrm {B} }T}{c^{2}}}.}\n\nThis last expression is the Rayleigh–Jeans law in the limit of small frequencies.\n\nWhen comparing the frequency- and wavelength-dependent expressions of the Rayleigh–Jeans law, it is important to remember that\n\n  \n    \n      \n        \n          \n            \n              d\n              P\n            \n            \n              d\n              λ\n            \n          \n        \n        =\n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle {\\frac {dP}{d\\lambda }}=B_{\\lambda }(T)}\n  \n\nand\n\n  \n    \n      \n        \n          \n            \n              d\n              P\n            \n            \n              d\n              ν\n            \n          \n        \n        =\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {dP}{d\\nu }}=B_{\\nu }(T).}\n  \n\nNote that these two expressions then have different units, as a step \n  \n    \n      \n        d\n        λ\n      \n    \n    {\\displaystyle d\\lambda }\n  \n in wavelength is not equivalent to a step \n  \n    \n      \n        d\n        ν\n      \n    \n    {\\displaystyle d\\nu }\n  \n in frequency. Therefore, \n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        ≠\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)\\neq B_{\\nu }(T),}\n  \n\neven after substituting the value \n  \n    \n      \n        λ\n        =\n        c\n        \n          /\n        \n        ν\n      \n    \n    {\\displaystyle \\lambda =c/\\nu }\n  \n, because \n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle B_{\\lambda }(T)}\n  \n has units of energy emitted per unit time per unit area of emitting surface, per unit solid angle, per unit wavelength, whereas \n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle B_{\\nu }(T)}\n  \n has units of energy emitted per unit time per unit area of emitting surface, per unit solid angle, per unit frequency. To be consistent, we must use the equality\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        \n        d\n        λ\n        =\n        d\n        P\n        =\n        \n          B\n          \n            ν\n          \n        \n        \n        d\n        ν\n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }\\,d\\lambda =dP=B_{\\nu }\\,d\\nu ,}\n  \n\nwhere both sides now have units of power (energy emitted per unit time) per unit area of emitting surface, per unit solid angle.\n\nStarting with the Rayleigh–Jeans law in terms of wavelength, we get\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        \n          \n            \n              d\n              ν\n            \n            \n              d\n              λ\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle B_{\\lambda }(T)=B_{\\nu }(T){\\frac {d\\nu }{d\\lambda }},}\n  \n\nwhere\n\n  \n    \n      \n        \n          \n            \n              d\n              ν\n            \n            \n              d\n              λ\n            \n          \n        \n        =\n        \n          \n            d\n            \n              d\n              λ\n            \n          \n        \n        \n          (\n          \n            \n              c\n              λ\n            \n          \n          )\n        \n        =\n        −\n        \n          \n            c\n            \n              λ\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d\\nu }{d\\lambda }}={\\frac {d}{d\\lambda }}\\left({\\frac {c}{\\lambda }}\\right)=-{\\frac {c}{\\lambda ^{2}}}.}\n  \n\nThis leads to\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              \n                k\n                \n                  B\n                \n              \n              T\n              \n                \n                  (\n                  \n                    \n                      c\n                      λ\n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        ×\n        \n          \n            c\n            \n              λ\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              2\n              c\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {2k_{\\text{B}}T\\left({\\frac {c}{\\lambda }}\\right)^{2}}{c^{2}}}\\times {\\frac {c}{\\lambda ^{2}}}={\\frac {2ck_{\\text{B}}T}{\\lambda ^{4}}}.}\n\nDepending on the application, the Planck function can be expressed in 3 different forms.  The first involves energy emitted per unit time per unit area of emitting surface, per unit solid angle, per spectral unit.  In this form, the Planck function and associated Rayleigh–Jeans limits are given by\n\n  \n    \n      \n        \n          B\n          \n            λ\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      c\n                    \n                    \n                      λ\n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              2\n              c\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n      \n    \n    {\\displaystyle B_{\\lambda }(T)={\\frac {2hc^{2}}{\\lambda ^{5}}}{\\frac {1}{e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}-1}}\\approx {\\frac {2ck_{\\text{B}}T}{\\lambda ^{4}}}}\n  \n\nor\n\n  \n    \n      \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              2\n              \n                k\n                \n                  B\n                \n              \n              T\n              \n                ν\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle B_{\\nu }(T)={\\frac {2h\\nu ^{3}}{c^{2}}}{\\frac {1}{e^{\\frac {h\\nu }{k_{\\text{B}}T}}-1}}\\approx {\\frac {2k_{\\text{B}}T\\nu ^{2}}{c^{2}}}.}\n\nAlternatively, Planck's law can be written as an expression \n  \n    \n      \n        I\n        (\n        ν\n        ,\n        T\n        )\n        =\n        π\n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle I(\\nu ,T)=\\pi B_{\\nu }(T)}\n  \n for emitted power integrated over all solid angles.  In this form, the Planck function and associated Rayleigh–Jeans limits are given by\n\n  \n    \n      \n        I\n        (\n        λ\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              π\n              h\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      c\n                    \n                    \n                      λ\n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              2\n              π\n              c\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n      \n    \n    {\\displaystyle I(\\lambda ,T)={\\frac {2\\pi hc^{2}}{\\lambda ^{5}}}{\\frac {1}{e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}-1}}\\approx {\\frac {2\\pi ck_{\\text{B}}T}{\\lambda ^{4}}}}\n  \n\nor\n\n  \n    \n      \n        I\n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              π\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              2\n              π\n              \n                k\n                \n                  B\n                \n              \n              T\n              \n                ν\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle I(\\nu ,T)={\\frac {2\\pi h\\nu ^{3}}{c^{2}}}{\\frac {1}{e^{\\frac {h\\nu }{k_{\\text{B}}T}}-1}}\\approx {\\frac {2\\pi k_{\\text{B}}T\\nu ^{2}}{c^{2}}}.}\n\nIn other cases, Planck's law is written as\n\n  \n    \n      \n        u\n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              4\n              π\n            \n            c\n          \n        \n        \n          B\n          \n            ν\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\textstyle u(\\nu ,T)={\\frac {4\\pi }{c}}B_{\\nu }(T)}\n  \n\nfor energy per unit volume (energy density).  In this form, the Planck function and associated Rayleigh–Jeans limits are given by\n\n  \n    \n      \n        u\n        (\n        λ\n        ,\n        T\n        )\n        =\n        \n          \n            \n              8\n              π\n              h\n              c\n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      c\n                    \n                    \n                      λ\n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              8\n              π\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              λ\n              \n                4\n              \n            \n          \n        \n      \n    \n    {\\displaystyle u(\\lambda ,T)={\\frac {8\\pi hc}{\\lambda ^{5}}}{\\frac {1}{e^{\\frac {hc}{\\lambda k_{\\text{B}}T}}-1}}\\approx {\\frac {8\\pi k_{\\text{B}}T}{\\lambda ^{4}}}}\n  \n\nor\n\n  \n    \n      \n        u\n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              8\n              π\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      \n                        k\n                        \n                          B\n                        \n                      \n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        ≈\n        \n          \n            \n              8\n              π\n              \n                k\n                \n                  B\n                \n              \n              T\n              \n                ν\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle u(\\nu ,T)={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}{\\frac {1}{e^{\\frac {h\\nu }{k_{\\text{B}}T}}-1}}\\approx {\\frac {8\\pi k_{\\text{B}}T\\nu ^{2}}{c^{3}}}.}",
        pageTitle: "Rayleigh–Jeans law",
    },
    {
        title: "Reed's law",
        link: "https://en.wikipedia.org/wiki/Reed%27s_law",
        content:
            "Reed's law is the assertion of David P. Reed that the utility of large networks, particularly social networks, can scale exponentially with the size of the network.[1]\n\nThe reason for this is that the number of possible sub-groups of network participants is 2N − N − 1, where N is the number of participants.  This grows much more rapidly than either\n\nso that even if the utility of groups available to be joined is very small on a per-group basis, eventually the network effect of potential group membership can dominate the overall economics of the system.\n\nGiven a set A of N people, it has 2N possible subsets. This is not difficult to see, since we can form each possible subset by simply choosing for each element of A one of two possibilities: whether to include that element, or not.\n\nHowever, this includes the (one) empty set, and N singletons, which are not properly subgroups. So 2N − N − 1 subsets remain, which is exponential, like 2N.\n\nFrom David P. Reed's, \"The Law of the Pack\" (Harvard Business Review, February 2001, pp 23–4):\n\nReed's Law is often mentioned when explaining competitive dynamics of internet platforms. As the law states that a network becomes more valuable when people can easily form subgroups to collaborate, while this value increases exponentially with the number of connections, business platform that reaches a sufficient number of members can generate network effects that dominate the overall economics of the system.[2]\n\nOther analysts of network value functions, including Andrew Odlyzko, have argued that both Reed's Law and Metcalfe's Law [3] overstate network value because they fail to account for the restrictive impact of human cognitive limits on network formation. According to this argument, the research around Dunbar's number implies a limit on the  number of inbound and outbound connections a human in a group-forming network can manage, so that the actual maximum-value structure is much sparser than the set-of-subsets measured by Reed's law or the complete graph measured by Metcalfe's law.",
        pageTitle: "Reed's law",
    },
    {
        title: "Reilly's law of retail gravitation",
        link: "https://en.wikipedia.org/wiki/Reilly%27s_law_of_retail_gravitation",
        content:
            "In economics, Reilly's law of retail gravitation is a heuristic developed by William J. Reilly in 1931.[1] According to Reilly's \"law,\" customers are willing to travel longer distances to larger retail centers given the higher attraction they present to customers. In Reilly's formulation, the attractiveness of the retail center becomes the analogy for size (mass) in the physical law of gravity.\n\nThe law presumes the geography of the area is flat without any rivers, roads or mountains to alter a consumer's decision of where to travel to buy goods. It also assumes consumers are otherwise indifferent between the actual cities. In analogy with Newton's law of gravitation, the point of indifference is the point at which the \"attractiveness\" of the two retail centres (postulated to be proportional to their size and inversely proportional to the square of the distance to them) is equal:\n\nWhere \n  \n    \n      \n        \n          d\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle d_{A}}\n  \n is the distance of the point of indifference from A, \n  \n    \n      \n        \n          d\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle d_{B}}\n  \n is its distance from B, and \n  \n    \n      \n        \n          P\n          \n            A\n          \n        \n        \n          /\n        \n        \n          P\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle P_{A}/P_{B}}\n  \n is the relative size of the two centres. If the customer is on the line connecting A and B, then if D is the distance between the centres, the point of indifference as measured from A on the line is\n\nAs expected, for centres of the same size, d=D/2, and if A is larger than B, the point of indifference is closer to B. As the size of A becomes very large with respect to B, d tends to D, meaning the customer will always prefer the larger centre unless they're very close to the smaller one.\n\nWilliam J. Reilly wrote Methods for the Study of Retail Relationships in 1929.[2]\n\nThe manuscript compiled the following information of that time:\n\nTwo laters later, he published The Law of Retail Gravitation (1931). [3] The latter publication goes into more mathematical detail.[4]\n\nIn addition to Newton's Law of Gravity in the physical sciences, there were other antecedents to Reilly's \"law\" of retail gravity. In particular, E.C. Young in 1924 described a formula for migration that was based on the physical law of gravity, and H.C. Carey had included a description of the tendency of humans to \"gravitate\" together in an 1858 summary of social science theory.[5]\n\nReilly's law has many variations, and extensions and applications are numerous. Among these include:",
        pageTitle: "Reilly's law of retail gravitation",
    },
    {
        title: "Ribot's law",
        link: "https://en.wikipedia.org/wiki/Ribot%27s_law",
        content:
            "Ribot's law of retrograde amnesia was hypothesized in 1881 by Théodule Ribot. It states that there is a time gradient in retrograde amnesia, so that recent memories are more likely to be lost than the more remote memories. Not all patients with retrograde amnesia report the symptoms of Ribot's law.\n\nRibot's law was first postulated by the French psychologist Théodule Ribot (1839–1916), who is recognized as one of the pioneer 19th century advocates for psychology as an objective and biologically based empirical field. Ribot's split from the mainstream \"Eclectic\" psychology of the era was associated with a transition from philosophical to evolutionary explanations of human psychology and behavior.[1] As Ribot was not a true experimentalist himself, this increased focus on the natural science basis of human mentality was manifested in an interest for case studies and diseases of dysfunction which helped to shape theories of psychological function. Ribot's law actually was first defined in terms of a broad generalization of functional decline in psychopathology: the observation that functions acquired most recently are the first to degenerate.[2] However, in the current context of neuroscience research, Ribot's law is used almost exclusively to describe the perceived effect of older memories being less prone to disruption.\n\nIn his 1882 book, \"Diseases of Memory: An Essay in the Positive Psychology\",[3] Ribot explained the retroactive phenomena of trauma or event-induced memory loss. Patients who incurred amnesia from a specific event such as an accident often also lost memory of the events leading up to the incident as well. In the case of some, this retrograde loss included several years leading up to the precipitating event of injury or trauma had occurred – yet left much older memories intact – suggesting that the effect was not just due to interference with consolidation of memories immediately before brain damage.\n\nOther historical accounts supporting the greater strength of older memories include some studies of aphasia starting as early as the late 1700s, in which bilingual patients recovered different languages with differential progress. In some cases, aphasics recover or preferentially improve only the first-acquired language, although this only seems to be the case mostly in people who were never truly fluent in their secondary language.[4] \nCurrently, Ribot's law is not universally accepted as a supporting example for memory consolidation and storage. As a component of the standard model memory of systems consolidation, it is challenged by the multiple trace theory which states that the hippocampus is always activated in the storage and retrieval of episodic memory regardless of memory age.\n\nA large body of research supports the predictions of Ribot's law. The theory concerns the relative strength of memories over time, which is not directly testable. Instead, scientists investigate the processes of forgetting (amnesia), and recollection.[5] Ribot's law states that following a disruptive event, patients will show a temporally graded retrograde amnesia that preferentially spares more distant memories.\n\nExperimental evidence largely confirms these predictions. In a study of electroconvulsive shock therapy patients, memories formed at least four years prior to treatment were unaffected, while more recent ones were impaired.[6] An experiment with rats showed similar results. Rats were conditioned to fear stimuli in two different contexts: one 50 days before receiving hippocampal brain lesions, and the other 1 day before lesioning. Subsequently, they only showed fear memory in the 50-day-old context.[7]\n\nMany neurological disorders, including Alzheimer's disease, are also associated with a temporally graded retrograde amnesia, indicating that older memories are somehow strengthened against degeneration while newer memories are not.[8] Although the mechanism for this strengthening is unclear, some models exist to explain the effects.[citation needed]\n\nInitially proposed in 1984 by Larry Squire, Neal Cohen, and Lynn Nadel, the standard model of systems consolidation is a contemporary theory used to explain the cognitive processes behind Ribot's law. In the model, interaction between the medial temporal hippocampus (MTH) and multiple areas in the neocortex lead to the formation of a cortical trace which represents a single memory. While this MTH-neocortex interaction is initially required to maintain the memory trace, the model predicts that over time the importance of the MTH becomes diminished and eventually is unnecessary for the storage of the memory trace.[9] The medial temporal hippocampus mediates memory formation by maintaining the connections between various neocortical regions that make up each memory trace. At first the associations between neocortical areas that make up a newly formed memory trace are weak, however repeated activation of these areas in succession lead to \"consolidation\" of the trace within the neocortex. Once consolidation is sufficiently complete, the memory trace becomes mediated through neocortical activity alone and the MTH is no longer necessary for re-activation.\n\nFigure 1 provides a visual explanation of the standard model. Initially, the memory trace (features of the experience represented by red circles) is weak in the neocortex and is reliant on its connections to the medial temporal hippocampal system (MTH) for retrieval. Over time, an intrinsic process results in the strengthening of the connections between memory trace representations in the neocortex. Since the connections are consolidated, the memory can now be retrieved without the hippocampus.\n\nWhile never explicitly described by Squire and colleagues, the timescale of MTH-dependence in memory formation and maintenance is believed to vary by species as well as by the extent of hippocampal damage. For example, hippocampal lesion experiments with mouse models have shown retrograde amnesia for approximately one week prior to surgery,[10] while case studies of human subjects with similar hippocampal damage have had retrograde amnesia limited to around two to three years prior to the accident.[11]\n\nThe standard model of systems consolidation largely applies to the formation of declarative memories, which include semantic, factual memories and episodic, autobiographical memories. This has been supported by case studies of human patients with MTH lesions who exhibit difficulties in remembering experiences and fact learned post-surgery, however are able to retain motor and skill memories such as how to ride a bike or perform mirror tracing tasks.[12]",
        pageTitle: "Ribot's law",
    },
    {
        title: "Ricco's law",
        link: "https://en.wikipedia.org/wiki/Ricco%27s_law",
        content:
            'Riccò\'s law, discovered by astronomer Annibale Riccò, is one of several laws that describe a human\'s ability to visually detect targets on a uniform background.[3][4] It says that for visual targets below a certain size, threshold visibility depends on the area of the target, and hence on the total light received. The "certain size" (called the "critical visual angle"), is small in daylight conditions, larger in low light levels. The law is of special significance in visual astronomy, since it concerns the ability to distinguish between faint point sources (e.g. stars) and small, faint extended objects ("DSOs").\n\nSuppose that an achromatic target of angular area \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is viewed against a uniform background luminance \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n (e.g. a disc of white light is projected on a white screen, or a nebula is seen through a telescope). For the target to be visible at all, there must be sufficient luminance contrast; i.e. the target must be brighter (or darker) than the background by some amount \n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n. If the target is at threshold (i.e. only just visible) then the threshold contrast is defined as \n  \n    \n      \n        C\n        =\n        Δ\n        B\n        \n          /\n        \n        B\n      \n    \n    {\\displaystyle C=\\Delta B/B}\n  \n. Riccò\'s law states that for targets below a certain size, threshold contrast is inversely proportional to target area, i.e.\n\n  \n    \n      \n        C\n        A\n        =\n        R\n      \n    \n    {\\displaystyle CA=R}\n  \n\nfor some constant \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n. Different values of background luminance \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n will yield different values of \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n.\n\nThis can be seen in contrast threshold data for different levels of background luminance, plotted on a single graph as \n  \n    \n      \n        log\n        ⁡\n        C\n      \n    \n    {\\displaystyle \\log C}\n  \n versus \n  \n    \n      \n        log\n        ⁡\n        A\n      \n    \n    {\\displaystyle \\log A}\n  \n. In each case (i.e. for each background \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n), the threshold curve for small targets is a straight line of gradient −1, i.e.\n\n  \n    \n      \n        log\n        ⁡\n        C\n        =\n        −\n        log\n        ⁡\n        A\n        +\n        \n          c\n          o\n          n\n          s\n          t\n          a\n          n\n          t\n        \n      \n    \n    {\\displaystyle \\log C=-\\log A+\\mathrm {constant} }\n  \n \n\n  \n    \n      \n        log\n        ⁡\n        (\n        C\n        A\n        )\n        =\n        \n          c\n          o\n          n\n          s\n          t\n          a\n          n\n          t\n        \n      \n    \n    {\\displaystyle \\log(CA)=\\mathrm {constant} }\n\nTargets for which the law holds are indistinguishable from point sources. Reading towards the right of each threshold curve, there is a target size at which the law begins to break down, i.e. the slope deviates from -1. This is called the "critical visual angle".[1] It is the size at which targets may begin to be seen as visibly extended (bearing in mind that the threshold data are averaged from multiple observers, and individual performance may vary). Notice that for any background \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, the threshold curve approaches a slope of zero for large target sizes; i.e. the curve is asymptotic at both ends. The "Ricco area" \n  \n    \n      \n        \n          A\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle A_{R}}\n  \n is conventionally defined by the intersection of the asymptotes.[2]: Eq. 22  The corresponding visual angle, \n  \n    \n      \n        2\n        \n          \n            \n              A\n              \n                R\n              \n            \n            \n              /\n            \n            π\n          \n        \n      \n    \n    {\\textstyle 2{\\sqrt {A_{R}/\\pi }}}\n  \n, is larger than the critical visual angle, but better defined, and sufficiently useful as an approximation of the least size at which an object is expected to be seen as clearly extended, for a given background luminance.[2]: §3.1\n\nRiccò\'s law is applicable for targets of angular area less than the size of the receptive field.  This region is variable based on the amount of background luminance.  Riccò\'s law is based on the fact that within a receptive field, the light energy (or the number of photons per second) required to lead to the target being detected is summed over the area and is thus proportional to the luminance and to the area.[5] Therefore, the contrast threshold required for detection is proportional to the signal-to-noise ratio multiplied by the noise divided by the area.  This leads to the above equation.\n\nThe "constant" R is actually a function of the background luminance B to which the eye is assumed to be adapted. It has been shown by Andrew Crumey[2] that for unconstrained vision (that is, observers could either look directly at the target or avert their gaze) an accurate empirical formula for R is\n\n  \n    \n      \n        R\n        =\n        (\n        \n          c\n          \n            1\n          \n        \n        \n          B\n          \n            −\n            1\n            \n              /\n            \n            4\n          \n        \n        +\n        \n          c\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle R=(c_{1}B^{-1/4}+c_{2})^{2}}\n  \n\nwhere c1, c2 are constants taking different values for scotopic and photopic vision. For low B this approximates to the De Vries-Rose Law[6] for threshold contrast C\n\n  \n    \n      \n        C\n        ≡\n        \n          \n            \n              Δ\n              B\n            \n            B\n          \n        \n        ∝\n        \n          \n            1\n            \n              A\n              \n                \n                  B\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle C\\equiv {\\frac {\\Delta B}{B}}\\propto {\\frac {1}{A{\\sqrt {B}}}}.}\n\nHowever, at very low background luminance (less than 10−5 candela per square metre), where the only perception is of \'dark light\' (neural noise), the threshold value for the illuminance\n\n  \n    \n      \n        Δ\n        I\n        =\n        A\n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta I=A\\Delta B}\n  \n\nis a constant (around 10−9 lux) and does not depend on B.[2]: §1.5,2.1,2.3  In that case\n\n  \n    \n      \n        C\n        =\n        \n          \n            \n              Δ\n              B\n            \n            B\n          \n        \n        =\n        \n          \n            \n              Δ\n              I\n            \n            \n              A\n              B\n            \n          \n        \n      \n    \n    {\\displaystyle C={\\frac {\\Delta B}{B}}={\\frac {\\Delta I}{AB}}}\n  \n\nor\n\n  \n    \n      \n        R\n        =\n        \n          \n            \n              Δ\n              I\n            \n            B\n          \n        \n        .\n      \n    \n    {\\displaystyle R={\\frac {\\Delta I}{B}}.}\n\nAt high B such as the daylight sky, Crumey\'s formula approaches an asymptotic value for R of 5.1×10−9 or 5.4×10−9 lux per nit.[a]',
        pageTitle: "Ricco's law",
    },
    {
        title: "Roemer's law",
        link: "https://en.wikipedia.org/wiki/Roemer%27s_law",
        content:
            'In health policy, Roemer\'s law may be expressed as "in an insured population, a hospital bed built is a bed filled."[1]\n\nThe rule was deduced by the American health services researcher Milton Roemer, working at the UCLA School of Public Health. Roemer and colleagues found a positive correlation between the number of short-term general hospital beds available per 1,000 population and the number of hospital days used per 1,000 population.[2]\n\nRoemer\'s law will clearly not always hold true (not every bed that is ever built will be filled), but it provides the underpinning for certificate of need laws and for health planning.[3][page needed]\n\nThe law is thought to be a consequence of induced demand: physicians encouraging patients to consume services that the patients would not have chosen if they had been fully informed.[4][better source needed] Health planning and certificate of need laws aim to prevent the waste that would otherwise occur because of Roemer\'s law.\n\n"One problem in this finding is that it could be the case that hospital stays are shorter in lower hospital bed per capita regions because of a deficit in supply (reverse causation). An increased number of beds may be due to patient preference for in-patient (rather than outpatient) care in a region."[5][self-published source]\n\nEnoch Powell, the British Minister of Health, propounded a similar proposition, which he called Parkinson\'s law of hospital beds: "the number of patients always tends to equality with the number of beds available for them to lie in."[6]',
        pageTitle: "Roemer's law",
    },
    {
        title: "Rothbard's law",
        link: "https://en.wikipedia.org/wiki/Rothbard%27s_law",
        content:
            'Murray Newton Rothbard (/ˈrɒθbɑːrd/; March 2, 1926 – January 7, 1995) was an American economist[1] of the Austrian School,[2][3][4][5] economic historian,[6][7] political theorist,[8] and activist. Rothbard was a central figure in the 20th-century American libertarian movement, particularly its right-wing strands, and was a founder and leading theoretician of anarcho-capitalism.[15](as termed by Rothbard; but later opposing the \'anarchism\' label on both etymological and historical grounds).[16] He wrote over twenty books on political theory, history, economics, and other subjects.[9]\n\nRothbard argued that all services provided by the "monopoly system of the corporate state"[17] could be provided more efficiently by the private sector and wrote that the state is "the organization of robbery systematized and writ large".[18][19][20] He called fractional-reserve banking a form of fraud and opposed central banking.[21] He categorically opposed all military, political, and economic interventionism in the affairs of other nations.[22][23]\n\nRothbard led a "fringe existence" in academia, as described by his protégé Hans-Hermann Hoppe.[24] Rothbard rejected mainstream economic methodologies and instead embraced the praxeology of Ludwig von Mises. Rothbard taught economics at a Wall Street division of New York University, later at Brooklyn Polytechnic, and after 1986 in an endowed position at the University of Nevada, Las Vegas.[8][25] Partnering with the oil billionaire Charles Koch, Rothbard was a founder of the Cato Institute and the Center for Libertarian Studies in the 1970s.[9] He broke with Cato and Koch, and in 1982 joined Lew Rockwell and Burton Blumert to establish the Mises Institute in Alabama.[26][5][27]\n\nRothbard opposed egalitarianism and the civil rights movement, and blamed women\'s voting and activism for the growth of the welfare state.[28][29][10][11] He promoted historical revisionism and befriended the Holocaust denier Harry Elmer Barnes.[30][31][32] Later in his career, Rothbard advocated a libertarian alliance with paleoconservatism (which he called paleolibertarianism), favoring right-wing populism and describing David Duke and Joseph McCarthy as models for political strategy.[33][34][28][35] In the 2010s, he received renewed attention as an influence on the alt-right.[36][10][37][38]\n\nRothbard\'s parents were David and Rae Rothbard, Jewish immigrants to the United States from Poland and Russia, respectively. David was a chemist.[39] He attended Birch Wathen Lenox School, a private school in New York City.[40] Rothbard later said he much preferred Birch Wathen to the "debasing and egalitarian public school system" he had attended in the Bronx.[41]\n\nRothbard wrote of having grown up as a "right-winger" (adherent of the "Old Right") among friends and neighbors who were "communists or fellow-travelers". He was a member of the New York Young Republican Club in his youth.[42] Rothbard described his father as an individualist who embraced minimal government, free enterprise, private property and "a determination to rise by one\'s own merits ...  [A]ll socialism seemed to me monstrously coercive and abhorrent."[41] In 1952, his father was trapped during a labor strike at the Tide Water Oil Refinery in New Jersey, which he managed, confirming their dislike of organized labor.[43]\n\nRothbard attended Columbia University, receiving a Bachelor of Arts degree in mathematics in 1945 and a PhD in economics in 1956. His first political activism came in 1948, on behalf of the segregationist South Carolinian Strom Thurmond\'s presidential campaign. In the 1948 presidential election, Rothbard, "as a Jewish student at Columbia, horrified his peers by organizing a Students for Strom Thurmond chapter, so staunchly did he believe in states\' rights", according to The American Conservative.[44] The delay in receiving his PhD was due in part to conflict with his advisor, Joseph Dorfman, and in part to Arthur Burns\'s rejecting his dissertation. Burns was a longtime friend of the Rothbard family and their neighbor at their Manhattan apartment building. It was only after Burns went on leave from the Columbia faculty to head President Eisenhower\'s Council of Economic Advisers that Rothbard\'s thesis was accepted, and he received his doctorate.[8]: 43–44 [45] Rothbard later said that all his fellow students were extreme leftists and that he was one of only two Republicans at Columbia at the time.[8]: 4\n\nDuring the 1940s, Rothbard vetted articles for Leonard Read at the Foundation for Economic Education think tank, became acquainted with Frank Chodorov, and read widely in libertarian-oriented works by Albert Jay Nock, Garet Garrett, Isabel Paterson, H. L. Mencken, and Austrian School economist Ludwig von Mises.[8]: 46 [43] Rothbard was greatly influenced by reading Mises\'s book Human Action in 1949.[27] In the 1950s, when Mises was teaching in the Wall Street division of the New York University Stern School of Business, Rothbard attended his unofficial seminar.[5][11] Rothbard wanted to promote libertarian activism; by the mid-1950s, he helped form the Circle Bastiat, a libertarian and anarchist social group in New York City.[5][43] He also joined the Mont Pelerin Society in the 1950s.[11]\n\nRothbard attracted the attention of the William Volker Fund, a group that provided financial backing to promote right-wing ideologies in the 1950s and early 1960s.[46][5] The Volker Fund paid Rothbard to write a textbook to explain Human Action in a form that could be used to introduce college undergraduates to Mises\'s views; a sample chapter he wrote on money and credit won Mises\'s approval. For ten years, the Volker Fund paid him a retainer as a "senior analyst".[8]: 54  As Rothbard continued his work, he enlarged the project. The result was his book Man, Economy, and State, published in 1962. Upon its publication, Mises praised Rothbard\'s work effusively.[47]: 14  In contrast to Mises, who considered security the primary justification for the state, Rothbard in the 1950s began to argue for a privatized market for the military, police and judiciary.[10] Rothbard\'s 1963 book America\'s Great Depression blamed government policy failures for the Great Depression, and challenged the widely-held view that capitalism is unstable.[48]\n\nIn 1953, Rothbard married JoAnn Beatrice Schumacher (1928–1999),[49] whom he called Joey, in New York City.[47]: 124  She was a historian, Rothbard\'s personal editor, and a close adviser as well as hostess of his Rothbard Salon. They enjoyed a loving marriage, and Rothbard often called her "the indispensable framework" of his life and achievements. According to her, the Volker Fund\'s patronage allowed Rothbard to work from home as a freelance theorist and pundit for the first 15 years of their marriage.[50]\n\nThe Volker Fund collapsed in 1962, leading Rothbard to seek employment at various New York academic institutions. He was offered a part-time position teaching economics to engineering students at Brooklyn Polytechnic Institute in 1966 at age 40. The institution had no economics department or economics majors, and Rothbard derided its social science department as "Marxist". Justin Raimondo, his biographer,[51] writes that Rothbard liked teaching at Brooklyn Polytechnic because working only two days a week gave him the freedom to contribute to developments in libertarian politics.[8] Rothbard continued in this role until 1986.[52][53] Then 60 years old, Rothbard left Brooklyn Polytechnic Institute for the Lee Business School at the University of Nevada, Las Vegas (UNLV), where he held the title of S.J. Hall Distinguished Professor of Economics, a chair endowed by a libertarian businessman.[54][25]\n\nAccording to Rothbard\'s friend, colleague, and fellow Misesian economist Hans-Hermann Hoppe, Rothbard led a "fringe existence" in academia, but he was able to attract a large number of "students and disciples" through his writings, thereby becoming "the creator and one of the principal agents of the contemporary libertarian movement".[24] Libertarian economist Jeffrey Herbener, who called Rothbard his friend and "intellectual mentor", said in a memoriam that Rothbard received "only ostracism" from mainstream academia.[55] Rothbard kept his position at UNLV from 1986 until his death.[52]\n\nThroughout his life, Rothbard engaged in a number of different political movements to promote Old Right and libertarian political principles. George Hawley writes that "unfortunately for Rothbard, the Old Right was ending as an intellectual and political force just as he was maturing as an intellectual", with the militantly anticommunist conservative movement exemplified by William F. Buckley Jr. supplanting the Old Right\'s isolationism.[28]\n\nRothbard was an admirer of Senator Joseph McCarthy—not for McCarthy\'s Cold War views, but for his demagoguery, which Rothbard credited for disrupting the establishment consensus of what Rothbard called "corporate liberalism".[28] Rothbard contributed many articles to Buckley\'s National Review, but his relations with Buckley and the magazine soured as he criticized the conservative movement for militarism.[28] Specifically, Rothbard opposed how such militarism could justify and expand the state\'s power.[10]\n\nRothbard befriended the Holocaust denier Harry Elmer Barnes in 1959.[31] In a 1966 issue of Robert LeFevre\'s Rampart Journal of Individualist Thought devoted to historical revisionism, Rothbard argued that Western democracies had been to blame for starting World War I, World War II, and the Cold War.[31] Rothbard published works by Barnes in his journals before and after Barnes died in 1968, including posthumously in the Cato Institute\'s journal.[31]\n\nIn 1954, Rothbard, along with several other attendees of Mises\'s seminar, joined the circle of novelist Ayn Rand, the founder of Objectivism. He soon parted from her, writing, among other things, that her ideas were not as original as she proclaimed but similar to those of Aristotle, Thomas Aquinas, and Herbert Spencer.[8]: 109–14  In 1958, after the publication of Rand\'s novel Atlas Shrugged, Rothbard wrote her a "fan letter", calling the book "an infinite treasure house" and "not merely the greatest novel ever written, [but] one of the very greatest books ever written, fiction or nonfiction." He also wrote: "[Y]ou introduced me to the whole field of natural rights and natural law philosophy," prompting him to learn "the glorious natural rights tradition."[8]: 121, 132–34 [56]: 145, 182 [57] Rothbard rejoined Rand\'s circle for a few months but soon broke with Rand again over various differences, including his defense of his interpretation of anarchism.\n\nRothbard later satirized Rand\'s acolytes in his unpublished one-act farce Mozart Was a Red[58] and his essay "The Sociology of the Ayn Rand Cult".[56]: 184 [59][60] He characterized Rand\'s circle as a "dogmatic, personality cult". His play parodies Rand (through the character Carson Sand) and her friends and is set during a visit from Keith Hackley, a fan of Sand\'s novel The Brow of Zeus (a play on Atlas Shrugged).[59]\n\nBy the late 1960s, according to The American Conservative, Rothbard\'s "long and winding yet somehow consistent road had taken him from anti-New Deal and anti-interventionist Robert A. Taft supporter into friendship with the quasi-pacifist Nebraska Republican Congressman Howard Buffett (father of Warren Buffett) then over to the League of (Adlai) Stevensonian Democrats and, by 1968, into tentative comradeship with the anarchist factions of the New Left."[61] Rothbard joined the Peace and Freedom Party and contributed writing to the New Left journal Ramparts.[28]\n\nRothbard later criticized the New Left for supporting a "People\'s Republic"-style Egalitarianism as a Revolt Against Nature and Other Essays .[independent source needed] It was during this phase that he associated with Karl Hess (a former Barry Goldwater speechwriter who had rejected conservatism)[28] and founded Left and Right: A Journal of Libertarian Thought with Leonard Liggio and George Resch. Raimondo described Rothbard during this time as "a man of the Old Culture: he believed that it was possible to be a revolutionary, an anarchist, and lead a bourgeois life", and wrote that the "respectably dressed, if a bit rumpled" Rothbard was "immune to the blandishments of sixties youth culture".[28] During this time, Rothbard proposed that black Americans should embrace racial separatism and secession.[11] He was frustrated that blacks and whites in the New Left instead decided to work together for egalitarian goals.[11] In the 1970s, Rothbard turned sharply against the left and described equality as evil.[28][11]\n\nFrom 1969 to 1984, Rothbard edited The Libertarian Forum, also initially with Hess (although Hess\'s involvement ended in 1971).[62] Despite its small readership, it engaged conservatives associated with the National Review in nationwide debate. Rothbard rejected the view that Ronald Reagan\'s 1980 presidential election was a victory for libertarian principles, and he attacked Reagan\'s economic program in a series of Libertarian Forum articles. In 1982, Rothbard called Reagan\'s claims of spending cuts a "fraud" and a "hoax" and accused Reaganites of doctoring the economic statistics to give a false impression that their policies successfully reduced inflation and unemployment.[63] He further criticized the "myths of Reaganomics" in 1987.[64]\n\nRothbard criticized the "frenzied nihilism" of left-wing libertarians but also criticized right-wing libertarians who were content to rely only on education to bring down the state; he believed that libertarians should adopt any moral tactic available to them to bring about liberty.[65] Imbibing Randolph Bourne\'s idea that "war is the health of the state", Rothbard opposed all wars in his lifetime and engaged in anti-war activism.[66]\n\nDuring the 1970s and 1980s, Rothbard was active in the Libertarian Party. He was frequently involved in the party\'s internal politics. Rothbard founded the Center for Libertarian Studies in 1976 and the Journal of Libertarian Studies in 1977. He was one of the founders of the Cato Institute in 1977 (whose funding by Charles Koch was a major infusion of money for libertarianism)[67] and "came up with the idea of naming this libertarian think tank after Cato\'s Letters, a powerful series of British newspaper essays by John Trenchard and Thomas Gordon which played a decisive influence upon America\'s Founding Fathers in fomenting the Revolution".[68][69]\n\nFrom 1978 to 1983, Rothbard was associated with the Libertarian Party Radical Caucus, allying himself with Justin Raimondo, Eric Garris and Williamson Evers. He opposed the "low-tax liberalism" espoused by 1980 Libertarian Party presidential candidate Ed Clark and Cato Institute president Edward H Crane III. According to Charles Burris, "Rothbard and Crane became bitter rivals after disputes emerging from the 1980 LP presidential campaign of Ed Clark carried over to strategic direction and management of Cato".[68]\n\nJanek Wasserman wrote, "The tempestuous tale of the Rothbard-Koch-Cato relationship has been told and retold because of its floridness."[27] Rothbard sought to cultivate radical anarcho-capitalists, while Crane and Koch wanted a more reformist approach to influence government and gain political power.[27] Rothbard was removed from Cato\'s board in 1981.[27] Wasserman described the split as "the first of many examples of Austrian and libertarian schisms in the United States".[27]\n\nIn 1982, following his split with the Cato Institute, Rothbard co-founded the Ludwig von Mises Institute in Auburn, Alabama, (with Lew Rockwell and Burton Blumert)[70] and was vice president of academic affairs until 1995.[52] Rothbard also founded the institute\'s Review of Austrian Economics, a heterodox economics[71] journal later renamed the Quarterly Journal of Austrian Economics, in 1987.[66] Rothbard "worked closely with Lew Rockwell (joined later by his long-time friend Blumert) in nurturing the Mises Institute and the publication, The Rothbard-Rockwell Report; which after Rothbard\'s 1995 death evolved into the website, LewRockwell.com", according to the website.[68]\n\nRothbard and other Mises Institute scholars criticized libertarian groups funded by the Koch brothers, referring to them as the "Kochtopus".[72] In contrast to some other libertarian groups, the Mises Institute "pushed more politically marginal positions like the virtues of secession, the need for a return to the gold standard, and opposition to racial integration", according to historian Quinn Slobodian.[11] Rothbard split with the Radical Caucus at the 1983 national convention over cultural issues and aligned himself with what he called the "right-wing populist" wing of the party, notably Lew Rockwell and Ron Paul, who ran for president on the Libertarian Party ticket in 1988.\n\nIn 1989, Rothbard left the Libertarian Party and began building bridges to the post-Cold War anti-interventionist right, calling himself a paleolibertarian, a conservative reaction against the cultural liberalism of mainstream libertarianism.[33][73] Paleolibertarianism sought to appeal to disaffected working-class whites through a synthesis of cultural conservatism and libertarian economics. According to Reason, Rothbard advocated right-wing populism in part because he was frustrated that mainstream thinkers were not adopting the libertarian view and suggested that former Ku Klux Klan Grand Wizard David Duke, as well as Wisconsin Senator Joseph McCarthy,[74] were models for an "Outreach to the Rednecks" effort that a broad libertarian/paleoconservative coalition could use. Working together, the coalition would expose the "unholy alliance of \'corporate liberal\' Big Business and media elites, who, through big government, have privileged and caused to rise up a parasitic Underclass". Rothbard blamed this "underclass" for "looting and oppressing the bulk of the middle and working classes in America".[33] Regarding Duke\'s political program, Rothbard asserted that there was "nothing" in it that "could not also be embraced by paleoconservatives or paleolibertarians; lower taxes, dismantling the bureaucracy, slashing the welfare system, attacking affirmative action and racial set-asides, calling for equal rights for all Americans, including whites".[75] He also praised the "racialist science" in Charles Murray\'s controversial book The Bell Curve.[76]\n\nRothbard co-founded and became a key figure in the John Randolph Club, which was an alliance between the Mises Institute and the paleoconservative Rockford Institute.[77][5] He supported the presidential campaign of Pat Buchanan in 1992, writing that "with Pat Buchanan as our leader, we shall break the clock of social democracy".[78] When Buchanan dropped out of the Republican primary race, Rothbard then shifted his interest and support to Ross Perot,[79] who Rothbard wrote had "brought an excitement, a verve, a sense of dynamics and of open possibilities to what had threatened to be a dreary race".[80] Rothbard eventually withdrew his support from Perot, and endorsed George H. W. Bush in the 1992 election.[81][82] Like Buchanan, Rothbard opposed the North American Free Trade Agreement (NAFTA);[83] however, he had become disillusioned with Buchanan by 1995, believing that the latter\'s "commitment to protectionism was mutating into an all-round faith in economic planning and the nation state".[84]\n\nJoey Rothbard said in a memoriam that her husband had a happy and bright spirit and that Rothbard, a night owl, "managed to make a living for 40 years without having to get up before noon. This was important to him." She said Rothbard would begin every day with a phone conversation with his colleague Lew Rockwell: "Gales of laughter would shake the house or apartment, as they checked in with each other. Murray thought it was the best possible way to start a day".[85]\n\nRothbard was irreligious and agnostic about God,[86][87] describing himself as a "mixture of an agnostic and a Reform Jew".[88] Despite identifying as an agnostic and an atheist, he was critical of the "left-libertarian hostility to religion".[89] In Rothbard\'s later years, many of his friends anticipated that he would convert to Catholicism, but he never did.[90]\n\nRothbard died of a heart attack on January 7, 1995, in St. Luke\'s-Roosevelt Hospital Center in Manhattan, at the age of 68.[1] The New York Times obituary called Rothbard "an economist and social philosopher who fiercely defended individual freedom against government intervention".[52] Lew Rockwell, president of the Mises Institute, told The New York Times that Rothbard was "the founder of right-wing anarchism".[52] William F. Buckley Jr. wrote a critical obituary in the National Review, criticizing Rothbard\'s "defective judgment" and views on the Cold War.[22]: 3–4  Hoppe, Rockwell, and Rothbard\'s other colleagues at the Mises Institute took a different view, arguing that he was one of the most important philosophers in history.[91]\n\nRothbard was an advocate and practitioner of the Austrian School tradition of his teacher Ludwig von Mises. Like Mises, Rothbard rejected the application of the scientific method to economics and dismissed econometrics, empirical and statistical analysis, and other tools of mainstream social science as outside the field (economic history might use those tools, but not Economics proper).[92] He instead embraced praxeology, the strictly a priori methodology of Mises. Praxeology conceives of economic laws as akin to geometric or mathematical axioms: fixed, unchanging, objective, and discernible through logical reasoning.[92][independent source needed]\n\nAccording to Misesian economist Hans-Hermann Hoppe, eschewing the scientific method and empiricism distinguishes the Misesian approach "from all other current economic schools", which dismiss the Misesian approach as "dogmatic and unscientific." Mark Skousen of Chapman University and the Foundation for Economic Education, a critic of mainstream economics,[93] praises Rothbard as brilliant, his writing style persuasive, his economic arguments nuanced and logically rigorous and his Misesian methodology sound.[94] But Skousen concedes that Rothbard was effectively "outside the discipline" of mainstream economics and that his work "fell on deaf ears" outside his ideological circles. Rothbard wrote extensively on Austrian business cycle theory and, as part of this approach, strongly opposed central banking, fiat money, and fractional-reserve banking, advocating a gold standard and a 100% reserve requirement for banks.[21]: 89–94, 96–97 [66][95][96]\n\nRothbard wrote a series of polemics in which he deprecated a number of leading modern economists. He vilified Adam Smith, calling him a "shameless plagiarist"[97] who set economics off track, ultimately leading to the rise of Marxism.[98] Rothbard praised Smith\'s contemporaries, including Richard Cantillon, Anne Robert Jacques Turgot and Étienne Bonnot de Condillac, for developing the subjective theory of value. In response to Rothbard\'s charge that Smith\'s The Wealth of Nations was largely plagiarized, David D. Friedman castigated Rothbard\'s scholarship and character, saying that he "was [either] deliberately dishonest or never really read the book he was criticizing".[99] Tony Endres called Rothbard\'s treatment of Smith a "travesty".[100]\n\nRothbard was equally scathing in his criticism of John Maynard Keynes,[101] calling him weak on economic theory and a shallow political opportunist. Rothbard also wrote more generally that Keynesian-style governmental regulation of money and credit created a "dismal monetary and banking situation". He called John Stuart Mill a "wooly man of mush" and speculated that Mill\'s "soft" personality led his economic thought astray.[102] Rothbard was critical of monetarist economist Milton Friedman. In his polemic "Milton Friedman Unraveled", he called Friedman a "statist", a "favorite of the establishment", a friend of and an "apologist" for Richard Nixon, and a "pernicious influence" on public policy.[103][104] Rothbard said that libertarians should scorn rather than celebrate Friedman\'s academic prestige and political influence. Noting that Rothbard has "been nasty to me and my work", Friedman responded to Rothbard\'s criticism by calling him a "cult builder and a dogmatist".[105]\n\nIn a memorial volume published by the Mises Institute, Rothbard\'s protégé and libertarian theorist Hans-Hermann Hoppe wrote that Man, Economy, and State "presented a blistering refutation of all variants of mathematical economics" and included it among Rothbard\'s "almost mind-boggling achievements". Hoppe lamented that, like Mises, Rothbard died without winning the Nobel Prize and, while acknowledging that Rothbard and his work were largely ignored by academia, called him an "intellectual giant" comparable to Aristotle, John Locke, and Immanuel Kant.[106]\n\nGeorgetown Professor Randy Barnett says, regarding Rothbard\'s "insistence on complete ideological purity", that "[a]lmost every intellectual who entered his orbit was eventually spun off, or self emancipated, for some deviation or another. For this reason, the circle around Rothbard was always small."[107] Although he self-identified as an Austrian economist, Rothbard\'s methodology was at odds with that of many other Austrians. In 1956, Rothbard deprecated the views of Austrian economist Fritz Machlup, stating that Machlup was no praxeologist and calling him instead a "positivist" who failed to represent the views of Ludwig von Mises. Rothbard noted that, in fact, Machlup shared the opposing positivist view associated with economist Milton Friedman.[108] Mises and Machlup had been colleagues in 1920s Vienna before each relocated to the United States, and Mises later urged his American protege Israel Kirzner to pursue his PhD studies with Machlup at Johns Hopkins University.[109][independent source needed]\n\nAccording to libertarian economists Tyler Cowen and Richard Fink,[110] Rothbard wrote that the term evenly rotating economy (ERE) could be used to analyze complexity in a world of change. Mises introduced ERE as an alternative nomenclature for the mainstream economic method of static equilibrium and general equilibrium analysis. Cowen and Fink found "serious inconsistencies in both the nature of the ERE and its suggested uses". With the sole exception of Rothbard, no other economist adopted Mises\' term, and the concept continued to be called "equilibrium analysis".[111]\n\nIn a 2011 article critical of Rothbard\'s "reflexive opposition" to inflation, The Economist noted that his views were increasingly gaining influence among politicians and laypeople on the right. The article contrasted Rothbard\'s categorical rejection of inflationary policies with the monetary views of "sophisticated Austrian-school monetary economists such as George Selgin and Lawrence H. White", [who] follow Hayek in treating stability of nominal spending as a monetary ideal—a position "not all that different from Mr [Scott] Sumner\'s".[112] According to economist Peter Boettke, Rothbard is better described as a property rights economist than as an Austrian economist. In 1988, Boettke noted that Rothbard "vehemently attacked all of the books of the younger Austrians".[113]\n\nAlthough Rothbard adopted Ludwig von Mises\' deductive methodology for his social theory and economics,[114] he parted with Mises on the question of ethics. Specifically, he rejected Mises\' conviction that ethical values remain subjective and opposed utilitarianism in favor of principle-based, natural law reasoning. In defense of his free-market views, Mises employed utilitarian economic arguments to contend that interventionist policies worsened society. Rothbard countered that interventionist policies do, in fact, benefit some people, including certain government employees and beneficiaries of social programs. Therefore, unlike Mises, Rothbard argued for an objective, natural-law basis for the free market.[47]: 87–89  He called this principle "self-ownership", loosely basing the idea on the writings of John Locke and also borrowing concepts from classical liberalism and the anti-imperialism of the Old Right.[8]: 134\n\nRothbard accepted the labor theory of property but rejected the Lockean proviso, arguing that if an individual mixes his labor with unowned land, then he becomes the proper owner eternally and that after that time, it is private property which may change hands only by trade or gift.[115] Rothbard was a strong critic of egalitarianism. The title essay of Rothbard\'s 1974 book Egalitarianism as a Revolt Against Nature and Other Essays held: "Equality is not in the natural order of things, and the crusade to make everyone equal in every respect (except before the law) is certain to have disastrous consequences."[116] In it, Rothbard wrote: "At the heart of the egalitarian left is the pathological belief that there is no structure of reality; that all the world is a tabula rasa that can be changed at any moment in any desired direction by the mere exercise of human will."[117] Noam Chomsky critiqued Rothbard\'s ideal society as "a world so full of hate that no human being would want to live in it ... First of all, it couldn\'t function for a second—and if it could, all you\'d want to do is get out, or commit suicide or something."[118] The philosopher James. W. Child has even questioned whether Rothbard and other \nsimilar libertarians can sustain a standard of fraud.[119]\n\nAccording to anarcho-capitalists, various theorists have espoused legal philosophies similar to anarcho-capitalism; however, Rothbard was credited with coining the terms "anarcho-capitalist" and "anarch-capitalism" in 1971 (though "anarchocapitalism [sic]" had been attested earliest in Karl Hess\'s 1969 essay The Death of Politics[120][121][self-published source?]).[122][123][self-published source?] He synthesized elements from the Austrian School of economics, classical liberalism and 19th-century American individualist anarchists into a right-wing form of anarchism.[124][125][10] According to his protégé Hans-Hermann Hoppe, "[t]here would be no anarcho-capitalist movement to speak of without Rothbard".[126] Lew Rockwell in a memoriam called Rothbard the "conscience" of all the various strains of what he described as "libertarian anarchism", and said their advocates had often been personally inspired by his example.[127]\n\nDuring his years at graduate school in the late 1940s, Rothbard considered whether strict adherence to libertarian and laissez-faire principles required the abolition of the state altogether. He visited Baldy Harper, a founder of the Foundation for Economic Education,[128] who doubted the need for any government whatsoever. Rothbard said that during this period, he was influenced by 19th-century American individualist anarchists like Lysander Spooner and Benjamin Tucker and the Belgian economist Gustave de Molinari who wrote about how such a system could work.[47]: 12–13  Thus, he "combined the laissez-faire economics of Mises with the absolutist views of human rights and rejection of the state" from individualist anarchists.[129] Edward Stringham opined that: "In the late 1940s, Murray Rothbard decided that that [sic] private-property anarchism was the logical conclusion of free-market thinking [...]."[130]\n\nRothbard began to consider himself a "private property anarchist"[citation needed] and published works about private property anarchism in 1954;[130] later, in 1971, he began to use "anarcho-capitalist" to describe his political ideology.[123][131][132] In his anarcho-capitalist model, the system of private property is upheld by private firms, such as hypothesized protection agencies, which compete in a free market and are voluntarily supported by consumers who choose to use their protective and judicial services. Anarcho-capitalists describe this as "the end of the state monopoly on force".[131] In this way, Rothbard differed from Mises, who favored a state to uphold markets.[10]\n\nIn an unpublished article, Rothbard wrote that economically speaking, individualist anarchism differs from anarcho-capitalism and jokingly pondered whether libertarians should adopt the term nonarchist. Rothbard concluded the article by affirming that he is neither an anarchist nor an "artist" but a middle-of-the-roader on the archy question.[133][independent source needed] In Man, Economy, and State, Rothbard divides the various kinds of state intervention into three categories: "autistic intervention" (interference with private non-economic activities), "binary intervention", (exchange between individuals and the state); and "triangular intervention" (state-mandated exchange between individuals). Sanford Ikeda wrote that Rothbard\'s typology "eliminates the gaps and inconsistencies that appear in Mises\'s original formulation".[134][135] Rothbard writes in Power and Market that the role of the economist in a free market is limited, but it is much larger in a government that solicits economic policy recommendations. Rothbard argues that self-interest, therefore, prejudices the views of many economists in favor of increased government intervention.[136][137]\n\nMichael O\'Malley, associate professor of history at George Mason University, describes Rothbard\'s tone toward the civil rights movement and the women\'s suffrage movement as "contemptuous and hostile".[29] Rothbard criticized women\'s rights activists, attributing the growth of the welfare state to politically active spinsters "whose busybody inclinations were not fettered by the responsibilities of home and hearth".[138] Rothbard argued that the progressive movement, which he regarded as a noxious influence on the United States, was spearheaded by a coalition of Yankee Protestants (people from the six New England states and upstate New York who were Protestants of English descent), Jewish women and "lesbian spinsters".[139]\n\nRothbard called for the elimination of "the entire \'civil rights\' structure," which he said "tramples on the property rights of every American." He consistently favored repeal of the 1964 Civil Rights Act, including Title VII regarding employment discrimination,[140] and called for overturning the Brown v. Board of Education decision on the grounds that state-mandated integration of schools violated libertarian principles.[141] In an essay called "Right-wing Populism", Rothbard proposed a set of measures to "reach out" to the "middle and working classes", which included urging the police to crack down on "street criminals", writing that "cops must be unleashed" and "allowed to administer instant punishment, subject of course to liability when they are in error". He also advocated that the police "clear the streets of bums and vagrants."[142][35]\n\nRothbard held strong opinions about many leaders of the civil rights movement. He considered black separatist Malcolm X to be a "great black leader" and integrationist Martin Luther King Jr. to be favored by whites because he "was the major restraining force on the developing Negro revolution".[8]: 167  Jacob Jensen writes that Rothbard\'s commentary from the 1960s, approving of both "black power" and "white power" in separated communities, amounted to support for racial segregation.[143] In 1993, Rothbard rejected the vision of a "separate black nation", asking, "Does anyone really believe that ... New Africa would be content to strike out on its own, with no massive "foreign aid" from the U.S.A.?".[144] Rothbard also suggested that opposition to Martin Luther King Jr., whom he demeaned as a "coercive integrationist", should be a litmus test for members of his "paleolibertarian" political movement.[145]\n\nRothbard is described by the historian John P. Jackson Jr. as espousing antisemitism despite Rothbard\'s own background as a secular Jew.[31] One former student described Rothbard as privately using the anti-Jewish slur "kikes" repeatedly.[31] Rothbard also befriended the Holocaust deniers Willis Carto and Harry Elmer Barnes.[31]\n\nLike Randolph Bourne, Rothbard believed that "war is the health of the state". According to David Gordon, this was the reason for Rothbard\'s opposition to aggressive foreign policy.[66] Rothbard believed that stopping new wars was necessary and that knowing how the government had led citizens into earlier wars was important. Two essays expanded on these views: "War, Peace, and the State" and "Anatomy of the State". Rothbard used insights from Vilfredo Pareto, Gaetano Mosca, and Robert Michels to build a model of state personnel, goals, and ideology.[146][147][independent source needed]\n\nRothbard\'s colleague Joseph Stromberg notes that Rothbard made two exceptions to his general condemnation of war: "the American Revolution and the War for Southern Independence, as viewed from the Confederate side", referring to the American Civil War.[148] Rothbard condemned the "Northern war against slavery", saying it was inspired by "fanatical" religious faith and characterized by "a cheerful willingness to uproot institutions, to commit mayhem and mass murder, to plunder and loot and destroy, all in the name of high moral principle".[149][150][151] He celebrated Jefferson Davis, Robert E. Lee, and other prominent Confederates as heroes while denouncing Abraham Lincoln, Ulysses S. Grant, and other Union leaders, who he said had "opened the Pandora\'s Box of genocide and the extermination of civilians".[152][153] Rothbard saw secession movements as a tool for undermining and disintegrating the state, according to historian Quinn Slobodian, who wrote that "Rothbard\'s life was marked by a search for signs of potential secession" and that "When he found them, he did his best to deepen them."[11]\n\nRothbard embraced "historical revisionism" as an antidote to what he perceived to be the dominant influence exerted by corrupt "court intellectuals" over mainstream historical narratives.[31][8]: 15, 62, 141 [154] His friend Harry Elmer Barnes, the Holocaust-denying historian, used similar language, "court historians".[31] Rothbard wrote that these mainstream intellectuals distorted the historical record in favor of "the state" in exchange for "wealth, power, and prestige" from the state.[8]: 15  Rothbard characterized the revisionist task as "penetrating the fog of lies and deception of the State and its Court Intellectuals, and to present to the public the true history".[154]\n\nRothbard worked with antisemitic writers in developing an isolationist revisionist history of World War II.[31] He was influenced by and called a champion of Barnes.[154][30][155] Rothbard favorably cited Barnes\' view that "the murder of Germans and Japanese was the overriding aim of World War II".[citation needed] In an obituary for Barnes, Rothbard wrote: "Our entry into World War II was the crucial act in foisting a permanent militarization upon the economy and society, in bringing to the country a permanent garrison state, an overweening military–industrial complex, a permanent system of conscription. It was the crucial act in creating a mixed economy run by Big Government, a system of state monopoly capitalism run by the central government in collaboration with Big Business and Big Unionism."[156] Besides broadly supporting his historical views, Rothbard promoted Barnes as an influence for future revisionists.[157]\n\nRothbard\'s endorsement of World War II revisionism and his association with Barnes and other Holocaust deniers have drawn criticism. Kevin D. Williamson wrote an opinion piece published by National Review which condemned Rothbard for "making common cause with the \'revisionist\' historians of the Third Reich", a term he used to describe American Holocaust deniers associated with Rothbard, such as James J. Martin of the Institute for Historical Review. The piece also characterized "Rothbard and his faction" as being "culpably indulgent" of Holocaust denial, the view which "specifically denies that the Holocaust actually happened or holds that it was in some way exaggerated".[32] In an article for Rothbard\'s 50th birthday, Rothbard\'s friend and Buffalo State College historian Ralph Raico stated that Rothbard "is the main reason that revisionism has become a crucial part of the whole libertarian position".[158]\n\nRothbard\'s The Libertarian Forum blamed the Middle East conflict on Israeli aggression "fueled by American arms and money". Rothbard warned that the Middle East conflict would draw the United States into a world war. He was anti-Zionist and opposed United States involvement in the Middle East. Rothbard said the Camp David Accords betrayed Palestinian aspirations and opposed Israel\'s 1982 invasion of Lebanon.[159]\n\nIn his essay, "War Guilt in the Middle East", Rothbard wrote that Israel refused "to let these refugees return and reclaim the property taken from them," [160] and took negative views of a two state solution for the Israeli–Palestinian conflict. He wrote: "On the one hand there are the Palestinian Arabs, who have tilled the soil or otherwise used the land of Palestine for centuries; and on the other, there are a group of external fanatics, who come from all over the world, and who claim the entire land area as \'given\' to them as a collective religion or tribe at some remote or legendary time in the past. There is no way the two claims can be resolved to the satisfaction of both parties. There can be no genuine settlement, no \'peace\' in the face of this irrepressible conflict; there can only be either a war to the death, or an uneasy practical compromise which can satisfy no one."[161]\n\nIn the Ethics of Liberty, Rothbard explores issues regarding children\'s rights regarding self-ownership and contract.[162] These include support for a woman\'s right to abortion, condemnation of parents showing aggression towards children and opposition to the state forcing parents to care for children. He also holds children have the right to run away from parents and seek new guardians as soon as they are able to choose to do so. He argued that parents have the right to put a child out for adoption or sell the rights to the child in a voluntary contract in what Rothbard suggests will be a "flourishing free market in children". He believes that selling children as consumer goods in accord with market forces—while "superficially monstrous"—will benefit "everyone" involved in the market: "the natural parents, the children, and the foster parents purchasing".[163][164]\n\nIn Rothbard\'s view of parenthood, "the parent should not have a legal obligation to feed, clothe, or educate his children, since such obligations would entail positive acts coerced upon the parent and depriving the parent of his rights."[163] Thus, Rothbard stated that parents should have the legal right to let any infant die by starvation and should be free to engage in other forms of child neglect. However, according to Rothbard, "the purely free society will have a flourishing free market in children". In a fully libertarian society, he wrote, "the existence of a free baby market will bring such \'neglect\' down to a minimum".[163] Economist Gene Callahan of Cardiff University, formerly a scholar at the Rothbard-affiliated Mises Institute, wrote that Rothbard allowed "the logical elegance of his legal theory" to "trump any arguments based on the moral reprehensibility of a parent idly watching her six-month-old child slowly starve to death in its crib".[165]\n\nIn The Ethics of Liberty, Rothbard advocates for a "frankly retributive theory of punishment" or a system of "a tooth (or two teeth) for a tooth".[166] Rothbard emphasizes that all punishment must be proportional, stating that "the criminal, or invader, loses his rights to the extent that he deprived another man of his".[167] Applying his retributive theory, Rothbard states that a thief "must pay double the extent of theft". Rothbard gives the example of a thief who stole $15,000 and says he would have to return the stolen money and provide the victim an additional $15,000, money to which the thief has forfeited his right. The thief would be "put in a [temporary] state of enslavement to his victim"[citation needed] if he is unable to pay him immediately. Rothbard also applies his theory to justify beating and torturing violent criminals, although the beatings are required to be proportional to the crimes for which they are being punished.\n\nIn chapter twelve of Ethics,[168] Rothbard turns his attention to suspects arrested by the police.[165] He argues that police should be able to torture certain types of criminal suspects, including accused murderers, for information related to their alleged crimes. Writes Rothbard: "Suppose ... police beat and torture a suspected murderer to find information (not to wring a confession, since obviously a coerced confession could never be considered valid). If the suspect turns out to be guilty, then the police should be exonerated, for then they have only ladled out to the murderer a parcel of what he deserves in return; his rights had already been forfeited by more than that extent. But if the suspect is not convicted, then that means that the police have beaten and tortured an innocent man, and that they in turn must be put into the dock for criminal assault".[168] Gene Callahan examines this position and concludes that Rothbard rejects the widely held belief that torture is inherently wrong, no matter who the victim. Callahan goes on to state that Rothbard\'s scheme gives the police a strong motive to frame the suspect after having tortured him or her.[165]\n\nIn an essay condemning "scientism in the study of man", Rothbard rejected the application of causal determinism to human beings, arguing that the actions of human beings—as opposed to those of everything else in nature—are not determined by prior causes, but by "free will".[169] He argued that "determinism as applied to man, is a self-contradictory thesis, since the man who employs it relies implicitly on the existence of free will"[citation needed]. Rothbard opposed what he considered the overspecialization of the academy and sought to fuse the disciplines of economics, history, ethics and political science to create a "science of liberty". Rothbard described the moral basis for his anarcho-capitalist position in two of his books: For a New Liberty, published in 1973; and The Ethics of Liberty, published in 1982. In his Power and Market (1970), Rothbard describes how a stateless economy might function.[independent source needed]',
        pageTitle: "Murray Rothbard",
    },
    {
        title: "Sanderson's Laws of Magic",
        link: "https://en.wikipedia.org/wiki/Brandon_Sanderson#Sanderson's_Laws_of_Magic",
        content:
            "Brandon Winn Sanderson (born December 19, 1975) is an American author of high fantasy, science fiction, and young adult books. He is best known for the Cosmere fictional universe, in which most of his fantasy novels, most notably the Mistborn series and The Stormlight Archive, are set. Outside of the Cosmere, he has written several young adult and juvenile series including The Reckoners, the Skyward series,[a] and the Alcatraz series. He is also known for finishing author Robert Jordan's high fantasy series The Wheel of Time. Sanderson has created two graphic novels, including White Sand and Dark One.\n\nSanderson created Sanderson's Laws of Magic and popularized the idea of \"hard magic\" and \"soft magic\" systems. In 2008, Sanderson started a podcast with the horror writer Dan Wells and the cartoonist Howard Tayler called Writing Excuses, involving topics about creating genre writing and webcomics. In 2016, the American media company DMG Entertainment licensed the film rights to Sanderson's entire Cosmere universe, but the rights have since reverted back to Sanderson. Sanderson's March 2022 Kickstarter campaign became the most successful in history, finishing with 185,341 backers pledging US$41,754,153.[3]\n\nSanderson was born on December 19, 1975, in Lincoln, Nebraska,[4][5] the eldest of four children born to Barbara and Winn Sanderson. He was a \"reluctant reader\" as a child but became a passionate reader of fantasy in his teens after a teacher gave him a copy of Dragonsbane by Barbara Hambly.[6] He made several early attempts at writing his own stories.[7] After graduating from high school in 1994, he went to Brigham Young University (BYU) as a biochemistry major. He took a two-year leave of absence from 1995 to 1997 to serve as a volunteer missionary for the Church of Jesus Christ of Latter-day Saints and was assigned to serve in South Korea.[7]\n\nAfter completing his missionary service, Sanderson returned to BYU and changed his major to English literature. While an undergraduate, Sanderson took a job as a night auditor at a local hotel in Provo, Utah, as it allowed him to write while working.[7] One of Sanderson's roommates at BYU was Ken Jennings, who nearly ten years later became famous during his 74-game win streak on the game show Jeopardy!.[8] Sanderson graduated from BYU in 2000 with a Bachelor of Arts. He continued on as a graduate student, receiving an M.A. in English with an emphasis in creative writing in 2004.[9] While at BYU, Sanderson was on the staff of Leading Edge, a semi-professional speculative fiction magazine published by the university, and served as its editor-in-chief for one year.[10]\n\nIn 2006, Sanderson married Emily Bushman, an English, Spanish, and ESL teacher and fellow BYU alumna; Emily later became his business manager.[7][11] They have three sons and reside in American Fork, Utah.[12]\n\nSanderson wrote consistently throughout his undergraduate and graduate studies; by 2003, he had written twelve novels, though no publisher had accepted any of them for publication.[13] While in the middle of a graduate program at BYU, he was contacted by Tor Books editor Moshe Feder, who wanted to acquire one of his books. Sanderson had submitted the manuscript of his sixth novel,[14] Elantris, a year and a half earlier.[7] Elantris was published by Tor Books on April 21, 2005, to generally positive reviews.[15][16] This was followed in 2006 by Mistborn: The Final Empire, the first book in his Mistborn fantasy trilogy, in which \"allomancers\"—people with the ability to 'burn' metals and alloys after ingesting them—gain enhanced senses and control over powerful supernatural forces.[17]\n\nHe published the second book of the Mistborn series The Well of Ascension in 2007.[18] Later that year, Sanderson published the children's novel Alcatraz Versus the Evil Librarians, about a boy named Alcatraz with a talent for breaking things.[19] Alcatraz confronts a group of evil librarians who are bent on taking over the world. The first of his \"laws of magic\" were first published in 2007, with the second and third published in 2012 and 2013 (respectively).[20][21][22] In 2008, the third and final book in the Mistborn trilogy was published, titled The Hero of Ages, as well as the second book in the Alcatraz series, titled Alcatraz Versus the Scrivener's Bones.[23] That same year, he started the podcast Writing Excuses with Howard Tayler and Dan Wells.[24]\n\nSanderson rose to prominence in late 2007 when Harriet McDougal, the wife and editor of author Robert Jordan, chose Sanderson to complete the final books in Jordan's epic fantasy series The Wheel of Time after Jordan's death. McDougal asked Sanderson to finish the series after being deeply impressed by his first Mistborn novel, The Final Empire.[25] Tor Books made the announcement on December 7, 2007.[26] After reviewing what was necessary to complete the series, Sanderson and Tor announced on March 30, 2009, that a final three books would be published instead of just one.\n\nThe first of these, The Gathering Storm, was published on October 27, 2009, and reached the number-one spot on the New York Times bestseller list for hardcover fiction.[27][28] Towers of Midnight, the second-to-last The Wheel of Time book, was published just over a year after The Gathering Storm on November 2, 2010, debuting at number one on the bestseller list.[29] In early 2013, the series was completed with the publication of A Memory of Light.[30]\n\nIn 2009, Tor Books published Warbreaker, which originally appeared serially on Sanderson's website while he was writing the novel from 2006 to 2009.[31][32] In the same year, the third Alcatraz book was published, titled Alcatraz Versus the Knights of Crystallia.[33] In 2010, Sanderson published The Way of Kings, the first of a planned ten-book series called The Stormlight Archive. It achieved the number seven slot on the New York Times hardcover fiction bestseller list.[34] The fourth Alcatraz novel, Alcatraz Versus the Shattered Lens, followed soon after.[35]\n\nIn October 2011, he finished a novella e-book, Infinity Blade: Awakening, based on the action role-playing, iOS video game Infinity Blade, developed by Chair Entertainment and Epic Games.[36] In November 2011, he published a sequel to the Mistborn trilogy, Mistborn: The Alloy of Law.[37] It was originally planned as a standalone novel set about 300 years after the original trilogy, but it was later expanded into a four-book series.[38][39] It debuted at number nine on the combined print and e-book New York Times Best Seller list.[40]\n\nOn August 31, 2012, Sanderson published a science fiction novella entitled Legion, followed by another short work titled The Emperor's Soul.[41][42] In 2013, Sanderson published two new young adult series. These series included The Rithmatist and the first of The Reckoners series titled Steelheart.[43][44][45] In March 2014, Words of Radiance, the second book in The Stormlight Archive, was published.[46]\n\nLater that year, Subterranean Press published the second novella in the Legion series, Legion: Skin Deep.[47] It was a preliminary nominee for the 2015 Hugo Awards, but did not make the final ballot.[47] In January 2015, the second book of The Reckoners, titled Firefight, was published.[48] Firefight won the 2015 Whitney Award in the Best Young Adult—Speculative category.[48] It also placed eighth in the Young Adult Fantasy & Science Fiction category of the Goodreads Choice Awards and was a finalist for the 2015 AML Award in the Young Adult Novel category.[48]\n\nNine months later, Sanderson published Mistborn: Shadows of Self as a direct sequel to The Alloy of Law.[49] The novel won the 2017 Neffy Award in the Best Novel category, placed third in the 2015 Goodreads Choice Awards in the Fantasy category, was a finalist in the Best Speculative Fiction category of the 2015 Whitney Awards, and was a preliminary nomineed for the 2016 Gemmell Legend Award.[49] On November 16, 2015, Sanderson's agency (JABberwocky Literary Agency) announced that Sanderson officially sold over 7 million copies worldwide.[50]\n\nOn January 26, 2016, Mistborn: The Bands of Mourning was published as the sequel to Shadows of Self. On February 16, 2016, the third and final book of the Reckoners trilogy, titled Calamity, was published. In June 2016, Sanderson's first graphic novel White Sand—written with Rik Hoskin—was released. The series is planned as a trilogy.[51] The graphic novels are based on an original manuscript by Sanderson.[52] On September 6, 2016, the fifth Alcatraz book was published, called Alcatraz Versus the Dark Talent.[53]\n\nDMG Entertainment optioned the Cosmere in 2016 for film and television.[54] On November 22, 2016, an anthology of Cosmere short stories and novellas was published, titled Arcanum Unbounded: A Cosmere Collection. The third book in The Stormlight Archive, Oathbringer, was published on November 14, 2017.[55] The first book of the Defiant series, Skyward, was published on November 6, 2018.[56] The second book in the series, Starsight, was released on November 26, 2019. In September 2020, a collaboration project with author Mary Robinette Kowal called The Original, was released. Rhythm of War, the fourth Stormlight novel, was published on November 17, 2020.[57] In 2020, Sanderson's agency updated his number of copies sold to over 18 million worldwide,[58] and in early 2021, to over 21 million.[59]\n\nIn March 2021, Brandon Sanderson announced a \"Weekly Update\" in his YouTube channel which will give updates on his current projects every week. On May 26, Brandon Sanderson revealed the title and cover for \"Cytonic\", the third book of his Skyward series, which was published on November 23, 2021. Sanderson started a new podcast in June 2021 called 'Intentionally Blank', with friend and fellow science fiction author Dan Wells.[60]\n\nSanderson announced in March 2022 that, over the previous two pandemic years, he had secretly written five otherwise-unannounced books (four full adult novels and a shorter junior novel). The full novels (three of which are set in the Cosmere) were made available through a Kickstarter subscription that releases them quarterly through 2023.[61] The Kickstarter campaign was highly successful, raising $15 million in its first 24 hours[62] and over $20 million within three days, becoming the all-time most successful campaign.[63] The Kickstarter campaign finished with 185,341 backers pledging $41,754,153.[64][65] Before the conclusion of his Kickstarter campaign, Sanderson also backed every other publishing project on Kickstarter, for a total of 316 projects.[66] One of the secret projects during the pandemic, Tress of the Emerald Sea, was released in book form in April 2023.[67]\n\nSanderson also collaborated with Unknown Worlds Entertainment to create the lore and setting for the video game Moonbreaker, which was released via early access in September 2022.[68]\n\nSanderson announced a further 'secret project' novel, set for a 2025 release, in March 2024.[69] But the \"biggest release the [fantasy] genre has seen in years\" came about in December 2024 with the unveiling of Wind and Truth.[70] This is Sanderson's fifth and final book in the first arc of The Stormlight Archive. Sanderson projects there will be at least five more books in the series, but the printing of these novels isn't estimated until 2031.[70]\n\nDragonsteel Entertainment, LLC is a company acting as publisher, storefront, and producer for various Sanderson-related products and projects. It is also known as Dragonsteel Books, according to Sanderson's personal blog as a rebranding tactic in 2021.[71]  It holds copyrights to many of Sanderson's novels and has self-published several of his stories in both digital and print formats.[72]\n\nIn 2024, Sanderson appeared before 5,000 fans at FanX in Salt Lake City, Utah at a 50-minute panel. During the panel, Sanderson announced that Dragonsteel Entertainment had purchased land to \"theoretically build a bookstore\" called Dragonsteel Plaza.[73][74]\n\nSanderson first turned to Kickstarter in 2020, when he generated $6.7 million from almost 30,000 backers to produce a collectable leather-bound 10th anniversary edition of the Stormlight Archive novel, The Way of Kings.[75]\n\nIn 2022, in his second Kickstarter project, Brandon Sanderson raised over $41.7 million for four secret books, all intended as stand-alone novels, through Dragonsteel Entertainment. The crowdfunding campaign became the largest in Kickstarter history by pledge volume, surpassing the previous record holder by more than double. It also set new records for the most funds raised in the first 24 hours, with $15.4 million, as well as the highest number of backers and total funding within the same time period.[76]\n\nIn August 2024, Sanderson teamed up with Brotherwise Games to create a tabletop role-playing game (RPG) based on the mythical universe the Cosmere, featured in many of his fantasy novels. With over $14.6 million in pledges, the Kickstarter campaign broke the previous record in pledges for a tabletop RPG.[76]\n\nIn 2015, Brandon Sanderson and wife Emily Sanderson created a charitable organization called The Lightweaver Foundation. Its mission is to \"Feed bodies. Fill minds. Fuel hope.\"[77] Jane Horne is the director of the organization.[77]\n\nThe Lightweaver Foundation's first project helped students at Utah Valley University (UVU) and Brigham Young University (BYU) publish their speculative fiction in journals. The foundation set up an endowment fund to support university journals and ensure future publications of these journals continued.[77]\n\nThe Lightweaver Foundation is also responsible for raising money to support people and programs, largely within their local community. One of the major beneficiaries for these efforts has been the Intermountain Primary Children’s Hospital. The Lightweaver Foundation has also promoted literacy by supporting a writing conference for teens called Teen Author Bootcamp and also supporting Book Drop, which hosts popular authors to speak at schools and give away copies of their published works.[77]\n\nIn the summer of 2024, Brandon Sanderson partnered with the Maryville University League of Legends team to financially support them in their involvement in the North American Challengers League (NACL), a tournament created by Riot Games that serves as the official path to becoming a professional League of Legends player in North America.[78]\n\nThe Cosmere is the name of the universe in which Elantris, Mistborn, Warbreaker, The Stormlight Archive, White Sand, Tress of the Emerald Sea, Yumi and the Nightmare Painter, The Sunlit Man, and stories contained in Arcanum Unbounded: The Cosmere Collection are all set. This idea came from Sanderson's desire to create an epic-length series without requiring readers to buy a ridiculous number of books. Because of that, he hides connections to his other works within each book, creating a \"hidden epic\".[79] Further, Sanderson has cited inspiration from the way Isaac Asimov's separate Robot and Foundation series were eventually tied together; the Cosmere is his attempt at an overarching superstory established at onset of the series. This is unlike Asimov's stories, which were connected ad hoc mid-series.[80] Sanderson has estimated that the Cosmere sequence could conclude with at least 40 books.[81]\n\nThe story of the Cosmere is about a mysterious being called Adonalsium, who existed on a world known as Yolen. Adonalsium was killed by a group of at least seventeen conspirators, causing its power to shatter into sixteen different Shards, each of which bears immense power.[82] Sixteen of those people—referred to as Vessels—then took these Shards and traveled to new worlds, populating them with different systems of magic or extending on ones already present. In one case, the Shards known as Ruin and Preservation worked together to create the planet Scadrial, the setting of the Mistborn series).[82]\n\nEach Shard has an Intent, such as Ambition or Honor, and a Vessel's personality is changed over time to bring them more in-line with their Intent. One such Shard, Odium, has killed—or \"splintered\"—several other Shards. On Sel, he splintered Devotion and Dominion, accidentally creating the Dor, from which Seons and Skaze have emerged. On Roshar, Odium splintered Honor and brought about the Everstorm and the True Desolation. He has also splintered Ambition, in the Threnody system. A man known as Hoid is seen or mentioned in most Cosmere books. He is from Yolen and travels the so-called Shardworlds, using the people of those worlds to further an unknown agenda.[83]\n\nSanderson has indicated that an upcoming work in the series will be in the Cyberpunk genre, a marked departure from the setting of the high-fantasy and urban-fantasy settings that have featured in the Cosmere universe to date.[84]\n\nSanderson makes an express distinction between \"soft\" and \"hard\" magic for purposes of world building and creating magic systems in fictional settings.[85][86][87] Both terms are approximate ways of characterizing two ends of a spectrum.[20][88] Hard magic systems follow specific rules, the magic is controlled and explained to the reader in the narrative detailing the mechanics behind the way the magic 'works' and can be used for building settings that revolve around the magic system.[89][90] Soft magic systems may not have clearly defined rules or limitations, or they may provide limited exposition regarding their workings. They are used to create a sense of wonder in the reader.[85][91]\n\nSanderson's three laws of magic are creative writing guidelines that can be used to create magic systems for fantasy stories:\n\nSanderson is adjunct faculty at Brigham Young University, teaching a creative writing course once per year.[94][95] Sanderson also participates in the weekly podcast Writing Excuses with authors Dan Wells, Mary Robinette Kowal, and web cartoonist Howard Tayler.[24] He began hosting the podcast Intentionally Blank with Dan Wells in June 2021, where they discuss random things they enjoy.[96]\n\nSanderson has been nominated for and also won multiple awards for his various works. See Writing Excuses for additional awards and nominations.",
        pageTitle: "Brandon Sanderson",
    },
    {
        title: "Sarnoff's law",
        link: "https://en.wikipedia.org/wiki/Sarnoff%27s_law",
        content:
            "David Sarnoff (February 27, 1891 – December 12, 1971) was a Russian[4] and American businessman who played an important role in the American history of radio and television. He led RCA for most of his career in various capacities from shortly after its founding in 1919 until his retirement in 1970.\n\nHe headed a conglomerate of telecommunications and media companies, including RCA and NBC, that became one of the largest in the world. Named a Reserve Brigadier General of the Signal Corps in 1945, Sarnoff thereafter was widely known as \"The General\".[3]\n\nDavid Sarnoff was born to a Jewish family in Uzlyany, a small town in Minsk Governorate, Russian Empire[4] (today part of Belarus), the son of Abraham Sarnoff and Leah Privin. Abraham emigrated to the United States and raised funds to bring the family. Sarnoff spent much of his early childhood in a cheder (or yeshiva) studying and memorizing the Torah. He emigrated with his mother and three brothers and one sister to New York City in 1900, where he helped support his family by selling newspapers before and after his classes at the Educational Alliance. In 1906 his father became incapacitated by tuberculosis, and at age 15 Sarnoff went to work to support the family.[5] He had planned to pursue a full-time career in the newspaper business, but a chance encounter led to a position as an office boy at the Commercial Cable Company. When his superior refused him  leave for Rosh Hashanah, he joined the Marconi Wireless Telegraph Company of America on September 30, 1906, and started a career of over 60 years in electronic communications.\n\nOver the next 13 years, Sarnoff rose from office boy to commercial manager of the company, learning about the technology and the business of electronic communications on the job and in libraries. He also served at Marconi stations on ships and posts on Siasconset, Nantucket and the New York Wanamaker Department Store. In 1911, he installed and operated the wireless equipment on a ship hunting seals off Newfoundland and Labrador, and used the technology to relay the first remote medical diagnosis from the ship's doctor to a radio operator at Belle Isle with an infected tooth.\n\nThe following year, he led two other operators at the Wanamaker station in an effort to confirm the fate of the Titanic.[1] Sarnoff later exaggerated his role as the sole hero who stayed by his telegraph key for three days to receive information on the Titanic's survivors.[5][6] Schwartz questions whether Sarnoff, who was a manager of the telegraphers by the time of the disaster, was working the key although that brushes aside concerns about corporate hierarchy. The event began on a Sunday when the store would have been closed.[7]\n\nOver the next two years Sarnoff earned promotions to chief inspector and contracts manager for a company whose revenues swelled after Congress passed legislation mandating continuous staffing of commercial shipboard radio stations. That same year Marconi won a patent suit that gave it the coastal stations of the United Wireless Telegraph Company. Sarnoff also demonstrated the first use of radio on a railroad line, the Lackawanna Railroad Company's link between Binghamton, New York, and Scranton, Pennsylvania; and permitted and observed Edwin Armstrong's demonstration of his regenerative receiver at the Marconi station at Belmar, New Jersey. Sarnoff used H. J. Round's hydrogen arc transmitter to demonstrate the broadcast of music from the New York Wanamaker station.\n\nThis demonstration and the AT&T demonstrations in 1915 of long-distance wireless telephony inspired the first of many memos to his superiors on applications of current and future radio technologies. Sometime late in 1915 or in 1916 he proposed to the company's president, Edward J. Nally, that the company develop a \"radio music box\" for the \"amateur\" market of radio enthusiasts.[6][8] Nally deferred on the proposal because of the expanded volume of business during World War I. Throughout the war years, Sarnoff remained Marconi's Commercial Manager,[3] including oversight of the company's factory in Roselle Park, New Jersey.\n\nUnlike many who were involved with early radio communications, who often viewed radio as point-to-point, Sarnoff saw the potential of radio as point-to-mass. One person (the broadcaster) could speak to many (the listeners).\n\nWhen Owen D. Young of General Electric arranged the purchase of American Marconi and reorganized it as the Radio Corporation of America, a radio patent monopoly, Sarnoff realized his dream and revived his proposal in a lengthy memo on the company's business and prospects. His superiors again ignored him but he contributed to the rising postwar radio boom by helping arrange for the broadcast of a heavyweight boxing match between Jack Dempsey and Georges Carpentier in July, 1921. Up to 300,000 people listened to the broadcast of the fight, and demand for home radio equipment bloomed that winter.[9] By the spring of 1922, Sarnoff's prediction of popular demand for radio broadcasting had come to pass and over the next few years, he gained much stature and influence.\n\nIn 1925, RCA purchased its first radio station (WEAF, New York) and launched the National Broadcasting Company (NBC), the first radio network in America. Four years later, Sarnoff became president of RCA. NBC had by that time split into two networks, the Red and the Blue. The Blue Network eventually became ABC Radio.[1] Sarnoff is often inaccurately referred to as the founder of both RCA and NBC, but he was in fact founder of only NBC.[5]\n\nSarnoff was instrumental in building and establishing the AM broadcasting radio business that became the preeminent public radio standard for the majority of the 20th century.\n\nSarnoff negotiated successful contracts to form Radio-Keith-Orpheum (RKO), a film production and distribution company.[5] Essential elements in that new company were RCA, the Film Booking Offices of America (FBO), and the Keith-Albee-Orpheum (KAO) theater chain.[10]\n\nWhen Sarnoff was put in charge of radio broadcasting at RCA, he soon recognized the potential for television, i.e., the combination of motion pictures with electronic transmission. Schemes for television had long been proposed (well before World War I) but with no practical outcome. Sarnoff was determined to lead his company in pioneering the medium and met with Westinghouse engineer Vladimir Zworykin in 1928. At the time Zworykin was attempting to develop an all-electronic television system at Westinghouse, but with little success. Zworykin had visited the laboratory of the inventor Philo T. Farnsworth, who had developed an Image Dissector, part of a system that could enable a working television. Zworykin was sufficiently impressed with Farnsworth's invention that he had his team at Westinghouse make several copies of the device for experimentation.[11]\n\nZworykin pitched the concept to Sarnoff, claiming a viable television system could be realized in two years with a mere $100,000 investment. Sarnoff opted to fund Zworkyin's research, most likely well-aware that Zworykin was underestimating the scope of his television effort. Seven years later, in late 1935, Zworykin's photograph appeared on the cover of the trade journal Electronics, holding an early RCA photomultiplier prototype. The photomultiplier, subject of intensive research at RCA and in Leningrad, Russia, would become an essential component within sensitive television cameras. On April 24, 1936, RCA demonstrated to the press a working iconoscope camera tube and kinescope receiver display tube (an early cathode-ray tube), two key components of all-electronic television.\n\nThe final cost of the enterprise was closer to $50 million. On the road to success they encountered a legal battle with Farnsworth, who had been granted patents in 1930 for his solution to broadcasting moving pictures. Despite Sarnoff's efforts to prove that he was the inventor of the television, he was ordered to pay Farnsworth $1,000,000 in royalties, a small price to settle the dispute for an invention that would profoundly revolutionize the world. However this sum was never paid to Farnsworth.\n\nIn 1929, Sarnoff engineered the purchase of the Victor Talking Machine Company, the nation's largest manufacturer of records and phonographs, merging radio-phonograph production at Victor's large manufacturing facility in Camden, New Jersey.\n\nSarnoff became president of RCA on January 3, 1930, succeeding General James Harbord. On May 30 the company was involved in an antitrust case concerning the original radio patent pool. Sarnoff negotiated an outcome where RCA was no longer partly owned by Westinghouse and General Electric, giving him final say in the company's affairs.\n\nInitially, the Great Depression caused RCA to cut costs, but Zworykin's project was protected. After nine years of Zworykin's hard work, Sarnoff's determination, and legal battles with Farnsworth (in which Farnsworth was proved in the right), they had a commercial system ready to launch. Finally, in April 1939, regularly scheduled, electronic television in America was initiated by RCA under the name of their broadcasting division at the time, The National Broadcasting Company (NBC). The first television broadcast aired was the dedication of the RCA pavilion at the 1939 New York World's Fairgrounds and was introduced by Sarnoff himself. Later that month on April 30, opening day ceremonies at The World's Fair were telecast in the medium's first major production, featuring a speech by President Franklin D. Roosevelt, the first US president to appear on television. These telecasts were seen only in New York City and the immediate vicinity, since NBC television had only one station at the time, W2XBS Channel 1, now WNBC Channel 4. The broadcast was seen by an estimated 1,000 viewers from the roughly 200 television sets which existed in the New York City area at the time.\n\nThe standard approved by the National Television System Committee (the NTSC) in 1941 differed from RCA's standard, but RCA quickly became the market leader of manufactured sets and NBC became the first television network in the United States, connecting their New York City station to stations in Philadelphia and Schenectady for occasional programs in the early 1940s.\n\nAccording to the book “Global Communication Since 1844” [12]by Peter J. Hughill, Sarnoff was part of a group of Russian Jewish scientists  in the 1930s who wanted their research to advance military technology with the possibility of a forthcoming war with Germany. The account, credited to British government scientist Brian Callick, is supported by other contemporary evidence[13].[14] The group, also comprising Simeon Aisenstein, Vladimir Zworykin and Isaac Shoenberg, knew each other well from Russia and saw possible military applications for their work on television. The group is said to have raised one million pounds sterling (about $5 million at the time) from US donors. The specific work took place at EMI-Marconi in the U.K. and resulted in Britain becoming significantly advanced in television development and able to launch a public service on 2nd November 1936. The military applications helped the development of radio-location (later named radar). In addition the design and production in quantity of television equipment and sets allowed the similar military technology (cathode ray tubes, VHF transmission and reception and wideband circuits to be advanced. A former British defence minister, Lord Orr-Ewing, referred to the work in a 1979 BBC interview and stated “that’s how we won the Battle of Britain”.\n\nMeanwhile, a system developed by EMI based on Russian research and Zworykin's work was adopted in Britain and the BBC had a regular television service from 1936 onwards. However, World War II put a halt to a dynamic growth of the early television development stages.\n\nAt the onset of World War II, Sarnoff served on Eisenhower's communications staff, arranging expanded radio circuits for NBC to transmit news from the invasion of France in June 1944. In France, Sarnoff arranged for the restoration of the Radio France station in Paris that the Germans destroyed and oversaw the construction of a radio transmitter powerful enough to reach all of the allied forces in Europe, called Radio Free Europe. In recognition of his achievements, Sarnoff was decorated with the Legion of Merit on October 11, 1944.[15]\n\nThanks to his communications skills and support he received the Brigadier General's star in December 1945, and thereafter was known as \"General Sarnoff.\"[16] The star, which he proudly and frequently wore, was buried with him.\n\nSarnoff anticipated that post-war America would need an international radio voice explaining its policies and positions. In 1943, he tried to influence Secretary of State Cordell Hull to include radio broadcasting in post-war planning. In 1947, he lobbied Secretary of State George Marshall to expand the roles of Radio Free Europe and Voice of America. His concerns and proposed solutions were eventually seen as prescient.[17]\n\nAfter the war, monochrome TV production began in earnest. Color TV was the next major development, and NBC once again won the battle. CBS had their electro-mechanical color television system approved by the FCC on October 10, 1950, but Sarnoff filed an unsuccessful suit in the United States district court to suspend that ruling. Subsequently, he made an appeal to the Supreme Court which eventually upheld the FCC decision. Sarnoff's tenacity and determination to win the \"Color War\" pushed his engineers to perfect an all-electronic color television system that used a signal that could be received on existing monochrome sets that prevailed. CBS was now unable to take advantage of the color market, due to lack of manufacturing capability and color programming, a system that could not be seen on the millions of black and white receivers and sets that were triple the cost of monochrome sets. A few days after CBS had its color premiere on June 14, 1951, RCA demonstrated a fully functional all-electronic color TV system and became the leading manufacturer of color TV sets in the US.\n\nCBS system color TV production was suspended in October 1951 for the duration of the Korean War. As more people bought monochrome sets, it was increasingly unlikely that CBS could achieve any success with its incompatible system. Few receivers were sold, and there were almost no color broadcasts, especially in prime time, when CBS could not run the risk of broadcasting a program which few could see. The NTSC was reformed and recommended a system virtually identical to RCA's in August 1952. On December 17, 1953, the FCC approved RCA's system as the new standard.\n\nIn 1955, Sarnoff received The Hundred Year Association of New York's Gold Medal Award \"in recognition of outstanding contributions to the City of New York.\"\n\nIn 1959 Sarnoff was a member of the Rockefeller Brothers Fund panel to report on U.S. foreign policy. As a member of that panel and in a subsequent essay published in Life as part of its \"The National Purpose\" series, he was critical of the tentative stand being taken by the United States in fighting the political and psychological warfare being waged by Soviet-led international Communism against the West. He strongly advocated an aggressive, multi-faceted fight in the ideological and political realms with a determination to decisively win the Cold War.[18]\n\nSarnoff retired in 1970, at the age of 79, and died the following year, aged 80. He is interred in a mausoleum featuring a stained-glass vacuum tube in Kensico Cemetery in Valhalla, New York.\n\nAfter his death, Sarnoff left behind an estate estimated to be worth over $1 million. The majority of the estate went to his widow, Lizette Hermant Sarnoff, who received $300,000, personal and household effects in addition to the Sarnoff home, located on 44 East 71st Street.[19]\n\nOn July 4, 1917, Sarnoff married Lizette Hermant, the daughter of a French-Jewish immigrant family who settled in the Bronx as one of his family's neighbors.[20][3] The Museum of Broadcast Communications describes their 54-year marriage as the bedrock of his life.[5] Lizette was often the first person to hear her husband's new ideas as radio and television became integral to American home life.[3]\n\nThe couple had three sons. Eldest son Robert W. Sarnoff (1918–1997)[21] succeeded his father at the helm of RCA in 1970.[22] Robert's third wife was operatic soprano Anna Moffo.[21] Edward Sarnoff, the middle child, headed Fleet Services of New York.[23] Thomas W. Sarnoff, the youngest, was NBC's West Coast President.[24]\n\nSarnoff was the maternal uncle of screenwriter Richard Baer.[25] Sarnoff was credited with sparking Baer's interest in television.[25] According to Baer's 2005 autobiography, Sarnoff called a vice president at NBC at 6 A.M. and ordered him to find Baer \"a job by 9 o'clock\" that same morning.[25] The NBC vice president complied with Sarnoff's request.\n\nDavid Sarnoff was initiated to the Scottish Rite Freemasonry[26][27] in the Renovation Lodge No. 97, Albion, NY.[28][29]\n\nThe David Sarnoff Library, a library and museum open to the public containing many historical items from David Sarnoff's life was in Princeton Junction, NJ. The David Sarnoff Library now exists as a virtual museum online. When the Library was operating, The David Sarnoff Radio Club composed of local amateur radio operators used to meet there, as did the New Jersey Antique Radio Club and other community organizations. The exhibits are now on display in Roscoe L. West Hall at The College of New Jersey.\n\nIn 1999, computer scientist David P. Reed coined Sarnoff's Law, which states that \"the value of a network grows in proportion to the number of viewers.\"[34] Sarnoff's Law, Metcalfe's Law and Reed's Law are frequently used in tandem in discussions of the value of networks.",
        pageTitle: "David Sarnoff",
    },
    {
        title: "Say's law",
        link: "https://en.wikipedia.org/wiki/Say%27s_law",
        content:
            "In classical economics, Say's law, or the law of markets, is the claim that the production of a product creates demand for another product by providing something of value which can be exchanged for that other product. So, production is the source of demand. In his principal work, A Treatise on Political Economy  \"A product is no sooner created, than it, from that instant, affords a market for other products to the full extent of its own value.\"[1] And also, \"As each of us can only purchase the productions of others with his/her own productions – as the value we can buy is equal to the value we can produce, the more men can produce, the more they will purchase.\"[2]\n\nSome maintain that Say further argued that this law of markets implies that a general glut (a widespread excess of supply over demand) cannot occur. If there is a surplus of one good, there must be unmet demand for another: \"If certain goods remain unsold, it is because other goods are not produced.\"[2] However, according to Petur Jonsson, Say does not claim a general glut cannot occur and in fact acknowledges that they can occur.[3] Say's law has been one of the principal doctrines used to support the laissez-faire belief that a capitalist economy will naturally tend toward full employment and prosperity without government intervention.[4][5]\n\nOver the years, at least two objections to Say's law have been raised:\n\nSay's law was generally accepted throughout the 19th century, though modified to incorporate the idea of a \"boom-and-bust\" cycle. During the worldwide Great Depression of the 1930s, the theories of Keynesian economics disputed Say's conclusions.\n\nScholars disagree on the question of whether it was Say who first stated the principle,[7][8] but by convention, Say's law has been another name for the law of markets ever since John Maynard Keynes used the term in the 1930s.\n\nSay argued that economic agents offer goods and services for sale so that they can spend the money they expect to obtain. Therefore, the fact that a quantity of goods and services is offered for sale is evidence of an equal quantity of demand. Essentially Say's argument was that money is just a medium, people pay for goods and services with other goods and services.[9][unreliable source?] This claim is often summarized as \"supply creates its own demand\", although that phrase does not appear in Say's writings.\n\nIt is worthwhile to remark that a product is no sooner created than it, from that instant, affords a market for other products to the full extent of its own value. When the producer has put the finishing hand to his product, he is most anxious to sell it immediately, lest its value should diminish in his hands. Nor is he less anxious to dispose of the money he may get for it; for the value of money is also perishable. But the only way of getting rid of money is in the purchase of some product or other. Thus the mere circumstance of creation of one product immediately opens a vent for other products.[10]\n\nSay further argued that because production necessarily creates demand, a \"general glut\" of unsold goods of all kinds is impossible. If there is an excess supply of one good, there must be a shortage of another: \"The superabundance of goods of one description arises from the deficiency of goods of another description.\"[11]\n\nTo further clarify, he wrote: \"Sales cannot be said to be dull because money is scarce, but because other products are so. ... To use a more hackneyed phrase, people have bought less, because they have made less profit.\"\n\nSay's law should therefore be formulated as: Supply of X creates demand for Y, subject to people being interested in buying X. The producer of X is able to buy Y, if his products are demanded.\n\nSay rejected the possibility that money obtained from the sale of goods could remain unspent, thereby reducing demand below supply. He viewed money only as a temporary medium of exchange.\n\nMoney performs but a momentary function in this double exchange; and when the transaction is finally closed, it will always be found, that one kind of commodity has been exchanged for another.[12]\n\nEarly writers on political economy held a variety of opinions on what we now call Say's law. James Mill and David Ricardo both supported the law in full. Thomas Malthus and John Stuart Mill questioned the doctrine that general gluts cannot occur.\n\nJames Mill and David Ricardo restated and developed Say's law. Mill wrote, \"The production of commodities creates, and is the one and universal cause which creates, a market for the commodities produced.\"[13] Ricardo wrote, \"Demand depends only on supply.\"[14]\n\nThomas Malthus, on the other hand, rejected Say's law because he saw evidence of general gluts.\n\nWe hear of glutted markets, falling prices, and cotton goods selling at Kamschatka lower than the costs of production. It may be said, perhaps, that the cotton trade happens to be glutted; and it is a tenet of the new doctrine on profits and demand, that if one trade be overstocked with capital, it is a certain sign that some other trade is understocked. But where, I would ask, is there any considerable trade that is confessedly under-stocked, and where high profits have been long pleading in vain for additional capital?[15]\n\nJohn Stuart Mill also recognized general gluts. He argued that during a general glut, there is insufficient demand for all non-monetary commodities and excess demand for money.\n\nWhen there is a general anxiety to sell, and a general disinclination to buy, commodities of all kinds remain for a long time unsold, and those which find an immediate market, do so at a very low price... At periods such as we have described... persons in general... liked better to possess money than any other commodity. Money, consequently, was in request, and all other commodities were in comparative disrepute... As there may be a temporary excess of any one article considered separately, so may there of commodities generally, not in consequence of over-production, but of a want of commercial confidence.[16]\n\nMill rescued the claim that there cannot be a simultaneous glut of all commodities by including money as one of the commodities.\n\nIn order to render the argument for the impossibility of an excess of all commodities applicable... money must itself be considered as a commodity. It must, undoubtedly, be admitted that there cannot be an excess of all other commodities, and an excess of money at the same time.[17]\n\nContemporary economist Brad DeLong believes that Mill's argument refutes the assertions that a general glut cannot occur, and that a market economy naturally tends towards an equilibrium in which general gluts do not occur.[18][19] What remains of Say's law, after Mill's modification, are a few less controversial assertions:\n\nSay himself never used many of the later, short definitions of Say's law, and thus the law actually developed through the work of many of his contemporaries and successors. The work of James Mill, David Ricardo, John Stuart Mill, and others evolved Say's law into what is sometimes called law of markets, which was a key element of the framework of macroeconomics from the mid-19th century until the 1930s.\n\nThe Great Depression posed a challenge to Say's law. In the United States, unemployment rose to 25%.[21] The quarter of the labor force that was unemployed constituted a supply of labor for which the demand predicted by Say's law did not exist.\n\nJohn Maynard Keynes argued in 1936 that Say's law is simply not true, and that demand, rather than supply, is the key variable that determines the overall level of economic activity. According to Keynes, demand depends on the propensity of individuals to consume and on the propensity of businesses to invest, both of which vary throughout the business cycle. There is no reason to expect enough aggregate demand to produce full employment.[22]\n\nSteven Kates, although a proponent of Say's law, writes:\n\nBefore the Keynesian Revolution, [the] denial of the validity of Say's Law placed an economist amongst the crackpots, people with no idea whatsoever about how an economy works. That the vast majority of the economics profession today would have been classified as crackpots in the 1930s and before is just how it is.[23]\n\nKeynesian economists, such as Paul Krugman, stress the role of money in negating Say's law: Money that is hoarded (held as cash or analogous financial instruments) is not spent on products.[24] To increase monetary holdings, someone may sell products or labor without immediately spending the proceeds. This can be a general phenomenon: from time to time, in response to changing economic circumstances, households and businesses in aggregate seek to increase net savings and thus decrease net debt. To increase net savings requires earning more than is spent—contrary to Say's law, which postulates that supply (sales, earning income) equals demand (purchases, requiring spending). Keynesian economists argue that the failure of Say's law, through an increased demand for monetary holdings, can result in a general glut due to falling demand for goods and services.\n\nMany economists today maintain that supply does not create its own demand, but instead, especially during recessions, demand creates its own supply. Krugman writes:\n\nNot only doesn't supply create its own demand; experience since 2008 suggests, if anything, that the reverse is largely true -- specifically, that inadequate demand destroys supply. Economies with persistently weak demand seem to suffer large declines in potential as well as actual output.[25]\n\nOlivier Blanchard and Larry Summers, observing persistently high and increasing unemployment rates in Europe in the 1970s and 1980s, argued that adverse demand shocks can lead to persistently high unemployment, therefore persistently reducing the supply of goods and services.[26]\nAntonio Fatás and Larry Summers argued that shortfalls in demand, resulting both from the global economic downturn of 2008 and 2009 and from subsequent attempts by governments to reduce government spending, have had large negative effects on both actual and potential world economic output.[27]\n\nA minority of economists still support Say's law. Some proponents of real business cycle theory maintain that high unemployment is due to a reduced labor supply rather than reduced demand. In other words, people choose to work less when economic conditions are poor, so that involuntary unemployment does not actually exist.[28]\n\nWhile economists have abandoned Say's law as a true law that must always hold, most still consider Say's law to be a useful rule of thumb which the economy will tend towards in the long run, so long as it is allowed to adjust to shocks such as financial crises without being exposed to any further such shocks.[29] The applicability of Say's law in theoretical long-run conditions is one motivation behind the study of general equilibrium theory in economics, which studies economies in the context where Say's law holds true.\n\nA number of laissez-faire consequences have been drawn from interpretations of Say's law. However, Say himself advocated public works to remedy unemployment and criticized Ricardo for neglecting the possibility of hoarding if there was a lack of investment opportunities.[30]\n\nSay argued against claims that businesses suffer because people do not have enough money. He argued that the power to purchase can only be increased through more production.\n\nJames Mill used Say's law against those who sought to give the economy a boost via unproductive consumption. In his view, consumption destroys wealth, in contrast to production, which is the source of economic growth. The demand for a product determines the price of the product.\n\nAccording to Keynes (see more below), if Say's law is correct, widespread involuntary unemployment (caused by inadequate demand) cannot occur. Classical economists in the context of Say's law explain unemployment as arising from insufficient demand for specialized labour—that is, the supply of viable labour exceeds demand in some segments of the economy.\n\nWhen more goods are produced by firms than are demanded in certain sectors, the suppliers in those sectors lose revenue as result. This loss of revenue, which would in turn have been used to purchase other goods from other firms, lowers demand for the products of firms in other sectors, causing an overall general reduction in output and thus lowering the demand for labour. This results in what contemporary macroeconomics call structural unemployment, the presumed mismatch between the overall demand for labour in jobs offered and the individual job skills and location of labour. This differs from the Keynesian concept of cyclical unemployment, which is presumed to arise because of inadequate aggregate demand.\n\nSuch economic losses and unemployment were seen by some economists, such as Marx and Keynes himself, as an intrinsic property of the capitalist system. The division of labor leads to a situation where one always has to anticipate what others will be willing to buy, and this leads to miscalculations.\n\nSay's law did not posit that (as per the Keynesian formulation) \"supply creates its own demand\". Nor was it based on the idea that everything that is saved will be exchanged. Rather, Say sought to refute the idea that production and employment were limited by low consumption.\n\nThus Say's law, in its original concept, was not intrinsically linked nor logically reliant on the neutrality of money (as has been alleged by those who wish to disagree with it[31][unreliable source?]), because the key proposition of the law is that no matter how much people save, production is still a possibility, as it is the prerequisite for the attainment of any additional consumption goods. Say's law states that in a market economy, goods and services are produced for exchange with other goods and services—\"employment multipliers\" therefore arise from production and not exchange alone—and that in the process a sufficient level of real income is created to purchase the economy's entire output, due to the truism that the means of consumption are limited ex vi termini by the level of production. That is, with regard to the exchange of products within a division of labour, the total supply of goods and services in a market economy will equal the total demand derived from consumption during any given time period. In modern terms, \"general gluts cannot exist\",[32][unreliable source?] although there may be local imbalances, with gluts in some markets balanced out by shortages in others.\n\nNevertheless, for some neoclassical economists,[33][unreliable source?] Say's law implies that economy is always at its full employment level. This is not necessarily what Say proposed.\n\nIn the Keynesian interpretation,[33][unreliable source?] the assumptions of Say's law are:\n\nUnder these assumptions, Say's law implies that there cannot be a general glut, so that a persistent state cannot exist in which demand is generally less than productive capacity and high unemployment results. Keynesians therefore argued[who?][when?] that the Great Depression demonstrated that Say's law is incorrect. Keynes, in his General Theory, argued that a country could go into a recession because of \"lack of aggregate demand\".[citation needed]\n\nBecause historically there have been many persistent economic crises, one may reject one or more of the assumptions of Say's law, its reasoning, or its conclusions. Taking the assumptions in turn:\n\nAs for the implication that dislocations cannot cause persistent unemployment, some theories of economic cycles accept Say's law and seek to explain high unemployment in other ways, considering depressed demand for labour as a form of local dislocation. For example, advocates of Real Business Cycle Theory[citation needed] argue that real shocks cause recessions and that the market responds efficiently to these real economic shocks.\n\nKrugman dismisses Say's law as, \"at best, a useless tautology when individuals have the option of accumulating money rather than purchasing real goods and services\".[34]\n\nIt is not easy to say what exactly Say's law says about the role of money apart from the claim that recession is not caused by lack of money. The phrase \"products are paid for with products\" is taken to mean that Say has a barter model of money; contrast with circuitist and post-Keynesian monetary theory.\n\nOne can read Say as stating simply that money is completely neutral, although he did not state this explicitly, and in fact did not concern himself with this subject. Say's central notion concerning money was that if one has money, it is irrational to hoard it.[citation needed]\n\nThe assumption that hoarding is irrational was attacked by underconsumptionist economists, such as John M. Robertson, in his 1892 book, The Fallacy of Saving:[35][36] where he called Say's law:\n\n[A] tenacious fallacy, consequent on the inveterate evasion of the plain fact that men want for their goods, not merely some other goods to consume, but further, some credit or abstract claim to future wealth, goods, or services. This all want as a surplus or bonus, and this surplus cannot be represented for all in present goods.\n\nHere Robertson identifies his critique as based on Say's theory of money: people wish to accumulate a \"claim to future wealth\", not simply present goods, and thus the hoarding of wealth may be rational.\n\nFor Say, as for other classical economists, it is possible for there to be a glut (excess supply, market surplus) for one product alongside a shortage (excess demand) of others. But there is no \"general glut\" in Say's view, since the gluts and shortages cancel out for the economy as a whole. But what if the excess demand is for money, because people are hoarding it? This creates an excess supply for all products, a general glut. Say's answer is simple: there is no reason to engage in hoarding money. According to Say, the only reason to have money is to buy products. It would not be a mistake, in his view, to treat the economy as if it were a barter economy. To quote Say:\n\nNor is [an individual] less anxious to dispose of the money he may get ... But the only way of getting rid of money is in the purchase of some product or other.[37]\n\nIn Keynesian terms, followers of Say's law would argue that on the aggregate level, there is only a transactions demand for money. That is, there is no precautionary, finance, or speculative demand for money. Money is held for spending, and increases in money supplies lead to increased spending.\n\nSome classical economists did see that a loss of confidence in business or a collapse of credit will increase the demand for money, which will decrease the demand for goods. This view was expressed both by Robert Torrens[citation needed] and John Stuart Mill.[citation needed] This would lead demand and supply to move out of phase and lead to an economic downturn in the same way that miscalculation in productions would, as described by William H. Beveridge in 1909.\n\nHowever, in classical economics, there was no reason for such a collapse to persist. In this view, persistent depressions, such as that of the 1930s, are impossible in a free market organized according to laissez-faire principles. The flexibility of markets under laissez faire allows prices, wages, and interest rates to adjust so as to abolish all excess supplies and demands; however, since all economies are a mixture of regulation and free-market elements, laissez-faire principles (which require a free market environment) cannot adjust effectively to excess supply and demand.\n\nThe whole of neoclassical equilibrium analysis implies that Say's law in the first place functioned to bring a market into this state: that is, Say's law is the mechanism through which markets equilibrate uniquely. Equilibrium analysis and its derivatives of optimization and efficiency in exchange live or die with Say's law. This is one of the major, fundamental points of contention between the neoclassical tradition, Keynes, and Marxians. Ultimately, from Say's law they deduced vastly different conclusions regarding the functioning of capitalist production.\n\nThe former, not to be confused with \"new Keynesian\" and the many offsprings and syntheses of the General Theory, take the fact that a commodity–commodity economy is substantially altered once it becomes a commodity–money–commodity economy, or once money becomes not only a facilitator of exchange (its only function in marginalist theory) but also a store of value and a means of payment. What this means is that money can be (and must be) hoarded: it may not re-enter the circulatory process for some time, and thus a general glut is not only possible but, to the extent that money is not rapidly turned over, probable.\n\nA response to this in defense of Say's law (echoing the debates between Ricardo and Malthus, in which the former denied the possibility of a general glut on its grounds) is that consumption that is abstained from through hoarding is simply transferred to a different consumer—overwhelmingly to factor (investment) markets, which, through financial institutions, function through the rate of interest.\n\nKeynes' innovation in this regard was twofold: First, he was to turn the mechanism that regulates savings and investment, the rate of interest, into a shell of its former self (relegating it to the price of money) by showing that supply and investment were not independent of one another and thus could not be related uniquely in terms of the balancing of disutility and utility. Second, after Say's law was dealt with and shown to be theoretically inconsistent, there was a gap to be filled. If Say's law was the logic by which we thought financial markets came to a unique position in the long run, and if Say's law were to be discarded, what were the real \"rules of the game\" of the financial markets? How did they function and remain stable?\n\nTo this Keynes responded with his famous notion of \"animal spirits\": markets are ruled by speculative behavior, influenced not only by one's own personal equation but also by one's perceptions of the speculative behavior of others. In turn, others' behavior is motivated by their perceptions of others' behavior, and so on. Without Say's law keeping them in balance, financial markets are thus inherently unstable. Through this identification, Keynes deduced the consequences for the macroeconomy of long-run equilibrium being attained not at only one unique position that represented a \"Pareto Optima\" (a special case), but through a possible range of many equilibria that could significantly under-employ human and natural resources (the general case).\n\nFor the Marxian critique, which is more fundamental, one must start at Marx's initial distinction between use value and exchange value—use value being the use somebody has for a commodity, and exchange value being what an item is traded for on a market. In Marx's theory, there is a gap between the creation of surplus value in production and the realization of that surplus value via a sale. To realize a sale, a commodity must have a use value for someone, so that they purchase the commodity and complete the cycle M–C–M'. Capitalism, which is interested in value (money as wealth), must create use value. The capitalist has no control over whether or not the value contained in the product is realized through the market mechanism. This gap between production and realization creates the possibility for capitalist crisis, but only if the value of any item is realised through the difference between its cost and final price. As the realization of capital is only possible through a market, Marx criticized other economists, such as David Ricardo, who argued that capital is realized via production. Thus, in Marx's theory, there can be general overproductive crises within capitalism.[38]\n\nGiven these concepts and their implications, Say's law does not hold in the Marxian framework. Moreover, the theoretical core of the Marxian framework contrasts with that of the neoclassical and Austrian traditions.\n\nConceptually, the distinction between Keynes and Marx is that for Keynes the theory is but a special case of his general theory, whereas for Marx it never existed at all.\n\nA modern way of expressing Say's law is that there can never be a general glut. Instead of there being an excess supply (glut or surplus) of goods in general, there may be an excess supply of one or more goods, but only when balanced by an excess demand (shortage) of yet other goods. Thus, there may be a glut of labor (\"cyclical\" unemployment), but this is balanced by an excess demand for produced goods. Modern advocates of Say's law see market forces as working quickly, via price adjustments, to abolish both gluts and shortages. The exception is when governments or other non-market forces prevent price adjustments.\n\nAccording to Keynes, the implication of Say's law is that a free-market economy is always at what Keynesian economists call full employment (see also Walras' law). Thus, Say's law is part of the general world view of laissez-faire economics—that is, that free markets can solve the economy's problems automatically. (These problems are recessions, stagnation, depression, and involuntary unemployment[broken anchor].)\n\nSome proponents of Say's law argue that such intervention is always counterproductive. Consider Keynesian-type policies aimed at stimulating the economy. Increased government purchases of goods (or lowered taxes) merely \"crowd out\" the production and purchase of goods by the private sector. Contradicting this view, Arthur Cecil Pigou, a self-proclaimed follower of Say's law, wrote a letter in 1932 signed by five other economists (among them Keynes) calling for more public spending to alleviate high levels of unemployment.\n\nKeynes summarized Say's law as \"supply creates its own demand\", or the assumption \"that the whole of the costs of production must necessarily be spent in the aggregate, directly or indirectly, on purchasing the product\" (from chapter 2 of his General Theory). See the article on The General Theory of Employment, Interest and Money for a summary of Keynes's view.\n\nAlthough hoarding of money was not a direct cause of unemployment in Keynes's theory, his concept of saving was unclear and some readers have filled the gap by assigning to hoarding the role Keynes gave to saving. An early example was Jacob Viner, who in his 1936 review of the General Theory said of hoarding that Keynes' attaches great importance to it as a barrier to \"full\" employment' (p152) while denying (pp158f) that it was capable of having that effect.[39]\n\nThe theory that hoarding is a cause of unemployment has been the subject of discussion. Some classical economists[who?] suggested that hoarding (increases in money-equivalent holdings) would always be balanced by dis-hoarding. This requires equality of saving (abstention from purchase of goods) and investment (the purchase of capital goods). However, Keynes and others argued that hoarding decisions are made by different people and for different reasons than are decisions to dis-hoard, so that hoarding and dis-hoarding are unlikely to be equal at all times, as indeed they are not. Decreasing demand (consumption) does not necessarily stimulate capital spending (investment).\n\nSome[who?] have argued that financial markets, and especially interest rates, could adjust to keep hoarding and dis-hoarding equal, so that Say's law could be maintained, or that prices could simply fall, to prevent a decrease in production. But Keynes argued that to play this role, interest rates would have to fall rapidly, and that there are limits on how quickly and how low they can fall (as in the liquidity trap, where interest rates approach zero and cannot fall further). To Keynes, in the short run, interest rates are determined more by the supply and demand for money than by saving and investment. Before interest rates can adjust sufficiently, excessive hoarding causes the vicious circle of falling aggregate production (recession). The recession itself lowers incomes so that hoarding (and saving) and dis-hoarding (and real investment) can reach a state of balance below full employment.\n\nWorse, a recession would hurt private real investment—by hurting profitability and business confidence—through what is called the accelerator effect. This means that the balance between hoarding and dis-hoarding would be pushed even further below the full-employment level of production.\n\nKeynes treats a fall in marginal efficiency of capital and an increase in the degree of liquidity preference (demand for money) as sparks leading to an insufficiency of effective demand. A decrease in MEC causes a reduction in investment, which reduces aggregate expenditure and income. A decline in the interest rate would offset the decline in investment, and stimulate propensity to consume.[40]",
        pageTitle: "Say's law",
    },
    {
        title: "Sayre's law",
        link: "https://en.wikipedia.org/wiki/Sayre%27s_law",
        content:
            'Sayre\'s law states, in a formulation quoted by Charles Philip Issawi: "In any dispute the intensity of feeling is inversely proportional to the value of the issues at stake." By way of corollary, it adds: "That is why academic politics are so bitter." Sayre\'s law is named after Wallace Stanley Sayre (1905–1972), U.S. political scientist and professor at Columbia University.\n\nOn 20 December 1973, the Wall Street Journal quoted Sayre as: "Academic politics is the most vicious and bitter form of politics, because the stakes are so low." Political scientist Herbert Kaufman, a colleague and coauthor of Sayre, has attested to Fred R. Shapiro, editor of The Yale Book of Quotations, that Sayre usually stated his claim as "The politics of the university are so intense because the stakes are so low", and that Sayre originated the quip by the early 1950s.\n\nMany other claimants attach to the thought behind Sayre\'s law. According to Arthur S. Link, Woodrow Wilson frequently complained about the personalized nature of academic politics, asserting that the "intensity" of academic squabbles was a function of the "triviality" of the issue at hand. Harvard political scientist Richard Neustadt (Sayre\'s former colleague  at Columbia University) was quoted to a similar effect: "Academic politics is much more vicious than real politics. We think it\'s because the stakes are so small."[1]  In his 1979 book Peter\'s People and Their Marvelous Ideas, Laurence J. Peter stated "Peter\'s Theory of Entrepreneurial Aggressiveness in Higher Education" as: "Competition in academia is so vicious because the stakes are so small." Another proverbial form is: "Academic politics are so vicious precisely because the stakes are so small." This observation is routinely attributed to Henry Kissinger who in a 1997 speech at the Ashbrook Center for Public Affairs at Ashland University, said: "I formulated the rule that the intensity of academic politics and the bitterness of it is in inverse proportion to the importance of the subject they\'re discussing. And I promise you at Harvard, they are passionately intense and the subjects are extremely unimportant."[2]\n\nVariations on the same thought have also been attributed to scientist-author C. P. Snow, professor-politician Daniel Patrick Moynihan, Business Analyst Cyril Northcote Parkinson, and politician Jesse Unruh,[3] among others.[1]',
        pageTitle: "Sayre's law",
    },
    {
        title: "Schneier's law",
        link: "https://en.wikipedia.org/wiki/Schneier%27s_law",
        content:
            'Bruce Schneier (/ˈʃnaɪ.ər/; born January 15, 1963) is an American cryptographer, computer security professional, privacy specialist, and writer. Schneier is an Adjunct Lecturer in Public Policy at the Harvard Kennedy School[2] and a Fellow at the Berkman Klein Center for Internet & Society as of November, 2013.[3] He is a board member of the Electronic Frontier Foundation, Access Now, and The Tor Project; and an advisory board member of Electronic Privacy Information Center and VerifiedVoting.org. He is the author of several books on general security topics, computer security and cryptography and is a squid enthusiast.[4]\n\nBruce Schneier is the son of Martin Schneier, a Brooklyn Supreme Court judge. He grew up in the Flatbush neighborhood of Brooklyn, New York, attending P.S. 139 and Hunter College High School.[5]\n\nAfter receiving a physics bachelor\'s degree from the University of Rochester in 1984,[6] he went to American University in Washington, D.C., and got his master\'s degree in computer science in 1988.[7]\n\nIn 1991, Schneier was laid off from his job and started writing for computer magazines. Later he decided to write a book on applied cryptography "since no such book existed". He took his articles, wrote a proposal to John Wiley and they bought the proposal.[8]\n\nIn 1994, Schneier published Applied Cryptography, which details the design, use, and implementation of cryptographic algorithms.\n\nThis book allowed me to write more, to start consulting, to start my companies, and really launched me as an expert in this field, and it really was because no one else has written this book. I wanted to read it so I had to write it. And it happened in a really lucky time when everything started to explode on the Internet.[8]\n\nIn 1999, Schneier was a founder and Chief technology officer of Counterpane Internet Security (now BT Managed Security Solutions).\n\nIn 2000, Schneier published Secrets and Lies: Digital Security in a Networked World; in 2003, Beyond Fear: Thinking Sensibly About Security in an Uncertain World and in 2012, Liars and Outliers: Enabling the Trust that Society Needs to Thrive\'.\n\nAs a Fellow of Berkman Center for Internet & Society at Harvard University since 2013,[3] Schneier has  been exploring the intersection of security, technology, and people, with an emphasis on power.[9]\n\nHe worked for IBM when they acquired Resilient Systems in 2016, where he was CTO.[10][11][12] until he left at the end of June 2019.[13]\n\nSchneier has been an Adjunct Lecturer in Public Policy at the Harvard Kennedy School.[2]\n\nSchneier was married to Karen Cooper in 1997 and lived in Minneapolis;[14] they published restaurant reviews in the Pulse of the Twin Cities. The couple divorced in 2022.\n\nSchneier has warned about misplaced trust in blockchain[15] and the lack of use cases, calling blockchain a solution in search of a problem.[16]\n\nWhat blockchain does is shift some of the trust in people and institutions to trust in technology. You need to trust the cryptography, the protocols, the software, the computers and the network. And you need to trust them absolutely, because they’re often single points of failure.\n\nI’ve never seen a legitimate use case for blockchain. I’ve never seen any system where blockchain provides security in a way that is impossible to provide in any other way.[17]\n\nHe goes on to say that cryptocurrencies are useless and are only used by speculators looking for quick riches.\n\nTo Schneier, peer review and expert analysis are important for the security of cryptographic systems.[18] Mathematical cryptography is usually not the weakest link in a security chain; effective security requires that cryptography be combined with other things.[19]\n\nThe term Schneier\'s law was coined by Cory Doctorow in a 2004 speech.[20] The law is phrased as:\n\nAny person can invent a security system so clever that she or he can\'t think of how to break it.\n\nHe attributes this to Bruce Schneier, who wrote in 1998: "Anyone, from the most clueless amateur to the best cryptographer, can create an algorithm that he himself can\'t break. It\'s not even hard. What is hard is creating an algorithm that no one else can break, even after years of analysis."[21]\n\nSimilar sentiments had been expressed by others before. In The Codebreakers, David Kahn states: "Few false ideas have more firmly gripped the minds of so many intelligent men than the one that, if they just tried, they could invent a cipher that no one could break", and in "A Few Words On Secret Writing", in July 1841, Edgar Allan Poe had stated: "Few persons can be made to believe that it is not quite an easy thing to invent a method of secret writing which shall baffle investigation. Yet it may be roundly asserted that human ingenuity cannot concoct a cipher which human ingenuity cannot resolve."[22]\n\nSchneier also coined the term "kid sister cryptography", writing in the Preface to Applied Cryptography[23] that:\n\nThere are two kinds of cryptography in this world: cryptography that will stop your kid sister from reading your files, and cryptography that will stop major governments from reading your files. This book is about the latter.\n\nSchneier is critical of digital rights management (DRM) and has said that it allows a vendor to increase lock-in.[24] Proper implementation of control-based security for the user via trusted computing is very difficult, and security is not the same thing as control.[24]\n\nSchneier insists that "owning your data is a different way of thinking about data."[25]\n\nSchneier is a proponent of full disclosure, i.e. making security issues public.\n\nIf researchers don\'t go public, things don’t get fixed. Companies don\'t see it as a security problem; they see it as a PR problem.[26]\n\nSchneier has said that homeland security money should be spent on intelligence, investigation, and emergency response.[27] Defending against the broad threat of terrorism is generally better than focusing on specific potential terrorist plots.[27] According to Schneier, analysis of intelligence data is difficult but is one of the better ways to deal with global terrorism.[28] Human intelligence has advantages over automated and computerized analysis, and increasing the amount of intelligence data that is gathered does not help to improve the analysis process.[28] Agencies that were designed around fighting the Cold War may have a culture that inhibits the sharing of information; the practice of sharing information is more important and less of a security threat in itself when dealing with more decentralized and poorly funded adversaries such as al Qaeda.[29]\n\nRegarding PETN—the explosive that has become terrorists\' weapon of choice—Schneier has written that only swabs and dogs can detect it. He also believes that changes to airport security since 11 September 2001 have done more harm than good and he defeated Kip Hawley, former head of the Transportation Security Administration, in an Economist online debate by 87% to 13% regarding the issue.[30] He is widely credited with coining the term "security theater" to describe some such changes.\n\n"Movie-plot threat" is a term Schneier coined that refers to very specific and dramatic terrorist attack scenarios, reminiscent of the behavior of terrorists in movies, rather than what terrorists actually do in the real world.[31] Security measures created to protect against movie plot threats do not provide a higher level of real security, because such preparation only pays off if terrorists choose that one particular avenue of attack, which may not even be feasible. Real-world terrorists would also be likely to notice the highly specific security measures, and simply attack in some other way. The specificity of movie plot threats gives them power in the public imagination, however, so even extremely unrealistic security theater countermeasures may receive strong support from the public and legislators. Among many other examples of movie plot threats, Schneier described banning baby carriers from subways, for fear that they may contain explosives.[32] Starting in April 2006, Schneier has had an annual contest to create the most fantastic movie-plot threat.[33] In 2015, during the 8th and as of 17 February 2022[update] the last one, he mentioned that the contest may have run its course.[34]\n\nSchneier has criticized security approaches that try to prevent any malicious incursion, instead arguing that designing systems to fail well is more important.[35] The designer of a system should not underestimate the capabilities of an attacker, as technology may make it possible in the future to do things that are not possible at the present.[18] Under Kerckhoffs\'s Principle, the need for one or more parts of a cryptographic system to remain secret increases the fragility of the system; whether details about a system should be obscured depends upon the availability of persons who can make use of the information for beneficial uses versus the potential for attackers to misuse the information.[36]\n\nSecrecy and security aren\'t the same, even though it may seem that way. Only bad security relies on secrecy; good security works even if all the details of it are public.[37]\n\nSchneier is a board member of the Electronic Frontier Foundation,[38] Access Now, and The Tor Project; and an advisory board member of Electronic Privacy Information Center and VerifiedVoting.org.\n\nIn 2015, Schneier received the EPIC Lifetime Achievement Award from Electronic Privacy Information Center.[39]\n\nIn 2011, he was awarded an honorary Ph.D from the University of Westminster in London, England, by the Department of Electronics and Computer Science in recognition of Schneier\'s \'hard work and contribution to industry and public life\'.[citation needed]\n\nSchneier has been involved in the creation of many cryptographic algorithms.\n\nSchneier writes a freely available monthly Internet newsletter on computer and other security issues, Crypto-Gram, as well as a security weblog, Schneier on Security.[40] The blog focuses on the latest threats, and his own thoughts. The weblog started out as a way to publish essays before they appeared in Crypto-Gram, making it possible for others to comment on them while the stories were still current, but over time the newsletter became a monthly email version of the blog, re-edited and re-organized.[41][citation needed] \nSchneier is frequently quoted in the press on computer and other security issues, pointing out flaws in security and cryptographic implementations ranging from biometrics to airline security after the September 11 attacks.[42]',
        pageTitle: "Bruce Schneier",
    },
    {
        title: "Segal's law",
        link: "https://en.wikipedia.org/wiki/Segal%27s_law",
        content:
            'Segal\'s law is an adage that states:\n.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nA man with a watch knows what time it is. A man with two watches is never sure.[1]\n\nAt surface level, the adage emphasizes the consistency that arises when information comes from a single source and points out the potential pitfalls of having too much conflicting information. However, the underlying message is to question the apparent certainty of anyone who only has one source of information. The man with one watch has no way to identify error or uncertainty.\n\nThe saying was coined by the San Diego Union on September 20, 1930: "Confusion.—Retail jewelers assert that every man should carry two watches. But a man with one watch knows what time it is, and a man with two watches could never be sure." Later this was — mistakenly — attributed to Lee Segall of KIXL, then to be misquoted again by Arthur Bloch as "Segal\'s Law".[2]',
        pageTitle: "Segal's law",
    },
    {
        title: "Shermer's last law",
        link: "https://en.wikipedia.org/wiki/Shermer%27s_last_law",
        content:
            'British science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke\'s three laws, of which the third law is the best known and most widely cited. They are part of his ideas in his extensive writings about the future.[1]\n\nOne account stated that Clarke\'s laws were developed after the editor of his works in French started numbering the author\'s assertions.[2] All three laws appear in Clarke\'s essay "Hazards of Prophecy: The Failure of Imagination", first published in Profiles of the Future (1962);[3] however, they were not all published at the same time. Clarke\'s first law was proposed in the 1962 edition of the essay, as "Clarke\'s Law" in Profiles of the Future.\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke\'s second law was conferred by others. It was initially a derivative of the first law and formally became Clarke\'s second law where the author proposed the third law in the 1973 revision of Profiles of the Future, which included an acknowledgement.[4] It was also here that Clarke wrote about the third law in these words: "As three laws were good enough for Newton, I have modestly decided to stop there".\n\nThe third law is the best known and most widely cited. It was published in a 1968 letter to Science magazine[5] and eventually added to the 1973 revision of the "Hazards of Prophecy" essay.[6]\n\nThe third law has inspired many snowclones and other variations:\n\n"When, however, the lay public rallies round an idea that is denounced by distinguished but elderly scientists and supports that idea with great fervour and emotion – the distinguished but elderly scientists are then, after all, probably right."[12]\n\nA contrapositive of the third law is "Any technology distinguishable from magic is insufficiently advanced." (Gehm\'s corollary)[13]',
        pageTitle: "Clarke's three laws",
    },
    {
        title: "Clarke's third law",
        link: "https://en.wikipedia.org/wiki/Clarke%27s_third_law",
        content:
            'British science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke\'s three laws, of which the third law is the best known and most widely cited. They are part of his ideas in his extensive writings about the future.[1]\n\nOne account stated that Clarke\'s laws were developed after the editor of his works in French started numbering the author\'s assertions.[2] All three laws appear in Clarke\'s essay "Hazards of Prophecy: The Failure of Imagination", first published in Profiles of the Future (1962);[3] however, they were not all published at the same time. Clarke\'s first law was proposed in the 1962 edition of the essay, as "Clarke\'s Law" in Profiles of the Future.\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke\'s second law was conferred by others. It was initially a derivative of the first law and formally became Clarke\'s second law where the author proposed the third law in the 1973 revision of Profiles of the Future, which included an acknowledgement.[4] It was also here that Clarke wrote about the third law in these words: "As three laws were good enough for Newton, I have modestly decided to stop there".\n\nThe third law is the best known and most widely cited. It was published in a 1968 letter to Science magazine[5] and eventually added to the 1973 revision of the "Hazards of Prophecy" essay.[6]\n\nThe third law has inspired many snowclones and other variations:\n\n"When, however, the lay public rallies round an idea that is denounced by distinguished but elderly scientists and supports that idea with great fervour and emotion – the distinguished but elderly scientists are then, after all, probably right."[12]\n\nA contrapositive of the third law is "Any technology distinguishable from magic is insufficiently advanced." (Gehm\'s corollary)[13]',
        pageTitle: "Clarke's three laws",
    },
    {
        title: "Sievers's law",
        link: "https://en.wikipedia.org/wiki/Sievers%27s_law",
        content:
            'Sievers\'s law in Indo-European linguistics accounts for the pronunciation of a consonant cluster with a glide (*w or *y) before a vowel as it was affected by the phonetics of the preceding syllable.  Specifically, it refers to the alternation between *iy and *y, and possibly *uw and *w as conditioned by the weight of the preceding syllable.  For instance, Proto-Indo-European (PIE) *kor-yo-s became Proto-Germanic *harjaz, Gothic harjis "army", but PIE *ḱerdh-yo-s became Proto-Germanic *hirdijaz,  Gothic hairdeis /hɛrdiːs/ "shepherd". It differs from ablaut in that the alternation has no morphological relevance but is phonologically context-sensitive: PIE *iy followed a heavy syllable (a syllable with a diphthong or long vowel, or ending in more than one consonant), but *y would follow a light syllable (a short vowel followed by a single consonant).\n\nThis situation was first noticed by the Germanic philologist Eduard Sievers (1859–1932), and his aim was to account for certain phenomena in the Germanic languages. He originally discussed only *y in medial position. He also noted, almost as an aside, that something similar seemed to be going on in the earliest Sanskrit texts. Thus in the Rigveda dāivya- "divine" actually had three syllables in scansion (dāiviya-) but satya- "true" was scanned as written.\n\nAfter Sievers, scholars would find similar alternations in Greek and Latin, and alternation between *uw and *u, though the evidence is poor for all of these. Through time, evidence was announced regarding similar alternations of syllabicity in the nasal and liquid consonants, though the evidence is extremely poor for these, despite the fact that such alternations would have left permanent, indeed irreversible, traces.[citation needed] For example, the Sanskrit "tool-suffix" -tra- (e.g. pā-tra- "drinking cup, vessel") almost always follows a consonant or long vowel and should have therefore been -tira-; but no such form as **pōtira-, either written as such or scanned thus, is actually attested in the Rigveda or any other Indic text. How a nearly universal suffix **-tira- would have been, or even could have been, uniformly replaced by -tra- is unobvious.\n\nThe most ambitious extension of Sievers\'s law was proposed by Franklin Edgerton (1885–1963) in a pair of articles in the journal Language (Edgerton 1934 and Edgerton 1943). He argued that not only was the syllabicity of prevocalic consonants by context applicable to all six Indo-European sonorants (*l *m *n *r *w *y), it was applicable in all positions in the word. Thus a form like *dyēws "sky" would have been pronounced like this only when it happened to follow a word ending with a short vowel. Everywhere else it would have had two syllables, *diyēws. Edgerton also maintained that the phonotactic rules in question applied to sequences arising across morpheme boundaries, such as when the bahuvrīhi prefix *su- occurred before a noun beginning with *w- (e.g. *su-wiHro- "well-heroed", Vedic suvīra-). According to Edgerton, the word should have had two forms, depending on what immediately preceded it: *suwiHro- and *swiHro-. This corollary he called the "converse" to Sievers\'s law, and is usually referred to as Edgerton\'s converse for short.\n\nThe evidence for alternation presented by Edgerton was of two sorts. He cited several hundred passages from the Rigveda, which he claimed should be rescanned to reveal hitherto unnoticed expressions of the syllable structure called for by his theory. But most forms show no such direct expressions; for them, Edgerton noted sharply skewed distributions that he interpreted as evidence for a lost alternation between syllabic and nonsyllabic consonants (commonly called "semivowels" in the literature). Thus say śiras "head" (from *śr̥ros) has no monosyllabic partner **śras  (from **śros), but Edgerton noted that śiras occurred 100% of the time in the environments where his theory called for the syllabification of the *r. Appealing to the "formulaic" nature of oral poetry, especially in tricky and demanding literary forms like sacred Vedic versification, he reasoned that this was direct evidence for the previous existence of an alternant *śras, on the assumption that when (for whatever reason) this *śras and other forms like it came to be shunned, the typical collocations in which they would have (correctly) occurred inevitably became obsolete at the same time as the loss of the form itself. And he was able to present a sizeable body of evidence in the form of these skewed distributions in both the 1934 and 1943 articles.\n\nParenthetically, many of Edgerton\'s data on this point are inappropriate: current scholarship takes śiras, for example, to be the regular reflex of PIE *ḱr̥Hos, the syllabicity of the resonant resulting from the fact that it was followed by a consonant in Proto-Indo-European; there never was, nor could have been, a form **ḱros to yield Indic **śras. How it might be that a form that is irrelevant to Edgerton\'s theory might seem to "behave" in accord with it is explained below.\n\nIn 1965, Fredrik Otto Lindeman (1936–) published an article (Lindeman 1965) proposing a significant modification of Edgerton\'s theory. Disregarding Edgerton\'s evidence (on the grounds that he was not prepared to judge the niceties of Rigvedic scansion) he took instead as the data to be analyzed the scansions in Hermann Grassmann\'s Wörterbuch zum Rig-Veda (Grassmann 1873). From these he concluded that Edgerton had been right, but only up to a point: the alternations he postulated did indeed apply to all sonorants; but in word-initial position, the alternation was limited to forms like *dyēws/diyēws "sky", as cited above – that is, words where the "short" form was monosyllabic.\n\nEdgerton\'s claims, once generally hailed, have not fared well. Regarding the skewed distributions in the Rigveda, Edgerton neglected to test his observations against controls, namely forms not susceptible to his theory but sharing other properties with the "test" forms such as part of speech, metrical configuration, and so on. The first scholar to look at controls was Franklin Eugene Horowitz (Horowitz 1974, but whose work actually dates from ten years earlier). Horowitz noted that for example all 65 occurrences of Vedic suvīra- "well-heroed" do occur in line-initial position or follow a heavy syllable (as if in accord with Edgerton\'s converse), but exactly the same thing is true of e.g. supatrá- "having beautiful wings" (which can have nothing to do with Edgerton\'s law). And indeed such skewing in distribution is pervasive in Vedic vocabulary: śatam "100", and dozens of other forms with no bearing on Edgerton\'s law, have exactly the same strong preference for not following a word ending with a short vowel that e.g. śiras "head" does, presumably by reason of beginning with a single consonant followed by a light syllable.\n\nA second difficulty has emerged much more recently (Sihler 2006): The actual passages from the Rigveda cited in Edgerton\'s two large articles in 1934 and 1943 as examples of the effects of his theory in action seriously misrepresent the facts in all but a handful of cases. No more than three Rigvedic passages cited in the 1934 article, and none at all in 1943, actually support the claims of Edgerton\'s law regarding word-initial sequences. This lies well within the operation of pure chance. And it has been shown also that the apparent success of Lindeman\'s more modest claims are not without problems, too, such as the limitation of the reliable examples to semivowels (the glides *y and *w) even though such alternations in the other four consonants should have left robust outcomes (for example, a disyllabic form of prá "forth, away" should have been much more frequent than the monosyllable, which would have occurred only after a word ending in a short vowel; but there is no evidence for such a disyllabic form as **pirá, in Vedic or any other form of Indic); and that the syllabified alternants (e.g. *diyēws) are rarer than they should be: they account for only fifteen to twenty percent of the total: they should account for at least eighty percent, since the monosyllabic form would have originally occurred, like prá, only after a word ending in a short vowel. Further, only the *diyēws alternants have a "distribution": the *dyēws shapes show no sensitivity to phonetic environment at all. (And even that disyllabic "distribution" can be inexplicable: disyllabic dyāus in the Rigveda always and only, with one exception, occurs in line-initial position, i.e., in only one of the four environments calling for syllabification of the resonant. Nothing in Lindeman\'s theory accounts for this striking distribution.)\n\nWithin the context of Indo-European, Sievers\'s law is generally held to be one-way.[1] That is, it applied only to create syllabic resonants from nonsyllabics after heavy syllables, but not the other way around after light syllables. In Proto-Germanic, however, the law came to be applied in both directions, with PIE syllabic *-iy- becoming nonsyllabic *-y- after light syllables. As a consequence, suffixal -j- and -ij- came to be in complementary distribution in Proto-Germanic, and were perceived as allophonic variants of the same suffix with the former following light syllables and the latter, heavy. Following the loss of j intervocalically, -ī- (from earlier -iji-) was also complementary to -i- in inflected forms.\n\nThe alternation is preserved in a number of the older languages. In addition to the Gothic nouns cited above, Gothic strong adjectives show a light suffix -ji- following a light stem, yielding the nominative singular masculine midjis "middle", while a heavy suffix -ī- (from -iji-/-ija-) follows a long stem: wilþeis /wilþīs/ "wild".\n\nIn Old Norse, nonsyllabic -j- is preserved word-medially, but syllabic -ij- is lost like all other medial-syllable vowels. This is seen in class 1 weak verbs, which end in -ja (from Germanic *-janą) following a short stem, but in -a (from Germanic *-ijaną) following a long stem. Word-finally, the distribution is reversed. For example, following the loss of final -ą, this left neuter ja-stem nouns with syllabic -i (from *-iją) after long stems but no ending (from *-ją) after short stems.\n\nThe West Germanic languages such as English largely lost the alternation because of the effects of the West Germanic gemination, but the gemination itself was conditioned only by -j- and not by -ij-, so that the alternation is indirectly preserved. There is also some evidence that the alternation was preserved and adapted to the new syllable structure that resulted from the gemination. In the oldest attested languages, medial syllabic -ij- tends to be lost in the same way as in Old Norse, while nonsyllabic -j- (occurring only after -r-, which was not geminated) is preserved. Compare for example:\n\nIt has been argued[by whom?] that Sievers\'s law is actually an innovation of Germanic. The reasons for this are two distinct innovations pertaining to Sievers\'s law outcomes.  The first is that the law works in both directions, not only yielding *-iya- following long stems, but instigating the reverse, decrementing etymological *-iya- to *-ya- following short stems. The second is an enlarged environment for the transformation. In Germanic, the syllabic shape *-iy- is found not only after heavy syllables, as in Vedic, but also after some polysyllabic stems. This is quite unlike anything in Indic.\n\nThe imposed conditions for the Sievers\'s law reversal are specifically Germanic, not Proto-Indo-European. Thus the following two verb forms show normal Germanic distributions in good order: Proto-Germanic *wurkīþi "(s)he works", *wurkijanþi "they work" become Gothic waurkeiþ /workīþ/, waurkjand (Gothic makes no distinction between -ij- and -j- in writing); and Proto-Germanic *satiþi "(s)he sets", *satjanþi "they set" become Gothic satjiþ, satjand. But the forms in their Proto-Indo-European shape were *wr̥g-yé-ti, *wr̥g-yó-nti and *sod-éye-ti, *sod-éyo-nti respectively.  Without Sievers\'s influence these would pass etymologically into Germanic as **wurkiþi, **wurkjanþi and **satīþi, **satijanþi. The regular Germanic evolution of *ur from *r̥ made a light root syllable heavy, and thus *wr̥g- > *wurk- created a triggering environment for a heavy suffix, *-iji-/*-ī-, yielding Gothic waurkeiþ. The opposite occurred regarding satjiþ, where the etymological *-iji-/*-ī- (PIE *-eye-) was decremented to *-i- because the light syllable created the environment for a light suffix.  So, a Proto-Germanic *satijiþi was turned to *satjiþi by Sievers\'s reversal, which in turn was simplified prehistorically to *satiþi.  Gothic re-inserts the -j- via analogy, yielding satjiþ (contrast Old English bideð, which does not re-insert the -j- therefore not yielding **biddeð). Hence, not only are Proto-Indo-European structures not needed to account for the facts of Germanic, they actually get in the way.\n\nDonald Ringe, in his book "From Proto-Indo-European to Proto-Germanic", characterizes the origins of the different features as follows:\n\nSievers\'s law in Germanic was clearly conditioned on morphological grounds as well as phonological, since suffixes were treated as separate words if they were recognised as separate morphological segments. For example, the suffix *-atjaną had a nonsyllabic -j- because the preceding -at- was light, as in Old English -ettan, where the gemination is evidence for -j-.[4] On the other hand, *-ārijaz had -ij- because the syllable -ār- was heavy, as in Gothic -areis, which would have been *-arjis if the suffix had contained -j- instead. This happened even though in fully formed words these -j- and -ij- would have been preceded by two syllables. Examples of the opposite - that is, multiple-syllable stems that were not segmentable - can also be found. *hamiþiją ("shirt") clearly contained -ij-, showing that *hamiþ- in its entirety was analysed as the stem, rather than just *-iþ- since there was no such suffix in Proto-Germanic. This is evidenced by the Old High German hemidi, where *hemiddi would be expected if the original form had -j-.',
        pageTitle: "Sievers's law",
    },
    {
        title: "Sieverts's law",
        link: "https://en.wikipedia.org/wiki/Sieverts%27s_law",
        content:
            "Sieverts's law, in physical metallurgy and in chemistry, is a rule to predict the solubility of gases in metals. It is named after German chemist Adolf Sieverts (1874–1947).[1]  The law states that the solubility of a diatomic gas in metal is proportional to the square root of the partial pressure of the gas in thermodynamic equilibrium.[2]  Hydrogen, oxygen and nitrogen are examples of dissolved diatomic gases of frequent interest in metallurgy.\n\nSieverts's law can be readily rationalized by considering the reaction of dissolution of the gas in the metal, which involves dissociation of the molecule of the gas.  For example, for nitrogen:\n\nFor the above reaction, the equilibrium constant is",
        pageTitle: "Sieverts's law",
    },
    {
        title: "Smeed's law",
        link: "https://en.wikipedia.org/wiki/Smeed%27s_law",
        content:
            "Smeed's law is an empirical rule suggested to relate traffic fatalities to traffic congestion as measured by the proxy of motor vehicle registrations and country population. The law proposes that increasing traffic volume (an increase in motor vehicle registrations) leads to an increase in fatalities per capita, but a decrease in fatalities per vehicle.\n\nThe relationship is named after statistician Reuben Smeed, who first proposed it in 1949. Smeed also predicted that the average speed of traffic in central London would always be nine miles per hour, because that is the minimum speed that people tolerate. He predicted that any intervention intended to speed traffic would only lead to more people driving at this \"tolerable\" speed unless there were any other disincentives against doing so.\n\nHis hypothesis in relation to road traffic safety has been refuted by several authors, who point out that fatalities per person have decreased in many countries, when according to Smeed's law requires they should increase as long as the number of vehicles per person continues to rise.\n\nwhere \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is annual road deaths, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is number of registered vehicles, and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is population.\n\nSmeed published his research for twenty different countries,[1] and, by his death in 1976, he had expanded this to 46 countries, all showing this result. Smeed became deputy director of the Road Research Laboratory and, later, Professor at University College London.\n\nSmeed claimed his law expresses a hypothesis of group psychology: people take advantage of improvements in automobiles or infrastructure to drive ever more recklessly in the interests of speed until deaths rise to a socially unacceptable level, at which point, safety becomes more important, and recklessness less tolerated.\n\nSmeed had a fatalistic view of traffic flow. He said that the average speed of traffic in central London would always be nine miles per hour, because that is the minimum speed that people will tolerate. Intelligent use of traffic lights might increase the number of cars on the roads but would not increase their speed. As soon as the traffic flowed faster, more drivers would come to slow it down.....Smeed interpreted his law as a law of human nature. The number of deaths is determined mainly by psychological factors that are independent of material circumstances. People will drive recklessly until the number of deaths reaches the maximum they can tolerate. When the number exceeds that limit, they drive more carefully. Smeed's Law merely defines the number of deaths that we find psychologically tolerable.[2]\n\nWhilst in charge of the RRL's traffic and safety division, Smeed's views on speeds and accidents were well reported at the time of the introduction of a mandatory speed limit on UK roads: \"If I wanted to stop all road accidents I would ban the car and introduce an overall speed limit, for there is no doubt that speed limits reduce accidents. Of course, roads with higher speeds often have lower accident rates. It is only on the safer, clear roads that you can drive fast - but that does not prove that you are driving more safely.\"\n\nHe recognised that few methods of reducing accidents were painless and thus preferred to report facts and not to make direct recommendations as: \"political, social and economic factors come in - but the people who make the decisions must know what the facts are on a subject.\".[3]\n\nAt the opposite end of this theory was Smeed's observations of heavily congested networks. He noted that at some minimum speed, motorists would simply choose not to drive. If speeds fell below 9 mph (14.5 km/h), then drivers would keep away; as speeds rose above this limit, it would draw more drivers out until the roads became congested again.\n\nThe Australian state of Victoria experienced deaths in excess of the Smeed formula until about 1970, but subsequently adopted a range of interventions which took it from being a poor performer in terms of road safety to one of the best. Deaths fell in absolute terms from a peak of 1000 in 1970 to below 300 in 2009, despite strong growth in population and the number of vehicles.[citation needed]\n\nCritics observe that fatality rates per vehicle are now decreasing faster than the formula would suggest, and that, in many cases, fatality rates per person are also falling, contrary to Smeed's prediction. They attribute this improvement to effective safety interventions. (see Andreassen,[4] Broughton,[5] Oppe,[6] and Ameen & Naji[7])\n\nHowever, John Adams of University College London argued in 1995 that Smeed's law linking deaths, vehicle-miles and population was still valid for a variety of countries over time, claiming that the relationship held for 62 countries.[8] He noted an enormous difference in fatality rates across different parts of the world in spite of safety interventions, and suggested that Smeed's Law was still useful in establishing general trends, especially when using a very long time period. Variations from the trend were normally better explained through economics, rather than claimed safety interventions. However, Adams found that Smeed's calculation of estimated deaths from vehicles per population was less successful than the calculation for vehicle-miles.[9]\n\nA 2007 study used a population of 139 different countries, where Smeed originally sampled 20 of the most developed countries. This study concluded that Smeed’s Law is still applicable in fatalities in countries with a ratio of 0.2 - 0.3 vehicle per person, but in countries with a higher ratio, the number of fatalities decreases greatly. This may mean that Smeed’s Law does not take into account any cultural differences nor laws that may decrease the number of fatalities.[citation needed]\n\nOther researchers have tried to find a way to describe the trends in countries with a higher vehicle per person ratio.[10] One study suggested that at higher ratios, a collective psychology is formed where people are more aware of the risk of motor vehicle related deaths and are inherently more careful in their driving. Since the 1940s, when Smeed's Law was first created, the number of motor vehicles has greatly increased, but so have the number of laws, stricter speed limits, and more car safety factors and features. Education on motor vehicles and safe driving habits are more common, but are not considered in Smeed's Law. High-income countries are typically able to invest in their road infrastructure and its maintenance, traffic safety research and maintenance, vehicle development, and driver education.[11]",
        pageTitle: "Smeed's law",
    },
    {
        title: "Snell's law",
        link: "https://en.wikipedia.org/wiki/Snell%27s_law",
        content:
            "Snell's law (also known as the Snell–Descartes law,  the ibn-Sahl law,[1] and the law of refraction) is a formula used to describe the relationship between the angles of incidence and refraction, when referring to light or other waves passing through a boundary between two different isotropic media, such as water, glass, or air.\nIn optics, the law is used in ray tracing to compute the angles of incidence or refraction, and in experimental optics to find the refractive index of a material. The law is also satisfied in meta-materials, which allow light to be bent \"backward\" at a negative angle of refraction with a negative refractive index.\n\nThe law states that, for a given pair of media, the ratio of the sines of angle of incidence (\n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \n) and angle of refraction (\n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n) is equal to the refractive index of the second medium with regard to the first (\n  \n    \n      \n        \n          n\n          \n            21\n          \n        \n      \n    \n    {\\displaystyle n_{21}}\n  \n) which is equal to the ratio of the refractive indices (\n  \n    \n      \n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                n\n                \n                  1\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {n_{2}}{n_{1}}}}\n  \n) of the two media, or equivalently, to the ratio of the phase velocities (\n  \n    \n      \n        \n          \n            \n              \n                v\n                \n                  1\n                \n              \n              \n                v\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {v_{1}}{v_{2}}}}\n  \n) in the two media.[2]\n\nThe law follows from Fermat's principle of least time, which in turn follows from the propagation of light as waves.\n\nPtolemy, in Alexandria, Egypt,[3] had found a relationship regarding refraction angles, but it was inaccurate for angles that were not small. Ptolemy was confident he had found an accurate empirical law, partially as a result of slightly altering his data to fit theory (see: confirmation bias).[4]\n\nThe law was eventually named after Snell, although it was first discovered by the Persian scientist Ibn Sahl, at Baghdad court in 984.[6][7][8] In the manuscript On Burning Mirrors and Lenses, Sahl used the law to derive lens shapes that focus light with no geometric aberration.[9]\n\nAlhazen, in his Book of Optics (1021), came close to rediscovering the law of refraction, but he did not take this step.[10]\n\nThe law was rediscovered by Thomas Harriot in 1602,[11] who however did not publish his results although he had corresponded with Kepler on this very subject. In 1621, the Dutch astronomer Willebrord Snellius (1580–1626)—Snell—derived a mathematically equivalent form, that remained unpublished during his lifetime. René Descartes independently derived the law using heuristic momentum conservation arguments in terms of sines in his 1637 essay Dioptrique, and used it to solve a range of optical problems. Rejecting Descartes' solution, Pierre de Fermat arrived at the same solution based solely on his principle of least time. Descartes assumed the speed of light was infinite, yet in his derivation of Snell's law he also assumed the denser the medium, the greater the speed of light. Fermat supported the opposing assumptions, i.e., the speed of light is finite, and his derivation depended upon the speed of light being slower in a denser medium.[12][13] Fermat's derivation also utilized his invention of adequality, a mathematical procedure equivalent to differential calculus, for finding maxima, minima, and tangents.[14][15]\n\nIn his influential mathematics book Geometry, Descartes solves a problem that was worked on by Apollonius of Perga and Pappus of Alexandria. Given n lines L and a point P(L) on each line, find the locus of points Q such that the lengths of the line segments QP(L) satisfy certain conditions.  For example, when n = 4, given the lines a, b, c, and d and a point A on a, B on b, and so on, find the locus of points Q such that the product QA*QB equals the product QC*QD. When the lines are not all parallel, Pappus showed that the loci are conics, but when Descartes considered larger n, he obtained cubic and higher degree curves. To show that the cubic curves were interesting, he showed that they arose naturally in optics from Snell's law.[16]\n\nAccording to Dijksterhuis,[17] \"In De natura lucis et proprietate (1662) Isaac Vossius said that Descartes had seen Snell's paper and concocted his own proof.  We now know this charge to be undeserved but it has been adopted many times since.\"  Both Fermat and Huygens repeated this accusation that Descartes had copied Snell. In French, Snell's Law is sometimes called  \"la loi de Descartes\" or more frequently \"loi de Snell-Descartes\".\n\nIn his 1678 Traité de la Lumière, Christiaan Huygens showed how Snell's law of sines could be explained by, or derived from, the wave nature of light, using what we have come to call the Huygens–Fresnel principle.\n\nWith the development of modern optical and electromagnetic theory, the ancient Snell's law was brought into a new stage.  In 1962, Nicolaas Bloembergen showed that at the boundary of nonlinear medium, the Snell's law should be written in a general form.[18] In 2008 and 2011, plasmonic metasurfaces were also demonstrated to change the reflection and refraction directions of light beam.[19][20]\n\nSnell's law is used to determine the direction of light rays through refractive media with varying indices of refraction. The indices of refraction of the media, labeled \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n  \n, \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{2}}\n  \n and so on, are used to represent the factor by which a light ray's speed decreases when traveling through a refractive medium, such as glass or water, as opposed to its velocity in a vacuum.\n\nAs light passes the border between media, depending upon the relative refractive indices of the two media, the light will either be refracted to a lesser angle, or a greater one. These angles are measured with respect to the normal line, represented perpendicular to the boundary. In the case of light traveling from air into water, light would be refracted towards the normal line, because the light is slowed down in water; light traveling from water to air would refract away from the normal line.\n\nRefraction between two surfaces is also referred to as reversible because if all conditions were identical, the angles would be the same for light propagating in the opposite direction.\n\nSnell's law is generally true only for isotropic or specular media (such as glass). In anisotropic media such as some crystals, birefringence may split the refracted ray into two rays, the ordinary or o-ray which follows Snell's law, and the other extraordinary or e-ray which may not be co-planar with the incident ray.\n\nWhen the light or other wave involved is monochromatic, that is, of a single frequency, Snell's law can also be expressed in terms of a ratio of wavelengths in the two media, \n  \n    \n      \n        \n          λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{1}}\n  \n and \n  \n    \n      \n        \n          λ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{2}}\n  \n:\n\nSnell's law can be derived from Fermat's principle, which states that the light travels the path which takes the least time. By taking the derivative of the optical path length, the stationary point is found giving the path taken by the light. (There are situations of light violating Fermat's principle by not taking the least time path, as in reflection in a (spherical) mirror.) In a classic analogy, the area of lower refractive index is replaced by a beach, the area of higher refractive index by the sea, and the fastest way for a rescuer on the beach to get to a drowning person in the sea is to run along a path that follows Snell's law.\n\nAs shown in the figure to the right, assume the refractive index of medium 1 and medium 2 are \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n  \n and \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{2}}\n  \n respectively. Light  enters medium 2 from medium 1 via point O.\n\nθ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \n is the angle of incidence, \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n is the angle of refraction with respect to the normal.\n\nThe phase velocities of light in medium 1 and medium 2 are\n\nc\n      \n    \n    {\\displaystyle c}\n  \n  is the speed of light in vacuum.\n\nLet T be the time required for the light to travel from point Q through point O to point P.\n\nwhere a, b, l and x are as denoted in the right-hand figure, x being the varying parameter.\n\nNote that\n\n  \n    \n      \n        \n          \n            x\n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              \n                a\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        sin\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\frac {x}{\\sqrt {x^{2}+a^{2}}}}=\\sin \\theta _{1}}\n\nand \n  \n    \n      \n        \n          \n            \n              l\n              −\n              x\n            \n            \n              (\n              l\n              −\n              x\n              \n                )\n                \n                  2\n                \n              \n              +\n              \n                b\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        sin\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {l-x}{\\sqrt {(l-x)^{2}+b^{2}}}}=\\sin \\theta _{2}}\n\nAlternatively, Snell's law can be derived using interference of all possible paths of light wave from source to observer—it results in destructive interference.\n\nAnother way to derive Snell's Law involves an application of the general boundary conditions of Maxwell equations for electromagnetic radiation and induction.\n\nYet another way to derive Snell's law is based on translation symmetry considerations.[21] For example, a homogeneous surface perpendicular to the z direction cannot change the transverse momentum. Since the propagation vector \n  \n    \n      \n        \n          \n            \n              k\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {k}}}\n  \n is proportional to the photon's momentum, the transverse propagation direction \n  \n    \n      \n        (\n        \n          k\n          \n            x\n          \n        \n        ,\n        \n          k\n          \n            y\n          \n        \n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (k_{x},k_{y},0)}\n  \n must remain the same in both regions. Assume without loss of generality a plane of incidence in the \n  \n    \n      \n        z\n        ,\n        x\n      \n    \n    {\\displaystyle z,x}\n  \n plane \n  \n    \n      \n        \n          k\n          \n            x\n            \n              \n                Region\n              \n              \n                1\n              \n            \n          \n        \n        =\n        \n          k\n          \n            x\n            \n              \n                Region\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle k_{x{\\text{Region}}_{1}}=k_{x{\\text{Region}}_{2}}}\n  \n. Using the well known dependence of the wavenumber on the refractive index of the medium, we derive Snell's law immediately.\n\nwhere \n  \n    \n      \n        \n          k\n          \n            0\n          \n        \n        =\n        \n          \n            \n              2\n              π\n            \n            \n              λ\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            ω\n            c\n          \n        \n      \n    \n    {\\displaystyle k_{0}={\\frac {2\\pi }{\\lambda _{0}}}={\\frac {\\omega }{c}}}\n  \n is the wavenumber in vacuum. Although no surface is truly homogeneous at the atomic scale, full translational symmetry is an excellent approximation whenever the region is homogeneous on the scale of the light wavelength.\n\nGiven a normalized light vector \n  \n    \n      \n        \n          \n            \n              l\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {l}}}\n  \n (pointing from the light source toward the surface) and a normalized plane normal vector \n  \n    \n      \n        \n          \n            \n              n\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {n}}}\n  \n, one can work out the normalized reflected and refracted rays, via the cosines of the angle of incidence \n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \n and angle of refraction \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n, without explicitly using the sine values or any trigonometric functions or angles:[22]\n\nNote: \n  \n    \n      \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{1}}\n  \n must be positive, which it will be if \n  \n    \n      \n        \n          \n            \n              n\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {n}}}\n  \n is the normal vector that points from the surface toward the side where the light is coming from, the region with index \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n  \n. If \n  \n    \n      \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{1}}\n  \n is negative, then \n  \n    \n      \n        \n          \n            \n              n\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {n}}}\n  \n points to the side without the light, so start over with \n  \n    \n      \n        \n          \n            \n              n\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {n}}}\n  \n replaced by its negative.\n\nThis reflected direction vector points back toward the side of the surface where the light came from.\n\nNow apply Snell's law to the ratio of sines to derive the formula for the refracted ray's direction vector:\n\nThe formula may appear simpler in terms of renamed simple values \n  \n    \n      \n        r\n        =\n        \n          n\n          \n            1\n          \n        \n        \n          /\n        \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle r=n_{1}/n_{2}}\n  \n and \n  \n    \n      \n        c\n        =\n        −\n        \n          \n            \n              n\n              →\n            \n          \n        \n        ⋅\n        \n          \n            \n              l\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle c=-{\\vec {n}}\\cdot {\\vec {l}}}\n  \n, avoiding any appearance of trig function  names or angle names:\n\nThe cosine values may be saved and used in the Fresnel equations for working out the intensity of the resulting rays.\n\nTotal internal reflection is indicated by a negative radicand in the equation for \n  \n    \n      \n        cos\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\cos \\theta _{2}}\n  \n, which can only happen for rays crossing into a less-dense medium (\n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n        <\n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{2}<n_{1}}\n  \n).\n\nWhen light travels from a medium with a higher refractive index to one with a lower refractive index, Snell's law seems to require in some cases (whenever the angle of incidence is large enough) that the sine of the angle of refraction be greater than one. This of course is impossible, and the light in such cases is completely reflected by the boundary, a phenomenon known as total internal reflection. The largest possible angle of incidence which still results in a refracted ray is called the critical angle; in this case the refracted ray travels along the boundary between the two media.\n\nFor example, consider a ray of light moving from water to air with an angle of incidence of 50°. The refractive indices of water and air are approximately 1.333 and 1, respectively, so Snell's law gives us the relation\n\nwhich is impossible to satisfy. The critical angle θcrit is the value of θ1 for which θ2 equals 90°:\n\nIn many wave-propagation media, wave velocity changes with frequency or wavelength of the waves; this is true of light propagation in most transparent substances other than a vacuum.  These media are called dispersive.  The result is that the angles determined by Snell's law also depend on frequency or wavelength, so that a ray of mixed wavelengths, such as white light, will spread or disperse.  Such dispersion of light in glass or water underlies the origin of rainbows and other optical phenomena, in which different wavelengths appear as different colors.\n\nIn optical instruments, dispersion leads to chromatic aberration; a color-dependent blurring that sometimes is the resolution-limiting effect.  This was especially true in refracting telescopes, before the invention of achromatic objective lenses.\n\nIn a conducting medium, permittivity and index of refraction are complex-valued. Consequently, so are the angle of refraction and the wave-vector. This implies that, while the surfaces of constant real phase are planes whose normals make an angle equal to the angle of refraction with the interface normal, the surfaces of constant amplitude, in contrast, are planes parallel to the interface itself. Since these two planes do not in general coincide with each other, the wave is said to be inhomogeneous.[23] The refracted wave is exponentially attenuated, with exponent proportional to the imaginary component of the index of refraction.[24][25]",
        pageTitle: "Snell's law",
    },
    {
        title: "Spearman's law of diminishing returns",
        link: "https://en.wikipedia.org/wiki/Spearman%27s_law_of_diminishing_returns",
        content:
            "The g factor[a] is a construct developed in psychometric investigations of cognitive abilities and human intelligence. It is a variable that summarizes positive correlations among different cognitive tasks, reflecting the assertion that an individual's performance on one type of cognitive task tends to be comparable to that person's performance on other kinds of cognitive tasks.[citation needed] The g factor typically accounts for 40 to 50 percent of the between-individual performance differences on a given cognitive test, and composite scores (\"IQ scores\") based on many tests are frequently regarded as estimates of individuals' standing on the g factor.[1] The terms IQ, general intelligence, general cognitive ability, general mental ability, and simply intelligence are often used interchangeably to refer to this common core shared by cognitive tests.[2] However, the g factor itself is a mathematical construct indicating the level of observed correlation between cognitive tasks.[3] The measured value of this construct depends on the cognitive tasks that are used, and little is known about the underlying causes of the observed correlations.\n\nThe existence of the g factor was originally proposed by the English psychologist Charles Spearman in the early years of the 20th century. He observed that children's performance ratings, across seemingly unrelated school subjects, were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. Spearman suggested that all mental performance could be conceptualized in terms of a single general ability factor, which he labeled g, and many narrow task-specific ability factors. Soon after Spearman proposed the existence of g, it was challenged by Godfrey Thomson, who presented evidence that such intercorrelations among test results could arise even if no g-factor existed.[4] Today's factor models of intelligence typically represent cognitive abilities as a three-level hierarchy, where there are many narrow factors at the bottom of the hierarchy, a handful of broad, more general factors at the intermediate level, and at the apex a single factor, referred to as the g factor, which represents the variance common to all cognitive tasks.\n\nTraditionally, research on g has concentrated on psychometric investigations of test data, with a special emphasis on factor analytic approaches. However, empirical research on the nature of g has also drawn upon experimental cognitive psychology and mental chronometry, brain anatomy and physiology, quantitative and molecular genetics, and primate evolution.[5] Research in the field of behavioral genetics has shown that the construct of g is highly heritable in measured populations. It has a number of other biological correlates, including brain size. It is also a significant predictor of individual differences in many social outcomes, particularly in education and employment.\n\nCritics have contended that an emphasis on g is misplaced and entails a devaluation of other important abilities. Some scientists, including Stephen J. Gould, have argued that the concept of g is a merely reified construct rather than a valid measure of human intelligence.\n\nCognitive ability tests are designed to measure different aspects of cognition. Specific domains assessed by tests include mathematical skill, verbal fluency, spatial visualization, and memory, among others. However, individuals who excel at one type of test tend to excel at other kinds of tests, too, while those who do poorly on one test tend to do so on all tests, regardless of the tests' contents.[8] The English psychologist Charles Spearman was the first to describe this phenomenon.[9] In a famous research paper published in 1904,[10] he observed that children's performance measures across seemingly unrelated school subjects were positively correlated. This finding has since been replicated numerous times. The consistent finding of universally positive correlation matrices of mental test results (or the \"positive manifold\"), despite large differences in tests' contents, has been described as \"arguably the most replicated result in all psychology\".[11] Zero or negative correlations between tests suggest the presence of sampling error or restriction of the range of ability in the sample studied.[12]\n\nUsing factor analysis or related statistical methods, it is possible to identify a single common factor that can be regarded as a summary variable characterizing the correlations between all the different tests in a test battery. Spearman referred to this common factor as the general factor, or simply g. (By convention, g is always printed as a lower case italic.) Mathematically, the g factor is a source of variance among individuals, which means that one cannot meaningfully speak of any one individual's mental abilities consisting of g or other factors to any specified degree. One can only speak of an individual's standing on g (or other factors) compared to other individuals in a relevant population.[12][13][14]\n\nDifferent tests in a test battery may correlate with (or \"load onto\") the g factor of the battery to different degrees. These correlations are known as g loadings. An individual test taker's g factor score, representing their relative standing on the g factor in the total group of individuals, can be estimated using the g loadings. Full-scale IQ scores from a test battery will usually be highly correlated with g factor scores, and they are often regarded as estimates of g. For example, the correlations between g factor scores and full-scale IQ scores from David Wechsler's tests have been found to be greater than .95.[1][12][15] The terms IQ, general intelligence, general cognitive ability, general mental ability, or simply intelligence are frequently used interchangeably to refer to the common core shared by cognitive tests.[2]\n\nThe g loadings of mental tests are always positive and usually range between .10 and .90, with a mean of about .60 and a standard deviation of about .15. Raven's Progressive Matrices is among the tests with the highest g loadings, around .80. Tests of vocabulary and general information are also typically found to have high g loadings.[16][17] However, the g loading of the same test may vary somewhat depending on the composition of the test battery.[18]\n\nThe complexity of tests and the demands they place on mental manipulation are related to the tests' g loadings. For example, in the forward digit span test the subject is asked to repeat a sequence of digits in the order of their presentation after hearing them once at a rate of one digit per second. The backward digit span test is otherwise the same except that the subject is asked to repeat the digits in the reverse order to that in which they were presented. The backward digit span test is more complex than the forward digit span test, and it has a significantly higher g loading. Similarly, the g loadings of arithmetic computation, spelling, and word reading tests are lower than those of arithmetic problem solving, text composition, and reading comprehension tests, respectively.[12][19]\n\nTest difficulty and g loadings are distinct concepts that may or may not be empirically related in any specific situation. Tests that have the same difficulty level, as indexed by the proportion of test items that are failed by test takers, may exhibit a wide range of g loadings. For example, tests of rote memory have been shown to have the same level of difficulty but considerably lower g loadings than many tests that involve reasoning.[19][20]\n\nWhile the existence of g as a statistical regularity is well-established and uncontroversial among experts, there is no consensus as to what causes the positive intercorrelations. Several explanations have been proposed.[21]\n\nCharles Spearman reasoned that correlations between tests reflected the influence of a common causal factor, a general mental ability that enters into performance on all kinds of mental tasks. However, he thought that the best indicators of g were those tests that reflected what he called the eduction of relations and correlates, which included abilities such as deduction, induction, problem solving, grasping relationships, inferring rules, and spotting differences and similarities. Spearman hypothesized that g was equivalent with \"mental energy\". However, this was more of a metaphorical explanation, and he remained agnostic about the physical basis of this energy, expecting that future research would uncover the exact physiological nature of g.[22]\n\nFollowing Spearman, Arthur Jensen maintained that all mental tasks tap into g to some degree. According to Jensen, the g factor represents a \"distillate\" of scores on different tests rather than a summation or an average of such scores, with factor analysis acting as the distillation procedure.[17] He argued that g cannot be described in terms of the item characteristics or information content of tests, pointing out that very dissimilar mental tasks may have nearly equal g loadings.  Wechsler similarly contended that g is not an ability at all but rather some general property of the brain. Jensen hypothesized that g corresponds to individual differences in the speed or efficiency of the neural processes associated with mental abilities.[23] He also suggested that given the associations between g and elementary cognitive tasks, it should be possible to construct a ratio scale test of g that uses time as the unit of measurement.[24]\n\nThe so-called sampling theory of g, originally developed by Edward Thorndike and Godfrey Thomson, proposes that the existence of the positive manifold can be explained without reference to a unitary underlying capacity. According to this theory, there are a number of uncorrelated mental processes, and all tests draw upon different samples of these processes. The inter correlations between tests are caused by an overlap between processes tapped by the tests.[25][26] Thus, the positive manifold arises due to a measurement problem, an inability to measure more fine-grained, presumably uncorrelated mental processes.[14]\n\nIt has been shown that it is not possible to distinguish statistically between Spearman's model of g and the sampling model; both are equally able to account for inter correlations among tests.[27] The sampling theory is also consistent with the observation that more complex mental tasks have higher g loading, because more complex tasks are expected to involve a larger sampling of neural elements and therefore have more of them in common with other tasks.[28]\n\nSome researchers have argued that the sampling model invalidates g as a psychological concept, because the model suggests that g factors derived from different test batteries simply reflect the shared elements of the particular tests contained in each battery rather than a g that is common to all tests. Similarly, high correlations between different batteries could be due to them measuring the same set of abilities rather than the same ability.[29]\n\nCritics have argued that the sampling theory is incongruent with certain empirical findings. Based on the sampling theory, one might expect that related cognitive tests share many elements and thus be highly correlated. However, some closely related tests, such as forward and backward digit span, are only modestly correlated, while some seemingly completely dissimilar tests, such as vocabulary tests and Raven's matrices, are consistently highly correlated. Another problematic finding is that brain damage frequently leads to specific cognitive impairments rather than a general impairment one might expect based on the sampling theory.[14][30]\n\nThe \"mutualism\" model of g proposes that cognitive processes are initially uncorrelated, but that the positive manifold arises during individual development due to mutual beneficial relations between cognitive processes. Thus there is no single process or capacity underlying the positive correlations between tests. During the course of development, the theory holds, any one particularly efficient process will benefit other processes, with the result that the processes will end up being correlated with one another. Thus similarly high IQs in different persons may stem from quite different initial advantages that they had.[14][31] Critics have argued that the observed correlations between the g loadings and the heritability coefficients of subtests are problematic for the mutualism theory.[32]\n\nFactor analysis is a family of mathematical techniques that can be used to represent correlations between intelligence tests in terms of a smaller number of variables known as factors. The purpose is to simplify the correlation matrix by using hypothetical underlying factors to explain the patterns in it. When all correlations in a matrix are positive, as they are in the case of IQ, factor analysis will yield a general factor common to all tests. The general factor of IQ tests is referred to as the g factor, and it typically accounts for 40 to 50 percent of the variance in IQ test batteries.[33] The presence of correlations between many widely varying cognitive tests has often been taken as evidence for the existence of g, but McFarland (2012) showed that such correlations do not provide any more or less support for the existence of g than for the existence of multiple factors of intelligence.[34]\n\nCharles Spearman developed factor analysis in order to study correlations between tests. Initially, he developed a model of intelligence in which variations in all intelligence test scores are explained by only two kinds of variables: first, factors that are specific to each test (denoted s); and second, a g factor that accounts for the positive correlations across tests. This is known as Spearman's two-factor theory. Later research based on more diverse test batteries than those used by Spearman demonstrated that g alone could not account for all correlations between tests. Specifically, it was found that even after controlling for g, some tests were still correlated with each other. This led to the postulation of group factors that represent variance that groups of tests with similar task demands (e.g., verbal, spatial, or numerical) have in common in addition to the shared g variance.[35]\n\nThrough factor rotation, it is, in principle, possible to produce an infinite number of different factor solutions that are mathematically equivalent in their ability to account for the intercorrelations among cognitive tests. These include solutions that do not contain a g factor. Thus factor analysis alone cannot establish what the underlying structure of intelligence is. In choosing between different factor solutions, researchers have to examine the results of factor analysis together with other information about the structure of cognitive abilities.[36]\n\nThere are many psychologically relevant reasons for preferring factor solutions that contain a g factor. These include the existence of the positive manifold, the fact that certain kinds of tests (generally the more complex ones) have consistently larger g loadings, the substantial invariance of g factors across different test batteries, the impossibility of constructing test batteries that do not yield a g factor, and the widespread practical validity of g as a predictor of individual outcomes. The g factor, together with group factors, best represents the empirically established fact that, on average, overall ability differences between individuals are greater than differences among abilities within individuals, while a factor solution with orthogonal factors without g obscures this fact. Moreover, g appears to be the most heritable component of intelligence.[37] Research utilizing the techniques of confirmatory factor analysis has also provided support for the existence of g.[36]\n\nA g factor can be computed from a correlation matrix of test results using several different methods. These include exploratory factor analysis, principal components analysis (PCA), and confirmatory factor analysis. Different factor-extraction methods produce highly consistent results, although PCA has sometimes been found to produce inflated estimates of the influence of g on test scores.[18][38]\n\nThere is a broad contemporary consensus that cognitive variance between people can be conceptualized at three hierarchical levels, distinguished by their degree of generality. At the lowest, least general level there are many narrow first-order factors; at a higher level, there are a relatively small number – somewhere between five and ten – of broad (i.e., more general) second-order factors (or group factors); and at the apex, there is a single third-order factor, g, the general factor common to all tests.[39][40][41] The g factor usually accounts for the majority of the total common factor variance of IQ test batteries.[42] Contemporary hierarchical models of intelligence include the three stratum theory and the Cattell–Horn–Carroll theory.[43]\n\nSpearman proposed the principle of the indifference of the indicator, according to which the precise content of intelligence tests is unimportant for the purposes of identifying g, because g enters into performance on all kinds of tests. Any test can therefore be used as an indicator of g.[44] Following Spearman, Arthur Jensen more recently argued that a g factor extracted from one test battery will always be the same, within the limits of measurement error, as that extracted from another battery, provided that the batteries are large and diverse.[45] According to this view, every mental test, no matter how distinctive, calls on g to some extent. Thus a composite score of a number of different tests will load onto g more strongly than any of the individual test scores, because the g components cumulate into the composite score, while the uncorrelated non-g components will cancel each other out. Theoretically, the composite score of an infinitely large, diverse test battery would, then, be a perfect measure of g.[46]\n\nIn contrast, L. L. Thurstone argued that a g factor extracted from a test battery reflects the average of all the abilities called for by the particular battery, and that g therefore varies from one battery to another and \"has no fundamental psychological significance.\"[47] Along similar lines, John Horn argued that g factors are meaningless because they are not invariant across test batteries, maintaining that correlations between different ability measures arise because it is difficult to define a human action that depends on just one ability.[48][49]\n\nTo show that different batteries reflect the same g, one must administer several test batteries to the same individuals, extract g factors from each battery, and show that the factors are highly correlated. This can be done within a confirmatory factor analysis framework.[21] Wendy Johnson and colleagues have published two such studies.[50][51] The first found that the correlations between g factors extracted from three different batteries were .99, .99, and 1.00, supporting the hypothesis that g factors from different batteries are the same and that the identification of g is not dependent on the specific abilities assessed. The second study found that g factors derived from four of five test batteries correlated at between .95–1.00, while the correlations ranged from .79 to .96 for the fifth battery, the Cattell Culture Fair Intelligence Test (the CFIT). They attributed the somewhat lower correlations with the CFIT battery to its lack of content diversity for it contains only matrix-type items, and interpreted the findings as supporting the contention that g factors derived from different test batteries are the same provided that the batteries are diverse enough. The results suggest that the same g can be consistently identified from different test batteries.[39][52] This approach has been criticized by psychologist Lazar Stankov in the Handbook of Understanding and Measuring Intelligence, who councluded \"Correlations between the g factors from different test batteries are not unity.\"[53]\n\nA study authored by Scott Barry Kaufman and colleagues showed that the general factor extracted from the Woodjock-Johnson cognitive abilities test, and the general factor extracted from the Achievement test batteries are highly correlated, but not isomorphic.[54]\n\nThe form of the population distribution of g is unknown, because g cannot be measured on a ratio scale[clarification needed]. (The distributions of scores on typical IQ tests are roughly normal, but this is achieved by construction, i.e., by normalizing the raw scores.) It has been argued[who?] that there are nevertheless good reasons for supposing that g is normally distributed in the general population, at least within a range of ±2 standard deviations from the mean. In particular, g can be thought of as a composite variable that reflects the additive effects of many independent genetic and environmental influences, and such a variable should, according to the central limit theorem, follow a normal distribution.[55]\n\nA number of researchers have suggested that the proportion of variation accounted for by g may not be uniform across all subgroups within a population. Spearman's law of diminishing returns (SLODR), also termed the cognitive ability differentiation hypothesis, predicts that the positive correlations among different cognitive abilities are weaker among more intelligent subgroups of individuals.  More specifically, SLODR predicts that the g factor will account for a smaller proportion of individual differences in cognitive tests scores at higher scores on the g factor.\n\nSLODR was originally proposed in 1927 by Charles Spearman,[56] who reported that the average correlation between 12 cognitive ability tests was .466 in 78 normal children, and .782 in 22 \"defective\" children.  Detterman and Daniel rediscovered this phenomenon in 1989.[57] They reported that for subtests of both the WAIS and the WISC, subtest intercorrelations decreased monotonically with ability group, ranging from approximately an average intercorrelation of .7 among individuals with IQs less than 78 to .4 among individuals with IQs greater than 122.[58]\n\nSLODR has been replicated in a variety of child and adult samples who have been measured using broad arrays of cognitive tests.  The most common approach has been to divide individuals into multiple ability groups using an observable proxy for their general intellectual ability, and then to either compare the average interrelation among the subtests across the different groups, or to compare the proportion of variation accounted for by a single common factor, in the different groups.[59] However, as both Deary et al. (1996).[59] and Tucker-Drob (2009)[60] have pointed out, dividing the continuous distribution of intelligence into an arbitrary number of discrete ability groups is less than ideal for examining SLODR.  Tucker-Drob (2009)[60] extensively reviewed the literature on SLODR and the various methods by which it had been previously tested, and proposed that SLODR could be most appropriately captured by fitting a common factor model that allows the relations between the factor and its indicators to be nonlinear in nature.  He applied such a factor model to a nationally representative data of children and adults in the United States and found consistent evidence for SLODR. For example, Tucker-Drob (2009) found that a general factor accounted for approximately 75% of the variation in seven different cognitive abilities among very low IQ adults, but only accounted for approximately 30% of the variation in the abilities among very high IQ adults.\n\nA recent meta-analytic study by Blum and Holling[61] also provided support for the differentiation hypothesis. As opposed to most research on the topic, this work made it possible to study ability and age variables as continuous predictors of the g saturation, and not just to compare lower- vs. higher-skilled or younger vs. older groups of testees. Results demonstrate that the mean correlation and g loadings of cognitive ability tests decrease with increasing ability, yet increase with respondent age. SLODR, as described by Charles Spearman, could be confirmed by a g-saturation decrease as a function of IQ as well as a g-saturation increase from middle age to senescence. Specifically speaking, for samples with a mean intelligence that is two standard deviations (i.e., 30 IQ-points) higher, the mean correlation to be expected is decreased by approximately .15 points. The question remains whether a difference of this magnitude could result in a greater apparent factorial complexity when cognitive data are factored for the higher-ability sample, as opposed to the lower-ability sample. It seems likely that greater factor dimensionality should tend to be observed for the case of higher ability, but the magnitude of this effect (i.e., how much more likely and how many more factors) remains uncertain.\n\nThe extent of the practical validity of g as a predictor of educational, economic, and social outcomes is the subject of ongoing debate.[62] Some researchers have argued that it is more far-ranging and universal than any other known psychological variable,[63] and that the validity of g increases as the complexity of the measured task increases.[64][65] Others have argued that tests of specific abilities outperform g factor in analyses fitted to certain real-world situations.[66][67][68]\n\nA test's practical validity is measured by its correlation with performance on some criterion external to the test, such as college grade-point average, or a rating of job performance. The correlation between test scores and a measure of some criterion is called the validity coefficient. One way to interpret a validity coefficient is to square it to obtain the variance accounted by the test. For example, a validity coefficient of .30 corresponds to 9 percent of variance explained. This approach has, however, been criticized as misleading and uninformative, and several alternatives have been proposed. One arguably more interpretable approach is to look at the percentage of test takers in each test score quintile who meet some agreed-upon standard of success. For example, if the correlation between test scores and performance is .30, the expectation is that 67 percent of those in the top quintile will be above-average performers, compared to 33 percent of those in the bottom quintile.[69][70]\n\nThe predictive validity of g is most conspicuous in the domain of scholastic performance. This is apparently because g is closely linked to the ability to learn novel material and understand concepts and meanings.[64]\n\nIn elementary school, the correlation between IQ and grades and achievement scores is between .60 and .70. At more advanced educational levels, more students from the lower end of the IQ distribution drop out, which restricts the range of IQs and results in lower validity coefficients. In high school, college, and graduate school the validity coefficients are .50–.60, .40–.50, and .30–.40, respectively. The g loadings of IQ scores are high, but it is possible that some of the validity of IQ in predicting scholastic achievement is attributable to factors measured by IQ independent of g. According to research by Robert L. Thorndike, 80 to 90 percent of the predictable variance in scholastic performance is due to g, with the rest attributed to non-g factors measured by IQ and other tests.[71]\n\nAchievement test scores are more highly correlated with IQ than school grades. This may be because grades are more influenced by the teacher's idiosyncratic perceptions of the student.[72] In a longitudinal English study, g scores measured at age 11 correlated with all the 25 subject tests of the national GCSE examination taken at age 16. The correlations ranged from .77 for the mathematics test to .42 for the art test. The correlation between g and a general educational factor computed from the GCSE tests was .81.[73]\n\nResearch suggests that the SAT, widely used in college admissions, is primarily a measure of g. A correlation of .82 has been found between g scores computed from an IQ test battery and SAT scores. In a study of 165,000 students at 41 U.S. colleges, SAT scores were found to be correlated at .47 with first-year college grade-point average after correcting for range restriction in SAT scores (the correlation rises to .55 when course difficulty is held constant, i.e., if all students attended the same set of classes).[69][74]\n\nThere is a high correlation of .90 to .95 between the prestige rankings of occupations, as rated by the general population, and the average general intelligence scores of people employed in each occupation. At the level of individual employees, the association between job prestige and g is lower – one large U.S. study reported a correlation of .65 (.72 corrected for attenuation). Mean level of g thus increases with perceived job prestige. It has also been found that the dispersion of general intelligence scores is smaller in more prestigious occupations than in lower level occupations, suggesting that higher level occupations have minimum g requirements.[75][76]\n\nResearch indicates that tests of g are the best single predictors of job performance, with an average validity coefficient of .55 across several meta-analyses of studies based on supervisor ratings and job samples. The average meta-analytic validity coefficient for performance in job training is .63.[77] The validity of g in the highest complexity jobs (professional, scientific, and upper management jobs) has been found to be greater than in the lowest complexity jobs, but g has predictive validity even for the simplest jobs. Research also shows that specific aptitude tests tailored for each job provide little or no increase in predictive validity over tests of general intelligence. It is believed that g affects job performance mainly by facilitating the acquisition of job-related knowledge. The predictive validity of g is greater than that of work experience, and increased experience on the job does not decrease the validity of g.[64][75]\n\nIn a 2011 meta-analysis, researchers found that general cognitive ability (GCA) predicted job performance better than personality (Five factor model) and three streams of emotional intelligence. They examined the relative importance of these constructs on predicting job performance and found that cognitive ability explained most of the variance in job performance.[78] Other studies suggested that GCA and emotional intelligence have a linear independent and complementary contribution to job performance. Côté and Miners (2015)[79] found that these constructs are interrelated when assessing their relationship with two aspects of job performance: organisational citizenship behaviour (OCB) and task performance. Emotional intelligence is a better predictor of task performance and OCB when GCA is low and vice versa. For instance, an employee with low GCA will compensate his/her task performance and OCB, if emotional intelligence is high.\n\nAlthough these compensatory effects favour emotional intelligence, GCA still remains as the best predictor of job performance. Several researchers have studied the correlation between GCA and job performance among different job positions. For instance, Ghiselli (1973)[80] found that salespersons had a higher correlation than sales clerk. The former obtained a correlation of 0.61 for GCA, 0.40 for perceptual ability and 0.29 for psychomotor abilities; whereas sales clerk obtained a correlation of 0.27 for GCA, 0.22 for perceptual ability and 0.17 for psychomotor abilities.[81] Other studies compared GCA – job performance correlation between jobs of different complexity. Hunter and Hunter (1984)[82] developed a meta-analysis with over 400 studies and found that this correlation was higher for jobs of high complexity (0.57). Followed by jobs of medium complexity (0.51) and low complexity (0.38).\n\nJob performance is measured by objective rating performance and subjective ratings. Although the former is better than subjective ratings, most of studies in job performance and GCA have been based on supervisor performance ratings. This rating criterion is considered problematic and unreliable, mainly because of its difficulty to define what is a good and bad performance. Rating of supervisors tends to be subjective and inconsistent among employees.[83] Additionally, supervisor rating of job performance is influenced by different factors, such as halo effect,[84] facial attractiveness,[85] racial or ethnic bias, and height of employees.[86] However, Vinchur, Schippmann, Switzer and Roth (1998)[81] found in their study with sales employees that objective sales performance had a correlation of 0.04 with GCA, while supervisor performance rating got a correlation of 0.40. These findings were surprising, considering that the main criterion for assessing these employees would be the objective sales.\n\nIn understanding how GCA is associated job performance, several researchers concluded that GCA affects acquisition of job knowledge, which in turn improves job performance. In other words, people high in GCA are capable to learn faster and acquire more job knowledge easily, which allow them to perform better. Conversely, lack of ability to acquire job knowledge will directly affect job performance. This is due to low levels of GCA. Also, GCA has a direct effect on job performance. In a daily basis, employees are exposed constantly to challenges and problem solving tasks, which success depends solely on their GCA. These findings are discouraging for governmental entities in charge of protecting rights of workers.[87] Because of the high correlation of GCA on job performance, companies are hiring employees based on GCA tests scores. Inevitably, this practice is denying the opportunity to work to many people with low GCA.[88] Previous researchers have found significant differences in GCA between race / ethnicity groups. For instance, there is a debate whether studies were biased against Afro-Americans, who scored significantly lower than white Americans in GCA tests.[89] However, findings on GCA-job performance correlation must be taken carefully. Some researchers have warned the existence of statistical artifacts related to measures of job performance and GCA test scores. For example, Viswesvaran, Ones and Schmidt (1996)[90] argued that is quite impossible to obtain perfect measures of job performance without incurring in any methodological error. Moreover, studies on GCA and job performance are always susceptible to range restriction, because data is gathered mostly from current employees, neglecting those that were not hired. Hence, sample comes from employees who successfully passed hiring process, including measures of GCA.[91]\n\nThe correlation between income and g, as measured by IQ scores, averages about .40 across studies. The correlation is higher at higher levels of education and it increases with age, stabilizing when people reach their highest career potential in middle age. Even when education, occupation and socioeconomic background are held constant, the correlation does not vanish.[92]\n\nThe g factor is reflected in many social outcomes. Many social behavior problems, such as dropping out of school, chronic welfare dependency, accident proneness, and crime, are negatively correlated with g independent of social class of origin.[93] Health and mortality outcomes are also linked to g, with higher childhood test scores predicting better health and mortality outcomes in adulthood (see Cognitive epidemiology).[94]\n\nIn 2004, psychologist Satoshi Kanazawa argued that g was a domain-specific, species-typical, information processing psychological adaptation,[95] and in 2010, Kanazawa argued that g correlated only with performance on evolutionarily unfamiliar rather than evolutionarily familiar problems, proposing what he termed the \"Savanna-IQ interaction hypothesis\".[96][97] In 2006, Psychological Review published a comment reviewing Kanazawa's 2004 article by psychologists Denny Borsboom and Conor Dolan that argued that Kanazawa's conception of g was empirically unsupported and purely hypothetical and that an evolutionary account of g must address it as a source of individual differences,[98] and in response to Kanazawa's 2010 article, psychologists Scott Barry Kaufman, Colin G. DeYoung, Deirdre Reis, and Jeremy R. Gray published a study in 2011 in Intelligence of 112 subjects taking a 70-item computer version of the Wason selection task (a logic puzzle) in a social relations context as proposed by evolutionary psychologists Leda Cosmides and John Tooby in The Adapted Mind,[99] and found instead that \"performance on non-arbitrary, evolutionarily familiar problems is more strongly related to general intelligence than performance on arbitrary, evolutionarily novel problems\".[100][101]\n\nHeritability is the proportion of phenotypic variance in a trait in a population that can be attributed to genetic factors. The heritability of g has been estimated to fall between 40 and 80 percent using twin, adoption, and other family study designs as well as molecular genetic methods. Estimates based on the totality of evidence place the heritability of g at about 50%.[102] It has been found to increase linearly with age. For example, a large study involving more than 11,000 pairs of twins from four countries reported the heritability of g to be 41 percent at age nine, 55 percent at age twelve, and 66 percent at age seventeen. Other studies have estimated that the heritability is as high as 80 percent in adulthood, although it may decline in old age. Most of the research on the heritability of g has been conducted in the United States and Western Europe, but studies in Russia (Moscow), the former East Germany, Japan, and rural India have yielded similar estimates of heritability as Western studies.[39][103][104][105]\n\nAs with heritability in general, the heritability of g can be understood in reference to a specific population at a specific place and time, and findings for one population do not apply to a different population that is exposed to different environmental factors.[106] A population that is exposed to strong environmental factors can be expected to have a lower level of heritability than a population that is exposed to only weak environmental factors. For example, one twin study found that genotype differences almost completely explain the variance in IQ scores within affluent families, but make close to zero contribution towards explaining IQ score differences in impoverished families.[107] Notably, heritability findings also only refer to total variation within a population and do not support a genetic explanation for differences between groups.[108] It is theoretically possible for the differences between the average g of two groups to be 100% due to environmental factors even if the variance within each group is 100% heritable.\n\nBehavioral genetic research has also established that the shared (or between-family) environmental effects on g are strong in childhood, but decline thereafter and are negligible in adulthood. This indicates that the environmental effects that are important to the development of g are unique and not shared between members of the same family.[104]\n\nThe genetic correlation is a statistic that indicates the extent to which the same genetic effects influence two different traits. If the genetic correlation between two traits is zero, the genetic effects on them are independent, whereas a correlation of 1.0 means that the same set of genes explains the heritability of both traits (regardless of how high or low the heritability of each is). Genetic correlations between specific mental abilities (such as verbal ability and spatial ability) have been consistently found to be very high, close to 1.0. This indicates that genetic variation in cognitive abilities is almost entirely due to genetic variation in whatever g is. It also suggests that what is common among cognitive abilities is largely caused by genes, and that independence among abilities is largely due to environmental effects. Thus it has been argued that when genes for intelligence are identified, they will be \"generalist genes\", each affecting many different cognitive abilities.[104][109][110]\n\nMuch research points to g being a highly polygenic trait influenced by many common genetic variants, each having only small effects. Another possibility is that heritable differences in g are due to individuals having different \"loads\" of rare, deleterious mutations, with genetic variation among individuals persisting due to mutation–selection balance.[110][111]\n\nA number of candidate genes have been reported to be associated with intelligence differences, but the effect sizes have been small and almost none of the findings have been replicated. No individual genetic variants have been conclusively linked to intelligence in the normal range so far. Many researchers believe that very large samples will be needed to reliably detect individual genetic polymorphisms associated with g.[39][111] However, while genes influencing variation in g in the normal range have proven difficult to find, many single-gene disorders with intellectual disability among their symptoms have been discovered.[112]\n\nIt has been suggested that the g loading of mental tests have been found to correlate with heritability,[32] but both the empirical data and statistical methodology bearing on this question are matters of active controversy.[113][114][115] Several studies suggest that tests with larger g loadings are more affected by inbreeding depression lowering test scores.[citation needed] There is also evidence that tests with larger g loadings are associated with larger positive heterotic effects on test scores, which has been suggested to indicate the presence of genetic dominance effects for g.[116]\n\ng has a number of correlates in the brain. Studies using magnetic resonance imaging (MRI) have established that g and total brain volume are moderately correlated (r~.3–.4). External head size has a correlation of ~.2 with g. MRI research on brain regions indicates that the volumes of frontal, parietal and temporal cortices, and the hippocampus are also correlated with g, generally at .25 or more, while the correlations, averaged over many studies, with overall grey matter and overall white matter have been found to be .31 and .27, respectively. Some but not all studies have also found positive correlations between g and cortical thickness. However, the underlying reasons for these associations between the quantity of brain tissue and differences in cognitive abilities remain largely unknown.[2]\n\nMost researchers believe that intelligence cannot be localized to a single brain region, such as the frontal lobe. Brain lesion studies have found small but consistent associations indicating that people with more white matter lesions tend to have lower cognitive ability. Research utilizing NMR spectroscopy has discovered somewhat inconsistent but generally positive correlations between intelligence and white matter integrity, supporting the notion that white matter is important for intelligence.[2]\n\nSome research suggests that aside from the integrity of white matter, also its organizational efficiency is related to intelligence. The hypothesis that brain efficiency has a role in intelligence is supported by functional MRI research showing that more intelligent people generally process information more efficiently, i.e., they use fewer brain resources for the same task than less intelligent people.[2]\n\nSmall but relatively consistent associations with intelligence test scores include also brain activity, as measured by EEG records or event-related potentials, and nerve conduction velocity.[117][118]\n\nEvidence of a general factor of intelligence has also been observed in non-human animals. Studies have shown that g  is responsible for 47% of the variance at the species level in primates[119] and around 55% of the individual variance observed in mice.[120][121] A review and meta-analysis of general intelligence, however, found that the average correlation among cognitive abilities was 0.18 and suggested that overall support for g is weak in non-human animals.[122]\n\nAlthough it is not assessable using the same intelligence measures used in humans, cognitive ability can be measured with a variety of interactive and observational tools focusing on innovation, habit reversal, social learning, and responses to novelty. Non-human models of g such as mice are used to study genetic influences on intelligence and neurological developmental research into the mechanisms behind and biological correlates of g.[123]\n\nSimilar to g for individuals, a new research path aims to extract a general collective intelligence factor c for groups displaying a group's general ability to perform a wide range of tasks.[124] Definition, operationalization and statistical approach for this c factor are derived from and similar to g. Causes, predictive validity as well as additional parallels to g are investigated.[125]\n\nHeight is correlated with intelligence (r~.2), but this correlation has not generally been found within families (i.e., among siblings), suggesting that it results from cross-assortative mating for height and intelligence, or from another factor that correlates with both (e.g. nutrition). Myopia is known to be associated with intelligence, with a correlation of around .2 to .25, and this association has been found within families, too.[126]\n\nCross-cultural studies indicate that the g factor can be observed whenever a battery of diverse, complex cognitive tests is administered to a human sample. The factor structure of IQ tests has also been found to be consistent across sexes and ethnic groups in the U.S. and elsewhere.[118] The g factor has been found to be the most invariant of all factors in cross-cultural comparisons. For example, when the g factors computed from an American standardization sample of Wechsler's IQ battery and from large samples who completed the Japanese translation of the same battery were compared, the congruence coefficient was .99, indicating virtual identity. Similarly, the congruence coefficient between the g factors obtained from white and black standardization samples of the WISC battery in the U.S. was .995, and the variance in test scores accounted for by g was highly similar for both groups.[127]\n\nMost studies suggest that there are negligible differences in the mean level of g between the sexes, but that sex differences in cognitive abilities are to be found in more narrow domains. For example, males generally outperform females in spatial tasks, while females generally outperform males in verbal tasks.[128] Another difference that has been found in many studies is that males show more variability in both general and specific abilities than females, with proportionately more males at both the low end and the high end of the test score distribution.[129]\n\nDifferences in g between racial and ethnic groups have been found, particularly in the U.S. between black- and white-identifying test takers, though these differences appear to have diminished significantly over time,[114] and to be attributable to environmental (rather than genetic) causes.[114][130] Some researchers have suggested that the magnitude of the black-white gap in cognitive test results is dependent on the magnitude of the test's g loading, with tests showing higher g loading producing larger gaps (see Spearman's hypothesis),[131] while others have criticized this view as methodologically unfounded.[132][133] Still others have noted that despite the increasing g loading of IQ test batteries over time, the performance gap between racial groups continues to diminish.[114] Comparative analysis has shown that while a gap of approximately 1.1 standard deviation in mean IQ (around 16 points) between white and black Americans existed in the late 1960s, between 1972 and 2002 black Americans gained between 4 and 7 IQ points relative to non-Hispanic Whites, and that \"the g gap between Blacks and Whites declined virtually in tandem with the IQ gap.\"[114] In contrast, Americans of East Asian descent generally slightly outscore white Americans.[134] It has been claimed that racial and ethnic differences similar to those found in the U.S. can be observed globally,[135] but the significance, methodological grounding, and truth of such claims have all been disputed.[136][137][138][139][140][141]\n\nElementary cognitive tasks (ECTs) also correlate strongly with g. ECTs are, as the name suggests, simple tasks that apparently require very little intelligence, but still correlate strongly with more exhaustive intelligence tests. Determining whether a light is red or blue and determining whether there are four or five squares drawn on a computer screen are two examples of ECTs. The answers to such questions are usually provided by quickly pressing buttons. Often, in addition to buttons for the two options provided, a third button is held down from the start of the test. When the stimulus is given to the subject, they remove their hand from the starting button to the button of the correct answer. This allows the examiner to determine how much time was spent thinking about the answer to the question (reaction time, usually measured in small fractions of second), and how much time was spent on physical hand movement to the correct button (movement time).  Reaction time correlates strongly with g, while movement time correlates less strongly.[142]\nECT testing has allowed quantitative examination of hypotheses concerning test bias, subject motivation, and group differences.  By virtue of their simplicity, ECTs provide a link between classical IQ testing and biological inquiries such as fMRI studies.\n\nOne theory holds that g is identical or nearly identical to working memory capacity. Among other evidence for this view, some studies have found factors representing g and working memory to be perfectly correlated. However, in a meta-analysis the correlation was found to be considerably lower.[143] One criticism that has been made of studies that identify g with working memory is that \"we do not advance understanding by showing that one mysterious concept is linked to another.\"[144]\n\nPsychometric theories of intelligence aim at quantifying intellectual growth and identifying ability differences between individuals and groups. In contrast, Jean Piaget's theory of cognitive development seeks to understand qualitative changes in children's intellectual development. Piaget designed a number of tasks to verify hypotheses arising from his theory. The tasks were not intended to measure individual differences, and they have no equivalent in psychometric intelligence tests.[145][146] For example, in one of the best-known Piagetian conservation tasks a child is asked if the amount of water in two identical glasses is the same. After the child agrees that the amount is the same, the investigator pours the water from one of the glasses into a glass of different shape so that the amount appears different although it remains the same. The child is then asked if the amount of water in the two glasses is the same or different.\n\nNotwithstanding the different research traditions in which psychometric tests and Piagetian tasks were developed, the correlations between the two types of measures have been found to be consistently positive and generally moderate in magnitude. A common general factor underlies them. It has been shown that it is possible to construct a battery consisting of Piagetian tasks that is as good a measure of g as standard IQ tests.[145][147]\n\nThe traditional view in psychology is that there is no meaningful relationship between personality and intelligence, and that the two should be studied separately. Intelligence can be understood in terms of what an individual can do, or what his or her maximal performance is, while personality can be thought of in terms of what an individual will typically do, or what his or her general tendencies of behavior are. Large-scale meta-analyses have found that there are hundreds of connections >.20 in magnitude between cognitive abilities and personality traits across the Big Five. This is despite the fact that correlations with the global Big Five factors themselves being small, except for Openness (.26).[148] More interesting relations emerge at other levels (e.g., .23 for the activity facet of extraversion with general mental ability, -.29 for the uneven-tempered facet of neuroticism, .32 for the industriousness aspect of conscientiousness, .26 for the compassion aspect of agreeableness).[149]\n\nThe associations between intelligence and personality have generally been interpreted in two main ways. The first perspective is that personality traits influence performance on intelligence tests. For example, a person may fail to perform at a maximal level on an IQ test due to his or her anxiety and stress-proneness. The second perspective considers intelligence and personality to be conceptually related, with personality traits determining how people apply and invest their cognitive abilities, leading to knowledge expansion and greater cognitive differentiation.[150][151] Other theories (e.g., Cybernetic Trait Complexes Theory) view personality and cognitive ability as intertwined parameters of individuals that co-evolved and are also co-influenced during development (e.g., by early life starvation).[152]\n\nSome researchers believe that there is a threshold level of g below which socially significant creativity is rare, but that otherwise there is no relationship between the two. It has been suggested that this threshold is at least one standard deviation above the population mean. Above the threshold, personality differences are believed to be important determinants of individual variation in creativity.[153][154]\n\nOthers have challenged the threshold theory. While not disputing that opportunity and personal attributes other than intelligence, such as energy and commitment, are important for creativity, they argue that g is positively associated with creativity even at the high end of the ability distribution. The longitudinal Study of Mathematically Precocious Youth has provided evidence for this contention. It has showed that individuals identified by standardized tests as intellectually gifted in early adolescence accomplish creative achievements (for example, securing patents or publishing literary or scientific works) at several times the rate of the general population, and that even within the top 1 percent of cognitive ability, those with higher ability are more likely to make outstanding achievements. The study has also suggested that the level of g acts as a predictor of the level of achievement, while specific cognitive ability patterns predict the realm of achievement.[155][156]\n\nResearch on the G-factor, as well as other psychometric values, has been widely criticized for not properly taking into account the eugenicist background of its research practices.[157] The reductionism of the G-factor has been attributed to having evolved from pseudoscientific theories about race and intelligence.[158] Spearman's g and the concept of inherited, immutable intelligence were a boon for eugenicists and pseudoscientists alike.[159]\n\nJoseph L. Graves Jr. and Amanda Johnson have argued that g \"...is to the psychometricians what Huygens' ether was to early physicists: a nonentity taken as an article of faith instead of one in need of verification by real data.\"[160]\n\nRaymond Cattell, a student of Charles Spearman's, modified the unitary g factor model and divided g into two broad, relatively independent domains: fluid intelligence (Gf) and crystallized intelligence (Gc). Gf is conceptualized as a capacity to figure out novel problems, and it is best assessed with tests with little cultural or scholastic content, such as Raven's matrices. Gc can be thought of as consolidated knowledge, reflecting the skills and information that an individual acquires and retains throughout his or her life. Gc is dependent on education and other forms of acculturation, and it is best assessed with tests that emphasize scholastic and cultural knowledge.[2][43][161] Gf can be thought to primarily consist of current reasoning and problem solving capabilities, while Gc reflects the outcome of previously executed cognitive processes.[162]\n\nThe rationale for the separation of Gf and Gc was to explain individuals' cognitive development over time. While Gf and Gc have been found to be highly correlated, they differ in the way they change over a lifetime. Gf tends to peak at around age 20, slowly declining thereafter. In contrast, Gc is stable or increases across adulthood. A single general factor has been criticized as obscuring this bifurcated pattern of development. Cattell argued that Gf reflected individual differences in the efficiency of the central nervous system. Gc was, in Cattell's thinking, the result of a person \"investing\" his or her Gf in learning experiences throughout life.[2][29][43][163]\n\nCattell, together with John Horn, later expanded the Gf-Gc model to include a number of other broad abilities, such as Gq (quantitative reasoning) and Gv (visual-spatial reasoning). While all the broad ability factors in the extended Gf-Gc model are positively correlated and thus would enable the extraction of a higher order g factor, Cattell and Horn maintained that it would be erroneous to posit that a general factor underlies these broad abilities. They argued that g factors computed from different test batteries are not invariant and would give different values of g, and that the correlations among tests arise because it is difficult to test just one ability at a time.[2][48][164]\n\nHowever, several researchers have suggested that the Gf-Gc model is compatible with a g-centered understanding of cognitive abilities. For example, John B. Carroll's three-stratum model of intelligence includes both Gf and Gc together with a higher-order g factor. Based on factor analyses of many data sets, some researchers have also argued that Gf and g are one and the same factor and that g factors from different test batteries are substantially invariant provided that the batteries are large and diverse.[43][165][166]\n\nSeveral theorists have proposed that there are intellectual abilities that are uncorrelated with each other. Among the earliest was L.L. Thurstone who created a model of primary mental abilities representing supposedly independent domains of intelligence. However, Thurstone's tests of these abilities were found to produce a strong general factor. He argued that the lack of independence among his tests reflected the difficulty of constructing \"factorially pure\" tests that measured just one ability. Similarly, J.P. Guilford proposed a model of intelligence that comprised up to 180 distinct, uncorrelated abilities, and claimed to be able to test all of them. Later analyses have shown that the factorial procedures Guilford presented as evidence for his theory did not provide support for it, and that the test data that he claimed provided evidence against g did in fact exhibit the usual pattern of intercorrelations after correction for statistical artifacts.[167][168]\n\nMore recently, Howard Gardner has developed the theory of multiple intelligences. He posits the existence of nine different and independent domains of intelligence, such as mathematical, linguistic, spatial, musical, bodily-kinesthetic, meta-cognitive, and existential intelligences, and contends that individuals who fail in some of them may excel in others. According to Gardner, tests and schools traditionally emphasize only linguistic and logical abilities while neglecting other forms of intelligence.\n\nWhile popular among educationalists, Gardner's theory has been much criticized by psychologists and psychometricians. One criticism is that the theory contradicts both scientific and everyday usages of the word intelligence. Several researchers have argued that not all of Gardner's intelligences fall within the cognitive sphere. For example, Gardner contends that a successful career in professional sports or popular music reflects bodily-kinesthetic intelligence and musical intelligence, respectively, even though one might usually talk of athletic and musical skills, talents, or abilities instead.\n\nAnother criticism of Gardner's theory is that many of his purportedly independent domains of intelligence are in fact correlated with each other. Responding to empirical analyses showing correlations between the domains, Gardner has argued that the correlations exist because of the common format of tests and because all tests require linguistic and logical skills. His critics have in turn pointed out that not all IQ tests are administered in the paper-and-pencil format, that aside from linguistic and logical abilities, IQ test batteries contain also measures of, for example, spatial abilities, and that elementary cognitive tasks (for example, inspection time and reaction time) that do not involve linguistic or logical reasoning correlate with conventional IQ batteries, too.[73][169][170][171]\n\nRobert Sternberg, working with various colleagues, has also suggested that intelligence has dimensions independent of g. He argues that there are three classes of intelligence: analytic, practical, and creative. According to Sternberg, traditional psychometric tests measure only analytic intelligence, and should be augmented to test creative and practical intelligence as well. He has devised several tests to this effect. Sternberg equates analytic intelligence with academic intelligence, and contrasts it with practical intelligence, defined as an ability to deal with ill-defined real-life problems. Tacit intelligence is an important component of practical intelligence, consisting of knowledge that is not explicitly taught but is required in many real-life situations. Assessing creativity independent of intelligence tests has traditionally proved difficult, but Sternberg and colleagues have claimed to have created valid tests of creativity, too.\n\nThe validation of Sternberg's theory requires that the three abilities tested are substantially uncorrelated and have independent predictive validity. Sternberg has conducted many experiments which he claims confirm the validity of his theory, but several researchers have disputed this conclusion. For example, in his reanalysis of a validation study of Sternberg's STAT test, Nathan Brody showed that the predictive validity of the STAT, a test of three allegedly independent abilities, was almost solely due to a single general factor underlying the tests, which Brody equated with the g factor.[172][173]\n\nJames Flynn has argued that intelligence should be conceptualized at three different levels: brain physiology, cognitive differences between individuals, and social trends in intelligence over time. According to this model, the g factor is a useful concept with respect to individual differences but its explanatory power is limited when the focus of investigation is either brain physiology, or, especially, the effect of social trends on intelligence. Flynn has criticized the notion that cognitive gains over time, or the Flynn effect, are \"hollow\" if they cannot be shown to be increases in g. He argues that the Flynn effect reflects shifting social priorities and individuals' adaptation to them. To apply the individual differences concept of g to the Flynn effect is to confuse different levels of analysis. On the other hand, according to Flynn, it is also fallacious to deny, by referring to trends in intelligence over time, that some individuals have \"better brains and minds\" to cope with the cognitive demands of their particular time. At the level of brain physiology, Flynn has emphasized both that localized neural clusters can be affected differently by cognitive exercise, and that there are important factors that affect all neural clusters.[174]\n\nPaleontologist and biologist Stephen Jay Gould presented a critique in his 1981 book The Mismeasure of Man. He argued that psychometricians fallaciously reified the g factor into an ineluctable \"thing\" that provided a convenient explanation for human intelligence, grounded only in mathematical theory rather than the rigorous application of mathematical theory to biological knowledge.[175] An example is provided in the work of Cyril Burt, published posthumously in 1972: \"The two main conclusions we have reached seem clear and beyond all question. The hypothesis of a general factor entering into every type of cognitive process, tentatively suggested by speculations derived from neurology and biology, is fully borne out by the statistical evidence; and the contention that differences in this general factor depend largely on the individual's genetic constitution appears incontestable.The concept of an innate, general cognitive ability, which follows from these two assumptions, though admittedly sheerly an abstraction, is thus wholly consistent with the empirical facts.\"[176]\n\nSeveral researchers have criticized Gould's arguments. For example, they have rejected the accusation of reification, maintaining that the use of extracted factors such as g as potential causal variables whose reality can be supported or rejected by further investigations constitutes a normal scientific practice that in no way distinguishes psychometrics from other sciences. Critics have also suggested that Gould did not understand the purpose of factor analysis, and that he was ignorant of relevant methodological advances in the field. While different factor solutions may be mathematically equivalent in their ability to account for intercorrelations among tests, solutions that yield a g factor are psychologically preferable for several reasons extrinsic to factor analysis, including the phenomenon of the positive manifold, the fact that the same g can emerge from quite different test batteries, the widespread practical validity of g, and the linkage of g to many biological variables.[36][37][page needed]\n\nJohn Horn and John McArdle have argued that the modern g theory, as espoused by, for example, Arthur Jensen, is unfalsifiable, because the existence of a common factor like g follows tautologically from positive correlations among tests. They contrasted the modern hierarchical theory of g with Spearman's original two-factor theory which was readily falsifiable (and indeed was falsified).[29]",
        pageTitle: "g factor (psychometrics)",
    },
    {
        title: "Stang's law",
        link: "https://en.wikipedia.org/wiki/Stang%27s_law",
        content:
            "Stang's law is a Proto-Indo-European (PIE) phonological rule named after the Norwegian linguist Christian Stang.\n\nThe law governs the word-final sequences of a vowel, followed by a semivowel (*y or *w) or a laryngeal (*h₁, *h₂ or *h₃), followed by a nasal. According to the law these sequences are simplified such that laryngeals and semivowels are dropped, with compensatory lengthening of a preceding vowel.\n\nThis rule is usually cited in more restricted form as: *Vwm > *Vːm and *Vh₂m > *Vːm (*V denoting a vowel and *Vː a long vowel).\n\nOften the rules *Vmm > *Vːm and also *Vyi > *Vːy are added:[1]\n\nThis phonology article is a stub. You can help Wikipedia by expanding it.\n\nThis Indo-European languages-related article is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Stang's law",
    },
    {
        title: "Stark–Einstein law",
        link: "https://en.wikipedia.org/wiki/Stark%E2%80%93Einstein_law",
        content:
            "Photoelectrochemical processes are processes in photoelectrochemistry; they usually involve transforming light into other forms of energy.[1] These processes apply to photochemistry, optically pumped lasers, sensitized solar cells, luminescence, and photochromism.\n\nElectron excitation is the movement of an electron to a higher energy state. This can either be done by photoexcitation (PE), where the original electron absorbs the photon and gains all the photon's energy or by electrical excitation (EE), where the original electron absorbs the energy of another, energetic electron. Within a semiconductor crystal lattice, thermal excitation is a process where lattice vibrations provide enough energy to move electrons to a higher energy band. When an excited electron falls back to a lower energy state again, it is called electron relaxation. This can be done by radiation of a photon or giving the energy to a third spectator particle as well.[2]\n\nIn physics there is a specific technical definition for energy level which is often associated with an atom being excited to an excited state. The excited state, in general, is in relation to the ground state, where the excited state is at a higher energy level than the ground state.\n\nPhotoexcitation is the mechanism of electron excitation by photon absorption, when the energy of the photon is too low to cause photoionization. The absorption of the photon takes place in accordance with Planck's quantum theory.\n\nPhotoexcitation plays role in photoisomerization. Photoexcitation is exploited in dye-sensitized solar cells, photochemistry, luminescence, optically pumped lasers, and in some photochromic applications.\n\nIn chemistry, photoisomerization is molecular behavior in which structural change between isomers is caused by photoexcitation. Both reversible and irreversible photoisomerization reactions exist. However, the word \"photoisomerization\" usually indicates a reversible process. Photoisomerizable molecules are already put to practical use, for instance, in pigments for rewritable CDs, DVDs, and 3D optical data storage solutions. In addition, recent interest in photoisomerizable molecules has been aimed at molecular devices, such as molecular switches,[3] molecular motors,[4] and molecular electronics.\n\nPhotoisomerization behavior can be roughly categorized into several classes. Two major classes are trans-cis (or 'E-'Z) conversion, and open-closed ring transition. Examples of the former include stilbene and azobenzene. This type of compounds has a double bond, and rotation or inversion around the double bond affords isomerization between the two states. Examples of the latter include fulgide and diarylethene. This type of compounds undergoes bond cleavage and bond creation upon irradiation with particular wavelengths of light. Still another class is the di-π-methane rearrangement.\n\nPhotoionization is the physical process in which an incident photon ejects one or more electrons from an atom, ion or molecule. This is essentially the same process that occurs with the photoelectric effect with metals. In the case of a gas or single atoms, the term photoionization is more common.[5]\n\nThe ejected electrons, known as photoelectrons, carry information about their pre-ionized states. For example, a single electron can have a kinetic energy equal to the energy of the incident photon minus the electron binding energy of the state it left. Photons with energies less than the electron binding energy may be absorbed or scattered but will not photoionize the atom or ion.[5]\n\nFor example, to ionize hydrogen, photons need an energy greater than 13.6 electronvolts (the Rydberg energy), which corresponds to a wavelength of 91.2 nm.[6] For photons with greater energy than this, the energy of the emitted photoelectron is given by:\n\nwhere h is the Planck constant and ν is the frequency of the photon.\n\nNot every photon which encounters an atom or ion will photoionize it. The probability of photoionization is related to the photoionization cross-section, which depends on the energy of the photon and the target being considered. For photon energies below the ionization threshold, the photoionization cross-section is near zero. But with the development of pulsed lasers it has become possible to create extremely intense, coherent light where multi-photon ionization may occur. At even higher intensities (around 1015 - 1016 W/cm2 of infrared or visible light), non-perturbative phenomena such as barrier suppression ionization[7] and rescattering ionization[8] are observed.\n\nSeveral photons of energy below the ionization threshold may actually combine their energies to ionize an atom. This probability decreases rapidly with the number of photons required, but the development of very intense, pulsed lasers still makes it possible. In the perturbative regime (below about 1014 W/cm2 at optical frequencies), the probability of absorbing N photons depends on the laser-light intensity I as IN .[9]\n\nAbove threshold ionization (ATI) [10] is an extension of multi-photon ionization where even more photons are absorbed than actually would be necessary to ionize the atom. The excess energy gives the released electron higher kinetic energy than the usual case of just-above threshold ionization. More precisely, the system will have multiple peaks in its photoelectron spectrum which are separated by the photon energies, this indicates that the emitted electron has more kinetic energy than in the normal (lowest possible number of photons) ionization case. The electrons released from the target will have approximately an integer number of photon-energies more kinetic energy. In intensity regions between 1014 W/cm2 and 1018 W/cm2, each of MPI, ATI, and barrier suppression ionization can occur simultaneously, each contributing to the overall ionization of the atoms involved.[11]\n\nIn semiconductor physics the Photo-Dember effect (named after its discoverer H. Dember) consists in the formation of a charge dipole in the vicinity of a semiconductor surface after ultra-fast photo-generation of charge carriers. The dipole forms owing to the difference of mobilities (or diffusion constants) for holes and electrons which combined with the break of symmetry provided by the surface lead to an effective charge separation in the direction perpendicular to the surface.[12]\n\nThe Grotthuss–Draper law (also called the principle of photochemical activation) states that only that light which is absorbed by a system can bring about a photochemical change. Materials such as dyes and phosphors must be able to absorb \"light\" at optical frequencies. This law provides a basis for fluorescence and phosphorescence. The law was first proposed in 1817 by Theodor Grotthuss and in 1842, independently, by John William Draper.[5]\n\nThis is considered to be one of the two basic laws of photochemistry. The second law is the Stark–Einstein law, which says that primary chemical or physical reactions occur with each photon absorbed.[5]\n\nThe Stark–Einstein law is named after German-born physicists Johannes Stark and Albert Einstein, who independently formulated the law between 1908 and 1913. It is also known as the photochemical equivalence law or photoequivalence law. In essence it says that every photon that is absorbed will cause a (primary) chemical or physical reaction.[13]\n\nThe photon is a quantum of radiation, or one unit of radiation. Therefore, this is a single unit of EM radiation that is equal to the Planck constant (h) times the frequency of light. This quantity is symbolized by γ, hν, or ħω.\n\nThe photochemical equivalence law is also restated as follows: for every mole of a substance that reacts, an equivalent mole of quanta of light are absorbed. The formula is:[13]\n\nThe photochemical equivalence law applies to the part of a light-induced reaction that is referred to as the primary process (i.e. absorption or fluorescence).[13]\n\nIn most photochemical reactions the primary process is usually followed by so-called secondary photochemical processes that are normal interactions between reactants not requiring absorption of light. As a result, such reactions do not appear to obey the one quantum–one molecule reactant relationship.[13]\n\nThe law is further restricted to conventional photochemical processes using light sources with moderate intensities; high-intensity light sources such as those used in flash photolysis and in laser experiments are known to cause so-called biphotonic processes; i.e., the absorption by a molecule of a substance of two photons of light.[13]\n\nIn physics, absorption of electromagnetic radiation is the way by which the energy of a photon is taken up by matter, typically the electrons of an atom. Thus, the electromagnetic energy is transformed to other forms of energy, for example, to heat. The absorption of light during wave propagation is often called attenuation. Usually, the absorption of waves does not depend on their intensity (linear absorption), although in certain conditions (usually, in optics), the medium changes its transparency dependently on the intensity of waves going through, and the Saturable absorption (or nonlinear absorption) occurs.\n\nPhotosensitization is a process of transferring the energy of absorbed light. After absorption, the energy is transferred to the (chosen) reactants. This is part of the work of photochemistry in general. In particular this process is commonly employed where reactions require light sources of certain wavelengths that are not readily available.[14]\n\nFor example, mercury absorbs radiation at 1849 and 2537 angstroms, and the source is often high-intensity mercury lamps. It is a commonly used sensitizer. When mercury vapor is mixed with ethylene, and the compound is irradiated with a mercury lamp, this results in the photodecomposition of ethylene to acetylene. This occurs on absorption of light to yield excited state mercury atoms, which are able to transfer this energy to the ethylene molecules, and are in turn deactivated to their initial energy state.[14]\n\nCadmium; some of the noble gases, for example xenon; zinc; benzophenone; and a large number of organic dyes, are also used as sensitizers.[14]\n\nPhotosensitisers are a key component of photodynamic therapy used to treat cancers.\n\nA sensitizer in chemiluminescence is a chemical compound, capable of light emission after it has received energy from a molecule, which became excited previously in the chemical reaction. A good example is this:\n\nWhen an alkaline solution of sodium hypochlorite and a concentrated solution of hydrogen peroxide are mixed, a reaction occurs:\n\nO2*is excited oxygen – meaning, one or more electrons in the O2 molecule have been promoted to higher-energy molecular orbitals. Hence, oxygen produced by this chemical reaction somehow 'absorbed' the energy released by the reaction and became excited. This energy state is unstable, therefore it will return to the ground state by lowering its energy. It can do that in more than one way:\n\nThe intensity, duration and color of emitted light depend on quantum and kinetical factors. However, excited molecules are frequently less capable of light emission in terms of brightness and duration when compared to sensitizers. This is because sensitizers can store energy (that is, be excited) for longer periods of time than other excited molecules. The energy is stored through means of quantum vibration, so sensitizers are usually compounds which either include systems of aromatic rings or many conjugated double and triple bonds in their structure. Hence, if an excited molecule transfers its energy to a sensitizer thus exciting it, longer and easier to quantify light emission is often observed.\n\nThe color (that is, the wavelength), brightness and duration of emission depend upon the sensitizer used. Usually, for a certain chemical reaction, many different sensitizers can be used.\n\nFluorescence spectroscopy aka fluorometry or spectrofluorometry, is a type of electromagnetic spectroscopy which analyzes fluorescence from a sample. It involves using a beam of light, usually ultraviolet light, that excites the electrons in molecules of certain compounds and causes them to emit light of a lower energy, typically, but not necessarily, visible light. A complementary technique is absorption spectroscopy.[15][16]\n\nDevices that measure fluorescence are called fluorometers or fluorimeters.\n\nAbsorption spectroscopy refers to spectroscopic techniques that measure the absorption of radiation, as a function of frequency or wavelength, due to its interaction with a sample. The sample absorbs energy, i.e., photons, from the radiating field. The intensity of the absorption varies as a function of frequency, and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum.[15][16]",
        pageTitle: "Photoelectrochemical process",
    },
    {
        title: "Stefan–Boltzmann law",
        link: "https://en.wikipedia.org/wiki/Stefan%E2%80%93Boltzmann_law",
        content:
            "The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature.  It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.\n\nFor an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T:\n\n  \n    \n      \n        \n          M\n          \n            ∘\n          \n        \n        =\n        σ\n        \n        \n          T\n          \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle M^{\\circ }=\\sigma \\,T^{4}.}\n\nThe constant of proportionality, \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n, is called the Stefan–Boltzmann constant. It has the value\n\nIn the general case, the Stefan–Boltzmann law for radiant exitance takes the form:\n\n  \n    \n      \n        M\n        =\n        ε\n        \n        \n          M\n          \n            ∘\n          \n        \n        =\n        ε\n        \n        σ\n        \n        \n          T\n          \n            4\n          \n        \n        ,\n      \n    \n    {\\displaystyle M=\\varepsilon \\,M^{\\circ }=\\varepsilon \\,\\sigma \\,T^{4},}\n  \n\nwhere \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is the emissivity of the surface emitting the radiation.  The emissivity is generally between zero and one.  An emissivity of one corresponds to a black body.\n\nThe radiant exitance (previously called radiant emittance), \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n, has dimensions of energy flux (energy per unit time per unit area), and the SI units of measure are joules per second per square metre (J⋅s−1⋅m−2), or equivalently, watts per square metre (W⋅m−2).[2] The SI unit for absolute temperature, T, is the kelvin (K).\n\nTo find the total power, \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, radiated from an object, multiply the radiant exitance by the object's surface area, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n:\n\n  \n    \n      \n        P\n        =\n        A\n        ⋅\n        M\n        =\n        A\n        \n        ε\n        \n        σ\n        \n        \n          T\n          \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle P=A\\cdot M=A\\,\\varepsilon \\,\\sigma \\,T^{4}.}\n\nMatter that does not absorb all incident radiation emits less total energy than a black body. Emissions are reduced by a factor \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, where the emissivity, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, is a material property which, for most matter, satisfies \n  \n    \n      \n        0\n        ≤\n        ε\n        ≤\n        1\n      \n    \n    {\\displaystyle 0\\leq \\varepsilon \\leq 1}\n  \n. Emissivity can in general depend on wavelength, direction, and polarization. However, the emissivity which appears in the non-directional form of the Stefan–Boltzmann law is the hemispherical total emissivity, which reflects emissions as totaled over all wavelengths, directions, and polarizations.[3]: 60\n\nThe form of the Stefan–Boltzmann law that includes emissivity is applicable to all matter, provided that matter is in a state of local thermodynamic equilibrium (LTE) so that its temperature is well-defined.[3]: 66n, 541  (This is a trivial conclusion, since the emissivity, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, is defined to be the quantity that makes this equation valid. What is non-trivial is the proposition that \n  \n    \n      \n        ε\n        ≤\n        1\n      \n    \n    {\\displaystyle \\varepsilon \\leq 1}\n  \n, which is a consequence of Kirchhoff's law of thermal radiation.[4]: 385 )\n\nA so-called grey body is a body for which the spectral emissivity is independent of wavelength, so that the total emissivity, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, is a constant.[3]: 71  In the more general (and realistic) case, the spectral emissivity depends on wavelength. The total emissivity, as applicable to the Stefan–Boltzmann law, may be calculated as a weighted average of the spectral emissivity, with the blackbody emission spectrum serving as the weighting function. It follows that if the spectral emissivity depends on wavelength then the total emissivity depends on the temperature, i.e., \n  \n    \n      \n        ε\n        =\n        ε\n        (\n        T\n        )\n      \n    \n    {\\displaystyle \\varepsilon =\\varepsilon (T)}\n  \n.[3]: 60  However, if the dependence on wavelength is small, then the dependence on temperature will be small as well.\n\nWavelength- and subwavelength-scale particles,[5] metamaterials,[6] and other nanostructures[7] are not subject to ray-optical limits and may be designed to have an emissivity greater than 1.\n\nIn national and international standards documents, the symbol \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n is recommended to denote radiant exitance; a superscript circle (°) indicates a term relate to a black body.[2] (A subscript \"e\" is added when it is important to distinguish the energetic (radiometric) quantity radiant exitance, \n  \n    \n      \n        \n          M\n          \n            \n              e\n            \n          \n        \n      \n    \n    {\\displaystyle M_{\\mathrm {e} }}\n  \n, from the analogous human vision (photometric) quantity, luminous exitance, denoted \n  \n    \n      \n        \n          M\n          \n            \n              v\n            \n          \n        \n      \n    \n    {\\displaystyle M_{\\mathrm {v} }}\n  \n.[8]) In common usage, the symbol used for radiant exitance (often called radiant emittance) varies among different texts and in different fields.\n\nThe Stefan–Boltzmann law may be expressed as a formula for radiance as a function of temperature. Radiance is measured in watts per square metre per steradian (W⋅m−2⋅sr−1). The Stefan–Boltzmann law for the radiance of a black body is:[9]: 26 [10]\n\n  \n    \n      \n        \n          L\n          \n            Ω\n          \n          \n            ∘\n          \n        \n        =\n        \n          \n            \n              M\n              \n                ∘\n              \n            \n            π\n          \n        \n        =\n        \n          \n            σ\n            π\n          \n        \n        \n        \n          T\n          \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle L_{\\Omega }^{\\circ }={\\frac {M^{\\circ }}{\\pi }}={\\frac {\\sigma }{\\pi }}\\,T^{4}.}\n\nThe Stefan–Boltzmann law expressed as a formula for radiation energy density is:[11]\n\n  \n    \n      \n        \n          w\n          \n            \n              e\n            \n          \n          \n            ∘\n          \n        \n        =\n        \n          \n            4\n            c\n          \n        \n        \n        \n          M\n          \n            ∘\n          \n        \n        =\n        \n          \n            4\n            c\n          \n        \n        \n        σ\n        \n        \n          T\n          \n            4\n          \n        \n        ,\n      \n    \n    {\\displaystyle w_{\\mathrm {e} }^{\\circ }={\\frac {4}{c}}\\,M^{\\circ }={\\frac {4}{c}}\\,\\sigma \\,T^{4},}\n  \n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the speed of light.\n\nIn 1864, John Tyndall presented measurements of the infrared emission by a platinum filament and the corresponding color of the filament.[12][13][14][15]\nThe proportionality to the fourth power of the absolute temperature was deduced by Josef Stefan (1835–1893) in 1877 on the basis of Tyndall's experimental measurements, in the article Über die Beziehung zwischen der Wärmestrahlung und der Temperatur (On the relationship between thermal radiation and temperature) in the Bulletins from the sessions of the Vienna Academy of Sciences.[16]\n\nA derivation of the law  from theoretical considerations was presented by Ludwig Boltzmann (1844–1906) in 1884, drawing upon the work of Adolfo Bartoli.[17]\nBartoli in 1876 had derived the existence of radiation pressure from the principles of thermodynamics. Following Bartoli, Boltzmann considered an ideal heat engine using electromagnetic radiation instead of an ideal gas as  working matter.\n\nThe law was almost immediately experimentally verified. Heinrich Weber in 1888 pointed out deviations at higher temperatures, but perfect accuracy within measurement uncertainties was confirmed up to temperatures of 1535 K by 1897.[18]\nThe law, including the theoretical prediction of the Stefan–Boltzmann constant as a function of the speed of light, the Boltzmann constant and the Planck constant, is a direct consequence of Planck's law as formulated in 1900.\n\nThe Stefan–Boltzmann constant, σ, is derived from other known physical constants:\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              2\n              \n                π\n                \n                  5\n                \n              \n              \n                k\n                \n                  4\n                \n              \n            \n            \n              15\n              \n                c\n                \n                  2\n                \n              \n              \n                h\n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\frac {2\\pi ^{5}k^{4}}{15c^{2}h^{3}}}}\n  \n\nwhere k is the Boltzmann constant, the h is the Planck constant, and c is the speed of light in vacuum.[19][4]: 388\n\nAs of the 2019 revision of the SI, which establishes exact fixed values for k, h, and c, the Stefan–Boltzmann constant is exactly:\n\n  \n    \n      \n        σ\n        =\n        \n          [\n          \n            \n              \n                2\n                \n                  π\n                  \n                    5\n                  \n                \n                \n                  \n                    (\n                    \n                      1.380\n                       \n                      649\n                      ×\n                      \n                        10\n                        \n                          −\n                          23\n                        \n                      \n                    \n                    )\n                  \n                  \n                    4\n                  \n                \n              \n              \n                15\n                \n                  \n                    (\n                    \n                      2.997\n                       \n                      924\n                       \n                      58\n                      ×\n                      \n                        10\n                        \n                          8\n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    (\n                    \n                      6.626\n                       \n                      070\n                       \n                      15\n                      ×\n                      \n                        10\n                        \n                          −\n                          34\n                        \n                      \n                    \n                    )\n                  \n                  \n                    3\n                  \n                \n              \n            \n          \n          ]\n        \n        \n        \n          \n            \n              W\n            \n            \n              \n                \n                  m\n                \n                \n                  2\n                \n              \n              \n                ⋅\n              \n              \n                \n                  K\n                \n                \n                  4\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma =\\left[{\\frac {2\\pi ^{5}\\left(1.380\\ 649\\times 10^{-23}\\right)^{4}}{15\\left(2.997\\ 924\\ 58\\times 10^{8}\\right)^{2}\\left(6.626\\ 070\\ 15\\times 10^{-34}\\right)^{3}}}\\right]\\,{\\frac {\\mathrm {W} }{\\mathrm {m} ^{2}{\\cdot }\\mathrm {K} ^{4}}}}\n  \n\nThus,\n\nPrior to this, the value of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n was calculated from the measured value of the gas constant.[20]\n\nThe numerical value of the Stefan–Boltzmann constant is different in other systems of units, as shown in the table below.\n\nWith his law, Stefan also determined the temperature of the Sun's surface.[22] He inferred from the data of Jacques-Louis Soret (1827–1890)[23] that the energy flux density from the Sun is 29 times greater than the energy flux density of a certain warmed metal lamella (a thin plate). A round lamella was placed at such a distance from the measuring device that it would be seen at the same angular diameter as the Sun. Soret estimated the temperature of the lamella to be approximately 1900 °C to 2000 °C. Stefan surmised that 1/3 of the energy flux from the Sun is absorbed by the Earth's atmosphere, so he took for the correct Sun's energy flux a value 3/2 times greater than Soret's value, namely 29 × 3/2 = 43.5.\n\nPrecise measurements of atmospheric absorption were not made until 1888 and 1904. The temperature Stefan obtained was a median value of previous ones, 1950 °C and the absolute thermodynamic one 2200 K. As 2.574 = 43.5, it follows from the law that the temperature of the Sun is 2.57 times greater than the temperature of the lamella, so Stefan got a value of 5430 °C or 5700 K. This was the first sensible value for the temperature of the Sun. Before this, values ranging from as low as 1800 °C to as high as 13000000 °C[24] were claimed. The lower value of 1800 °C was determined by Claude Pouillet (1790–1868) in 1838 using the Dulong–Petit law.[25][26] Pouillet also took just half the value of the Sun's correct energy flux.\n\nThe temperature of stars other than the Sun can be approximated using a similar means by treating the emitted energy as a black body radiation.[27] So:\n\n  \n    \n      \n        L\n        =\n        4\n        π\n        \n          R\n          \n            2\n          \n        \n        σ\n        \n          T\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle L=4\\pi R^{2}\\sigma T^{4}}\n  \n\nwhere L is the luminosity, σ is the Stefan–Boltzmann constant, R is the stellar radius and T is the effective temperature. This formula can then be rearranged to calculate the temperature:\n\n  \n    \n      \n        T\n        =\n        \n          \n            \n              L\n              \n                4\n                π\n                \n                  R\n                  \n                    2\n                  \n                \n                σ\n              \n            \n            \n              4\n            \n          \n        \n      \n    \n    {\\displaystyle T={\\sqrt[{4}]{\\frac {L}{4\\pi R^{2}\\sigma }}}}\n  \n\nor alternatively the radius:\n\n  \n    \n      \n        R\n        =\n        \n          \n            \n              L\n              \n                4\n                π\n                σ\n                \n                  T\n                  \n                    4\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle R={\\sqrt {\\frac {L}{4\\pi \\sigma T^{4}}}}}\n\nThe same formulae can also be simplified to compute the parameters relative to the Sun:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    L\n                    \n                      L\n                      \n                        ⊙\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    (\n                    \n                      \n                        R\n                        \n                          R\n                          \n                            ⊙\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        T\n                        \n                          T\n                          \n                            ⊙\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    4\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    T\n                    \n                      T\n                      \n                        ⊙\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    (\n                    \n                      \n                        L\n                        \n                          L\n                          \n                            ⊙\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    1\n                    \n                      /\n                    \n                    4\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        \n                          R\n                          \n                            ⊙\n                          \n                        \n                        R\n                      \n                    \n                    )\n                  \n                  \n                    1\n                    \n                      /\n                    \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    R\n                    \n                      R\n                      \n                        ⊙\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    (\n                    \n                      \n                        \n                          T\n                          \n                            ⊙\n                          \n                        \n                        T\n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        L\n                        \n                          L\n                          \n                            ⊙\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    1\n                    \n                      /\n                    \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {L}{L_{\\odot }}}&=\\left({\\frac {R}{R_{\\odot }}}\\right)^{2}\\left({\\frac {T}{T_{\\odot }}}\\right)^{4}\\\\[1ex]{\\frac {T}{T_{\\odot }}}&=\\left({\\frac {L}{L_{\\odot }}}\\right)^{1/4}\\left({\\frac {R_{\\odot }}{R}}\\right)^{1/2}\\\\[1ex]{\\frac {R}{R_{\\odot }}}&=\\left({\\frac {T_{\\odot }}{T}}\\right)^{2}\\left({\\frac {L}{L_{\\odot }}}\\right)^{1/2}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          R\n          \n            ⊙\n          \n        \n      \n    \n    {\\displaystyle R_{\\odot }}\n  \n is the solar radius, and so forth. They can also be rewritten in terms of the surface area A and radiant exitance \n  \n    \n      \n        \n          M\n          \n            ∘\n          \n        \n      \n    \n    {\\displaystyle M^{\\circ }}\n  \n:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n              \n                \n                =\n                A\n                \n                  M\n                  \n                    ∘\n                  \n                \n              \n            \n            \n              \n                \n                  M\n                  \n                    ∘\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    L\n                    A\n                  \n                \n              \n            \n            \n              \n                A\n              \n              \n                \n                =\n                \n                  \n                    L\n                    \n                      M\n                      \n                        ∘\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}L&=AM^{\\circ }\\\\[1ex]M^{\\circ }&={\\frac {L}{A}}\\\\[1ex]A&={\\frac {L}{M^{\\circ }}}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        A\n        =\n        4\n        π\n        \n          R\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A=4\\pi R^{2}}\n  \n and \n  \n    \n      \n        \n          M\n          \n            ∘\n          \n        \n        =\n        σ\n        \n          T\n          \n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle M^{\\circ }=\\sigma T^{4}.}\n\nWith the Stefan–Boltzmann law, astronomers can easily infer the radii of stars. The law is also met in the thermodynamics of black holes in so-called Hawking radiation.\n\nSimilarly we can calculate the effective temperature of the Earth T⊕ by equating the energy received from the Sun and the energy radiated by the Earth, under the black-body approximation (Earth's own production of energy being small enough to be negligible).  The luminosity of the Sun, L⊙, is given by:\n\n  \n    \n      \n        \n          L\n          \n            ⊙\n          \n        \n        =\n        4\n        π\n        \n          R\n          \n            ⊙\n          \n          \n            2\n          \n        \n        σ\n        \n          T\n          \n            ⊙\n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle L_{\\odot }=4\\pi R_{\\odot }^{2}\\sigma T_{\\odot }^{4}}\n\nAt Earth, this energy is passing through a sphere with a radius of a0, the distance between the Earth and the Sun, and the irradiance (received power per unit area) is given by\n\n  \n    \n      \n        \n          E\n          \n            ⊕\n          \n        \n        =\n        \n          \n            \n              L\n              \n                ⊙\n              \n            \n            \n              4\n              π\n              \n                a\n                \n                  0\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\oplus }={\\frac {L_{\\odot }}{4\\pi a_{0}^{2}}}}\n\nThe Earth has a radius of R⊕, and therefore has a cross-section of \n  \n    \n      \n        π\n        \n          R\n          \n            ⊕\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\pi R_{\\oplus }^{2}}\n  \n.  The radiant flux (i.e. solar power) absorbed by the Earth is thus given by:\n\n  \n    \n      \n        \n          Φ\n          \n            abs\n          \n        \n        =\n        π\n        \n          R\n          \n            ⊕\n          \n          \n            2\n          \n        \n        ×\n        \n          E\n          \n            ⊕\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{\\text{abs}}=\\pi R_{\\oplus }^{2}\\times E_{\\oplus }}\n\nBecause the Stefan–Boltzmann law uses a fourth power, it has a stabilizing effect on the exchange and the flux emitted by Earth tends to be equal to the flux absorbed, close to the steady state where:\n\n  \n    \n      \n        \n          \n            \n              \n                4\n                π\n                \n                  R\n                  \n                    ⊕\n                  \n                  \n                    2\n                  \n                \n                σ\n                \n                  T\n                  \n                    ⊕\n                  \n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                π\n                \n                  R\n                  \n                    ⊕\n                  \n                  \n                    2\n                  \n                \n                ×\n                \n                  E\n                  \n                    ⊕\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                π\n                \n                  R\n                  \n                    ⊕\n                  \n                  \n                    2\n                  \n                \n                ×\n                \n                  \n                    \n                      4\n                      π\n                      \n                        R\n                        \n                          ⊙\n                        \n                        \n                          2\n                        \n                      \n                      σ\n                      \n                        T\n                        \n                          ⊙\n                        \n                        \n                          4\n                        \n                      \n                    \n                    \n                      4\n                      π\n                      \n                        a\n                        \n                          0\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}4\\pi R_{\\oplus }^{2}\\sigma T_{\\oplus }^{4}&=\\pi R_{\\oplus }^{2}\\times E_{\\oplus }\\\\&=\\pi R_{\\oplus }^{2}\\times {\\frac {4\\pi R_{\\odot }^{2}\\sigma T_{\\odot }^{4}}{4\\pi a_{0}^{2}}}\\\\\\end{aligned}}}\n\nT⊕ can then be found:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  T\n                  \n                    ⊕\n                  \n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        R\n                        \n                          ⊙\n                        \n                        \n                          2\n                        \n                      \n                      \n                        T\n                        \n                          ⊙\n                        \n                        \n                          4\n                        \n                      \n                    \n                    \n                      4\n                      \n                        a\n                        \n                          0\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  T\n                  \n                    ⊕\n                  \n                \n              \n              \n                \n                =\n                \n                  T\n                  \n                    ⊙\n                  \n                \n                ×\n                \n                  \n                    \n                      \n                        R\n                        \n                          ⊙\n                        \n                      \n                      \n                        2\n                        \n                          a\n                          \n                            0\n                          \n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                5780\n                \n                \n                  \n                    K\n                  \n                \n                ×\n                \n                  \n                    \n                      \n                        6.957\n                        ×\n                        \n                          10\n                          \n                            8\n                          \n                        \n                        \n                        \n                          \n                            m\n                          \n                        \n                      \n                      \n                        2\n                        ×\n                        1.495\n                         \n                        978\n                         \n                        707\n                        ×\n                        \n                          10\n                          \n                            11\n                          \n                        \n                        \n                        \n                          \n                            m\n                          \n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                ≈\n                279\n                \n                \n                  \n                    K\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}T_{\\oplus }^{4}&={\\frac {R_{\\odot }^{2}T_{\\odot }^{4}}{4a_{0}^{2}}}\\\\T_{\\oplus }&=T_{\\odot }\\times {\\sqrt {\\frac {R_{\\odot }}{2a_{0}}}}\\\\&=5780\\;{\\rm {K}}\\times {\\sqrt {6.957\\times 10^{8}\\;{\\rm {m}} \\over 2\\times 1.495\\ 978\\ 707\\times 10^{11}\\;{\\rm {m}}}}\\\\&\\approx 279\\;{\\rm {K}}\\end{aligned}}}\n  \n\nwhere T⊙ is the temperature of the Sun, R⊙ the radius of the Sun, and a0 is the distance between the Earth and the Sun. This gives an effective temperature of 6 °C on the surface of the Earth, assuming that it perfectly absorbs all emission falling on it and has no atmosphere.\n\nThe Earth has an albedo of 0.3, meaning that 30% of the solar radiation that hits the planet gets scattered back into space without absorption.  The effect of albedo on temperature can be approximated by assuming that the energy absorbed is multiplied by 0.7, but that the planet still radiates as a black body (the latter by definition of effective temperature, which is what we are calculating).  This approximation reduces the temperature by a factor of 0.71/4, giving 255 K (−18 °C; −1 °F).[28][29]\n\nThe above temperature is Earth's as seen from space, not ground temperature but an average over all emitting bodies of Earth from surface to high altitude. Because of the greenhouse effect, the Earth's actual average surface temperature is about 288 K (15 °C; 59 °F), which is higher than the 255 K (−18 °C; −1 °F) effective temperature, and even higher than the 279 K (6 °C; 43 °F) temperature that a black body would have.\n\nIn the above discussion, we have assumed that the whole surface of the earth is at one temperature. Another interesting question is to ask what the temperature of a blackbody surface on the earth would be assuming that it reaches equilibrium with the sunlight falling on it. This of course depends on the angle of the sun on the surface and on how much air the sunlight has gone through. When the sun is at the zenith and the surface is horizontal, the irradiance can be as high as 1120 W/m2.[30] The Stefan–Boltzmann law then gives a temperature of\n\n  \n    \n      \n        T\n        =\n        \n          \n            (\n            \n              \n                \n                  1120\n                  \n                    \n                       W/m\n                    \n                    \n                      2\n                    \n                  \n                \n                σ\n              \n            \n            )\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n        ≈\n        375\n        \n           K\n        \n      \n    \n    {\\displaystyle T=\\left({\\frac {1120{\\text{ W/m}}^{2}}{\\sigma }}\\right)^{1/4}\\approx 375{\\text{ K}}}\n  \n\nor 102 °C (216 °F). (Above the atmosphere, the result is even higher: 394 K (121 °C; 250 °F).) We can think of the earth's surface as \"trying\" to reach equilibrium temperature during the day, but being cooled by the atmosphere, and \"trying\" to reach equilibrium with starlight and possibly moonlight at night, but being warmed by the atmosphere.\n\nThe fact that the energy density of the box containing radiation is proportional to \n  \n    \n      \n        \n          T\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle T^{4}}\n  \n can be derived using thermodynamics.[31][15] This derivation uses the relation between the radiation pressure p and the internal energy density \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n, a relation that can be shown using the form of the electromagnetic stress–energy tensor. This relation is:\n\n  \n    \n      \n        p\n        =\n        \n          \n            u\n            3\n          \n        \n        .\n      \n    \n    {\\displaystyle p={\\frac {u}{3}}.}\n\nNow, from the fundamental thermodynamic relation\n\n  \n    \n      \n        d\n        U\n        =\n        T\n        \n        d\n        S\n        −\n        p\n        \n        d\n        V\n        ,\n      \n    \n    {\\displaystyle dU=T\\,dS-p\\,dV,}\n  \n\nwe obtain the following expression, after dividing by \n  \n    \n      \n        d\n        V\n      \n    \n    {\\displaystyle dV}\n  \n and fixing \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n:\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  U\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        =\n        T\n        \n          \n            (\n            \n              \n                \n                  ∂\n                  S\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        −\n        p\n        =\n        T\n        \n          \n            (\n            \n              \n                \n                  ∂\n                  p\n                \n                \n                  ∂\n                  T\n                \n              \n            \n            )\n          \n          \n            V\n          \n        \n        −\n        p\n        .\n      \n    \n    {\\displaystyle \\left({\\frac {\\partial U}{\\partial V}}\\right)_{T}=T\\left({\\frac {\\partial S}{\\partial V}}\\right)_{T}-p=T\\left({\\frac {\\partial p}{\\partial T}}\\right)_{V}-p.}\n\nThe last equality comes from the following Maxwell relation:\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  S\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  ∂\n                  p\n                \n                \n                  ∂\n                  T\n                \n              \n            \n            )\n          \n          \n            V\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left({\\frac {\\partial S}{\\partial V}}\\right)_{T}=\\left({\\frac {\\partial p}{\\partial T}}\\right)_{V}.}\n\nFrom the definition of energy density it follows that\n\n  \n    \n      \n        U\n        =\n        u\n        V\n      \n    \n    {\\displaystyle U=uV}\n  \n\nwhere the energy density of radiation only depends on the temperature, therefore\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  U\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        =\n        u\n        \n          \n            (\n            \n              \n                \n                  ∂\n                  V\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        =\n        u\n        .\n      \n    \n    {\\displaystyle \\left({\\frac {\\partial U}{\\partial V}}\\right)_{T}=u\\left({\\frac {\\partial V}{\\partial V}}\\right)_{T}=u.}\n\nNow, the equality is\n\n  \n    \n      \n        u\n        =\n        T\n        \n          \n            (\n            \n              \n                \n                  ∂\n                  p\n                \n                \n                  ∂\n                  T\n                \n              \n            \n            )\n          \n          \n            V\n          \n        \n        −\n        p\n        ,\n      \n    \n    {\\displaystyle u=T\\left({\\frac {\\partial p}{\\partial T}}\\right)_{V}-p,}\n  \n after substitution of \n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  U\n                \n                \n                  ∂\n                  V\n                \n              \n            \n            )\n          \n          \n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left({\\frac {\\partial U}{\\partial V}}\\right)_{T}.}\n\nMeanwhile, the pressure is the rate of momentum change per unit area.  Since the momentum of a photon is the same as the energy divided by the speed of light,\n\n  \n    \n      \n        u\n        =\n        \n          \n            T\n            3\n          \n        \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  u\n                \n                \n                  ∂\n                  T\n                \n              \n            \n            )\n          \n          \n            V\n          \n        \n        −\n        \n          \n            u\n            3\n          \n        \n        ,\n      \n    \n    {\\displaystyle u={\\frac {T}{3}}\\left({\\frac {\\partial u}{\\partial T}}\\right)_{V}-{\\frac {u}{3}},}\n  \n\nwhere the factor 1/3 comes from the projection of the momentum transfer onto the normal to the wall of the container.\n\nSince the partial derivative \n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ∂\n                  u\n                \n                \n                  ∂\n                  T\n                \n              \n            \n            )\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle \\left({\\frac {\\partial u}{\\partial T}}\\right)_{V}}\n  \n can be expressed as a relationship between only \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n (if one isolates it on one side of the equality), the partial derivative can be replaced by the ordinary derivative. After separating the differentials the equality becomes\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              4\n              u\n            \n          \n        \n        =\n        \n          \n            \n              d\n              T\n            \n            T\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {du}{4u}}={\\frac {dT}{T}},}\n  \n\nwhich leads immediately to \n  \n    \n      \n        u\n        =\n        A\n        \n          T\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle u=AT^{4}}\n  \n, with \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n as some constant of integration.\n\nThe law can be derived by considering a small flat black body surface radiating out into a half-sphere. This derivation uses spherical coordinates, with θ as the zenith angle and φ as the azimuthal angle; and the small flat blackbody surface lies on the xy-plane, where θ = π/2.\n\nThe intensity of the light emitted from the blackbody surface is given by Planck's law,\n\n  \n    \n      \n        I\n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  h\n                  ν\n                  \n                    /\n                  \n                  (\n                  k\n                  T\n                  )\n                \n              \n              −\n              1\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I(\\nu ,T)={\\frac {2h\\nu ^{3}}{c^{2}}}{\\frac {1}{e^{h\\nu /(kT)}-1}},}\n  \n\nwhere\n\nThe quantity \n  \n    \n      \n        I\n        (\n        ν\n        ,\n        T\n        )\n         \n        A\n        cos\n        ⁡\n        θ\n         \n        d\n        ν\n         \n        d\n        Ω\n      \n    \n    {\\displaystyle I(\\nu ,T)~A\\cos \\theta ~d\\nu ~d\\Omega }\n  \n is the power radiated by a surface of area A through a solid angle dΩ in the frequency range between ν and ν + dν.\n\nThe Stefan–Boltzmann law gives the power emitted per unit area of the emitting body,\n\n  \n    \n      \n        \n          \n            P\n            A\n          \n        \n        =\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        I\n        (\n        ν\n        ,\n        T\n        )\n        \n        d\n        ν\n        ∫\n        cos\n        ⁡\n        θ\n        \n        d\n        Ω\n      \n    \n    {\\displaystyle {\\frac {P}{A}}=\\int _{0}^{\\infty }I(\\nu ,T)\\,d\\nu \\int \\cos \\theta \\,d\\Omega }\n\nNote that the cosine appears because black bodies are Lambertian (i.e. they obey Lambert's cosine law), meaning that the intensity observed along the sphere will be the actual intensity times the cosine of the zenith angle.\nTo derive the Stefan–Boltzmann law, we must integrate \n  \n    \n      \n        d\n        Ω\n        =\n        sin\n        ⁡\n        θ\n        \n        d\n        θ\n        \n        d\n        φ\n      \n    \n    {\\textstyle d\\Omega =\\sin \\theta \\,d\\theta \\,d\\varphi }\n  \n over the half-sphere and integrate \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n from 0 to ∞.\n\nP\n                    A\n                  \n                \n              \n              \n                \n                =\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    ∞\n                  \n                \n                I\n                (\n                ν\n                ,\n                T\n                )\n                \n                d\n                ν\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    2\n                    π\n                  \n                \n                \n                d\n                φ\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    π\n                    \n                      /\n                    \n                    2\n                  \n                \n                cos\n                ⁡\n                θ\n                sin\n                ⁡\n                θ\n                \n                d\n                θ\n              \n            \n            \n              \n              \n                \n                =\n                π\n                \n                  ∫\n                  \n                    0\n                  \n                  \n                    ∞\n                  \n                \n                I\n                (\n                ν\n                ,\n                T\n                )\n                \n                d\n                ν\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {P}{A}}&=\\int _{0}^{\\infty }I(\\nu ,T)\\,d\\nu \\int _{0}^{2\\pi }\\,d\\varphi \\int _{0}^{\\pi /2}\\cos \\theta \\sin \\theta \\,d\\theta \\\\&=\\pi \\int _{0}^{\\infty }I(\\nu ,T)\\,d\\nu \\end{aligned}}}\n\nThen we plug in for I:\n\n  \n    \n      \n        \n          \n            P\n            A\n          \n        \n        =\n        \n          \n            \n              2\n              π\n              h\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              ν\n              \n                3\n              \n            \n            \n              \n                e\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      k\n                      T\n                    \n                  \n                \n              \n              −\n              1\n            \n          \n        \n        \n        d\n        ν\n      \n    \n    {\\displaystyle {\\frac {P}{A}}={\\frac {2\\pi h}{c^{2}}}\\int _{0}^{\\infty }{\\frac {\\nu ^{3}}{e^{\\frac {h\\nu }{kT}}-1}}\\,d\\nu }\n\nTo evaluate this integral, do a substitution,\n\n  \n    \n      \n        \n          \n            \n              \n                u\n              \n              \n                \n                =\n                \n                  \n                    \n                      h\n                      ν\n                    \n                    \n                      k\n                      T\n                    \n                  \n                \n              \n            \n            \n              \n                d\n                u\n              \n              \n                \n                =\n                \n                  \n                    h\n                    \n                      k\n                      T\n                    \n                  \n                \n                \n                d\n                ν\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}u&={\\frac {h\\nu }{kT}}\\\\[6pt]du&={\\frac {h}{kT}}\\,d\\nu \\end{aligned}}}\n  \n\nwhich gives:\n\n  \n    \n      \n        \n          \n            P\n            A\n          \n        \n        =\n        \n          \n            \n              2\n              π\n              h\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  k\n                  T\n                \n                h\n              \n            \n            )\n          \n          \n            4\n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              u\n              \n                3\n              \n            \n            \n              \n                e\n                \n                  u\n                \n              \n              −\n              1\n            \n          \n        \n        \n        d\n        u\n        .\n      \n    \n    {\\displaystyle {\\frac {P}{A}}={\\frac {2\\pi h}{c^{2}}}\\left({\\frac {kT}{h}}\\right)^{4}\\int _{0}^{\\infty }{\\frac {u^{3}}{e^{u}-1}}\\,du.}\n\nThe integral on the right is standard and goes by many names: it is a particular case of a Bose–Einstein integral, the polylogarithm, or the Riemann zeta function \n  \n    \n      \n        ζ\n        (\n        s\n        )\n      \n    \n    {\\displaystyle \\zeta (s)}\n  \n. The value of the integral is \n  \n    \n      \n        Γ\n        (\n        4\n        )\n        ζ\n        (\n        4\n        )\n        =\n        \n          \n            \n              π\n              \n                4\n              \n            \n            15\n          \n        \n      \n    \n    {\\displaystyle \\Gamma (4)\\zeta (4)={\\frac {\\pi ^{4}}{15}}}\n  \n (where \n  \n    \n      \n        Γ\n        (\n        s\n        )\n      \n    \n    {\\displaystyle \\Gamma (s)}\n  \n is the Gamma function), giving the result that, for a perfect blackbody surface:\n\n  \n    \n      \n        \n          M\n          \n            ∘\n          \n        \n        =\n        σ\n        \n          T\n          \n            4\n          \n        \n         \n        ,\n         \n         \n        σ\n        =\n        \n          \n            \n              2\n              \n                π\n                \n                  5\n                \n              \n              \n                k\n                \n                  4\n                \n              \n            \n            \n              15\n              \n                c\n                \n                  2\n                \n              \n              \n                h\n                \n                  3\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                π\n                \n                  2\n                \n              \n              \n                k\n                \n                  4\n                \n              \n            \n            \n              60\n              \n                ℏ\n                \n                  3\n                \n              \n              \n                c\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle M^{\\circ }=\\sigma T^{4}~,~~\\sigma ={\\frac {2\\pi ^{5}k^{4}}{15c^{2}h^{3}}}={\\frac {\\pi ^{2}k^{4}}{60\\hbar ^{3}c^{2}}}.}\n\nFinally, this proof started out only considering a small flat surface. However, any differentiable surface can be approximated by a collection of small flat surfaces. So long as the geometry of the surface does not cause the blackbody to reabsorb its own radiation, the total energy radiated is just the sum of the energies radiated by each surface; and the total surface area is just the sum of the areas of each surface—so this law holds for all convex blackbodies, too, so long as the surface has the same temperature throughout.  The law extends to radiation from non-convex bodies by using the fact that the convex hull of a black body radiates as though it were itself a black body.\n\nThe total energy density U can be similarly calculated, except the integration is over the whole sphere and there is no cosine, and the energy flux (U c) should be divided by the velocity c to give the energy density U:\n\n  \n    \n      \n        U\n        =\n        \n          \n            1\n            c\n          \n        \n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        I\n        (\n        ν\n        ,\n        T\n        )\n        \n        d\n        ν\n        ∫\n        \n        d\n        Ω\n      \n    \n    {\\displaystyle U={\\frac {1}{c}}\\int _{0}^{\\infty }I(\\nu ,T)\\,d\\nu \\int \\,d\\Omega }\n  \n\nThus \n  \n    \n      \n        \n          ∫\n          \n            0\n          \n          \n            π\n            \n              /\n            \n            2\n          \n        \n        cos\n        ⁡\n        θ\n        sin\n        ⁡\n        θ\n        \n        d\n        θ\n      \n    \n    {\\textstyle \\int _{0}^{\\pi /2}\\cos \\theta \\sin \\theta \\,d\\theta }\n  \n is replaced by \n  \n    \n      \n        \n          ∫\n          \n            0\n          \n          \n            π\n          \n        \n        sin\n        ⁡\n        θ\n        \n        d\n        θ\n      \n    \n    {\\textstyle \\int _{0}^{\\pi }\\sin \\theta \\,d\\theta }\n  \n, giving an extra factor of 4.\n\nThus, in total:\n\n  \n    \n      \n        U\n        =\n        \n          \n            4\n            c\n          \n        \n        \n        σ\n        \n        \n          T\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle U={\\frac {4}{c}}\\,\\sigma \\,T^{4}}\n  \n\nThe product \n  \n    \n      \n        \n          \n            4\n            c\n          \n        \n        σ\n      \n    \n    {\\displaystyle {\\frac {4}{c}}\\sigma }\n  \n is sometimes known as the radiation constant or radiation density constant.[32][33]\n\nThe Stefan–Boltzmann law can be expressed as[34]\n\n  \n    \n      \n        \n          M\n          \n            ∘\n          \n        \n        =\n        σ\n        \n        \n          T\n          \n            4\n          \n        \n        =\n        \n          N\n          \n            \n              p\n              h\n              o\n              t\n            \n          \n        \n        \n        ⟨\n        \n          E\n          \n            \n              p\n              h\n              o\n              t\n            \n          \n        \n        ⟩\n      \n    \n    {\\displaystyle M^{\\circ }=\\sigma \\,T^{4}=N_{\\mathrm {phot} }\\,\\langle E_{\\mathrm {phot} }\\rangle }\n  \n\nwhere the flux of photons, \n  \n    \n      \n        \n          N\n          \n            \n              p\n              h\n              o\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {phot} }}\n  \n, is given by\n\n  \n    \n      \n        \n          N\n          \n            \n              p\n              h\n              o\n              t\n            \n          \n        \n        =\n        π\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              B\n              \n                ν\n              \n            \n            \n              h\n              ν\n            \n          \n        \n        \n        \n          d\n        \n        ν\n      \n    \n    {\\displaystyle N_{\\mathrm {phot} }=\\pi \\int _{0}^{\\infty }{\\frac {B_{\\nu }}{h\\nu }}\\,\\mathrm {d} \\nu }\n  \n\n\n  \n    \n      \n        \n          N\n          \n            \n              p\n              h\n              o\n              t\n            \n          \n        \n        =\n        \n          (\n          \n            \n              1.5205\n              ×\n              \n                10\n                \n                  15\n                \n              \n            \n            \n            \n              \n                photons\n              \n            \n            \n              ⋅\n            \n            \n              \n                \n                  s\n                \n              \n              \n                −\n                1\n              \n            \n            \n              ⋅\n            \n            \n              \n                \n                  m\n                \n              \n              \n                −\n                2\n              \n            \n            \n              ⋅\n            \n            \n              \n                K\n              \n              \n                −\n                3\n              \n            \n          \n          )\n        \n        ⋅\n        \n          T\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle N_{\\mathrm {phot} }=\\left({1.5205\\times 10^{15}}\\;{\\textrm {photons}}{\\cdot }{\\textrm {s}}^{-1}{\\cdot }{\\textrm {m}}^{-2}{\\cdot }\\mathrm {K} ^{-3}\\right)\\cdot T^{3}}\n  \n\nand the average energy per photon,\n  \n    \n      \n        ⟨\n        \n          E\n          \n            \n              phot\n            \n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle E_{\\textrm {phot}}\\rangle }\n  \n, is given by\n\n  \n    \n      \n        ⟨\n        \n          E\n          \n            \n              phot\n            \n          \n        \n        ⟩\n        =\n        \n          \n            \n              π\n              \n                4\n              \n            \n            \n              30\n              \n              ζ\n              (\n              3\n              )\n            \n          \n        \n        k\n        \n        T\n        =\n        \n          (\n          \n            \n              3.7294\n              ×\n              \n                10\n                \n                  −\n                  23\n                \n              \n            \n            \n              J\n            \n            \n              ⋅\n            \n            \n              \n                K\n              \n              \n                −\n                1\n              \n            \n          \n          )\n        \n        ⋅\n        T\n        \n        .\n      \n    \n    {\\displaystyle \\langle E_{\\textrm {phot}}\\rangle ={\\frac {\\pi ^{4}}{30\\,\\zeta (3)}}k\\,T=\\left({3.7294\\times 10^{-23}}\\mathrm {J} {\\cdot }\\mathrm {K} ^{-1}\\right)\\cdot T\\,.}\n\nMarr and Wilkin (2012) recommend that students be taught about \n  \n    \n      \n        ⟨\n        \n          E\n          \n            \n              phot\n            \n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle E_{\\textrm {phot}}\\rangle }\n  \n instead of being taught Wien's displacement law, and that the above decomposition be taught when the Stefan–Boltzmann law is taught.[34]",
        pageTitle: "Stefan–Boltzmann law",
    },
    {
        title: "Stein's law",
        link: "https://en.wikipedia.org/wiki/Stein%27s_law",
        content:
            'Herbert Stein (August 27, 1916 – September 8, 1999) was an American economist, a senior fellow at the American Enterprise Institute, and a member of the board of contributors of The Wall Street Journal. He was the chairman of the Council of Economic Advisers under Richard Nixon and Gerald Ford. From 1974 to 1984, he was the A. Willis Robertson Professor of Economics at the University of Virginia.[1]\n\nStein was born on August 27, 1916, in Detroit, Michigan, and his family moved to New York during the Great Depression.  He enrolled in Williams College just before he turned 16. After graduating with Phi Beta Kappa honors, he went to Washington, DC, to work as an economist in various agencies. He received his PhD in economics from the University of Chicago in 1958.[2]\n\nStein, who died September 8, 1999, in Washington, DC, was married to Mildred Stein, who died in 1997 after 61 years of marriage. He is the father of the lawyer, author, and actor Ben Stein and the writer Rachel Stein. Herbert Stein was also the original writer for the advice column Dear Prudence.\n\nStein was known as a pragmatic conservative and was referred to as "a liberal\'s conservative and a conservative\'s liberal."[3] He was the author of The Fiscal Revolution in America.\n\nIn one article, Stein wrote that the people who wore an "Adam Smith necktie" did so to:\n\nmake a statement of their devotion to the idea of free markets and limited government. What stands out in [Smith\'s seminal work] Wealth of Nations, however, is that their patron saint was not pure or doctrinaire about this idea. He viewed government intervention in the market with great skepticism. He regarded his exposition of the virtues of the free market as his main contribution to policy, and the purpose for which his economic analysis was developed. Yet he was prepared to accept or propose qualifications to that policy in the specific cases where he judged that their net effect would be beneficial and would not undermine the basically free character of the system.[4]\n\nStein propounded Stein\'s Law, which he expressed in 1986 as "If something cannot go on forever, it will stop."[5][6] Stein observed this logic in analyzing economic trends (such as rising US federal debt in proportion to GDP, or increasing international balance of payments deficits, in his analysis): if such a process is limited by external factors, there is no urgency for government intervention to stop it, much less to make it stop immediately, but it will stop of its own accord.[7] A paraphrase, not attributed to Stein, is "Trends that can\'t continue indefinitely won\'t."',
        pageTitle: "Herbert Stein",
    },
    {
        title: "Stevens's power law",
        link: "https://en.wikipedia.org/wiki/Stevens%27s_power_law",
        content:
            "Stevens' power law is an empirical relationship in psychophysics between an increased intensity or strength in a physical stimulus and the perceived magnitude increase in the sensation created by the stimulus. It is often considered to supersede the Weber–Fechner law, which is based on a logarithmic relationship between stimulus and sensation, because the power law describes a wider range of sensory comparisons, down to zero intensity.[1]\n\nThe theory is named after psychophysicist Stanley Smith Stevens (1906–1973).  Although the idea of a power law had been suggested by 19th-century researchers, Stevens is credited with reviving the law and publishing a body of psychophysical data to support it in 1957.\n\nwhere I is the intensity or strength of the stimulus in physical units (energy, weight, pressure, mixture proportions, etc.), ψ(I) is the magnitude of the sensation evoked by the stimulus, a is an exponent that depends on the type of stimulation or sensory modality, and k is a proportionality constant that depends on the units used.\n\nA distinction has been made between local psychophysics, where stimuli can only be discriminated with a probability around 50%, and global psychophysics, where the stimuli can be discriminated correctly with near certainty (Luce & Krumhansl, 1988). The Weber–Fechner law and methods described by L. L. Thurstone are generally applied in local psychophysics, whereas Stevens' methods are usually applied in global psychophysics.\n\nThe adjacent table lists the exponents reported by Stevens.\n\nThe principal methods used by Stevens to measure the perceived intensity of a stimulus were magnitude estimation and magnitude production. In magnitude estimation with a standard, the experimenter presents a stimulus called a standard and assigns it a number called the modulus. For subsequent stimuli, subjects report numerically their perceived intensity relative to the standard so as to preserve the ratio between the sensations and the numerical estimates (e.g., a sound perceived twice as loud as the standard should be given a number twice the modulus). In magnitude estimation without a standard (usually just magnitude estimation), subjects are free to choose their own standard, assigning any number to the first stimulus and all subsequent ones with the only requirement being that the ratio between sensations and numbers is preserved. In magnitude production a number and a reference stimulus is given and subjects produce a stimulus that is perceived as that number times the reference. Also used is cross-modality matching, which generally involves subjects altering the magnitude of one physical quantity, such as the brightness of a light, so that its perceived intensity is equal to the perceived intensity of another type of quantity, such as warmth or pressure.\n\nStevens generally collected magnitude estimation data from multiple observers, averaged the data across subjects, and then fitted a power function to the data.  Because the fit was generally reasonable, he concluded the power law was correct.\n\nA principal criticism has been that Stevens' approach provides neither a direct test of the power law itself nor the underlying assumptions of the magnitude estimation/production method: it simply fits curves to data points. In addition, the power law can be deduced mathematically from the Weber-Fechner logarithmic function (Mackay, 1963[2]), and the relation makes predictions consistent with data (Staddon, 1978[3]). As with all psychometric studies, Stevens' approach ignores individual differences in the stimulus-sensation relationship, and there are generally large individual differences in this relationship that averaging the data will obscure (Greem & Luce 1974).\n\nStevens' main assertion was that using magnitude estimations/productions respondents were able to make judgements on a ratio scale (i.e., if x and y are values on a given ratio scale, then there exists a constant k such that x = ky). In the context of axiomatic psychophysics, (Narens 1996) formulated a testable property capturing the implicit underlying assumption this assertion entailed. Specifically, for two proportions p and q, and three stimuli, x, y, z, if y is judged p times x, z is judged q times y, then t = pq times x should be equal to z. This amounts to assuming that respondents interpret numbers in a veridical way. This property was unambiguously rejected (Ellermeier & Faulhammer 2000, Zimmer 2005). Without assuming veridical interpretation of numbers, (Narens 1996) formulated another property that, if sustained, meant that respondents could make ratio scaled judgments, namely, if y is judged p times x, z is judged q times y, and if y' is judged q times x, z' is judged p times y', then z should equal z'. This property has been sustained in a variety of situations (Ellermeier & Faulhammer 2000, Zimmer 2005).\n\nCritics of the power law also point out that the validity of the law is contingent on the measurement of perceived stimulus intensity that is employed in the relevant experiments. Luce (2002), under the condition that respondents' numerical distortion function and the psychophysical functions could be separated, formulated a behavioral condition equivalent to the psychophysical function being a power function. This condition was confirmed for just over half the respondents, and the power form was found to be a reasonable approximation for the rest (Steingrimsson & Luce 2006).\n\nIt has also been questioned, particularly in terms of signal detection theory, whether any given stimulus is actually associated with a particular and absolute perceived intensity; i.e. one that is independent of contextual factors and conditions. Consistent with this, Luce (1990, p. 73) observed that \"by introducing contexts such as background noise in loudness judgements, the shape of the magnitude estimation functions certainly deviates sharply from a power function\". Indeed, nearly all sensory judgments can be changed by the context in which a stimulus is perceived.",
        pageTitle: "Stevens's power law",
    },
    {
        title: "Weber–Fechner law",
        link: "https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law",
        content:
            "The Weber–Fechner laws are two related scientific laws in the field of psychophysics, known as Weber's law and Fechner's law. Both relate to human perception, more specifically the relation between the actual change in a physical stimulus and the perceived change. This includes stimuli to all senses: vision, hearing, taste, touch, and smell.\n\nErnst Heinrich Weber states that \"the minimum increase of stimulus which will produce a perceptible increase of sensation is proportional to the pre-existent stimulus,\" while Gustav Fechner's law is an inference from Weber's law (with additional assumptions) which states that the intensity of our sensation increases as the logarithm of an increase in energy rather than as rapidly as the increase.[1]\n\nBoth Weber's law and Fechner's law were formulated by Gustav Theodor Fechner (1801–1887). They were first published in 1860 in the work Elemente der Psychophysik (Elements of Psychophysics). This publication was the first work ever in this field, and where Fechner coined the term psychophysics to describe the interdisciplinary study of how humans perceive physical magnitudes.[2] He made the claim that \"...psycho-physics is an exact doctrine of the relation of function or dependence between body and soul.\"[3]\n\nErnst Heinrich Weber (1795–1878) was one of the first persons to approach the study of the human response to a physical stimulus in a quantitative fashion. Fechner was a student of Weber and named his first law in honor of his mentor, since it was Weber who had conducted the experiments needed to formulate the law.[4]\n\nFechner formulated several versions of the law, all communicating the same idea. One formulation states:\n\nSimple differential sensitivity is inversely proportional to the size of the components of the difference; relative differential sensitivity remains the same regardless of size.[2]\n\nWhat this means is that the perceived change in stimuli is inversely proportional to the initial stimuli.\n\nWeber's law also incorporates the just-noticeable difference (JND). This is the smallest change in stimuli that can be perceived. As stated above, the JND dS is proportional to the initial stimuli intensity S. Mathematically, it can be described as \n  \n    \n      \n        d\n        S\n        =\n        K\n        ⋅\n        S\n      \n    \n    {\\displaystyle dS=K\\cdot S}\n  \n\nwhere \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the reference stimulus and \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n is a constant.[5] When plotted, this relation is a straight line with zero intercept. It may be written as Ψ = k logS, with Ψ being the sensation, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n being a constant, and \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n being the physical intensity of the stimulus.\n\nWeber's law always fails at low intensities, near and below the absolute detection threshold, and often also at high intensities, but may be approximately true across a wide middle range of intensities.[6]\n\nAlthough Weber's law includes a statement of the proportionality of a perceived change to initial stimuli, Weber only refers to this as a rule of thumb regarding human perception. It was Fechner who formulated this statement as a mathematical expression referred to as Weber contrast.[2][7][8][9]\n\nd\n        p\n        =\n        \n          \n            \n              d\n              S\n            \n            S\n          \n        \n        \n        \n      \n    \n    {\\displaystyle dp={\\frac {dS}{S}}\\,\\!}\n  \n\nWeber contrast is not part of Weber's law.[2][7]\n\nFechner noticed in his own studies that different individuals have different sensitivity to certain stimuli. For example, the ability to perceive differences in light intensity could be related to how good that individual's vision is.[2] He also noted that how the human sensitivity to stimuli changes depends on which sense is affected. He used this to formulate another version of Weber's law that he named die Maßformel, the \"measurement formula\". Fechner's law states that the subjective sensation is proportional to the logarithm of the stimulus intensity. According to this law, human perceptions of sight and sound work as follows: Perceived loudness/brightness is proportional to logarithm of the actual intensity measured with an accurate nonhuman instrument.[7]\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              S\n              \n                0\n              \n            \n          \n        \n        \n        \n      \n    \n    {\\displaystyle p=k\\ln {\\frac {S}{S_{0}}}\\,\\!}\n\nThe relationship between stimulus and perception is logarithmic. This logarithmic relationship means that if a stimulus varies as a geometric progression (i.e., multiplied by a fixed factor), the corresponding perception is altered in an arithmetic progression (i.e., in additive constant amounts). For example, if a stimulus is tripled in strength (i.e., 3 × 1), the corresponding perception may be two times as strong as its original value (i.e., 1 + 1). If the stimulus is again tripled in strength (i.e., 3 × 3 × 1), the corresponding perception will be three times as strong as its original value (i.e., 1 + 1 + 1). Hence, for multiplications in stimulus strength, the strength of perception only adds. The mathematical derivations of the torques on a simple beam balance produce a description that is strictly compatible with Weber's law.[10][11]\n\nSince Weber's law fails at low intensity, so does Fechner's law.[6]\n\nAn early reference to \"Fechner's ... law\" was in 1875 by Ludimar Hermann  in Elements of Human Physiology.[12]\n\nFechner's law is a mathematical derivation of Weber contrast.\n\nd\n        p\n        =\n        k\n        \n          \n            \n              d\n              S\n            \n            S\n          \n        \n      \n    \n    {\\displaystyle dp=k{\\frac {dS}{S}}}\n\nIntegrating the mathematical expression for Weber contrast gives:\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          S\n        \n        +\n        C\n      \n    \n    {\\displaystyle p=k\\ln {S}+C}\n\nwhere \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a constant of integration and ln is the natural logarithm.\n\nTo solve for \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, assume that the perceived stimulus becomes zero at some threshold stimulus \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle S_{0}}\n  \n. Using this as a constraint, set \n  \n    \n      \n        p\n        =\n        0\n      \n    \n    {\\displaystyle p=0}\n  \n and \n  \n    \n      \n        S\n        =\n        \n          S\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle S=S_{0}}\n  \n. This gives:\n\nC\n        =\n        −\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              0\n            \n          \n        \n      \n    \n    {\\displaystyle C=-k\\ln {S_{0}}}\n\nSubstituting \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n in the integrated expression for Weber's law, the expression can be written as:\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              S\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p=k\\ln {\\frac {S}{S_{0}}}}\n\nThe constant k is sense-specific and must be determined depending on the sense and type of stimulus.[7]\n\nWeber and Fechner conducted research on differences in light intensity and the perceived difference in weight.[2] Other sense modalities provide only mixed support for either Weber's law or Fechner's law.\n\nWeber found that the just noticeable difference (JND) between two weights was approximately proportional to the weights. Thus, if the weight of 105 g can (only just) be distinguished from that of 100 g, the JND (or differential threshold) is 5 g. If the mass is doubled, the differential threshold also doubles to 10 g, so that 210 g can be distinguished from 200 g. In this example, a weight (any weight) seems to have to increase by 5% for someone to be able to reliably detect the increase, and this minimum required fractional increase (of 5/100 of the original weight) is referred to as the \"Weber fraction\" for detecting changes in weight.  Other discrimination tasks, such as detecting changes in brightness, or in tone height (pure tone frequency), or in the length of a line shown on a screen, may have different Weber fractions, but they all obey Weber's law in that observed values need to change by at least some small but constant proportion of the current value to ensure human observers will reliably be able to detect that change.\n\nFechner did not conduct any experiments on how perceived heaviness increased with the mass of the stimulus. Instead, he assumed that all JNDs are subjectively equal, and argued mathematically that this would produce a logarithmic relation between the stimulus intensity and the sensation. These assumptions have both been questioned.[13][14] \nFollowing the work of S. S. Stevens, many researchers came to believe in the 1960s that the Stevens's power law was a more general psychophysical principle than Fechner's logarithmic law.\n\nWeber's law does not quite hold for loudness. It is a fair approximation for higher intensities, but not for lower amplitudes.[15]\n\nWeber's law does not hold at perception of higher intensities. Intensity discrimination improves at higher intensities.  The first demonstration of the phenomena was presented by Riesz in 1928, in Physical Review. This deviation of the Weber's law is known as the \"near miss\" of the Weber's law.  This term was coined by McGill and Goldberg in their paper of 1968 in Perception & Psychophysics. Their study consisted of intensity discrimination in pure tones. Further studies have shown that the near miss is observed in noise stimuli as well. Jesteadt et al. (1977)[16] demonstrated that the near miss holds across all the frequencies, and that the intensity  discrimination is not a function of frequency, and that the change in discrimination with level can be represented by a single function across all frequencies: \n  \n    \n      \n        Δ\n        I\n        \n          /\n        \n        I\n        =\n        0.463\n        \n          \n            (\n            I\n            \n              /\n            \n            \n              I\n              \n                0\n              \n            \n            )\n          \n          \n            −\n            0.072\n          \n        \n      \n    \n    {\\displaystyle \\Delta I/I=0.463{(I/I_{0})}^{-0.072}}\n  \n.[16]\n\nThe eye senses brightness approximately logarithmically over a moderate range and stellar magnitude is measured on a logarithmic scale.[17]\nThis magnitude scale was invented by the ancient Greek astronomer Hipparchus in about 150 B.C. He ranked the stars he could see in terms of their brightness, with 1 representing the brightest down to 6 representing the faintest, though now the scale has been extended beyond these limits; an increase in 5 magnitudes corresponds to a decrease in brightness by a factor of 100.[17]\nModern researchers have attempted to incorporate such perceptual effects into mathematical models of vision.[18][19]\n\nPerception of Glass patterns[20] and mirror symmetries in the presence of noise follows Weber's law in the middle range of regularity-to-noise ratios (S), but in both outer ranges, sensitivity to variations is disproportionally lower. As Maloney, Mitchison, & Barlow (1987)[21] showed for Glass patterns, and as van der Helm (2010)[22] showed for mirror symmetries, perception of these visual regularities in the whole range of regularity-to-noise ratios follows the law p = g/(2+1/S) with parameter g to be estimated using experimental data.\n\nFor vision, Weber's law implies constancy of luminance contrast. Suppose a target object is set against a background luminance \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. In order to be just visible, the target must be brighter or fainter than the background by some small amount \n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n. The Weber contrast is defined as \n  \n    \n      \n        C\n        =\n        Δ\n        B\n        \n          /\n        \n        B\n      \n    \n    {\\displaystyle C=\\Delta B/B}\n  \n, and Weber's law says that \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n should be constant for all \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n.\n\nHuman vision follows Weber's law closely at normal daylight levels (i.e. in the photopic range) but begins to break down at twilight levels (the mesopic range) and is completely inapplicable at low light levels (scotopic vision). This can be seen in data collected by Blackwell[23] and plotted by Crumey,[24] showing threshold increment log\n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n versus background luminance log\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n for various targets sizes. At daylight levels, the curves are approximately straight with slope 1, i.e. log\n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n = log\n  \n    \n      \n        B\n        +\n        c\n        o\n        n\n        s\n        t\n        .\n      \n    \n    {\\displaystyle B+const.}\n  \n, implying \n  \n    \n      \n        C\n        =\n        Δ\n        B\n        \n          /\n        \n        B\n      \n    \n    {\\displaystyle C=\\Delta B/B}\n  \n is constant. At the very darkest background levels (\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n ≲ 10− 5 cd m−2, approximately 25 mag arcsec−2)[24] the curves are flat - this is where the only visual perception is the observer's own neural noise ('dark light'). In the intermediate range, a portion can be approximated by the De Vries - Rose law, related to Ricco's law.\n\nActivation of neurons by sensory stimuli in many parts of the brain is by a proportional law: neurons change their spike rate by about 10–30%, when a stimulus (e.g. a natural scene for vision) has been applied. However, as Scheler (2017)[25] showed,\nthe population distribution of the intrinsic excitability or gain of a neuron is a heavy tail distribution, more precisely a lognormal shape, which is equivalent to a logarithmic coding scheme. Neurons may therefore spike with 5–10 fold different mean rates. Obviously, this increases the dynamic range of a neuronal population, while stimulus-derived changes remain small and linear proportional.\n\nAn analysis[26] of the length of comments in internet discussion boards across several languages shows that comment lengths obey the lognormal distribution with great precision. The authors explain the distribution as a manifestation of the Weber–Fechner law.\n\nThe Weber–Fechner law has been applied in other fields of research than just the human senses.\n\nPsychological studies show that it becomes increasingly difficult to discriminate between two numbers as the difference between them decreases. This is called the distance effect.[27][28] This is important in areas of magnitude estimation, such as dealing with large scales and estimating distances.  It may also play a role in explaining why consumers neglect to shop around to save a small percentage on a large purchase, but will shop around to save a large percentage on a small purchase which represents a much smaller absolute dollar amount.[29]\n\nIt has been hypothesized that dose-response relationships can follow Weber's Law[30] which suggests this law – which is often applied at the sensory level – originates from underlying chemoreceptor responses to cellular signaling dose relationships within the body. Dose response can be related to the Hill equation, which is closer to a power law.\n\nThere is a new branch of the literature on public finance hypothesizing that the Weber–Fechner law can explain the increasing levels of public expenditures in mature democracies. Election after election, voters demand more public goods to be effectively impressed; therefore, politicians try to increase the magnitude of this \"signal\" of competence – the size and composition of public expenditures – in order to collect more votes.[31]\n\nPreliminary research has found that pleasant emotions adhere to Weber’s Law, with accuracy in judging their intensity decreasing as pleasantness increases. However, this pattern wasn't observed for unpleasant emotions, suggesting a survival-related need for accurately discerning high-intensity negative emotions.[32]",
        pageTitle: "Weber–Fechner law",
    },
    {
        title: "Stigler's law of eponymy",
        link: "https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy",
        content:
            'Stigler\'s law of eponymy, proposed by University of Chicago statistics professor Stephen Stigler in his 1980 publication "Stigler\'s law of eponymy",[1] states that "no scientific discovery is named after its original discoverer." Examples include Hubble\'s law, which was derived by Georges Lemaître two years before Edwin Hubble; the Pythagorean theorem, which was known to Babylonian mathematicians before Pythagoras; and Halley\'s Comet, which was observed by astronomers since at least 240 BC (although its official designation is due to the first ever mathematical prediction of such astronomical phenomenon in the sky, not to its discovery).\n\nStigler attributed the discovery of Stigler\'s law to sociologist Robert K. Merton, from whom Stigler stole credit so that it would be an example of the law. The same observation had previously also been made by many others.[2]\n\nHistorical acclaim for discoveries is often assigned to persons of note who bring attention to an idea that is not yet widely known, whether or not that person was its original inventor – theories may be named long after their discovery. In the case of eponymy, the idea becomes named after that person, even if that person is acknowledged by historians of science not to be the one who discovered it. Often, several people will arrive at a new idea around the same time, as in the case of calculus. It can be dependent on the publicity of the new work and the fame of its publisher as to whether the scientist\'s name becomes historically associated.\n\nIt takes a thousand men to invent a telegraph, or a steam engine, or a phonograph, or a photograph, or a telephone or any other important thing—and the last man gets the credit and we forget the others. He added his little mite—that is all he did. These object lessons should teach us that ninety-nine parts of all things that proceed from the intellect are plagiarisms, pure and simple; and the lesson ought to make us modest. But nothing can do that.[3]\n\nStephen Stigler\'s father, the economist George Stigler, also examined the process of discovery in economics. He said, "If an earlier, valid statement of a theory falls on deaf ears, and a later restatement is accepted by the science, this is surely proof that the science accepts ideas only when they fit into the then-current state of the science." He gave several examples in which the original discoverer was not recognized as such.[4] Similar arguments were made in regards to accepted ideas relative to the state of science by Thomas Kuhn in The Structure of Scientific Revolutions.[5]\n\nThe Matthew effect was coined by Robert K. Merton to describe how eminent scientists get more credit than a comparatively unknown researcher, even if their work is similar, so that credit will usually be given to researchers who are already famous. Merton notes: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\n\nThis pattern of recognition, skewed in favor of the established scientist, appears principally\n\n(ii) in cases of independent multiple discoveries made by scientists of distinctly different rank.[6]\n\nThe effect applies specifically to women through the Matilda effect.\n\nBoyer\'s law was named by Hubert Kennedy in 1972. It says, "Mathematical formulas and theorems are usually not named after their original discoverers" and was named after Carl Boyer, whose book A History of Mathematics contains many examples of this law. Kennedy observed that "it is perhaps interesting to note that this is probably a rare instance of a law whose statement confirms its own validity".[7]\n\n"Everything of importance has been said before by somebody who did not discover it" is an adage attributed to Alfred North Whitehead.[8]',
        pageTitle: "Stigler's law of eponymy",
    },
    {
        title: "Stokes's law",
        link: "https://en.wikipedia.org/wiki/Stokes%27s_law",
        content:
            "In fluid dynamics, Stokes' law gives the frictional force – also called drag force – exerted on spherical objects moving at very small Reynolds numbers in a viscous fluid.[1]  It was derived by George Gabriel Stokes in 1851 by solving the Stokes flow limit for small Reynolds numbers of the Navier–Stokes equations.[2]\n\nThe force of viscosity on a small sphere moving through a viscous fluid is given by:[3][4]\n\nStokes' law makes the following assumptions for the behavior of a particle in a fluid:\n\nDepending on desired accuracy, the failure to meet these assumptions may or may not require the use of a more complicated model. To 10% error, for instance, velocities need be limited to those giving Re < 1.\n\nFor molecules Stokes' law is used to define their Stokes radius and diameter.\n\nThe CGS unit of kinematic viscosity was named \"stokes\" after his work.\n\nStokes' law is the basis of the falling-sphere viscometer, in which the fluid is stationary in a vertical glass tube. A sphere of known size and density is allowed to descend through the liquid. If correctly selected, it reaches terminal velocity, which can be measured by the time it takes to pass two marks on the tube. Electronic sensing can be used for opaque fluids. Knowing the terminal velocity, the size and density of the sphere, and the density of the liquid, Stokes' law can be used to calculate the viscosity of the fluid. A series of steel ball bearings of different diameters are normally used in the classic experiment to improve the accuracy of the calculation. The school experiment uses glycerine or golden syrup as the fluid, and the technique is used industrially to check the viscosity of fluids used in processes. Several school experiments often involve varying the temperature and/or concentration of the substances used in order to demonstrate the effects this has on the viscosity. Industrial methods include many different oils, and polymer liquids such as solutions.\n\nThe importance of Stokes' law is illustrated by the fact that it played a critical role in the research leading to at least three Nobel Prizes.[5]\n\nStokes' law is important for understanding the swimming of microorganisms and sperm; also, the sedimentation of small particles and organisms in water, under the force of gravity.[5]\n\nIn air, the same theory can be used to explain why small water droplets (or ice crystals) can remain suspended in air (as clouds) until they grow to a critical size and start falling as rain (or snow and hail).[6] Similar use of the equation can be made in the settling of fine particles in water or other fluids.[citation needed]\n\nAt terminal (or settling) velocity, the excess force Fe due to the difference between the weight and buoyancy of the sphere (both caused by gravity[7]) is given by:\n\nRequiring the force balance Fd = Fe and solving for the velocity v gives the terminal velocity vs. Note that since the excess force increases as R3 and Stokes' drag increases as R, the terminal velocity increases as R2 and thus varies greatly with particle size as shown below. If a particle only experiences its own weight while falling in a viscous fluid, then a terminal velocity is reached when the sum of the frictional and the buoyant forces on the particle due to the fluid exactly balances the gravitational force. This velocity v [m/s] is given by:[7]\n\nIn Stokes flow, at very low Reynolds number, the convective acceleration terms in the Navier–Stokes equations are neglected. Then the flow equations become, for an incompressible steady flow:[8]\n\nBy using some vector calculus identities, these equations can be shown to result in Laplace's equations for the pressure and each of the components of the vorticity vector:[8]\n\nAdditional forces like those by gravity and buoyancy have not been taken into account, but can easily be added since the above equations are linear, so linear superposition of solutions and associated forces can be applied.\n\nFor the case of a sphere in a uniform far field flow, it is advantageous to use a cylindrical coordinate system (r, φ, z). The z–axis is through the centre of the sphere and aligned with the mean flow direction, while r is the radius as measured perpendicular to the z–axis. The origin is at the sphere centre. Because the flow is axisymmetric around the z–axis, it is independent of the azimuth φ.\n\nIn this cylindrical coordinate system, the incompressible flow can be described with a Stokes stream function ψ, depending on r and z:[9][10]\n\nwith ur and uz the flow velocity components in the r and z direction, respectively. The azimuthal velocity component in the φ–direction is equal to zero, in this axisymmetric case. The volume flux, through a tube bounded by a surface of some constant value ψ, is equal to 2πψ and is constant.[9]\n\nFor this case of an axisymmetric flow, the only non-zero component of the vorticity vector ω is the azimuthal φ–component ωφ[11][12]\n\nThe Laplace operator, applied to the vorticity ωφ, becomes in this cylindrical coordinate system with axisymmetry:[12]\n\nFrom the previous two equations, and with the appropriate boundary conditions, for a far-field uniform-flow velocity u in the z–direction and a sphere of radius R, the solution is found to be[13]\n\nThe solution of velocity in cylindrical coordinates and components follows as:\n\nThe solution of vorticity in cylindrical coordinates follows as:\n\nThe solution of pressure in cylindrical coordinates follows as:\n\nThe solution of pressure in spherical coordinates follows as:\n\nThe formula of pressure is also called dipole potential analogous to the concept in electrostatics.\n\nA more general formulation, with arbitrary far-field velocity-vector \n  \n    \n      \n        \n          \n            u\n          \n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {u} _{\\infty }}\n  \n, in cartesian coordinates \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        x\n        ,\n        y\n        ,\n        z\n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} =(x,y,z)^{T}}\n  \n follows with:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  u\n                \n                (\n                \n                  x\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \n                          \n                            \n                              \n                                \n                                  \n                                    \n                                      R\n                                      \n                                        3\n                                      \n                                    \n                                    4\n                                  \n                                \n                                ⋅\n                                \n                                  (\n                                  \n                                    \n                                      \n                                        \n                                          3\n                                          \n                                            (\n                                            \n                                              \n                                                \n                                                  u\n                                                \n                                                \n                                                  ∞\n                                                \n                                              \n                                              ⋅\n                                              \n                                                x\n                                              \n                                            \n                                            )\n                                          \n                                          ⋅\n                                          \n                                            x\n                                          \n                                        \n                                        \n                                          ‖\n                                          \n                                            x\n                                          \n                                          \n                                            ‖\n                                            \n                                              5\n                                            \n                                          \n                                        \n                                      \n                                    \n                                    −\n                                    \n                                      \n                                        \n                                          \n                                            u\n                                          \n                                          \n                                            ∞\n                                          \n                                        \n                                        \n                                          ‖\n                                          \n                                            x\n                                          \n                                          \n                                            ‖\n                                            \n                                              3\n                                            \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  )\n                                \n                              \n                              ⏟\n                            \n                          \n                          \n                            \n                              conservative: curl=0,\n                            \n                             \n                            \n                              ∇\n                              \n                                2\n                              \n                            \n                            \n                              u\n                            \n                            =\n                            0\n                          \n                        \n                        +\n                        \n                          \n                            \n                              \n                                \n                                  u\n                                \n                                \n                                  ∞\n                                \n                              \n                              ⏟\n                            \n                          \n                          \n                            far-field\n                          \n                        \n                      \n                      ⏟\n                    \n                  \n                  \n                    Terms of Boundary-Condition\n                  \n                \n                \n                \n                  \n                    \n                      \n                        −\n                        \n                          \n                            \n                              3\n                              R\n                            \n                            4\n                          \n                        \n                        ⋅\n                        \n                          (\n                          \n                            \n                              \n                                \n                                  \n                                    u\n                                  \n                                  \n                                    ∞\n                                  \n                                \n                                \n                                  ‖\n                                  \n                                    x\n                                  \n                                  ‖\n                                \n                              \n                            \n                            +\n                            \n                              \n                                \n                                  \n                                    (\n                                    \n                                      \n                                        \n                                          u\n                                        \n                                        \n                                          ∞\n                                        \n                                      \n                                      ⋅\n                                      \n                                        x\n                                      \n                                    \n                                    )\n                                  \n                                  ⋅\n                                  \n                                    x\n                                  \n                                \n                                \n                                  ‖\n                                  \n                                    x\n                                  \n                                  \n                                    ‖\n                                    \n                                      3\n                                    \n                                  \n                                \n                              \n                            \n                          \n                          )\n                        \n                      \n                      ⏟\n                    \n                  \n                  \n                    \n                      non-conservative: curl\n                    \n                    =\n                    \n                      ω\n                    \n                    (\n                    \n                      x\n                    \n                    )\n                    ,\n                     \n                    μ\n                    \n                      ∇\n                      \n                        2\n                      \n                    \n                    \n                      u\n                    \n                    =\n                    ∇\n                    p\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  [\n                  \n                    \n                      \n                        \n                          3\n                          \n                            R\n                            \n                              3\n                            \n                          \n                        \n                        4\n                      \n                    \n                    \n                      \n                        \n                          x\n                          ⊗\n                          \n                            x\n                          \n                        \n                        \n                          ‖\n                          \n                            x\n                          \n                          \n                            ‖\n                            \n                              5\n                            \n                          \n                        \n                      \n                    \n                    −\n                    \n                      \n                        \n                          R\n                          \n                            3\n                          \n                        \n                        4\n                      \n                    \n                    \n                      \n                        \n                          I\n                        \n                        \n                          ‖\n                          \n                            x\n                          \n                          \n                            ‖\n                            \n                              3\n                            \n                          \n                        \n                      \n                    \n                    −\n                    \n                      \n                        \n                          3\n                          R\n                        \n                        4\n                      \n                    \n                    \n                      \n                        \n                          \n                            x\n                          \n                          ⊗\n                          \n                            x\n                          \n                        \n                        \n                          ‖\n                          \n                            x\n                          \n                          \n                            ‖\n                            \n                              3\n                            \n                          \n                        \n                      \n                    \n                    −\n                    \n                      \n                        \n                          3\n                          R\n                        \n                        4\n                      \n                    \n                    \n                      \n                        \n                          I\n                        \n                        \n                          ‖\n                          \n                            x\n                          \n                          ‖\n                        \n                      \n                    \n                    +\n                    \n                      I\n                    \n                  \n                  ]\n                \n                ⋅\n                \n                  \n                    u\n                  \n                  \n                    ∞\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {u} (\\mathbf {x} )&=\\underbrace {\\underbrace {{\\frac {R^{3}}{4}}\\cdot \\left({\\frac {3\\left(\\mathbf {u} _{\\infty }\\cdot \\mathbf {x} \\right)\\cdot \\mathbf {x} }{\\|\\mathbf {x} \\|^{5}}}-{\\frac {\\mathbf {u} _{\\infty }}{\\|\\mathbf {x} \\|^{3}}}\\right)} _{{\\text{conservative: curl=0,}}\\ \\nabla ^{2}\\mathbf {u} =0}+\\underbrace {\\mathbf {u} _{\\infty }} _{\\text{far-field}}} _{\\text{Terms of Boundary-Condition}}\\;\\underbrace {-{\\frac {3R}{4}}\\cdot \\left({\\frac {\\mathbf {u} _{\\infty }}{\\|\\mathbf {x} \\|}}+{\\frac {\\left(\\mathbf {u} _{\\infty }\\cdot \\mathbf {x} \\right)\\cdot \\mathbf {x} }{\\|\\mathbf {x} \\|^{3}}}\\right)} _{{\\text{non-conservative: curl}}={\\boldsymbol {\\omega }}(\\mathbf {x} ),\\ \\mu \\nabla ^{2}\\mathbf {u} =\\nabla p}\\\\[8pt]&=\\left[{\\frac {3R^{3}}{4}}{\\frac {\\mathbf {x\\otimes \\mathbf {x} } }{\\|\\mathbf {x} \\|^{5}}}-{\\frac {R^{3}}{4}}{\\frac {\\mathbf {I} }{\\|\\mathbf {x} \\|^{3}}}-{\\frac {3R}{4}}{\\frac {\\mathbf {x} \\otimes \\mathbf {x} }{\\|\\mathbf {x} \\|^{3}}}-{\\frac {3R}{4}}{\\frac {\\mathbf {I} }{\\|\\mathbf {x} \\|}}+\\mathbf {I} \\right]\\cdot \\mathbf {u} _{\\infty }\\end{aligned}}}\n\nIn this formulation the non-conservative term represents a kind of so-called Stokeslet. The Stokeslet is the Green's function of the Stokes-Flow-Equations. The conservative term is equal to the dipole gradient field. The formula of vorticity is analogous to the Biot–Savart law  in electromagnetism.\n\nAlternatively, in a more compact way, one can formulate the velocity field as follows:\n\nwhere \n  \n    \n      \n        \n          H\n        \n        =\n        ∇\n        ⊗\n        ∇\n      \n    \n    {\\displaystyle \\mathrm {H} =\\nabla \\otimes \\nabla }\n  \n is the Hessian matrix differential operator and \n  \n    \n      \n        \n          S\n        \n        =\n        \n          I\n        \n        \n          ∇\n          \n            2\n          \n        \n        −\n        \n          H\n        \n      \n    \n    {\\displaystyle \\mathrm {S} =\\mathbf {I} \\nabla ^{2}-\\mathrm {H} }\n  \n is a differential operator composed as the difference of the Laplacian and the Hessian. In this way it becomes explicitly clear, that the solution is composed from derivatives of a Coulomb-type potential (\n  \n    \n      \n        1\n        \n          /\n        \n        ‖\n        \n          x\n        \n        ‖\n      \n    \n    {\\displaystyle 1/\\|\\mathbf {x} \\|}\n  \n) and a Biharmonic-type potential (\n  \n    \n      \n        ‖\n        \n          x\n        \n        ‖\n      \n    \n    {\\displaystyle \\|\\mathbf {x} \\|}\n  \n). The differential operator \n  \n    \n      \n        \n          S\n        \n      \n    \n    {\\displaystyle \\mathrm {S} }\n  \n applied to the vector norm \n  \n    \n      \n        ‖\n        \n          x\n        \n        ‖\n      \n    \n    {\\displaystyle \\|\\mathbf {x} \\|}\n  \n generates the Stokeslet.\n\nThe following formula describes the viscous stress tensor for the special case of Stokes flow. It is needed in the calculation of the force acting on the particle. In Cartesian coordinates the vector-gradient \n  \n    \n      \n        ∇\n        \n          u\n        \n      \n    \n    {\\displaystyle \\nabla \\mathbf {u} }\n  \n is identical to the Jacobian matrix. The matrix I represents the identity-matrix.\n\nThe force acting on the sphere can be calculated via the integral of the stress tensor over the surface of the sphere, where er represents the radial unit-vector of spherical-coordinates:\n\nAlthough the liquid is static and the sphere is moving with a certain velocity, with respect to the frame of sphere, the sphere is at rest and liquid is flowing in the opposite direction to the motion of the sphere.",
        pageTitle: "Stokes' law",
    },
    {
        title: "Stokes's law of sound attenuation",
        link: "https://en.wikipedia.org/wiki/Stokes%27s_law_of_sound_attenuation",
        content:
            "In acoustics, Stokes's law of sound attenuation is a formula for the attenuation of sound in a Newtonian fluid, such as water or air, due to the fluid's viscosity.  It states that the amplitude of a plane wave decreases exponentially with distance traveled, at a rate α given by\n\n  \n    \n      \n        α\n        =\n        \n          \n            \n              2\n              η\n              \n                ω\n                \n                  2\n                \n              \n            \n            \n              3\n              ρ\n              \n                V\n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {2\\eta \\omega ^{2}}{3\\rho V^{3}}}}\n  \n\nwhere η is the dynamic viscosity coefficient of the fluid, ω is the sound's angular frequency, ρ is the fluid density, and V is the speed of sound in the medium.[1]\n\nThe law and its derivation were published in 1845 by the Anglo-Irish physicist G. G. Stokes, who also developed Stokes's law for the friction force in fluid motion. A generalisation of Stokes attenuation taking into account the effect of thermal conductivity was proposed by the German physicist Gustav Kirchhoff in 1868.[2][3]\n\nSound attenuation in fluids is also accompanied by acoustic dispersion, meaning that the different frequencies are propagating at different sound speeds.[1]\n\nStokes's law of sound attenuation applies to sound propagation in an isotropic and homogeneous Newtonian medium.  Consider a plane sinusoidal pressure wave that has amplitude A0 at some point. After traveling a distance d from that point, its amplitude A(d) will be\n\n  \n    \n      \n        A\n        (\n        d\n        )\n        =\n        \n          A\n          \n            0\n          \n        \n        \n          e\n          \n            −\n            α\n            d\n          \n        \n      \n    \n    {\\displaystyle A(d)=A_{0}e^{-\\alpha d}}\n\nThe parameter α is a kind of attenuation constant, dimensionally the reciprocal of length.\nIn the International System of Units (SI), it is expressed in neper per meter or simply reciprocal of meter (m–1). That is, if α = 1 m–1, the wave's amplitude decreases by a factor of 1/e for each meter traveled.\n\nThe law is amended to include a contribution by the volume viscosity ζ:\n\n  \n    \n      \n        α\n        =\n        \n          \n            \n              \n                (\n                \n                  2\n                  η\n                  +\n                  \n                    \n                      3\n                      2\n                    \n                  \n                  ζ\n                \n                )\n              \n              \n                ω\n                \n                  2\n                \n              \n            \n            \n              3\n              ρ\n              \n                V\n                \n                  3\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                (\n                \n                  \n                    \n                      4\n                      3\n                    \n                  \n                  η\n                  +\n                  ζ\n                \n                )\n              \n              \n                ω\n                \n                  2\n                \n              \n            \n            \n              2\n              ρ\n              \n                V\n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {\\left(2\\eta +{\\frac {3}{2}}\\zeta \\right)\\omega ^{2}}{3\\rho V^{3}}}={\\frac {\\left({\\frac {4}{3}}\\eta +\\zeta \\right)\\omega ^{2}}{2\\rho V^{3}}}}\n  \n\nThe volume viscosity coefficient is relevant when the fluid's compressibility cannot be ignored, such as in the case of ultrasound in water.[4][5][6][7] The volume viscosity of water at 15 C is 3.09 centipoise.[8]\n\nStokes's law is actually an asymptotic approximation for low frequencies of a more general formula involving relaxation time τ:\n\n  \n    \n      \n        \n          \n            \n              \n                2\n                \n                  \n                    (\n                    \n                      \n                        \n                          α\n                          V\n                        \n                        ω\n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      1\n                      +\n                      \n                        \n                          (\n                          \n                            ω\n                            τ\n                          \n                          )\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                −\n                \n                  \n                    1\n                    \n                      1\n                      +\n                      \n                        \n                          (\n                          \n                            ω\n                            τ\n                          \n                          )\n                        \n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                α\n              \n              \n                \n                =\n                \n                  \n                    ω\n                    V\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          \n                            1\n                            +\n                            \n                              \n                                (\n                                \n                                  ω\n                                  τ\n                                \n                                )\n                              \n                              \n                                2\n                              \n                            \n                          \n                        \n                        −\n                        1\n                      \n                      \n                        2\n                        \n                          (\n                          \n                            1\n                            +\n                            \n                              \n                                (\n                                \n                                  ω\n                                  τ\n                                \n                                )\n                              \n                              \n                                2\n                              \n                            \n                          \n                          )\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                τ\n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \n                          \n                            4\n                            η\n                          \n                          3\n                        \n                      \n                      +\n                      ζ\n                    \n                    \n                      ρ\n                      \n                        V\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                =\n                \n                  \n                    \n                      4\n                      η\n                      +\n                      3\n                      ζ\n                    \n                    \n                      3\n                      ρ\n                      \n                        V\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                α\n              \n              \n                \n                =\n                ω\n                \n                  \n                    \n                      3\n                      2\n                    \n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        \n                          ρ\n                          \n                            (\n                            \n                              \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        ω\n                                        \n                                          (\n                                          \n                                            4\n                                            η\n                                            +\n                                            3\n                                            ζ\n                                          \n                                          )\n                                        \n                                      \n                                      )\n                                    \n                                    \n                                      2\n                                    \n                                  \n                                  +\n                                  \n                                    \n                                      (\n                                      \n                                        3\n                                        ρ\n                                        \n                                          V\n                                          \n                                            2\n                                          \n                                        \n                                      \n                                      )\n                                    \n                                    \n                                      2\n                                    \n                                  \n                                \n                              \n                              −\n                              3\n                              ρ\n                              \n                                V\n                                \n                                  2\n                                \n                              \n                            \n                            )\n                          \n                        \n                        \n                          \n                            \n                              (\n                              \n                                ω\n                                \n                                  (\n                                  \n                                    4\n                                    η\n                                    +\n                                    3\n                                    ζ\n                                  \n                                  )\n                                \n                              \n                              )\n                            \n                            \n                              2\n                            \n                          \n                          +\n                          \n                            \n                              (\n                              \n                                3\n                                ρ\n                                \n                                  V\n                                  \n                                    2\n                                  \n                                \n                              \n                              )\n                            \n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}2\\left({\\frac {\\alpha V}{\\omega }}\\right)^{2}&={\\frac {1}{\\sqrt {1+\\left(\\omega \\tau \\right)^{2}}}}-{\\frac {1}{1+\\left(\\omega \\tau \\right)^{2}}}\\\\\\alpha &={\\frac {\\omega }{V}}{\\sqrt {\\frac {{\\sqrt {1+\\left(\\omega \\tau \\right)^{2}}}-1}{2\\left(1+\\left(\\omega \\tau \\right)^{2}\\right)}}}\\\\\\tau &={\\frac {{\\frac {4\\eta }{3}}+\\zeta }{\\rho V^{2}}}={\\frac {4\\eta +3\\zeta }{3\\rho V^{2}}}\\\\\\alpha &=\\omega {\\sqrt {\\frac {3}{2}}}\\left({\\frac {\\rho \\left({\\sqrt {\\left(\\omega \\left(4\\eta +3\\zeta \\right)\\right)^{2}+\\left(3\\rho V^{2}\\right)^{2}}}-3\\rho V^{2}\\right)}{\\left(\\omega \\left(4\\eta +3\\zeta \\right)\\right)^{2}+\\left(3\\rho V^{2}\\right)^{2}}}\\right)^{\\frac {1}{2}}\\\\\\end{aligned}}}\n  \n\nThe relaxation time for water is about 2.0×10−12 seconds (2 picoseconds) per radian[citation needed], corresponding to an angular frequency ω of 5×1011 radians (500 gigaradians) per second and therefore a frequency of about 3.14×1012 hertz (3.14 terahertz).",
        pageTitle: "Stokes's law of sound attenuation",
    },
    {
        title: "Sturgeon's law",
        link: "https://en.wikipedia.org/wiki/Sturgeon%27s_law",
        content:
            'Sturgeon\'s law (or Sturgeon\'s revelation) is an adage stating "ninety percent of everything is crap".  It was coined by Theodore Sturgeon, an American science fiction author and critic, and was inspired by his observation that, while science fiction was often derided for its low quality by critics, most work in other fields was low-quality too, and so science fiction was no different.[1]\n\nSturgeon deemed Sturgeon\'s law to mean "nothing is always absolutely so".[2] This adage previously appeared in his story "The Claustrophile" in a 1956 issue of Galaxy.[3]\n\nThe second adage, variously rendered as "ninety percent of everything is crud" or "ninety percent of everything is crap", was published as "Sturgeon\'s Revelation" in his book review column for Venture[4] in 1957. However, almost all modern uses of the term Sturgeon\'s law refer to the second,[citation needed] including the definition listed in the Oxford English Dictionary.[5]\n\nAccording to science fiction author William Tenn, Sturgeon first expressed his law circa 1951, at a talk at New York University attended by Tenn.[6] The statement was subsequently included in a talk Sturgeon gave at a 1953 Labor Day weekend session of the World Science Fiction Convention in Philadelphia.[7]\n\nThe first written reference to the adage is in the September 1957 issue of Venture:\n\nAnd on that hangs Sturgeon’s revelation. It came to him that [science fiction] is indeed ninety-percent crud, but that also – Eureka! – ninety-percent of everything is crud. All things –  cars, books, cheeses, hairstyles, people, and pins are, to the expert and discerning eye, crud, except for the acceptable tithe which we each happen to like.[4]\n\nThe adage appears again in the March 1958 issue of Venture, where Sturgeon wrote:\n\nIt is in this vein that I repeat Sturgeon\'s Revelation, which was wrung out of me after twenty years of wearying defense of science fiction against attacks of people who used the worst examples of the field for ammunition, and whose conclusion was that ninety percent of S.F. is crud.\n\nIn the 1870 novel, Lothair, by Benjamin Disraeli, it is asserted that:\n\nNine-tenths of existing books are nonsense, and the clever books are the refutation of that nonsense.[9]\n\nA similar adage appears in Rudyard Kipling\'s The Light That Failed, published in 1890.\n\nFour-fifths of everybody\'s work must be bad. But the remnant is worth the trouble for its own sake.[10]\n\nA 1946 essay Confessions of a Book Reviewer by George Orwell asserts about books:\n\nIn much more than nine cases out of ten the only objectively truthful criticism would be "This book is worthless ..."[11]\n\nIn 2009, a paper published in The Lancet estimated that over 85% of health and medical research is wasted.[12]\n\nIn 2013, philosopher Daniel Dennett championed Sturgeon\'s law as one of his seven tools for critical thinking.[13]\n\n90% of everything is crap. That is true, whether you are talking about physics, chemistry, evolutionary psychology, sociology, medicine –  you name it –  rock music, country western. 90% of everything is crap.[14]',
        pageTitle: "Sturgeon's law",
    },
    {
        title: "Sutton's law",
        link: "https://en.wikipedia.org/wiki/Sutton%27s_law",
        content:
            "Sutton's law states that when diagnosing, one should first consider the obvious. It suggests that one should first conduct those tests which could confirm (or rule out) the most likely diagnosis. It is taught in medical schools to suggest to medical students that they might best order tests in that sequence which is most likely to result in a quick diagnosis, hence treatment, while minimizing unnecessary costs. It is also applied in pharmacology, when choosing a drug to treat a specific disease you want the drug to reach the disease. It is applicable to any process of diagnosis, e.g. debugging computer programs.  Computer-aided diagnosis provides a statistical and quantitative approach.\n\nA more thorough analysis will consider the false positive rate of the test and the possibility that a less likely diagnosis might have more serious consequences. A competing principle is the idea of performing simple tests before more complex and expensive tests, moving from bedside tests to blood results and simple imaging such as ultrasound and then more complex such as MRI then specialty imaging.  The law can also be applied in prioritizing tests when resources are limited, so a test for a treatable condition should be performed before an equally probable but less treatable condition.\n\nThe law is named after the bank robber Willie Sutton, who reputedly replied to a reporter's inquiry as to why he robbed banks by saying \"because that's where the money is.\" In Sutton's 1976 book Where the Money Was, Sutton denies having said this,[1] but added that \"If anybody had asked me, I'd have probably said it. That's what almost anybody would say.... it couldn't be more obvious.\"[2]\n\nA similar idea is contained in the physician's adage, \"When you hear hoofbeats, think horses, not zebras.\"",
        pageTitle: "Sutton's law",
    },
    {
        title: "Swanson's law",
        link: "https://en.wikipedia.org/wiki/Swanson%27s_law",
        content:
            "Swanson's law is the observation that the price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. At present rates, costs go down 75% about every 10 years.[3]\n\nIt is named after Richard Swanson, the founder of SunPower Corporation, a solar panel manufacturer.[4] The term Swanson's Law appears to have originated with an article in The Economist published in late 2012.[5][6] Swanson had been presenting such curves at technical conferences for several years.[7]\n\nSwanson's law has been compared to Moore's law, which predicts the growing computing power of processors. Swanson's Law is a solar industry specific application of the more general Wright's Law which states there will be a fixed cost reduction for each doubling of manufacturing volume.\n\nThe method used by Swanson is more commonly referred to as learning curve or more precise experience curve analysis. It was first developed and applied to the aeronautics industry in 1936 by Theodore Paul Wright.[8] There are reports of it first being applied to the photovoltaics industry in 1975, and saw wider use starting in the early 1990s.[9]\n\nCrystalline silicon photovoltaic cell prices have fallen from $76.67 per watt in 1977 to $0.36 per watt in 2014.[5][6][10] Plotting the module price (in $/Wp) versus time shows a dropping by 10% per year.[11]",
        pageTitle: "Swanson's law",
    },
    {
        title: "Szemerényi's law",
        link: "https://en.wikipedia.org/wiki/Szemer%C3%A9nyi%27s_law",
        content:
            "Szemerényi's law (.mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}Hungarian pronunciation: [ˈsɛmɛreːɲi]) is both a sound change and a synchronic phonological rule that operated during an early stage of the Proto-Indo-European language (PIE). Though its effects are evident in many reconstructed as well as attested forms, it did not operate in late PIE, having become morphologized (with exceptions reconstructible via the comparative method). It is named for Hungarian-British linguist Oswald Szemerényi.\n\nThe rule deleted coda fricatives *s or laryngeals *h₁, *h₂ or *h₃ (cover symbol *H), with compensatory lengthening occurring in a word-final position after resonants. In other words:\n\nThe law affected the nominative singular forms of the many masculine and feminine nouns whose stem ended in a resonant:\n\nThe rule also affected the nominative-accusative forms of neuter plural/collective nouns, which ended in *-h₂:\n\nAccording to another synchronic PIE phonological rule, word-final *n  was deleted after *ō, usually by the operation of Szemerényi's law:\n\nThe PIE reconstruction for \"heart\" is the single instance where *d is deleted after *r, with compensatory lengthening of the preceding vowel. It is not clear whether that is an isolated example or a part of a broader process.\n\nSome cases were apparently not affected by Szemerényi's law:\n\nIn PIE, the resulting long vowels had already begun to spread analogically to other nominative singular forms that were not phonologically justified by the law (PIE *pṓds 'foot'). The word-final sonorants other than *-n were sometimes dropped as well, which demonstrates that this law was already morphologized in the period of \"PIE proper\", and the long vowel produced was no longer synchronically viewed as the outcome of a process of fricative deletion. Exceptions to Szemerényi's law are found in word-final:\n\nThe forms without a laryngeal are considered to be more archaic and were likely to have been lexicalized at a later stage of PIE.[citation needed]",
        pageTitle: "Szemerényi's law",
    },
    {
        title: "Taylor's law",
        link: "https://en.wikipedia.org/wiki/Taylor%27s_law",
        content:
            "Taylor's power law is an empirical law in ecology that relates the variance of the number of individuals of a species per unit area of habitat to the corresponding mean by a power law relationship.[1] It is named after the ecologist who first proposed it in 1961, Lionel Roy Taylor (1924–2007).[2] Taylor's original name for this relationship was the law of the mean.[1]  The name Taylor's law was coined by Southwood in 1966.[2]\n\nThis law was originally defined for ecological systems, specifically to assess the spatial clustering of organisms. For a population count \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  \n with mean \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n and variance \n  \n    \n      \n        var\n        ⁡\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle \\operatorname {var} (Y)}\n  \n, Taylor's law is written\n\nwhere a and b are both positive constants. Taylor proposed this relationship in 1961, suggesting that the exponent b be considered a species specific index of aggregation.[1] This power law has subsequently been confirmed for many hundreds of species.[3][4]\n\nTaylor's law has also been applied to assess the time dependent changes of population distributions.[3] Related variance to mean power laws have also been demonstrated in several non-ecological systems:\n\nThe first use of a double log-log plot was by Reynolds in 1879 on thermal aerodynamics.[17] Pareto used a similar plot to study the proportion of a population and their income.[18]\n\nThe term variance was coined by Fisher in 1918.[19]\n\nPearson[20] in 1921 proposed the equation (also studied by Neyman[21])\n\nSmith in 1938 while studying crop yields proposed a relationship similar to Taylor's.[22] This relationship was\n\nwhere Vx is the variance of yield for plots of x units, V1 is the variance of yield per unit area and x is the size of plots. The slope (b) is the index of heterogeneity. The value of b in this relationship lies between 0 and 1. Where the yield are highly correlated b tends to 0; when they are uncorrelated b tends to 1.\n\nBliss[23] in 1941, Fracker and Brischle[24] in 1941 and Hayman & Lowe [25] in 1961 also described what is now known as Taylor's law, but in the context of data from single species.\n\nTaylor's 1961 paper used data from 24 papers, published between 1936 and 1960, that considered a variety of biological settings: virus lesions, macro-zooplankton, worms and symphylids in soil, insects in soil, on plants and in the air, mites on leaves, ticks on sheep and fish in the sea.;[1] the b value lay between 1 and 3. Taylor proposed the power law as a general feature of the spatial distribution of these species. He also proposed a mechanistic hypothesis to explain this law.\n\nInitial attempts to explain the spatial distribution of animals had been based on approaches like Bartlett's stochastic population models and the negative binomial distribution that could result from birth–death processes.[26]  Taylor's explanation was based the assumption of a balanced migratory and congregatory behavior of animals.[1]  His hypothesis was initially qualitative, but as it evolved it became semi-quantitative and was supported by simulations.[27]\n\nMany alternative hypotheses for the power law have been advanced. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction.[28] Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values.[3][4]\n\nAnderson et al formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function.[29] As a response to this model Taylor argued that such a Markov process would predict that the power law exponent would vary considerably between replicate observations, and that such variability had not been observed.[30]\n\nKemp reviewed a number of discrete stochastic models based on the negative binomial, Neyman type A, and Polya–Aeppli distributions that with suitable adjustment of parameters could produce a variance to mean power law.[31]  Kemp, however, did not explain the parameterizations of his models in mechanistic terms.  Other relatively abstract models for Taylor's law followed.[6][32]\n\nStatistical concerns were raised regarding Taylor's law, based on the difficulty with real data in distinguishing between Taylor's law and other variance to mean functions, as well the inaccuracy of standard regression methods.[33][34]\n\nTaylor's law has been applied to time series data, and Perry showed, using simulations, that chaos theory could yield Taylor's law.[35]\n\nTaylor's law has been applied to the spatial distribution of plants[36] and bacterial populations[37]  As with the observations of Tobacco necrosis virus mentioned earlier, these observations were not consistent with Taylor's animal behavioral model.\n\nA variance to mean power function had been applied to non-ecological systems, under the rubric of Taylor's law.  A more general explanation for the range of manifestations of the power law a hypothesis has been proposed based on the Tweedie distributions,[38] a family of probabilistic models that express an inherent power function relationship between the variance and the mean.[11][13][39]\n\nSeveral alternative hypotheses for the power law have been proposed. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction.[28] Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values.[3][4] Anderson et al formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function.[29] The Lewontin Cohen growth model.[40] is another proposed explanation. The possibility that observations of a power law might reflect more mathematical artifact than a mechanistic process was raised.[41] Variation in the exponents of Taylor's Law applied to ecological populations cannot be explained or predicted based solely on statistical grounds however.[42] Research has shown that variation within the Taylor's law exponents for the North Sea fish community varies with the external environment, suggesting ecological processes at least partially determine the form of Taylor's law.[43]\n\nIn the physics literature Taylor's law has been referred to as fluctuation scaling. Eisler et al, in a further attempt to find a general explanation for fluctuation scaling, proposed a process they called impact inhomogeneity in which frequent events are associated with larger impacts.[44] In appendix B of the Eisler article, however, the authors noted that the equations for impact inhomogeneity yielded the same mathematical relationships as found with the Tweedie distributions.\n\nAnother group of physicists, Fronczak and Fronczak, derived Taylor's power law for fluctuation scaling from principles of equilibrium and non-equilibrium statistical physics.[45] Their derivation was based on assumptions of physical quantities like free energy and an external field that caused the clustering of biological organisms. Direct experimental demonstration of these postulated physical quantities in relationship to animal or plant aggregation has yet to be achieved, though. Shortly thereafter, an analysis of Fronczak and Fronczak's model was presented that showed their equations directly lead to the Tweedie distributions, a finding that suggested that Fronczak and Fronczak had possibly provided a maximum entropy derivation of these distributions.[14]\n\nTaylor's law has been shown to hold for prime numbers not exceeding a given real number.[46] This result has been shown to hold for the first 11 million primes. If the Hardy–Littlewood twin primes conjecture is true then this law also holds for twin primes.\n\nAbout the time that Taylor was substantiating his ecological observations, MCK Tweedie, a British statistician and medical physicist, was investigating a family of probabilistic models that are now known as the Tweedie distributions.[47][48] As mentioned above, these distributions are all characterized by a variance to mean power law mathematically identical to Taylor's law.\n\nThe Tweedie distribution most applicable to ecological observations is the compound Poisson-gamma distribution, which represents the sum of N independent and identically distributed random variables with a gamma distribution where N is a random variable distributed in accordance with a Poisson distribution. In the additive form its cumulant generating function (CGF) is:\n\ns is the generating function variable, and θ and λ are the canonical and index parameters, respectively.[38]\n\nThese last two parameters are analogous to the scale and shape parameters used in probability theory. The cumulants of this distribution can be determined by successive differentiations of the CGF and then substituting s=0 into the resultant equations. The first and second cumulants are the mean and variance, respectively, and thus the compound Poisson-gamma CGF yields Taylor's law with the proportionality constant\n\nThe compound Poisson-gamma cumulative distribution function has been verified for limited ecological data through the comparison of the theoretical distribution function with the empirical distribution function.[39] A number of other systems, demonstrating variance to mean power laws related to Taylor's law, have been similarly tested for the compound Poisson-gamma distribution.[12][13][14][16]\n\nThe main justification for the Tweedie hypothesis rests with the mathematical convergence properties of the Tweedie distributions.[13] The Tweedie convergence theorem requires the Tweedie distributions to act as foci of convergence for a wide range of statistical processes.[49] As a consequence of this convergence theorem, processes based on the sum of multiple independent small jumps will tend to express Taylor's law and obey a Tweedie distribution. A limit theorem for independent and identically distributed variables, as with the Tweedie convergence theorem, might then be considered as being fundamental relative to the ad hoc population models, or models proposed on the basis of simulation or approximation.[14][16]\n\nThis hypothesis remains controversial; more conventional population dynamic approaches seem preferred amongst ecologists, despite the fact that the Tweedie compound Poisson distribution can be directly applied to population dynamic mechanisms.[6]\n\nOne difficulty with the Tweedie hypothesis is that the value of b does not range between 0 and 1. Values of b < 1 are rare but have been reported.[50]\n\nwhere si2 is the variance of the density of the ith sample, mi is the mean density of the ith sample and a and b are constants.\n\nThe exponent in Taylor's law is scale invariant: If the unit of measurement is changed by a constant factor \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n, the exponent (\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n) remains unchanged.\n\nTaylor's law expressed in the original variable (x) is\n\nThus, \n  \n    \n      \n        \n          σ\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}^{2}}\n  \n is still proportional to \n  \n    \n      \n        \n          μ\n          \n            2\n          \n          \n            b\n          \n        \n      \n    \n    {\\displaystyle \\mu _{2}^{b}}\n  \n (even though the proportionality constant has changed).\n\nIt has been shown that Taylor's law is the only relationship between the mean and variance that is scale invariant.[51]\n\nA refinement in the estimation of the slope b has been proposed by Rayner.[52]\n\nwhere \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n is the Pearson moment correlation coefficient between \n  \n    \n      \n        log\n        ⁡\n        (\n        \n          s\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\log(s^{2})}\n  \n and  \n  \n    \n      \n        log\n        ⁡\n        m\n      \n    \n    {\\displaystyle \\log m}\n  \n, \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is the ratio of sample variances in \n  \n    \n      \n        log\n        ⁡\n        (\n        \n          s\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\log(s^{2})}\n  \n and \n  \n    \n      \n        log\n        ⁡\n        m\n      \n    \n    {\\displaystyle \\log m}\n  \n and \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is the ratio of the errors in \n  \n    \n      \n        log\n        ⁡\n        (\n        \n          s\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\log(s^{2})}\n  \n and \n  \n    \n      \n        log\n        ⁡\n        m\n      \n    \n    {\\displaystyle \\log m}\n  \n.\n\nOrdinary least squares regression assumes that φ = ∞. This tends to underestimate the value of b because the estimates of both \n  \n    \n      \n        log\n        ⁡\n        (\n        \n          s\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\log(s^{2})}\n  \n and \n  \n    \n      \n        log\n        ⁡\n        m\n      \n    \n    {\\displaystyle \\log m}\n  \n are subject to error.\n\nAn extension of Taylor's law has been proposed by Ferris et al when multiple samples are taken[53]\n\nwhere s2 and m are the variance and mean respectively, b, c and d are constants and n is the number of samples taken. To date, this proposed extension has not been verified to be as applicable as the original version of Taylor's law.\n\nAn extension to this law for small samples has been proposed by Hanski.[54] For small samples the Poisson variation (P) - the variation that can be ascribed to sampling variation - may be significant. Let S be the total variance and let V be the biological (real) variance. Then\n\nBecause in the Poisson distribution the mean equals the variance, we have\n\nThis closely resembles Barlett's original suggestion.\n\nSlope values (b) significantly > 1 indicate clumping of the organisms.\n\nIn Poisson-distributed data, b = 1.[30] If the population follows a lognormal or gamma distribution, then b = 2.\n\nFor populations that are experiencing constant per capita environmental variability, the regression of log( variance ) versus log( mean abundance ) should have a line with b = 2.\n\nMost populations that have been studied have b < 2 (usually 1.5–1.6) but values of 2 have been reported.[55] Occasionally cases with b > 2 have been reported.[3] b values below 1 are uncommon but have also been reported ( b = 0.93 ).[50]\n\nIt has been suggested that the exponent of the law (b) is proportional to the skewness of the underlying distribution.[56] This proposal has criticised: additional work seems to be indicated.[57][58]\n\nThe origin of the slope (b) in this regression remains unclear. Two hypotheses have been proposed to explain it. One suggests that b arises from the species behavior and is a constant for that species. The alternative suggests that it is dependent on the sampled population. Despite the considerable number of studies carried out on this law (over 1000), this question remains open.\n\nIt is known that both a and b are subject to change due to age-specific dispersal, mortality and sample unit size.[59]\n\nThis law may be a poor fit if the values are small. For this reason an extension to Taylor's law has been proposed by Hanski which improves the fit of Taylor's law at low densities.[54]\n\nA form of Taylor's law applicable to binary data in clusters (e.q., quadrats) has been proposed.[60] In a binomial distribution, the theoretical variance is\n\nwhere (varbin) is the binomial variance, n is the sample size per cluster, and p is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual having that trait.\n\nOne difficulty with binary data is that the mean and variance, in general, have a particular relationship: as the mean proportion of individuals infected increases above 0.5, the variance deceases.\n\nIt is now known that the observed variance (varobs) changes as a power function of (varbin).[60]\n\nHughes and Madden noted that if the distribution is Poisson, the mean and variance are equal.[60] As this is clearly not the case in many observed proportion samples, they instead assumed a binomial distribution. They replaced the mean in Taylor's law with the binomial variance and then compared this theoretical variance with the observed variance. For binomial data, they showed that varobs = varbin with overdispersion, varobs > varbin.\n\nIn symbols, Hughes and Madden's modification to Tyalor's law was\n\nThis latter version is known as the binary power law.\n\nA key step in the derivation of the binary power law by Hughes and Madden was the observation made by Patil and Stiteler[61] that the variance-to-mean ratio used for assessing over-dispersion of unbounded counts in a single sample is actually the ratio of two variances: the observed variance and the theoretical variance for a random distribution. For unbounded counts, the random distribution is the Poisson. Thus, the Taylor power law for a collection of samples can be considered as a relationship between the observed variance and the Poisson variance.\n\nMore broadly, Madden and Hughes[60] considered the power law as the relationship between two variances, the observed variance and the theoretical variance for a random distribution. With binary data, the random distribution is the binomial (not the Poisson). Thus the Taylor power law and the binary power law are two special cases of a general power-law relationships for heterogeneity.\n\nWhen both a and b are equal to 1, then a small-scale random spatial pattern is suggested and is best described by the binomial distribution. When b = 1 and a > 1, there is over-dispersion (small-scale aggregation). When b is > 1, the degree of aggregation varies with p. Turechek et al.[62] have shown that the binary power law describes numerous data sets in plant pathology. In general, b is greater than 1 and less than 2.\n\nThe fit of this law has been tested by simulations.[63] These results suggest that rather than a single regression line for the data set, a segmental regression may be a better model for genuinely random distributions. However, this segmentation only occurs for very short-range dispersal distances and large quadrat sizes.[62] The break in the line occurs only at p very close to 0.\n\nAn extension to this law has been proposed.[64] The original form of this law is symmetrical but it can be extended to an asymmetrical form.[64] Using simulations the symmetrical form fits the data when there is positive correlation of disease status of neighbors. Where there is a negative correlation between the likelihood of neighbours being infected, the asymmetrical version is a better fit to the data.\n\nBecause of the ubiquitous occurrence of Taylor's law in biology it has found a variety of uses some of which are listed here.\n\nIt has been recommended based on simulation studies[65] in applications testing the validity of Taylor's law to a data sample that:\n\n(1) the total number of organisms studied be > 15\n(2) the minimum number of groups of organisms studied be > 5 \n(3) the density of the organisms should vary by at least 2 orders of magnitude within the sample\n\nIt is common assumed (at least initially) that a population is randomly distributed in the environment. If a population is randomly distributed then the mean ( m ) and variance ( s2 ) of the population are equal and the proportion of samples that contain at least one individual ( p ) is\n\nWhen a species with a clumped pattern is compared with one that is randomly distributed with equal overall densities, p will be less for the species having the clumped distribution pattern. Conversely when comparing a uniformly and a randomly distributed species but at equal overall densities, p will be greater for the randomly distributed population. This can be graphically tested by plotting p against m.\n\nWilson and Room developed a binomial model that incorporates Taylor's law.[66] The basic relationship is\n\nIncorporating Taylor's law this relationship becomes\n\nThe common dispersion parameter (k) of the negative binomial distribution is\n\nwhere \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is the sample mean and \n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s^{2}}\n  \n is the variance.[67] If 1 / k is > 0 the population is considered to be aggregated; 1 / k = 0 ( s2 = m ) the population is considered to be randomly (Poisson) distributed and if 1 / k is < 0 the population is considered to be uniformly distributed. No comment on the distribution can be made if k = 0.\n\nWilson and Room assuming that Taylor's law applied to the population gave an alternative estimator for k:[66]\n\nJones[68] using the estimate for k above along with the relationship Wilson and Room developed for the probability of finding a sample having at least one individual[66]\n\nderived an estimator for the probability of a sample containing x individuals per sampling unit. Jones's formula is\n\nwhere P( x ) is the probability of finding x individuals per sampling unit, k is estimated from the Wilon and Room equation and m is the sample mean. The probability of finding zero individuals P( 0 ) is estimated with the negative binomial distribution\n\nJones also gives confidence intervals for these probabilities.\n\nwhere CI is the confidence interval, t is the critical value taken from the t distribution and N is the total sample size.\n\nKatz proposed a family of distributions (the Katz family) with 2 parameters ( w1, w2 ).[69] This family of distributions includes the Bernoulli, Geometric, Pascal and Poisson distributions as special cases. The mean and variance of a Katz distribution are\n\nwhere m is the mean and s2 is the variance of the sample. The parameters can be estimated by the method of moments from which we have\n\nFor a Poisson distribution w2 = 0 and w1 = λ the parameter of the Possion distribution. This family of distributions is also sometimes known as the Panjer family of distributions.\n\nThe Katz family is related to the Sundt-Jewel family of distributions:[70]\n\nThe only members of the Sundt-Jewel family are the Poisson, binomial, negative binomial (Pascal), extended truncated negative binomial and logarithmic series distributions.\n\nIf the population obeys a Katz distribution then the coefficients of Taylor's law are\n\nwhere Jn is the test statistic, s2 is the variance of the sample, m is the mean of the sample and n is the sample size. Jn is asymptotically normally distributed with a zero mean and unit variance. If the sample is Poisson distributed Jn = 0; values of Jn < 0 and > 0 indicate under and over dispersion respectively. Overdispersion is often caused by latent heterogeneity - the presence of multiple sub populations within the population the sample is drawn from.\n\nThis statistic is related to the Neyman–Scott statistic\n\nwhich is known to be asymptotically normal and the conditional chi-squared statistic (Poisson dispersion test)\n\nwhich is known to have an asymptotic chi squared distribution with n − 1 degrees of freedom when the population is Poisson distributed.\n\nIf Taylor's law is assumed to apply it is possible to determine the mean time to local extinction. This model assumes a simple random walk in time and the absence of density dependent population regulation.[71]\n\nLet \n  \n    \n      \n        \n          N\n          \n            t\n            +\n            1\n          \n        \n        =\n        r\n        \n          N\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle N_{t+1}=rN_{t}}\n  \n where Nt+1 and Nt are the population sizes at time t + 1 and t respectively and r is parameter equal to the annual increase (decrease in population). Then\n\nwhere \n  \n    \n      \n        \n          var\n        \n        (\n        r\n        )\n      \n    \n    {\\displaystyle {\\text{var}}(r)}\n  \n is the variance of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n.\n\nLet \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n be a measure of the species abundance (organisms per unit area). Then\n\nIf a population is lognormally distributed then the harmonic mean of the population size (H) is related to the arithmetic mean (m)[72]\n\nGiven that H must be > 0 for the population to persist then rearranging we have\n\nis the minimum size of population for the species to persist.\n\nThe assumption of a lognormal distribution appears to apply to about half of a sample of 544 species.[73] suggesting that it is at least a plausible assumption.\n\nThe degree of precision (D) is defined to be s / m where s is the standard deviation and m is the mean. The degree of precision is known as the coefficient of variation in other contexts. In ecology research it is recommended that D be in the range 10–25%.[74] The desired degree of precision is important in estimating the required sample size where an investigator wishes to test if Taylor's law applies to the data. The required sample size has been estimated for a number of simple distributions but where the population distribution is not known or cannot be assumed more complex formulae may needed to determine the required sample size.\n\nWhere the population is Poisson distributed the sample size (n) needed is\n\nwhere t is critical level of the t distribution for the type 1 error with the degrees of freedom that the mean (m) was calculated with.\n\nIf the population is distributed as a negative binomial distribution then the required sample size is\n\nwhere k is the parameter of the negative binomial distribution.\n\nA more general sample size estimator has also been proposed[75]\n\nwhere n is the required sample size, a and b are the Taylor's law coefficients and D is the desired degree of precision.\n\nKarandinos proposed two similar estimators for n.[77] The first was modified by Ruesink to incorporate Taylor's law.[78]\n\nwhere d is the ratio of half the desired confidence interval (CI) to the mean. In symbols\n\nThe second estimator is used in binomial (presence-absence) sampling. The desired sample size (n) is\n\nwhere the dp is ratio of half the desired confidence interval to the proportion of sample units with individuals, p is proportion of samples containing individuals and q = 1 − p. In symbols\n\nFor binary (presence/absence) sampling, Schulthess et al modified Karandinos' equation\n\nwhere N is the required sample size, p is the proportion of units containing the organisms of interest, t is the chosen level of significance and Dip is a parameter derived from Taylor's law.[79]\n\nSequential analysis is a method of statistical analysis where the sample size is not fixed in advance. Instead samples are taken in accordance with a predefined stopping rule. Taylor's law has been used to derive a number of stopping rules.\n\nA formula for fixed precision in serial sampling to test Taylor's law was derived by Green in 1970.[80]\n\nwhere T is the cumulative sample total, D is the level of precision, n is the sample size and a and b are obtained from Taylor's law.\n\nAs an aid to pest control Wilson et al developed a test that incorporated a threshold level where action should be taken.[81] The required sample size is\n\nwhere a and b are the Taylor coefficients, || is the absolute value, m is the sample mean, T is the threshold level and t is the critical level of the t distribution. The authors also provided a similar test for binomial (presence-absence) sampling\n\nwhere p is the probability of finding a sample with pests present and q = 1 − p.\n\nGreen derived another sampling formula for sequential sampling based on Taylor's law[82]\n\nwhere D is the degree of precision, a and b are the Taylor's law coefficients, n is the sample size and T is the total number of individuals sampled.\n\nSerra et al have proposed a stopping rule based on Taylor's law.[83]\n\nwhere a and b are the parameters from Taylor's law, D is the desired level of precision and Tn is the total sample size.\n\nSerra et al also proposed a second stopping rule based on Iwoa's regression\n\nwhere α and β are the parameters of the regression line, D is the desired level of precision and Tn is the total sample size.\n\nThe authors recommended that D be set at 0.1 for studies of population dynamics and D = 0.25 for pest control.\n\nIt is considered to be good practice to estimate at least one additional analysis of aggregation (other than Taylor's law) because the use of only a single index may be misleading.[84] Although a number of other methods for detecting relationships between the variance and mean in biological samples have been proposed, to date none have achieved the popularity of Taylor's law. The most popular analysis used in conjunction with Taylor's law is probably Iwao's Patchiness regression test but all the methods listed here have been used in the literature.\n\nBarlett in 1936[85] and later Iwao independently in 1968[86] both proposed an alternative relationship between the variance and the mean. In symbols\n\nwhere s is the variance in the ith sample and mi is the mean of the ith sample\n\nWhen the population follows a negative binomial distribution, a = 1 and b = k (the exponent of the negative binomial distribution).\n\nThis alternative formulation has not been found to be as good a fit as Taylor's law in most studies.\n\nNachman proposed a relationship between the mean density and the proportion of samples with zero counts:[87]\n\nwhere p0 is the proportion of the sample with zero counts, m is the mean density, a is a scale parameter and b is a dispersion parameter. If a = b = 1 the distribution is random. This relationship is usually tested in its logarithmic form\n\nAllsop used this relationship along with Taylor's law to derive an expression for the proportion of infested units in a sample[88]\n\nwhere D2 is the degree of precision desired, zα/2 is the upper α/2 of the normal distribution, a and b are the Taylor's law coefficients, c and d are the Nachman coefficients, n is the sample size and N is the number of infested units.\n\nBinary sampling is not uncommonly used in ecology. In 1958 Kono and Sugino derived an equation that relates the proportion of samples without individuals to the mean density of the samples.[89]\n\nwhere p0 is the proportion of the sample with no individuals, m is the mean sample density, a and b are constants. Like Taylor's law this equation has been found to fit a variety of populations including ones that obey Taylor's law. Unlike the negative binomial distribution this model is independent of the mean density.\n\nThe derivation of this equation is straightforward. Let the proportion of empty units be p0 and assume that these are distributed exponentially. Then\n\nTaking logs twice and rearranging, we obtain the equation above. This model is the same as that proposed by Nachman.\n\nThe advantage of this model is that it does not require counting the individuals but rather their presence or absence. Counting individuals may not be possible in many cases particularly where insects are the matter of study.\n\nThe equation was derived while examining the relationship between the proportion P of a series of rice hills infested and the mean severity of infestation m. The model studied was\n\nwhere a and b are empirical constants. Based on this model the constants a and b were derived and a table prepared relating the values of P and m\n\nThe predicted estimates of m from this equation are subject to bias[90] and it is recommended that the adjusted mean ( ma ) be used instead[91]\n\nwhere var is the variance of the sample unit means mi and m is the overall mean.\n\nAn alternative adjustment to the mean estimates is[91]\n\nwhere MSE is the mean square error of the regression.\n\nThis model may also be used to estimate stop lines for enumerative (sequential) sampling. The variance of the estimated means is[92]\n\nwhere MSE is the mean square error of the regression, α and β are the constant and slope of the regression respectively, sβ2 is the variance of the slope of the regression, N is the number of points in the regression, n is the number of sample units and p is the mean value of p0 in the regression. The parameters a and b are estimated from Taylor's law:\n\nHughes and Madden have proposed testing a similar relationship applicable to binary observations in cluster, where each cluster contains from 0 to n individuals.[60]\n\nwhere a, b and c are constants, varobs is the observed variance, and p is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual with a trait. In logarithmic form, this relationship is\n\nIn most cases, it is assumed that b = c, leading to a simple model\n\nThis relationship has been subjected to less extensive testing than Taylor's law. However, it has accurately described over 100 data sets, and there are no published examples reporting that it does not works.[62]\n\nA variant of this equation was proposed by Shiyomi et al. ([93]) who suggested testing the regression\n\nwhere varobs is the variance, a and b are the constants of the regression, n here is the sample size (not sample per cluster) and p is the probability of a sample containing at least one individual.\n\nA negative binomial model has also been proposed.[94] The dispersion parameter (k) using the method of moments is m2 / ( s2 – m ) and pi is the proportion of samples with counts > 0. The s2 used in the calculation of k are the values predicted by Taylor's law. pi is plotted against 1 − (k(k + m)−1)k and the fit of the data is visually inspected.\n\nPerry and Taylor have proposed an alternative estimator of k based on Taylor's law.[95]\n\nA better estimate of the dispersion parameter can be made with the method of maximum likelihood. For the negative binomial it can be estimated from the equation[67]\n\nwhere Ax is the total number of samples with more than x individuals, N is the total number of individuals, x is the number of individuals in a sample, m is the mean number of individuals per sample and k is the exponent. The value of k has to be estimated numerically.\n\nGoodness of fit of this model can be tested in a number of ways including using the chi square test. As these may be biased by small samples an alternative is the U statistic – the difference between the variance expected under the negative binomial distribution and that of the sample. The expected variance of this distribution is m + m2 / k and\n\nwhere s2 is the sample variance, m is the sample mean and k is the negative binomial parameter.\n\nwhere p = m / k, q = 1 + p, R = p / q and N is the total number of individuals in the sample. The expected value of U is 0. For large sample sizes U is distributed normally.\n\nNote: The negative binomial is actually a family of distributions defined by the relation of the mean to the variance\n\nσ\n          \n            2\n          \n        \n        =\n        μ\n        +\n        a\n        \n          μ\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}=\\mu +a\\mu ^{p}}\n\nwhere a and p are constants. When a = 0 this defines the Poisson distribution. With p = 1 and p = 2, the distribution is known as the NB1 and NB2 distribution respectively.\n\nThis model is a version of that proposed earlier by Barlett.\n\nwhere m is the sample mean and s2 is the variance. If k−1 is > 0 the population is considered to be aggregated; k−1 = 0 the population is considered to be random; and if k−1 is < 0 the population is considered to be uniformly distributed.\n\nSouthwood has recommended regressing k against the mean and a constant[76]\n\nwhere ki and mi are the dispersion parameter and the mean of the ith sample respectively to test for the existence of a common dispersion parameter (kc). A slope (b) value significantly > 0 indicates the dependence of k on the mean density.\n\nAn alternative method was proposed by Elliot who suggested plotting ( s2 − m ) against ( m2 − s2 / n ).[96] kc is equal to 1/slope of this regression.\n\nIf the population can be assumed to be distributed in a negative binomial fashion, then C = 100 (1/k)0.5 where k is the dispersion parameter of the distribution.\n\nThe usual interpretation of this index is as follows: values of Ic < 1, = 1, > 1 are taken to mean a uniform distribution, a random distribution or an aggregated distribution.\n\nBecause s2 = Σ x2 − (Σx)2, the index can also be written\n\nLloyd's index of mean crowding (IMC) is the average number of other points contained in the sample unit that contains a randomly chosen point.[98]\n\nIt is a measure of pattern intensity that is unaffected by thinning (random removal of points). This index was also proposed by Pielou in 1988 and is sometimes known by this name also.\n\nBecause an estimate of the variance of IP is extremely difficult to estimate from the formula itself, LLyod suggested fitting a negative binomial distribution to the data. This method gives a parameter k\n\nwhere \n  \n    \n      \n        S\n        E\n        (\n        I\n        P\n        )\n      \n    \n    {\\displaystyle SE(IP)}\n  \n is the standard error of the index of patchiness, \n  \n    \n      \n        \n          var\n        \n        (\n        k\n        )\n      \n    \n    {\\displaystyle {\\text{var}}(k)}\n  \n is the variance of the parameter k and q is the number of quadrats sampled..\n\nIwao proposed a patchiness regression to test for clumping[99][100]\n\nyi here is Lloyd's index of mean crowding.[98] Perform an ordinary least squares regression of mi against y.\n\nIn this regression the value of the slope (b) is an indicator of clumping: the slope = 1 if the data is Poisson-distributed. The constant (a) is the number of individuals that share a unit of habitat at infinitesimal density and may be < 0, 0 or > 0. These values represent regularity, randomness and aggregation of populations in spatial patterns respectively. A value of a < 1 is taken to mean that the basic unit of the distribution is a single individual.\n\nWhere the statistic s2/m is not constant it has been recommended to use instead to regress Lloyd's index against am + bm2 where a and b are constants.[101]\n\nThe sample size (n) for a given degree of precision (D) for this regression is given by[101]\n\nwhere a is the constant in this regression, b is the slope, m is the mean and t is the critical value of the t distribution.\n\nIwao has proposed a sequential sampling test based on this regression.[102] The upper and lower limits of this test are based on critical densities mc where control of a pest requires action to be taken.\n\nwhere Nu and Nl are the upper and lower bounds respectively, a is the constant from the regression, b is the slope and i is the number of samples.\n\nKuno has proposed an alternative sequential stopping test also based on this regression.[103]\n\nwhere Tn is the total sample size, D is the degree of precision, n is the number of samples units, a is the constant and b is the slope from the regression respectively.\n\nKuno's test is subject to the condition that n ≥ (b − 1) / D2\n\nParrella and Jones have proposed an alternative but related stop line[104]\n\nwhere a and b are the parameters from the regression, N is the maximum number of sampled units and n is the individual sample size.\n\nMasaaki Morisita's index of dispersion ( Im ) is the scaled probability that two points chosen at random from the whole population are in the same sample.[105] Higher values indicate a more clumped distribution.\n\nwhere n is the total sample size, m is the sample mean and x are the individual values with the sum taken over the whole sample.\nIt is also equal to\n\nThis index is relatively independent of the population density but is affected by the sample size. Values > 1 indicate clumping; values < 1 indicate a uniformity of distribution and a value of 1 indicates a random sample.\n\nis distributed as a chi squared variable with n − 1 degrees of freedom.\n\nAn alternative significance test for this index has been developed for large samples.[106]\n\nwhere m is the overall sample mean, n is the number of sample units and z is the normal distribution abscissa. Significance is tested by comparing the value of z against the values of the normal distribution.\n\nA function for its calculation is available in the statistical R language in the vegan package.\n\nNote, not to be confused with Morisita's overlap index.\n\nSmith-Gill developed a statistic based on Morisita's index which is independent of both sample size and population density and bounded by −1 and +1. This statistic is calculated as follows[107]\n\nFirst determine Morisita's index ( Id ) in the usual fashion. Then let k be the number of units the population was sampled from. Calculate the two critical values\n\nwhere χ2 is the chi square value for n − 1 degrees of freedom at the 97.5% and 2.5% levels of confidence.\n\nThe standardised index ( Ip ) is then calculated from one of the formulae below.\n\nIp ranges between +1 and −1 with 95% confidence intervals of ±0.5. Ip has the value of 0 if the pattern is random; if the pattern is uniform, Ip < 0 and if the pattern shows aggregation, Ip > 0.\n\nSouthwood's index of spatial aggregation (k) is defined as\n\nwhere m is the mean of the sample and m* is Lloyd's index of crowding.[76]\n\nThis index may be used to test for over dispersion of the population. It is recommended that in applications n > 5[110] and that the sample total divided by the number of samples is > 3. In symbols\n\nwhere x is an individual sample value. The expectation of the index is equal to n and it is distributed as the chi-square distribution with n − 1 degrees of freedom when the population is Poisson distributed.[110] It is equal to the scale parameter when the population obeys the gamma distribution.\n\nIt can be applied both to the overall population and to the individual areas sampled individually. The use of this test on the individual sample areas should also include the use of a Bonferroni correction factor.\n\nThe index of cluster size (ICS) was created by David and Moore.[111] Under a random (Poisson) distribution ICS is expected to equal 0. Positive values indicate a clumped distribution; negative values indicate a uniform distribution.\n\nThe ICS is also equal to Katz's test statistic divided by ( n / 2 )1/2 where n is the sample size. It is also related to Clapham's test statistic. It is also sometimes referred to as the clumping index.\n\nGreen's index (GI) is a modification of the index of cluster size that is independent of n the number of sample units.[112]\n\nThis index equals 0 if the distribution is random, 1 if it is maximally aggregated and −1 / ( nm − 1 ) if it is uniform.\n\nThe distribution of Green's index is not currently known so statistical tests have been difficult to devise for it.\n\nBinary sampling (presence/absence) is frequently used where it is difficult to obtain accurate counts. The dispersal index (D) is used when the study population is divided into a series of equal samples ( number of units = N: number of units per sample = n: total population size = n x N ).[113] The theoretical variance of a sample from a population with a binomial distribution is\n\nwhere s2 is the variance, n is the number of units sampled and p is the mean proportion of sampling units with at least one individual present. The dispersal index (D) is defined as the ratio of observed variance to the expected variance. In symbols\n\nwhere varobs is the observed variance and varbin is the expected variance. The expected variance is calculated with the overall mean of the population. Values of D > 1 are considered to suggest aggregation. D( n − 1 ) is distributed as the chi squared variable with n − 1 degrees of freedom where n is the number of units sampled.\n\nwhere D is the dispersal index, n is the number of units per sample and N is the number of samples. C is distributed normally. A statistically significant value of C indicates overdispersion of the population.\n\nD is also related to intraclass correlation (ρ) which is defined as[115]\n\nwhere T is the number of organisms per sample, p is the likelihood of the organism having the sought after property (diseased, pest free, etc), and xi is the number of organism in the ith unit with this property. T must be the same for all sampled units. In this case with n constant\n\nIf the data can be fitted with a beta-binomial distribution then[115]\n\nMa has proposed a parameter (m0) − the population aggregation critical density - to relate population density to Taylor's law.[116]\n\nA number of statistical tests are known that may be of use in applications.\n\nA related statistic suggested by de Oliveria[117] is the difference of the variance and the mean.[118] If the population is Poisson distributed then\n\nwhere t is the Poisson parameter, s2 is the variance, m is the mean and n is the sample size. The expected value of s2 - m is zero. This statistic is distributed normally.[119]\n\nIf the Poisson parameter in this equation is estimated by putting t = m, after a little manipulation this statistic can be written\n\nThis is almost identical to Katz's statistic with ( n - 1 ) replacing n. Again OT is normally distributed with mean 0 and unit variance for large n. This statistic is the same as the Neyman-Scott statistic.\n\nde Oliveria actually suggested that the variance of s2 - m was ( 1 - 2t1/2 + 3t ) / n where t is the Poisson parameter. He suggested that t could be estimated by putting it equal to the mean (m) of the sample. Further investigation by Bohning[118] showed that this estimate of the variance was incorrect. Bohning's correction is given in the equations above.\n\nIn 1936 Clapham proposed using the ratio of the variance to the mean as a test statistic (the relative variance).[120] In symbols\n\nFor a Possion distribution this ratio equals 1. To test for deviations from this value he proposed testing its value against the chi square distribution with n degrees of freedom where n is the number of sample units. The distribution of this statistic was studied further by Blackman[121] who noted that it was approximately normally distributed with a mean of 1 and a variance ( Vθ ) of\n\nThe derivation of the variance was re analysed by Bartlett[122] who considered it to be\n\nFor large samples these two formulae are in approximate agreement. This test is related to the later Katz's Jn statistic.\n\nA refinement on this test has also been published[123] These authors noted that the original test tends to detect overdispersion at higher scales even when this was not present in the data. They noted that the use of the multinomial distribution may be more appropriate than the use of a Poisson distribution for such data. The statistic θ is distributed\n\nwhere N is the number of sample units, n is the total number of samples examined and xi are the individual data values.\n\nIf the number of individuals sampled (n) is large this estimate of the variance is in agreement with those derived earlier. However, for smaller samples these latter estimates are more precise and should be used.",
        pageTitle: "Taylor's law",
    },
    {
        title: "Teeter's law",
        link: "https://en.wikipedia.org/wiki/Teeter%27s_law",
        content:
            "Teeter's law is a wry observation about the biases of historical linguists, explaining how different investigators can arrive at radically divergent conceptions of the proto-language of a family:[1][2]\n\nThe language of the family you know best always turns out to be the most archaic.\n\nAlthough the law is named after the Americanist linguist Karl Teeter, it apparently does not appear in any of Teeter's works.[3]\nIt is customarily quoted from a 1976 review by the Indo-European linguist Calvert Watkins of Paul Friedrich's Proto-Indo-European syntax: the order of meaningful elements.[3]\nWatkins argued that Friedrich, after criticizing other scholars for overemphasizing particular branches of the family, had based his reconstruction of Proto-Indo-European syntax entirely on Homeric Greek.[1]\n\nThis article about historical linguistics is a stub. You can help Wikipedia by expanding it.",
        pageTitle: "Teeter's law",
    },
    {
        title: "law of conservation of complexity",
        link: "https://en.wikipedia.org/wiki/Law_of_conservation_of_complexity",
        content:
            "The law of conservation of complexity, also known as Tesler's Law,[1][2][3] or Waterbed Theory,[4] is an adage in human–computer interaction stating that every application has an inherent amount of complexity that cannot be removed or hidden. Instead, it must be dealt with, either in product development or in user interaction.\n\nThis poses the question of who should be exposed to the complexity. For example, should a software developer add complexity to the software code to make the  interaction simpler for the user or should the user deal with a complex interface so that the software code can be simple?[5]\n\nWhile working for Xerox PARC in the mid-1980s, Larry Tesler realized that the way users interact with applications was just as important as the application itself.[5] The book Designing for Interaction by Dan Saffer[6] includes an interview with Larry Tesler that describes the law of conservation of complexity.[5] The interview is popular among user experience and interaction designers.\n\nLarry Tesler argues that, in most cases, an engineer should spend an extra week reducing the complexity of an application vis-à-vis making millions of users spend an extra minute using the program because of the extra complexity.[5] However, Bruce Tognazzini proposes that people resist reductions to the amount of complexity in their lives.[7] Thus, when an application is simplified, users begin attempting more complex tasks.",
        pageTitle: "Law of conservation of complexity",
    },
    {
        title: "Thirlwall's law",
        link: "https://en.wikipedia.org/wiki/Thirlwall%27s_Law",
        content:
            'Thirlwall\'s law (named after Anthony Thirlwall) states that if long-run balance of payments equilibrium on current account is a requirement, and the real exchange rate stays relatively constant, then the long run growth of a country can be approximated by the ratio of the growth of exports to the income elasticity of demand for imports (Thirlwall, 1979).\n\nIf the real exchange rate varies considerably, but the price elasticities of demand for imports and exports are low, the long run growth of the economy will then be determined by the growth of world income times the ratio of the income elasticity of demand for exports and imports which are determined by the structural characteristics of countries. One important example of this is that if developing countries produce mainly primary products and low value manufactured goods with a low income elasticity of demand, while developed countries specialise in high income elasticity manufactured goods the developing countries will grow at a relatively slower rate (Davidson, 1991).\n\nThirlwall’s balance of payments constrained growth model –or Thirlwall’s Law- is often called the dynamic Harrod trade multiplier result following Roy Harrod’s (1933) static foreign trade multiplier result that Y = X/m, where Y is national income; X is exports and m is the marginal propensity to import, which is derived under the same assumptions as Thirlwall’s Law (O’Hara, 1999).\n\nThe assumption of balance of payments equilibrium on current account can be relaxed to allow capital flows (see Thirlwall and Nureldin Hussein, 1982), but for reasonable values of sustainable flows (e.g. 3% of GDP), capital flows make little empirical difference to the growth predictions of the basic model.\n\nSince 1979, the model has been extensively tested (for surveys of the literature see McCombie and Thirlwall 1994, 2004) with broad support for both developed and developing countries. See also "PSL Quarterly Review Vol. 64 No.259 (2011)" and "Models of Balance of Payments Constrained Growth: History, Theory and Evidence (2012)"\n\nThe model provides an alternative to the supply side models of neo-classical growth theory which are close economy models with no demand constraints. In the Thirlwall model the ultimate constraint on growth is a shortage of foreign exchange or the growth of exports to which factor supplies can adapt. It is changes in growth that equilibrate the balance of payments, not changes in relative prices in international trade.',
        pageTitle: "Thirlwall's Law",
    },
    {
        title: "Titius–Bode law",
        link: "https://en.wikipedia.org/wiki/Titius%E2%80%93Bode_law",
        content:
            "The Titius–Bode law (sometimes termed simply Bode's law) is a formulaic prediction of spacing between planets in any given planetary system. The formula suggests that, extending outward, each planet should be approximately twice as far from the Sun as the one before. The hypothesis correctly anticipated the orbits of Ceres (in the asteroid belt) and Uranus, but failed as a predictor of Neptune's orbit. It is named after Johann Daniel Titius and Johann Elert Bode.\n\nLater work by Mary Adela Blagg and D.E. Richardson significantly revised the original formula, and made predictions that were subsequently validated by new discoveries and observations. It is these re-formulations that offer \"the best phenomenological representations of distances with which to investigate the theoretical significance of Titius–Bode type Laws\".[1]\n\nThe law relates the semi-major axis \n  \n    \n      \n         \n        \n          a\n          \n            n\n          \n        \n         \n      \n    \n    {\\displaystyle ~a_{n}~}\n  \n of each planet's orbit outward from the Sun in units such that the Earth's semi-major axis is equal to 10:\n\nwhere \n  \n    \n      \n         \n        x\n        =\n        0\n        ,\n        3\n        ,\n        6\n        ,\n        12\n        ,\n        24\n        ,\n        48\n        ,\n        96\n        ,\n        192\n        ,\n        384\n        ,\n        768\n        …\n         \n      \n    \n    {\\displaystyle ~x=0,3,6,12,24,48,96,192,384,768\\ldots ~}\n  \n such that, with the exception of the first step, each value is twice the previous value.\nThere is another representation of the formula:\n\nwhere \n  \n    \n      \n         \n        n\n        =\n        −\n        ∞\n        ,\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n         \n        .\n      \n    \n    {\\displaystyle ~n=-\\infty ,0,1,2,\\ldots ~.}\n  \n\nThe resulting values can be divided by 10 to convert them into astronomical units (AU), resulting in the expression:\n\nFor the far outer planets, beyond Saturn, each planet is predicted to be roughly twice as far from the Sun as the previous object. Whereas the Titius–Bode law predicts Saturn, Uranus, Neptune, and Pluto at about 10, 20, 39, and 77 AU, the actual values are closer to 10, 19, 30, 40 AU.[a]\n\nThe first mention of a series approximating Bode's law is found in a textbook by D. Gregory (1715):[2]\n\nA similar sentence, likely paraphrased from Gregory (1715),[2][3] appears in a work published by C. Wolff in 1724.\n\nIn his 1766 translation of Bonnet's work, J.D. Titius added two of his own paragraphs to the statement above. The insertions were placed at the bottom of page 7 and at the top of page 8. The new paragraph is not in Bonnet's original French text, nor in translations of the work into Italian and English.\n\nThere are two parts to Titius's inserted text. The first part explains the succession of planetary distances from the Sun:\n\nIn 1772, J.E. Bode, then aged twenty-five, published an astronomical compendium,[5] in which he included the following footnote, citing Titius (in later editions):[b][6]\n\nThese two statements, for all their peculiar expression, and from the radii used for the orbits, seem to stem from an antique algorithm by a cossist.[c]\n\nMany precedents were found that predate the seventeenth century.[citation needed] Titius was a disciple of the German philosopher C.F. von Wolf (1679–1754), and the second part of the text that Titius inserted into Bonnet's work is in a book by von Wolf (1723),[7] suggesting that Titius learned the relation from him. Twentieth-century literature about Titius–Bode law attributes authorship to von Wolf.[citation needed] A prior version was written by D. Gregory (1702),[8] in which the succession of planetary distances 4, 7, 10, 16, 52, and 100 became a geometric progression with ratio 2. This is the nearest Newtonian formula, which was also cited by Benjamin Martin (1747)[9] and Tomàs Cerdà (c. 1760)[10] years before Titius's expanded translation of Bonnet's book into German (1766). Over the next two centuries, subsequent authors continued to present their own modified versions, apparently unaware of prior work.[1]\n\nTitius and Bode hoped that the law would lead to the discovery of new planets, and indeed the discovery of Uranus and Ceres – both of whose distances fit well with the law – contributed to the law's fame. Neptune's distance was very discrepant, however, and indeed Pluto – no longer considered a planet – is at a mean distance that roughly corresponds to that the Titius–Bode law predicted for the next planet out from Uranus.\n\nWhen originally published, the law was approximately satisfied by all the planets then known – i.e., Mercury through Saturn – with a gap between the fourth and fifth planets. Vikarius (Johann Friedrich) Wurm (1787) proposed a modified version of the Titius–Bode Law that accounted for the then-known satellites of Jupiter and Saturn, and better predicted the distance for Mercury.[11]\n\nThe Titius–Bode law was regarded as interesting, but of no great importance until the discovery of Uranus in 1781, which happens to fit into the series nearly exactly. Based on this discovery, Bode urged his contemporaries to search for a fifth planet. Ceres, the largest object in the asteroid belt, was found at Bode's predicted position in 1801.\n\nBode's law was widely accepted at that point, until in 1846 Neptune was discovered in a location that does not conform to the law. Simultaneously, due to the large number of asteroids discovered in the belt, Ceres was no longer a major planet. In 1898 the astronomer and logician C.S. Peirce used Bode's law as an example of fallacious reasoning.[12]\n\nThe discovery of Pluto in 1930 confounded the issue still further: Although nowhere near its predicted position according to Bode's law, it was very nearly at the position the law had designated for Neptune. The subsequent discovery of the Kuiper belt – and in particular the object Eris, which is more massive than Pluto, yet does not fit Bode's law – further discredited the formula.[13]\n\nThe Titius–Bode law predicts planets will be present at specific distances in astronomical units, which can be compared to the observed data for the planets and two dwarf planets in the Solar System:\n\nIn 1913, M.A. Blagg, an Oxford astronomer, re-visited the law.[14]\nShe analyzed the orbits of the planetary system and those of the satellite systems of the outer gas giants, Jupiter, Saturn and Uranus. She examined the log of the distances, trying to find the best 'average' difference.\n\nNote in particular that in Blagg's formula, the law for the Solar System was best represented by a progression in 1.7275, rather than the original value 2 used by Titius, Bode, and others.\n\nBlagg examined the satellite system of Jupiter, Saturn, and Uranus, and discovered the same progression ratio 1.7275, in each.\n\nHowever, the final form of the correction function  f  was not given in Blagg's 1913 paper, with Blagg noting that the empirical figures given were only for illustration. The empirical form was provided in the form of a graph (the reason that points on the curve are such a close match for empirical data, for objects discovered prior to 1913, is that they are the empirical data).\n\nFinding a formula that closely fit the empircal curve turned out to be difficult. Fourier analysis of the shape resulted in the following seven term approximation:[14]\n\nf\n                \n                  \n                    (\n                  \n                \n                 \n                θ\n                 \n                \n                  \n                    )\n                  \n                \n                \n                =\n                \n                0.4594\n                \n                +\n                \n                \n              \n              \n                0.396\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                θ\n                −\n                \n                  27.4\n                  \n                    ∘\n                  \n                \n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.168\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                2\n                 \n                (\n                 \n                θ\n                −\n                \n                  60.4\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.062\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                3\n                 \n                (\n                 \n                θ\n                −\n                \n                  28.1\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n              \n            \n            \n              \n                \n                +\n                \n                \n              \n              \n                0.053\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                4\n                 \n                (\n                 \n                θ\n                −\n                \n                  77.2\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.009\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                5\n                 \n                (\n                 \n                θ\n                −\n                \n                  22\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                \n                +\n                \n                0.012\n                 \n                cos\n                \n                \n                  \n                    (\n                  \n                \n                 \n                7\n                 \n                (\n                 \n                θ\n                −\n                \n                  40.4\n                  \n                    ∘\n                  \n                \n                )\n                 \n                \n                  \n                    )\n                  \n                \n                 \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\ f{\\bigl (}\\ \\theta \\ {\\bigr )}\\;=\\;0.4594\\;+\\;\\;&0.396\\ \\cos \\!{\\bigl (}\\ \\theta -27.4^{\\circ }\\ {\\bigr )}\\;+\\;0.168\\ \\cos \\!{\\bigl (}\\ 2\\ (\\ \\theta -60.4^{\\circ })\\ {\\bigr )}\\;+\\;0.062\\ \\cos \\!{\\bigl (}\\ 3\\ (\\ \\theta -28.1^{\\circ })\\ {\\bigr )}\\;+\\;\\\\\\;+\\;\\;&0.053\\ \\cos \\!{\\bigl (}\\ 4\\ (\\ \\theta -77.2^{\\circ })\\ {\\bigr )}\\;+\\;0.009\\ \\cos \\!{\\bigl (}\\ 5\\ (\\ \\theta -22^{\\circ })\\ {\\bigr )}\\;+\\;0.012\\ \\cos \\!{\\bigl (}\\ 7\\ (\\ \\theta -40.4^{\\circ })\\ {\\bigr )}~.\\end{aligned}}}\n\nAfter further analysis, Blagg gave the following simpler formula; however the price for the simpler form is that it produces a less accurate fit to the empirical data. Blagg gave it in an un-normalized form in her paper, which leaves the relative sizes of A, B, and f  ambiguous; it is shown here in normalized form (i.e. this version of  f  is scaled to produce values ranging from 0 to 1, inclusive):[15]\n\nf\n        \n          \n            (\n          \n        \n         \n        θ\n         \n        \n          \n            )\n          \n        \n        \n        =\n        \n        0.249\n        \n        +\n        \n        0.860\n         \n        \n          (\n          \n            \n              \n                \n                   \n                  cos\n                  ⁡\n                   \n                  Ψ\n                   \n                \n                \n                   \n                  3\n                  −\n                  cos\n                  \n                  \n                    (\n                    \n                       \n                      2\n                       \n                      Ψ\n                       \n                    \n                    )\n                  \n                   \n                \n              \n            \n            \n            +\n            \n            \n              \n                1\n                \n                   \n                  6\n                  −\n                  4\n                   \n                  cos\n                  \n                  \n                    (\n                    \n                       \n                      2\n                       \n                      Ψ\n                      −\n                      \n                        60\n                        \n                          ∘\n                        \n                      \n                    \n                    )\n                  \n                   \n                \n              \n            \n          \n          )\n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ f{\\bigl (}\\ \\theta \\ {\\bigr )}\\;=\\;0.249\\;+\\;0.860\\ \\left({\\frac {\\ \\cos \\ \\Psi \\ }{\\ 3-\\cos \\!\\left(\\ 2\\ \\Psi \\ \\right)\\ }}\\;+\\;{\\frac {1}{\\ 6-4\\ \\cos \\!\\left(\\ 2\\ \\Psi -60^{\\circ }\\right)\\ }}\\right)\\ ,}\n\nwhere \n  \n    \n      \n         \n        Ψ\n        ≡\n        θ\n        −\n        \n          27.5\n          \n            ∘\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\ \\Psi \\equiv \\theta -27.5^{\\circ }~.}\n\nNeither of these formulas for function  f  are used in the calculations below: The calculations here are based on a graph of function  f  which was drawn based on observed data.\n\nHer paper was published in 1913, and was forgotten until 1953, when A.E. Roy came across it while researching another problem.[16]\nRoy noted that Blagg herself had suggested that her formula could give approximate mean distances of other bodies still undiscovered in 1913. Since then, six bodies in three systems examined by Blagg had been discovered: Pluto, Sinope (Jupiter IX), Lysithea (J X), Carme (J XI), Ananke (J XII), and Miranda (Uranus V).\n\nRoy found that all six fitted very closely. This might have been an exaggeration: out of these six bodies, four were sharing positions with objects that were already known in 1913; concerning the two others, there was a ~6% overestimate for Pluto; and later, a 6% underestimate for Miranda became apparent.[15]\n\nBodies in parentheses were not known in 1913, when Blagg wrote her paper. Some of the calculated distances in the Saturn and Uranus systems are not very accurate. This is because the low values of constant B in the table above make them very sensitive to the exact form of the  function  f .\n\nIn a 1945 Popular Astronomy magazine article,[17]\nthe science writer D.E. Richardson apparently independently arrived at the same conclusion as Blagg: That the progression ratio is 1.728 rather than 2. His spacing law is in the form:\n\nR\n          \n            n\n          \n        \n        =\n        \n          \n            (\n          \n        \n         \n        1.728\n         \n        \n          \n            \n              )\n            \n          \n          \n            n\n          \n        \n         \n        \n          ϱ\n          \n            n\n          \n        \n        (\n        \n          θ\n          \n            n\n          \n        \n        )\n         \n        ,\n      \n    \n    {\\displaystyle \\ R_{n}={\\bigl (}\\ 1.728\\ {\\bigr )}^{n}\\ \\varrho _{n}(\\theta _{n})\\ ,}\n\nwhere \n  \n    \n      \n        \n          ϱ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\varrho _{n}}\n  \n is an oscillatory function with period \n  \n    \n      \n        2\n        π\n      \n    \n    {\\displaystyle 2\\pi }\n  \n, representing distances \n  \n    \n      \n        \n          ϱ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\varrho _{n}}\n  \n from an off-centered origin to points on an ellipse.\n\nNieto, who conducted the first modern comprehensive review of the Titius–Bode Law,[18] noted that \"The psychological hold of the Law on astronomy has been such that people have always tended to regard its original form as the one on which to base theories.\" He was emphatic that \"future theories must rid themselves of the bias of trying to explain a progression ratio of 2\":\n\nOne thing which needs to be emphasized is that the historical bias towards a progression ratio of 2 must be abandoned. It ought to be clear that the first formulation of Titius (with its asymmetric first term) should be viewed as a good first guess. Certainly, it should not necessarily be viewed as the best guess to refer theories to. But in astronomy the weight of history is heavy ... Despite the fact that the number 1.73 is much better, astronomers cling to the original number 2.[1]\n\nNo solid theoretical explanation underlies the Titius–Bode law – but it is possible that, given a combination of orbital resonance and shortage of degrees of freedom, any stable planetary system has a high probability of satisfying a Titius–Bode-type relationship. Since it may be a mathematical coincidence rather than a \"law of nature\", it is sometimes referred to as a rule instead of \"law\".[19] Astrophysicist Alan Boss states that it is just a coincidence, and the planetary science journal Icarus no longer accepts papers attempting to provide improved versions of the \"law\".[13]\n\nOrbital resonance from major orbiting bodies creates regions around the Sun that are free of long-term stable orbits. Results from simulations of planetary formation support the idea that a randomly chosen, stable planetary system will likely satisfy a Titius–Bode law.[20]\n\nDubrulle and Graner[21][22] showed that power-law distance rules can be a consequence of collapsing-cloud models of planetary systems possessing two symmetries: rotational invariance (i.e., the cloud and its contents are axially symmetric) and scale invariance (i.e., the cloud and its contents look the same on all scales). The latter is a feature of many phenomena considered to play a role in planetary formation, such as turbulence.\n\nOnly a limited number of systems are available upon which Bode's law can presently be tested; two solar planets have enough large moons that probably formed in a process similar to that which formed the planets: The four large satellites of Jupiter and the biggest inner satellite (i.e., Amalthea) cling to a regular, but non-Titius-Bode, spacing, with the four innermost satellites locked into orbital periods that are each twice that of the next inner satellite. Similarly, the large moons of Uranus have a regular but non-Titius-Bode spacing.[23]\nHowever, according to Martin Harwit\n\nOf the recent discoveries of extrasolar planetary systems, few have enough known planets to test whether similar rules apply. An attempt with 55 Cancri suggested the equation\n\nand controversially[25]\npredicts an undiscovered planet or asteroid field for \n  \n    \n      \n         \n        n\n        =\n        5\n         \n      \n    \n    {\\displaystyle ~n=5~}\n  \n at 2 AU.[26]\nFurthermore, the orbital period and semi-major axis of the innermost planet in the 55 Cancri system have been greatly revised (from 2.817 days to 0.737 days and from 0.038 AU to 0.016 AU, respectively) since the publication of these studies.[27]\n\nRecent astronomical research suggests that planetary systems around some other stars may follow Titius-Bode-like laws.[28][29]\nBovaird & Lineweaver (2013)[30]\napplied a generalized Titius-Bode relation to 68 exoplanet systems that contain four or more planets. They showed that 96% of these exoplanet systems adhere to a generalized Titius-Bode relation to a similar or greater extent than the Solar System does. The locations of potentially undetected exoplanets are predicted in each system.[30]\n\nSubsequent research detected 5 candidate planets from the 97 planets predicted for the 68 planetary systems. The study showed that the actual number of planets could be larger. The occurrence rates of Mars- and Mercury-sized planets are unknown, so many planets could be missed due to their small size. Other possible reasons that may account for apparent discrepancies include planets that do not transit the star or circumstances in which the predicted space is occupied by circumstellar disks. Despite these types of allowances, the number of planets found with Titius–Bode law predictions was lower than expected.[31]\n\nIn a 2018 paper, the idea of a hypothetical eighth planet around TRAPPIST-1 named \"TRAPPIST‑1i\", was proposed by using the Titius–Bode law. TRAPPIST‑1i had a prediction based exclusively on the Titius–Bode law with an orbital period of 27.53 ± 0.83 days.[32]\n\nFinally, raw statistics from exoplanetary orbits strongly point to a general fulfillment of Titius-Bode-like laws (with exponential increase of semi-major axes as a function of planetary index) in all the exoplanetary systems; when making a blind histogram of orbital semi-major axes for all the known exoplanets for which this magnitude is known,[33] and comparing it with what should be expected if planets distribute according to Titius-Bode-like laws, a significant degree of agreement (i.e., 78%)[34]\nis obtained.",
        pageTitle: "Titius–Bode law",
    },
    {
        title: "Tobler's first law of geography",
        link: "https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography",
        content:
            'The First Law of Geography, according to Waldo Tobler, is "everything is related to everything else, but near things are more related than distant things."[1] This first law is the foundation of the fundamental concepts of spatial dependence and spatial autocorrelation and is utilized specifically for the inverse distance weighting method for spatial interpolation and to support the regionalized variable theory for kriging.[2] The first law of geography is the fundamental assumption used in all spatial analysis.[3]\n\nTobler first presented his seminal idea during a meeting of the International Geographical Union\'s Commission on Qualitative Methods held in 1969 and later published by him in 1970 in his publication "A Computer Movie Simulating Urban Growth in the Detroit Region".[1] In this paper Tobler created a model of the population growth in Detroit, and was discussing variables included within the model, and Tobler was probably not extremely serious when he originally invoked the first law and instead was explaining limitations brought about by computers of the 1970s.[1][3] He certainly did not think it would be as prominent in geography as it is today.[3] Though simple in its presentation, this idea is profound. Without it, "the full range of conditions anywhere on the Earth\'s surface could be packed within any small area. There would be no regions of approximately homogeneous conditions to be described by giving attributes to area objects. Topographic surfaces would vary chaotically, with infinite slopes everywhere, and the contours of such surfaces would be infinitely dense and contorted. Spatial analysis, and indeed life itself, would be impossible."[4]\n\nWhile Tobler is the first to present the concept as the first law of geography, it existed in some form as a concept before him. In 1935, R.A. Fisher said "the widely verified fact that patches in close proximity are commonly more alike, as judged by the yield of crops, than those which are further apart."[5][6] Tobler was made aware of this by a peer-reviewer, and seems to have come up with the first law independently.[5]\n\nTobler\'s law was proposed towards the end of the quantitative revolution in geography, which saw a shift towards using systematic and scientific methods in geography. This paradigm shifted the discipline from idiographic geography to an empirical law-making nomothetic geography.[7][8] This law-making approach was conducive to the acceptance of Tobler\'s law, and Tobler\'s law can be seen as a direct product of the quantitative revolution.[9]\n\nIn 2003, the American Association of Geographers held a panel titled "On Tobler\'s First Law of Geography", with panelists selected to represent diverse geographic interests and their philosophical perspective on Tobler\'s First Law.[10] In 2004, the peer reviewed journal Annals of the Association of American Geographers included a section titled "Methods, Models, and GIS Forum: On Tobler\'s First Law of Geography" that contained several peer-reviewed papers from members of the 2003 panel.[11] Of note, this section also had a paper by Tobler titled "On the First Law of Geography: A Reply", which contained his response to the 2003 panel and insight into the first law.[5] In this publication, Tobler discussed his less well known second law, which complements the first:\n\n"The phenomenon external to an area of interest affects what goes on inside."\n\nThe theory is based upon the concept of the friction of distance "where distance itself hinders interaction between places. The farther two places are apart, the greater the hindrance",[12] or cost. For example, one is less likely to travel across town to purchase a sandwich than walk to the corner store for the same sandwich. In this example, hindrance, or cost, can readily be counted in time (amount of time as well as the value of time), transportation costs, and personal muscle energy loss which are added to the purchase price and thus result in high levels of friction. The friction of distance and the increase in cost combine, causing the distance decay effect.\n\nSome have disputed the usefulness and validity of Tobler\'s first law.[5][13]  In general, some also dispute the entire concept of laws in geography and the social sciences. These criticisms have been addressed by Tobler and others.[5]\n\nAn anonymous reviewer pointed out that Tobler\'s first law is remarkably close to a phrase in a book by R.A. Fisher in 1935.[5][6] Tobler seems to have come up with the first law independently.[5]\n\nSome view Tobler\'s first law to be limited and have proposed amendments. One example of such an amendment proposed by Robert T. Walker combines Tobler\'s first law with von Thünen\'s concept of accessibility to offer an explanation for the description provided by Tobler.[9] The resulting law, referred to by Walker as "The Tobler-von Thünen law", is:\n\n″Everything is related to everything else, but near things are more related than distant things,\nas a consequence of accessibility."',
        pageTitle: "Tobler's first law of geography",
    },
    {
        title: "Twyman's law",
        link: "https://en.wikipedia.org/wiki/Twyman%27s_law",
        content:
            'Twyman\'s law states that "Any figure that looks interesting or different is usually wrong",[1] following the principle that "the more unusual or interesting the data, the more likely they are to have been the result of an error of one kind or another". It is named after the media and market researcher Tony Twyman and has been described as one of the most important laws of data analysis.[2][3][4]\n\nThe law is based on the fact that errors in data measurement and analysis can lead to observed quantities that are wildly different from typical values. These errors are usually more common than real changes of similar magnitude in the underlying process being measured. For example, if an analyst at a software company notices that the number of users has doubled overnight, the most likely explanation is a bug in logging, rather than a true increase in users.[3]\n\nThe law can also be extended to situations where the underlying data is influenced by unexpected factors that differ from what was intended to be measured. For example, when schools show unusually large improvements in test scores, subsequent investigation often reveals that those scores were driven by fraud.[5][6]',
        pageTitle: "Twyman's law",
    },
    {
        title: "Van Loon's law",
        link: "https://en.wikipedia.org/wiki/Van_Loon%27s_law",
        content:
            'Van Loon\'s law appears to be a poorly attributed statement drawn from a book published in 1929. It may originate in Hendrik Willem van Loon.[1]\n\n"The amount of mechanical development will always be in inverse ratio to the number of slaves that happen to be at a country’s disposal."[2]\n\nAttributed as quoted in: Stuart Chase; Men and Machines; (The Macmillan Company, N. Y.; 1929).[3][4]\n\nThis economic term article is a stub. You can help Wikipedia by expanding it.',
        pageTitle: "Van Loon's law",
    },
    {
        title: "Vegard's law",
        link: "https://en.wikipedia.org/wiki/Vegard%27s_law",
        content:
            "In crystallography, materials science and metallurgy, Vegard's law is an empirical finding (heuristic approach) resembling the rule of mixtures. In 1921, Lars Vegard discovered that the lattice parameter of a solid solution of two constituents is approximately a weighted mean of the two constituents' lattice parameters at the same temperature:[1][2]\n\ne.g., in the case of a mixed oxide of uranium and plutonium as used in the fabrication of MOX nuclear fuel:\n\nVegard's law assumes that both components A and B in their pure form (i.e., before mixing) have the same crystal structure. Here, aA(1-x)Bx is the lattice parameter of the solid solution, aA and aB are the lattice parameters of the pure constituents, and x is the molar fraction of B in the solid solution.\n\nVegard's law is seldom perfectly obeyed; often deviations from the linear behavior are observed. A detailed study of such deviations was conducted by King.[3] However, it is often used in practice to obtain rough estimates when experimental data are not available for the lattice parameter for the system of interest.\n\nFor systems known to approximately obey Vegard's law, the approximation may also be used to estimate the composition of a solution from knowledge of its lattice parameters, which are easily obtained from diffraction data.[4] For example, consider the semiconductor compound InPxAs(1-x). A relation exists between the constituent elements and their associated lattice parameters, a, such that:\n\nWhen variations in lattice parameters are very small across the entire composition range, Vegard's law becomes equivalent to Amagat's law.\n\nIn many binary semiconducting systems, the band gap in semiconductors is approximately a linear function of the lattice parameter. Therefore, if the lattice parameter of a semiconducting system follows Vegard's law, one can also write a linear relationship between the band gap and composition. Using InPxAs(1-x) as before, the band gap energy, \n  \n    \n      \n        \n          E\n          \n            g\n          \n        \n      \n    \n    {\\displaystyle E_{g}}\n  \n,  can be written as:\n\nSometimes, the linear interpolation between the band gap energies is not accurate enough, and a second term to account for the curvature of the band gap energies as a function of composition is added. This curvature correction is characterized by the bowing parameter, b:\n\nThe following excerpt from Takashi Fujii (1960)[5] summarises well the limits of Vegard’s law in the context of mineralogy and also makes the link with the Gladstone–Dale equation:\n\nIn mineralogy, the tacit assumption for the linear correlation of the density and the chemical composition of a solid solution is twofold: one is an ideal solid solution and the other identical or nearly identical molar volumes of the components. …\nCoefficients of thermal expansion and compressibilities of the ideal solid solution can be discussed in the same manner. But when the solid solution is ideal, the linear correlation of molar heat capacities and chemical composition is possible. The linear correlation of refractive index and chemical composition of an isotropic solid solution can be derived from the Gladstone–Dale equation, but it is required that the system must be ideal and the molar volumes of the components are equal or nearly equal. If the concept of the volume fraction is introduced, density, coefficient of thermal expansion, compressibility and refractive index can be correlated linearly with the volume fraction in an ideal system.“[6]\n\nWhen considering the empirical correlation of some physical properties and the chemical composition of solid compounds, other relationships, rules, or laws, also closely resembles Vegard's law, and in fact the more general rule of mixtures:",
        pageTitle: "Vegard's law",
    },
    {
        title: "Verdoorn's law",
        link: "https://en.wikipedia.org/wiki/Verdoorn%27s_law",
        content:
            "Verdoorn's law is named after Dutch economist Petrus Johannes Verdoorn.[1][2][3] It states that in the long run productivity generally grows proportionally to the square root of output. In economics, this law pertains to the relationship between the growth of output and the growth of productivity. According to the law, faster growth in output increases productivity due to increasing returns. Verdoorn argued[4] that \"in the long run a change in the volume of production, say about 10 per cent, tends to be associated with an average increase in labor productivity of 4.5 per cent.\" The Verdoorn coefficient close to 0.5 (0.484) is also found in subsequent estimations of the law.[5]\n\nVerdoorn's law describes a simple long-run relation between productivity and output growth, whose coefficients were empirically estimated in 1949 by the Dutch economist. The relation takes the following form: \n  \n    \n      \n        p\n        =\n        a\n        +\n        b\n        Q\n      \n    \n    {\\displaystyle p=a+bQ}\n\nwhere p is the labor productivity growth, Q the output growth (value-added), b is the Verdoorn coefficient and a is the exogenous productivity growth rate.[6]\n\nVerdoorn's law differs from \"the usual hypothesis […] that the growth of productivity is mainly to be explained by the progress of knowledge in science and technology\",[7] as it typically is in neoclassical models of growth (notably the Solow model). Verdoorn's law is usually associated with cumulative causation models of growth, in which demand rather than supply determine the pace of accumulation.\n\nNicholas Kaldor and Anthony Thirlwall developed models of export-led growth based on Verdoorn's law. For a given country an expansion of the export sector may cause specialisation in the production of export products, which increase the productivity level, and increase the level of skills in the export sector. This may then lead to a reallocation of resources from the less efficient non-trade sector to the more productive export sector, lower prices for traded goods and higher competitiveness. This productivity change may then lead expanded exports and to output growth.\n\nThirlwall shows[8] that for several countries the rate of growth never exceeds the ratio of the rate of growth of exports to income elasticity of demand for imports. This implies that growth is limited by the balance of payments equilibrium. This result is known as Thirlwall's Law.\n\nSometimes Verdoorn's law is called Kaldor-Verdoorn's law or effect.",
        pageTitle: "Verdoorn's law",
    },
    {
        title: "Verner's law",
        link: "https://en.wikipedia.org/wiki/Verner%27s_law",
        content:
            "Verner's law describes a historical sound change in the Proto-Germanic language whereby consonants that would usually have been the voiceless fricatives *f, *þ, *s, *h, *hʷ, following an unstressed syllable, became the voiced fricatives *β, *ð, *z, *ɣ, *ɣʷ.[a] The law was formulated by Karl Verner, and first published in 1877.\n\nA seminal insight into how the Germanic languages diverged from their Indo-European ancestor had been established in the early nineteenth century, and had been formulated as Grimm's law. Amongst other things, Grimm's law described how the Proto-Indo-European voiceless stops *p, *t, *k, and *kʷ  regularly changed into Proto-Germanic *f (bilabial fricative [ɸ]), *þ (dental fricative [θ]), *h (velar fricative [x]), and *hʷ (labio-velar fricative [xw]).[1]\n\nHowever, there appeared to be a large set of words in which the agreement of Latin, Greek, Sanskrit, Baltic, Slavic etc. guaranteed Proto-Indo-European *p, *t or *k, and yet the Germanic reflex was not the expected, unvoiced fricatives *f, *þ, *h, *hʷ  but rather their voiced counterparts *β, *ð, *ɣ, *ɣʷ. A similar problem obtained with Proto-Indo-European *s, which sometimes appeared as Proto-Germanic *z.[2]\n\nAt first, irregularities did not cause concern for scholars since there were many examples of the regular outcome. Increasingly, however, it became the ambition of linguists like the Neogrammarians to formulate general and exceptionless rules of sound change that would account for all the data (or as close to all the data as possible), not merely for a well-behaved subset of it.\n\nOne classic example of Proto-Indo-European *t → Proto-Germanic *ð is the word for 'father'. Proto-Indo-European *ph₂tḗr (here, the macron marks vowel length) → Proto-Germanic *faðēr (instead of expected *faþēr).[2] In the structurally similar family term *bʰréh₂tēr 'brother', Proto-Indo-European *t did indeed develop as predicted by Grimm's Law (Germanic *brōþēr).[3] Even more curiously, scholars often found both *þ and *ð as reflexes of Proto-Indo-European *t in different forms of one and the same root, e.g. *werþaną 'to turn', preterite third-person singular *warþ 'he turned', but preterite third-person plural *wurðun and past participle *wurðanaz.\n\nKarl Verner is traditionally credited as the first scholar to note the factor governing the distribution of the two outcomes. Verner observed that the apparently unexpected voicing of Proto-Indo-European voiceless stops occurred if they were non-word-initial and if the vowel preceding them carried no stress in Proto-Indo-European. The original location of stress was often retained in Greek and early Sanskrit; in Germanic, though, stress eventually became fixed on the initial (root) syllable of all words.\n\nThe following table illustrates the sound changes according to Verner. In the bottom row, for each pair, the sound on the right represents the sound changed according to Verner's Law.\n\nThe crucial difference between *patḗr and *bʰrā́tēr was therefore one of second-syllable versus first-syllable stress (compare Sanskrit pitā́ versus bhrā́tā).[2]\n\nThe *werþaną : *wurðun contrast is likewise explained as due to stress on the root versus stress on the inflectional suffix (leaving the first syllable unstressed). There are also other Vernerian alternations, as illustrated by modern German ziehen 'to draw, pull': Old High German zogōn 'to tug, drag' ← Proto-Germanic *teuhaną : *tugōną ← Pre-Germanic *déwk-o-nom : *duk-éh₂-yo-nom 'lead'.[2]\n\nThe change described by Verner's Law also accounts for Proto-Germanic *z as the development of Proto-Indo-European *s in some words. Since this *z changed to *r in the North Germanics and in West Germanic (German, Dutch, English, Frisian), Verner's Law resulted in alternation of *s and *r in some inflectional paradigms, known as grammatischer Wechsel.[4] For example, the Old English verb ceosan 'choose' had the past plural form curon and the past participle (ge)coren. These three forms derived from Proto-Germanic *keusaną : *kuzun ~ *kuzanaz, which again derived from Pre-Germanic *géws-o-nom : *gus-únt ~ *gus-o-nós 'taste, try'. We would have **corn for chosen in Modern English if the consonants of choose and chose had not been morphologically levelled (compare the Dutch kiezen 'to choose' : verkoren 'chosen'). On the other hand, Vernerian *r has not been levelled out in English were ← Proto-Germanic *wēzun, related to English was. Similarly, English lose, though it has the weak form lost, also has the archaic form †lorn (now seen in the compounds forlorn and lovelorn) (compare Dutch verliezen : verloren); in German, on the other hand, the *s has been levelled out both in war 'was' (plural waren 'were') and verlieren 'lose' (participle verloren 'lost').\n\nWhereas the North Germanic and West Germanic languages clearly show the effects of Verner's law, those patterns seldom appear in Gothic, the representative of East Germanic. This is usually thought to be because Gothic eliminated most Verner's law variants through analogy with the unaffected consonants.[5]\n\nKarl Verner published his discovery in the article \"Eine Ausnahme der ersten Lautverschiebung\" (An exception to the first sound shift) in Kuhn's Journal of Comparative Linguistic Research in 1877,[6] but he had already presented his theory on 1 May 1875 in a comprehensive personal letter to his friend and mentor, Vilhelm Thomsen.[citation needed]\n\nA letter shows that Eduard Sievers had hit on the same explanation by 1874, but did not publish it.[7]\n\nVerner's theory was received with great enthusiasm by the young generation of comparative philologists, the so-called Junggrammatiker, because it was an important argument in favour of the Neogrammarian dogma that the sound laws were without exceptions (\"die Ausnahmslosigkeit der Lautgesetze\").\n\nThe change in pronunciation described by Verner's Law must have occurred before the shift of stress to the first syllable: the voicing of the new consonant in Proto-Germanic is conditioned by which syllable is stressed in Proto-Indo-European, yet this syllabic stress has disappeared in Proto-Germanic, so the change in the consonant must have occurred at a time when the syllabic stress in earlier Proto-Germanic still conformed to the Indo-European pattern. However, the syllabic stress shift erased the conditioning environment, and made the variation between voiceless fricatives and their voiced alternants look mysteriously haphazard.\n\nUntil recently it was assumed that Verner's law was productive after Grimm's Law, and this remains the standard account: R. D. Fulk's 2018 Comparative Grammar of the Early Germanic Languages, for example, finds that 'Grimm's law should be assumed to antecede Verner's law'.[8]\n\nBut it has been pointed out that, even if the sequence is reversed, the result can be just the same given certain conditions, and the thesis that Verner's Law might have been valid before Grimm's Law—maybe long before it—has been finding more and more acceptance.[9] Accordingly, this order now would have to be assumed:\n\nThis chronological reordering would have far-reaching implications for the shape and development of the Proto-Germanic language. If Verner's law operated before Grimm's law, one would expect the voicing of Proto-Indo-European *p, *t, *k, and *kʷ  to produce *b, *d, *g, and *gʷ, which would have been identical with the existing Proto-Indo-European voiced stops. Yet it is clear that consonants affected by Verner's law merged with the descendants of the Proto-Indo-European voiced aspirate stops, not of the plain voiced stops. The usual proposed explanation for this is to postulate aspiration in the voiceless stops of the dialect of Indo-European that gave rise to Proto-Germanic.\n\nHere is a table describing the sequence of changes in this alternative ordering:\n\n(This can however be bypassed in the glottalic theory framework, where the voiced aspirate stops are replaced with plain voiced stops, and plain voiced stops with glottalized stops.)\n\nMeanwhile, Noske (2012) argued that Grimm's Law and Verner's Law must have been part of a single bifurcating chain shift.\n\nAn exact parallel to Verner's law is found in the neighboring Finnic languages, where it forms a part of the system of consonant gradation: a single voiceless consonant (*p, *t, *k, *s) becomes weakened (*b, *d, *g; *h < *z) when occurring after an unstressed syllable. As word stress in Finnic is predictable (primary stress on the initial syllable, secondary stress on odd-numbered non-final syllables), and has remained so since Proto-Uralic, this change did not produce any alternation in the shape of word roots. However, it manifests in the shape of numerous inflectional or derivational suffixes, and is therefore called \"suffixal gradation\".[10]\n\nLauri Posti argued that suffixal gradation in Finnic represents Germanic influence, in particular reflecting the pronunciation of Proto-Finnic by a hypothetical Germanic-speaking superstrate (often assumed to account for the great number of Germanic loanwords already in Proto-Finnic).[11] On the contrary, consonant gradation has also been viewed as inheritance from Proto-Uralic, as it occurs also in other Uralic languages. In particular, suffixal gradation under identical conditions also exists in Nganasan. The possibility of the opposite direction of influence – from Finnic to Germanic – has also been suggested.[12]",
        pageTitle: "Verner's law",
    },
    {
        title: "Vierordt's law",
        link: "https://en.wikipedia.org/wiki/Vierordt%27s_law",
        content:
            'Karl von Vierordt in 1868[1] was the first to record a law of time perception which relates perceived duration to actual duration over different interval magnitudes, and according to task complexity.  It states that, retrospectively, "short" intervals of time (e.g., 10 seconds) tend to be overestimated, and "long" intervals of time tend to be underestimated. The other major paradigm of time estimation methodology measures time prospectively.\n\nModern research suggests that "Vierordt’s law is caused by an unnatural yet widely used experimental protocol".[2]\n\nThis time-related article is a stub. You can help Wikipedia by expanding it.',
        pageTitle: "Vierordt's law",
    },
    {
        title: "Wagner's law",
        link: "https://en.wikipedia.org/wiki/Wagner%27s_law",
        content:
            "Wagner's law, also known as the law of increasing[a] state activity,[2] is the observation that public expenditure increases as national income rises.[3] It is named after the German economist Adolph Wagner (1835–1917), who first observed the effect in his own country and then for other countries.[4]\n\nThe principle is closely tied to industrialization. It predicts that the development of an industrial economy will be accompanied by an increased share of public expenditure in gross national product:\n\nThe advent of modern industrial society will result in increasing political pressure for social progress and increased allowance for social consideration by industry.\n\nWagner's law suggests that welfare states evolves from free-market capitalism because the population votes for ever-increasing social services as income grows. In spite of some ambiguity, Wagner's statement in formal terms has been interpreted by Richard Musgrave as follows:\n\nAs progressive nations industrialize, the share of the public sector in the national economy grows continually. The increase in State Expenditure is needed because of three main reasons. Wagner himself identified these as (i) social activities of the state, (ii) administrative and protective actions, and (iii) welfare functions. The material below is an apparently much more generous interpretation of Wagner's original premise.\n\nA 1961 study by the British economists Alan T. Peacock and Jack Wiseman found that Wagner's Law aptly described public expenditure in the United Kingdom in the period between 1891 and 1955.[6] They further stated:\n\nOther studies have likewise found a strong relationship between public expenditure and per-capita gross domestic product.[8] Studies have tended to show support for Wagner's law in developing countries, though some have found only weak support.[1][9]\n\nThere have been a variety of studies testing Wagner's law in individual countries:",
        pageTitle: "Wagner's law",
    },
    {
        title: "Walras's law",
        link: "https://en.wikipedia.org/wiki/Walras%27s_law",
        content:
            "Walras's law is a principle in general equilibrium theory asserting that budget constraints imply that the values of excess demand (or, conversely, excess market supplies) must sum to zero regardless of whether the prices are general equilibrium prices. That is:\n\nwhere \n  \n    \n      \n        \n          p\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle p_{j}}\n  \n is the price of good j and \n  \n    \n      \n        \n          D\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle D_{j}}\n  \n and \n  \n    \n      \n        \n          S\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle S_{j}}\n  \n are the demand and supply respectively of good j.\n\nWalras's law is named after the economist Léon Walras[1] of the University of Lausanne who formulated the concept in his Elements of Pure Economics of 1874.[2] Although the concept was expressed earlier but in a less mathematically rigorous fashion by John Stuart Mill in his Essays on Some Unsettled Questions of Political Economy (1844),[3]  Walras noted the mathematically equivalent proposition that when considering any particular market, if all other markets in an economy are in equilibrium, then that specific market must also be in equilibrium. The term \"Walras's law\" was coined by Oskar Lange[4] to distinguish it from Say's law. Some economic theorists[5] also use the term to refer to the weaker proposition that the total value of excess demands cannot exceed the total value of excess supplies.\n\nWalras's law is a consequence of finite budgets. If a consumer spends more on good A then they must spend and therefore demand less of good B, reducing B's price. The sum of the values of excess demands across all markets must equal zero, whether or not the economy is in a general equilibrium. This implies that if positive excess demand exists in one market, negative excess demand must exist in some other market. Thus, if all markets but one are in equilibrium, then that last market must also be in equilibrium.\n\nThis last implication is often applied in formal general equilibrium models. In particular, to characterize general equilibrium in a model with m agents and n commodities, a modeler may impose market clearing for n – 1 commodities and \"drop the n-th market-clearing condition.\" In this case, the modeler should include the budget constraints of all m agents (with equality). Imposing the budget constraints for all m agents ensures that Walras's law holds, rendering the n-th market-clearing condition redundant. In other words, suppose there are 100 markets, and someone saw that 99 are in equilibrium[note 1], they would know the remaining market must also be in equilibrium without having to look.\n\nIn the former example, suppose that the only commodities in the economy are cherries and apples, and that no other markets exist. This is an exchange economy with no money, so cherries are traded for apples and vice versa. If excess demand for cherries is zero, then by Walras's law, excess demand for apples is also zero. If there is excess demand for cherries, then there will be a surplus (excess supply, or negative excess demand) for apples; and the market value of the excess demand for cherries will equal the market value of the excess supply of apples.\n\nWalras's law is ensured if every agent's budget constraint holds with equality. An agent's budget constraint is an equation stating that the total market value of the agent's planned expenditures, including saving for future consumption, must be less than or equal to the total market value of the agent's expected revenue, including sales of financial assets such as bonds or money.  When an agent's budget constraint holds with equality, the agent neither plans to acquire goods for free (e.g., by stealing), nor does the agent plan to give away any goods for free. If every agent's budget constraint holds with equality, then the total market value of all agents' planned outlays for all commodities (including saving, which represents future purchases) must equal the total market value of all agents' planned sales of all commodities and assets. It follows that the market value of total excess demand in the economy must be zero, which is the statement of Walras's law. Walras's law implies that if there are n markets and n – 1 of these are in equilibrium, then the last market must also be in equilibrium, a property which is essential in the proof of the existence of equilibrium.\n\nConsider an exchange economy with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n agents and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n divisible goods.\n\nFor every agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, let \n  \n    \n      \n        \n          E\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle E_{i}}\n  \n be their initial endowment vector and \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n their Marshallian demand function (demand vector as a function of prices and income).\n\nGiven a price vector \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, the income of consumer \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is \n  \n    \n      \n        p\n        ⋅\n        \n          E\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p\\cdot E_{i}}\n  \n. Hence, their demand vector is \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        (\n        p\n        ,\n        p\n        ⋅\n        \n          E\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle x_{i}(p,p\\cdot E_{i})}\n  \n.\n\nThis can be proven using the definition of excess demand:\n\nThe Marshallian demand is a bundle \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n that maximizes the agent's utility, given the budget constraint. The budget constraint here is:\n\nHence, all terms in the sum are 0 so the sum itself is 0.[6]: 317–318\n\nNeoclassical macroeconomic reasoning concludes that because of Walras's law, if all markets for goods are in equilibrium, the market for labor must also be in equilibrium. Thus, by neoclassical reasoning, Walras's law contradicts the Keynesian conclusion that negative excess demand and consequently, involuntary unemployment, may exist in the labor market, even when all markets for goods are in equilibrium. The Keynesian rebuttal[dubious – discuss] is that this neoclassical perspective ignores financial markets, which may experience excess demand (such as a \"liquidity trap\")[clarification needed] that permits an excess supply of labor and consequently, temporary involuntary unemployment, even if markets for goods are in equilibrium.[dubious – discuss][citation needed]",
        pageTitle: "Walras's law",
    },
    {
        title: "Weber–Fechner law",
        link: "https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law",
        content:
            "The Weber–Fechner laws are two related scientific laws in the field of psychophysics, known as Weber's law and Fechner's law. Both relate to human perception, more specifically the relation between the actual change in a physical stimulus and the perceived change. This includes stimuli to all senses: vision, hearing, taste, touch, and smell.\n\nErnst Heinrich Weber states that \"the minimum increase of stimulus which will produce a perceptible increase of sensation is proportional to the pre-existent stimulus,\" while Gustav Fechner's law is an inference from Weber's law (with additional assumptions) which states that the intensity of our sensation increases as the logarithm of an increase in energy rather than as rapidly as the increase.[1]\n\nBoth Weber's law and Fechner's law were formulated by Gustav Theodor Fechner (1801–1887). They were first published in 1860 in the work Elemente der Psychophysik (Elements of Psychophysics). This publication was the first work ever in this field, and where Fechner coined the term psychophysics to describe the interdisciplinary study of how humans perceive physical magnitudes.[2] He made the claim that \"...psycho-physics is an exact doctrine of the relation of function or dependence between body and soul.\"[3]\n\nErnst Heinrich Weber (1795–1878) was one of the first persons to approach the study of the human response to a physical stimulus in a quantitative fashion. Fechner was a student of Weber and named his first law in honor of his mentor, since it was Weber who had conducted the experiments needed to formulate the law.[4]\n\nFechner formulated several versions of the law, all communicating the same idea. One formulation states:\n\nSimple differential sensitivity is inversely proportional to the size of the components of the difference; relative differential sensitivity remains the same regardless of size.[2]\n\nWhat this means is that the perceived change in stimuli is inversely proportional to the initial stimuli.\n\nWeber's law also incorporates the just-noticeable difference (JND). This is the smallest change in stimuli that can be perceived. As stated above, the JND dS is proportional to the initial stimuli intensity S. Mathematically, it can be described as \n  \n    \n      \n        d\n        S\n        =\n        K\n        ⋅\n        S\n      \n    \n    {\\displaystyle dS=K\\cdot S}\n  \n\nwhere \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the reference stimulus and \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n is a constant.[5] When plotted, this relation is a straight line with zero intercept. It may be written as Ψ = k logS, with Ψ being the sensation, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n being a constant, and \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n being the physical intensity of the stimulus.\n\nWeber's law always fails at low intensities, near and below the absolute detection threshold, and often also at high intensities, but may be approximately true across a wide middle range of intensities.[6]\n\nAlthough Weber's law includes a statement of the proportionality of a perceived change to initial stimuli, Weber only refers to this as a rule of thumb regarding human perception. It was Fechner who formulated this statement as a mathematical expression referred to as Weber contrast.[2][7][8][9]\n\nd\n        p\n        =\n        \n          \n            \n              d\n              S\n            \n            S\n          \n        \n        \n        \n      \n    \n    {\\displaystyle dp={\\frac {dS}{S}}\\,\\!}\n  \n\nWeber contrast is not part of Weber's law.[2][7]\n\nFechner noticed in his own studies that different individuals have different sensitivity to certain stimuli. For example, the ability to perceive differences in light intensity could be related to how good that individual's vision is.[2] He also noted that how the human sensitivity to stimuli changes depends on which sense is affected. He used this to formulate another version of Weber's law that he named die Maßformel, the \"measurement formula\". Fechner's law states that the subjective sensation is proportional to the logarithm of the stimulus intensity. According to this law, human perceptions of sight and sound work as follows: Perceived loudness/brightness is proportional to logarithm of the actual intensity measured with an accurate nonhuman instrument.[7]\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              S\n              \n                0\n              \n            \n          \n        \n        \n        \n      \n    \n    {\\displaystyle p=k\\ln {\\frac {S}{S_{0}}}\\,\\!}\n\nThe relationship between stimulus and perception is logarithmic. This logarithmic relationship means that if a stimulus varies as a geometric progression (i.e., multiplied by a fixed factor), the corresponding perception is altered in an arithmetic progression (i.e., in additive constant amounts). For example, if a stimulus is tripled in strength (i.e., 3 × 1), the corresponding perception may be two times as strong as its original value (i.e., 1 + 1). If the stimulus is again tripled in strength (i.e., 3 × 3 × 1), the corresponding perception will be three times as strong as its original value (i.e., 1 + 1 + 1). Hence, for multiplications in stimulus strength, the strength of perception only adds. The mathematical derivations of the torques on a simple beam balance produce a description that is strictly compatible with Weber's law.[10][11]\n\nSince Weber's law fails at low intensity, so does Fechner's law.[6]\n\nAn early reference to \"Fechner's ... law\" was in 1875 by Ludimar Hermann  in Elements of Human Physiology.[12]\n\nFechner's law is a mathematical derivation of Weber contrast.\n\nd\n        p\n        =\n        k\n        \n          \n            \n              d\n              S\n            \n            S\n          \n        \n      \n    \n    {\\displaystyle dp=k{\\frac {dS}{S}}}\n\nIntegrating the mathematical expression for Weber contrast gives:\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          S\n        \n        +\n        C\n      \n    \n    {\\displaystyle p=k\\ln {S}+C}\n\nwhere \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a constant of integration and ln is the natural logarithm.\n\nTo solve for \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, assume that the perceived stimulus becomes zero at some threshold stimulus \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle S_{0}}\n  \n. Using this as a constraint, set \n  \n    \n      \n        p\n        =\n        0\n      \n    \n    {\\displaystyle p=0}\n  \n and \n  \n    \n      \n        S\n        =\n        \n          S\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle S=S_{0}}\n  \n. This gives:\n\nC\n        =\n        −\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              0\n            \n          \n        \n      \n    \n    {\\displaystyle C=-k\\ln {S_{0}}}\n\nSubstituting \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n in the integrated expression for Weber's law, the expression can be written as:\n\np\n        =\n        k\n        ln\n        ⁡\n        \n          \n            S\n            \n              S\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p=k\\ln {\\frac {S}{S_{0}}}}\n\nThe constant k is sense-specific and must be determined depending on the sense and type of stimulus.[7]\n\nWeber and Fechner conducted research on differences in light intensity and the perceived difference in weight.[2] Other sense modalities provide only mixed support for either Weber's law or Fechner's law.\n\nWeber found that the just noticeable difference (JND) between two weights was approximately proportional to the weights. Thus, if the weight of 105 g can (only just) be distinguished from that of 100 g, the JND (or differential threshold) is 5 g. If the mass is doubled, the differential threshold also doubles to 10 g, so that 210 g can be distinguished from 200 g. In this example, a weight (any weight) seems to have to increase by 5% for someone to be able to reliably detect the increase, and this minimum required fractional increase (of 5/100 of the original weight) is referred to as the \"Weber fraction\" for detecting changes in weight.  Other discrimination tasks, such as detecting changes in brightness, or in tone height (pure tone frequency), or in the length of a line shown on a screen, may have different Weber fractions, but they all obey Weber's law in that observed values need to change by at least some small but constant proportion of the current value to ensure human observers will reliably be able to detect that change.\n\nFechner did not conduct any experiments on how perceived heaviness increased with the mass of the stimulus. Instead, he assumed that all JNDs are subjectively equal, and argued mathematically that this would produce a logarithmic relation between the stimulus intensity and the sensation. These assumptions have both been questioned.[13][14] \nFollowing the work of S. S. Stevens, many researchers came to believe in the 1960s that the Stevens's power law was a more general psychophysical principle than Fechner's logarithmic law.\n\nWeber's law does not quite hold for loudness. It is a fair approximation for higher intensities, but not for lower amplitudes.[15]\n\nWeber's law does not hold at perception of higher intensities. Intensity discrimination improves at higher intensities.  The first demonstration of the phenomena was presented by Riesz in 1928, in Physical Review. This deviation of the Weber's law is known as the \"near miss\" of the Weber's law.  This term was coined by McGill and Goldberg in their paper of 1968 in Perception & Psychophysics. Their study consisted of intensity discrimination in pure tones. Further studies have shown that the near miss is observed in noise stimuli as well. Jesteadt et al. (1977)[16] demonstrated that the near miss holds across all the frequencies, and that the intensity  discrimination is not a function of frequency, and that the change in discrimination with level can be represented by a single function across all frequencies: \n  \n    \n      \n        Δ\n        I\n        \n          /\n        \n        I\n        =\n        0.463\n        \n          \n            (\n            I\n            \n              /\n            \n            \n              I\n              \n                0\n              \n            \n            )\n          \n          \n            −\n            0.072\n          \n        \n      \n    \n    {\\displaystyle \\Delta I/I=0.463{(I/I_{0})}^{-0.072}}\n  \n.[16]\n\nThe eye senses brightness approximately logarithmically over a moderate range and stellar magnitude is measured on a logarithmic scale.[17]\nThis magnitude scale was invented by the ancient Greek astronomer Hipparchus in about 150 B.C. He ranked the stars he could see in terms of their brightness, with 1 representing the brightest down to 6 representing the faintest, though now the scale has been extended beyond these limits; an increase in 5 magnitudes corresponds to a decrease in brightness by a factor of 100.[17]\nModern researchers have attempted to incorporate such perceptual effects into mathematical models of vision.[18][19]\n\nPerception of Glass patterns[20] and mirror symmetries in the presence of noise follows Weber's law in the middle range of regularity-to-noise ratios (S), but in both outer ranges, sensitivity to variations is disproportionally lower. As Maloney, Mitchison, & Barlow (1987)[21] showed for Glass patterns, and as van der Helm (2010)[22] showed for mirror symmetries, perception of these visual regularities in the whole range of regularity-to-noise ratios follows the law p = g/(2+1/S) with parameter g to be estimated using experimental data.\n\nFor vision, Weber's law implies constancy of luminance contrast. Suppose a target object is set against a background luminance \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. In order to be just visible, the target must be brighter or fainter than the background by some small amount \n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n. The Weber contrast is defined as \n  \n    \n      \n        C\n        =\n        Δ\n        B\n        \n          /\n        \n        B\n      \n    \n    {\\displaystyle C=\\Delta B/B}\n  \n, and Weber's law says that \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n should be constant for all \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n.\n\nHuman vision follows Weber's law closely at normal daylight levels (i.e. in the photopic range) but begins to break down at twilight levels (the mesopic range) and is completely inapplicable at low light levels (scotopic vision). This can be seen in data collected by Blackwell[23] and plotted by Crumey,[24] showing threshold increment log\n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n versus background luminance log\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n for various targets sizes. At daylight levels, the curves are approximately straight with slope 1, i.e. log\n  \n    \n      \n        Δ\n        B\n      \n    \n    {\\displaystyle \\Delta B}\n  \n = log\n  \n    \n      \n        B\n        +\n        c\n        o\n        n\n        s\n        t\n        .\n      \n    \n    {\\displaystyle B+const.}\n  \n, implying \n  \n    \n      \n        C\n        =\n        Δ\n        B\n        \n          /\n        \n        B\n      \n    \n    {\\displaystyle C=\\Delta B/B}\n  \n is constant. At the very darkest background levels (\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n ≲ 10− 5 cd m−2, approximately 25 mag arcsec−2)[24] the curves are flat - this is where the only visual perception is the observer's own neural noise ('dark light'). In the intermediate range, a portion can be approximated by the De Vries - Rose law, related to Ricco's law.\n\nActivation of neurons by sensory stimuli in many parts of the brain is by a proportional law: neurons change their spike rate by about 10–30%, when a stimulus (e.g. a natural scene for vision) has been applied. However, as Scheler (2017)[25] showed,\nthe population distribution of the intrinsic excitability or gain of a neuron is a heavy tail distribution, more precisely a lognormal shape, which is equivalent to a logarithmic coding scheme. Neurons may therefore spike with 5–10 fold different mean rates. Obviously, this increases the dynamic range of a neuronal population, while stimulus-derived changes remain small and linear proportional.\n\nAn analysis[26] of the length of comments in internet discussion boards across several languages shows that comment lengths obey the lognormal distribution with great precision. The authors explain the distribution as a manifestation of the Weber–Fechner law.\n\nThe Weber–Fechner law has been applied in other fields of research than just the human senses.\n\nPsychological studies show that it becomes increasingly difficult to discriminate between two numbers as the difference between them decreases. This is called the distance effect.[27][28] This is important in areas of magnitude estimation, such as dealing with large scales and estimating distances.  It may also play a role in explaining why consumers neglect to shop around to save a small percentage on a large purchase, but will shop around to save a large percentage on a small purchase which represents a much smaller absolute dollar amount.[29]\n\nIt has been hypothesized that dose-response relationships can follow Weber's Law[30] which suggests this law – which is often applied at the sensory level – originates from underlying chemoreceptor responses to cellular signaling dose relationships within the body. Dose response can be related to the Hill equation, which is closer to a power law.\n\nThere is a new branch of the literature on public finance hypothesizing that the Weber–Fechner law can explain the increasing levels of public expenditures in mature democracies. Election after election, voters demand more public goods to be effectively impressed; therefore, politicians try to increase the magnitude of this \"signal\" of competence – the size and composition of public expenditures – in order to collect more votes.[31]\n\nPreliminary research has found that pleasant emotions adhere to Weber’s Law, with accuracy in judging their intensity decreasing as pleasantness increases. However, this pattern wasn't observed for unpleasant emotions, suggesting a survival-related need for accurately discerning high-intensity negative emotions.[32]",
        pageTitle: "Weber–Fechner law",
    },
    {
        title: "Stevens's power law",
        link: "https://en.wikipedia.org/wiki/Stevens%27s_power_law",
        content:
            "Stevens' power law is an empirical relationship in psychophysics between an increased intensity or strength in a physical stimulus and the perceived magnitude increase in the sensation created by the stimulus. It is often considered to supersede the Weber–Fechner law, which is based on a logarithmic relationship between stimulus and sensation, because the power law describes a wider range of sensory comparisons, down to zero intensity.[1]\n\nThe theory is named after psychophysicist Stanley Smith Stevens (1906–1973).  Although the idea of a power law had been suggested by 19th-century researchers, Stevens is credited with reviving the law and publishing a body of psychophysical data to support it in 1957.\n\nwhere I is the intensity or strength of the stimulus in physical units (energy, weight, pressure, mixture proportions, etc.), ψ(I) is the magnitude of the sensation evoked by the stimulus, a is an exponent that depends on the type of stimulation or sensory modality, and k is a proportionality constant that depends on the units used.\n\nA distinction has been made between local psychophysics, where stimuli can only be discriminated with a probability around 50%, and global psychophysics, where the stimuli can be discriminated correctly with near certainty (Luce & Krumhansl, 1988). The Weber–Fechner law and methods described by L. L. Thurstone are generally applied in local psychophysics, whereas Stevens' methods are usually applied in global psychophysics.\n\nThe adjacent table lists the exponents reported by Stevens.\n\nThe principal methods used by Stevens to measure the perceived intensity of a stimulus were magnitude estimation and magnitude production. In magnitude estimation with a standard, the experimenter presents a stimulus called a standard and assigns it a number called the modulus. For subsequent stimuli, subjects report numerically their perceived intensity relative to the standard so as to preserve the ratio between the sensations and the numerical estimates (e.g., a sound perceived twice as loud as the standard should be given a number twice the modulus). In magnitude estimation without a standard (usually just magnitude estimation), subjects are free to choose their own standard, assigning any number to the first stimulus and all subsequent ones with the only requirement being that the ratio between sensations and numbers is preserved. In magnitude production a number and a reference stimulus is given and subjects produce a stimulus that is perceived as that number times the reference. Also used is cross-modality matching, which generally involves subjects altering the magnitude of one physical quantity, such as the brightness of a light, so that its perceived intensity is equal to the perceived intensity of another type of quantity, such as warmth or pressure.\n\nStevens generally collected magnitude estimation data from multiple observers, averaged the data across subjects, and then fitted a power function to the data.  Because the fit was generally reasonable, he concluded the power law was correct.\n\nA principal criticism has been that Stevens' approach provides neither a direct test of the power law itself nor the underlying assumptions of the magnitude estimation/production method: it simply fits curves to data points. In addition, the power law can be deduced mathematically from the Weber-Fechner logarithmic function (Mackay, 1963[2]), and the relation makes predictions consistent with data (Staddon, 1978[3]). As with all psychometric studies, Stevens' approach ignores individual differences in the stimulus-sensation relationship, and there are generally large individual differences in this relationship that averaging the data will obscure (Greem & Luce 1974).\n\nStevens' main assertion was that using magnitude estimations/productions respondents were able to make judgements on a ratio scale (i.e., if x and y are values on a given ratio scale, then there exists a constant k such that x = ky). In the context of axiomatic psychophysics, (Narens 1996) formulated a testable property capturing the implicit underlying assumption this assertion entailed. Specifically, for two proportions p and q, and three stimuli, x, y, z, if y is judged p times x, z is judged q times y, then t = pq times x should be equal to z. This amounts to assuming that respondents interpret numbers in a veridical way. This property was unambiguously rejected (Ellermeier & Faulhammer 2000, Zimmer 2005). Without assuming veridical interpretation of numbers, (Narens 1996) formulated another property that, if sustained, meant that respondents could make ratio scaled judgments, namely, if y is judged p times x, z is judged q times y, and if y' is judged q times x, z' is judged p times y', then z should equal z'. This property has been sustained in a variety of situations (Ellermeier & Faulhammer 2000, Zimmer 2005).\n\nCritics of the power law also point out that the validity of the law is contingent on the measurement of perceived stimulus intensity that is employed in the relevant experiments. Luce (2002), under the condition that respondents' numerical distortion function and the psychophysical functions could be separated, formulated a behavioral condition equivalent to the psychophysical function being a power function. This condition was confirmed for just over half the respondents, and the power form was found to be a reasonable approximation for the rest (Steingrimsson & Luce 2006).\n\nIt has also been questioned, particularly in terms of signal detection theory, whether any given stimulus is actually associated with a particular and absolute perceived intensity; i.e. one that is independent of contextual factors and conditions. Consistent with this, Luce (1990, p. 73) observed that \"by introducing contexts such as background noise in loudness judgements, the shape of the magnitude estimation functions certainly deviates sharply from a power function\". Indeed, nearly all sensory judgments can be changed by the context in which a stimulus is perceived.",
        pageTitle: "Stevens's power law",
    },
    {
        title: "Weyl law",
        link: "https://en.wikipedia.org/wiki/Weyl_law",
        content:
            'In mathematics, especially spectral theory, Weyl\'s law describes the asymptotic behavior of eigenvalues of the Laplace–Beltrami operator. This description was discovered in 1911 (in the \n  \n    \n      \n        d\n        =\n        2\n        ,\n        3\n      \n    \n    {\\displaystyle d=2,3}\n  \n case) by Hermann Weyl for eigenvalues for the Laplace–Beltrami operator acting on functions that vanish at the boundary of a bounded domain \n  \n    \n      \n        Ω\n        ⊂\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\Omega \\subset \\mathbb {R} ^{d}}\n  \n. In particular, he proved that the number, \n  \n    \n      \n        N\n        (\n        λ\n        )\n      \n    \n    {\\displaystyle N(\\lambda )}\n  \n, of Dirichlet eigenvalues (counting their multiplicities) less than or equal to \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n satisfies\n\nwhere  \n  \n    \n      \n        \n          ω\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\omega _{d}}\n  \n is a volume of the unit ball in \n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d}}\n  \n.[1] In 1912 he provided a new proof based on variational methods.[2][3] Weyl\'s law can be extended to closed Riemannian manifolds, where another proof can be given using the Minakshisundaram–Pleijel zeta function.\n\nThe Weyl law has been extended to more general domains and operators.  For the Schrödinger operator\n\nas \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n tending to \n  \n    \n      \n        +\n        ∞\n      \n    \n    {\\displaystyle +\\infty }\n  \n or to a bottom of essential spectrum and/or \n  \n    \n      \n        h\n        →\n        +\n        0\n      \n    \n    {\\displaystyle h\\to +0}\n  \n.\n\nHere \n  \n    \n      \n        N\n        (\n        E\n        ,\n        h\n        )\n      \n    \n    {\\displaystyle N(E,h)}\n  \n is the number of eigenvalues of \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n below \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n unless there is essential spectrum below \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n in which case \n  \n    \n      \n        N\n        (\n        E\n        ,\n        h\n        )\n        =\n        +\n        ∞\n      \n    \n    {\\displaystyle N(E,h)=+\\infty }\n  \n.\n\nIn the development of spectral asymptotics, the crucial role was played by variational methods and microlocal analysis.\n\nThe extended Weyl law fails in certain situations. In particular, the extended Weyl law "claims" that there is no essential spectrum  if and only if  the right-hand expression is finite for all \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n.\n\nIf one considers domains with cusps (i.e. "shrinking exits to infinity") then the (extended) Weyl law claims that there is no essential spectrum if and only if the volume is finite. However for the Dirichlet Laplacian there is no essential spectrum even if the volume is infinite as long as cusps shrinks at infinity (so the finiteness of the volume is not necessary).\n\nOn the other hand, for the Neumann Laplacian there is an essential spectrum unless cusps shrinks at infinity faster than the negative exponent (so the finiteness of the volume is not sufficient).\n\nwhere the remainder term is negative for Dirichlet boundary conditions and positive for Neumann.\nThe remainder estimate was improved upon by many mathematicians.\n\nIn 1922, Richard Courant proved a bound of \n  \n    \n      \n        O\n        (\n        \n          λ\n          \n            (\n            d\n            −\n            1\n            )\n            \n              /\n            \n            2\n          \n        \n        log\n        ⁡\n        λ\n        )\n      \n    \n    {\\displaystyle O(\\lambda ^{(d-1)/2}\\log \\lambda )}\n  \n.\nIn 1952, Boris Levitan proved the tighter bound of \n  \n    \n      \n        O\n        (\n        \n          λ\n          \n            (\n            d\n            −\n            1\n            )\n            \n              /\n            \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(\\lambda ^{(d-1)/2})}\n  \n for compact closed manifolds. Robert Seeley extended this to include certain Euclidean domains in 1978.[4]\nIn 1975, Hans Duistermaat and Victor Guillemin proved the bound of\n\n  \n    \n      \n        o\n        (\n        \n          λ\n          \n            (\n            d\n            −\n            1\n            )\n            \n              /\n            \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle o(\\lambda ^{(d-1)/2})}\n  \n when the set of periodic bicharacteristics has measure 0.[5]  This was finally generalized by Victor Ivrii in 1980.[6]  This generalization assumes that the set of periodic trajectories of a billiard in \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n has measure 0, which Ivrii conjectured is fulfilled for all bounded Euclidean domains with smooth boundaries.  Since then, similar results have been obtained for wider classes of operators.',
        pageTitle: "Weyl law",
    },
    {
        title: "Wiedemann–Franz law",
        link: "https://en.wikipedia.org/wiki/Wiedemann%E2%80%93Franz_law",
        content:
            "In physics, the Wiedemann–Franz law states that the ratio of the electronic contribution of the thermal conductivity (κ) to the electrical conductivity (σ) of a metal is proportional to the temperature (T).[1]\n\nTheoretically, the proportionality constant L, known as the Lorenz number, is equal to\n\nwhere kB is the Boltzmann constant and e is the elementary charge.\n\nThis empirical law is named after Gustav Wiedemann and Rudolph Franz, who in 1853 reported that κ/σ has approximately the same value for different metals at the same temperature.[2] The proportionality of κ/σ with temperature was discovered by Ludvig Lorenz in 1872.[3]\n\nQualitatively, this relationship is based upon the fact that the heat and electrical transport both involve the free electrons in the metal.\n\nThe mathematical expression of the law can be derived as following. Electrical conduction of metals is a well-known phenomenon and is attributed to the free conduction electrons, which can be measured as sketched in the figure. The current density j is observed to be proportional to the applied electric field and follows Ohm's law where the prefactor is the specific electrical conductivity. Since the electric field and the current density are vectors Ohm's law is expressed here in bold face. The conductivity can in general be expressed as a tensor of the second rank (3×3 matrix). Here we restrict the discussion to isotropic, i.e. scalar conductivity. The specific resistivity is the inverse of the conductivity. Both parameters will be used in the following.\n\nPaul Drude (c. 1900) realized that the phenomenological description of conductivity can be formulated quite generally (electron-, ion-, heat- etc. conductivity). Although the phenomenological description is incorrect for conduction electrons, it can serve as a preliminary treatment.[4]\n\nThe assumption is that the electrons move freely in the solid like in an ideal gas. The force applied to the electron by the electric field leads to an acceleration according to\n\nThis would lead, however, to a constant acceleration and, ultimately, to an infinite velocity. The further assumption therefore is that the electrons bump into obstacles (like defects or phonons) once in a while which limits their free flight. This establishes an average or drift velocity Vd. The drift velocity is related to the average scattering time as becomes evident from the following relations.\n\nFrom kinetic theory of gases, \n  \n    \n      \n        κ\n        =\n        \n          \n            1\n            3\n          \n        \n        c\n        n\n        \n        ℓ\n        \n        ⟨\n        v\n        ⟩\n      \n    \n    {\\displaystyle \\kappa ={\\frac {1}{3}}cn\\,\\ell \\,\\langle v\\rangle }\n  \n, where \n  \n    \n      \n        c\n        =\n        \n          \n            3\n            2\n          \n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle c={\\frac {3}{2}}k_{\\rm {B}}}\n  \n is the heat capacity per electron, \n  \n    \n      \n        ℓ\n      \n    \n    {\\displaystyle \\ell }\n  \n is the mean free path of the electrons, and\n\nwhich is the Wiedemann–Franz law with an erroneous proportionality constant \n  \n    \n      \n        \n          \n            4\n            π\n          \n        \n        ≈\n        1.27\n      \n    \n    {\\displaystyle {\\frac {4}{\\pi }}\\approx 1.27}\n  \n.\n\nIn Drude's original paper he used \n  \n    \n      \n        ⟨\n        \n          v\n          \n            2\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle v^{2}\\rangle }\n  \n instead of \n  \n    \n      \n        ⟨\n        v\n        \n          ⟩\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\langle v\\rangle ^{2}}\n  \n, and also accidentally used a factor of 2. This meant his result is \n  \n    \n      \n        L\n        =\n        3\n        \n          \n            (\n            \n              \n                \n                  k\n                  \n                    \n                      B\n                    \n                  \n                \n                e\n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        2.22\n        ×\n        \n          10\n          \n            −\n            8\n          \n        \n        \n        \n          \n            V\n          \n          \n            2\n          \n        \n        \n          \n            K\n          \n          \n            −\n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle L=3\\left({\\frac {k_{\\rm {B}}}{e}}\\right)^{2}=2.22\\times 10^{-8}\\;\\mathrm {V} ^{2}\\mathrm {K} ^{-2},}\n  \nwhich is very close to experimental values. This is in fact due to 3 mistakes that conspired to make his result more accurate than warranted: the factor of 2 mistake; the specific heat per electron is in fact about 100 times less than \n  \n    \n      \n        \n          \n            3\n            2\n          \n        \n        \n          k\n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {3}{2}}k_{\\rm {B}}}\n  \n; the mean squared velocity of an electron is in fact about 100 times larger.[5]\n\nAfter taking into account the quantum effects, as in the free electron model, the heat capacity, mean free path and average speed of electrons are modified and the proportionality constant is then corrected to \n  \n    \n      \n        \n          \n            \n              π\n              \n                2\n              \n            \n            3\n          \n        \n        ≈\n        3.29\n      \n    \n    {\\displaystyle {\\frac {\\pi ^{2}}{3}}\\approx 3.29}\n  \n, which agrees with experimental values.\n\nThe value L0 = 2.44×10−8 V2⋅K−2 results from the fact that at low temperatures (\n  \n    \n      \n        T\n        →\n        0\n      \n    \n    {\\displaystyle T\\rightarrow 0}\n  \n K) the heat and charge currents are carried by the same quasi-particles: electrons or holes. At finite temperatures two mechanisms produce a deviation of the ratio \n  \n    \n      \n        L\n        =\n        κ\n        \n          /\n        \n        (\n        σ\n        T\n        )\n      \n    \n    {\\displaystyle L=\\kappa /(\\sigma T)}\n  \n from the theoretical Lorenz value L0: (i) other thermal carriers such as phonons or magnons, (ii) Inelastic scattering.\nAs the temperature tends to 0 K, inelastic scattering becomes weak and promotes large q scattering values (trajectory a in the figure). For each electron transported, a thermal excitation is also carried and the Lorenz number is reached L = L0. Note that in a perfect metal, inelastic scattering would be completely absent in the limit \n  \n    \n      \n        T\n        →\n        0\n      \n    \n    {\\displaystyle T\\rightarrow 0}\n  \n K and the thermal conductivity would vanish \n  \n    \n      \n        κ\n        →\n        0\n        ;\n        L\n        →\n        0\n      \n    \n    {\\displaystyle \\kappa \\rightarrow 0;L\\rightarrow 0}\n  \n.\nAt finite temperature small q scattering values are possible (trajectory b in the figure) and electrons can be transported without the transport of a thermal excitation L(T) < L0.\nAt higher temperatures, the contribution of phonons to thermal transport in a system becomes important. This can lead to L(T) > L0. Above the Debye temperature the phonon contribution to thermal transport is constant and the ratio L(T) is again found constant.\n\nExperiments have shown that the value of L, while roughly constant, is not exactly the same for all materials. \nKittel[8] gives some values of L ranging from L = 2.23×10−8V2K−2  for copper at 0 °C to L = 3.2×10−8V2K−2 for tungsten at 100 °C.  Rosenberg[9] notes that the Wiedemann–Franz law is generally valid for high temperatures and for low (i.e., a few Kelvins) temperatures, but may not hold at intermediate temperatures.\n\nIn many high purity metals both the electrical and thermal conductivities rise as temperature is decreased. In certain materials (such as silver or aluminum) however, the value of L also may decrease with temperature. In the purest samples of silver and at very low temperatures, L can drop by as much as a factor of 10.[10]\n\nIn degenerate semiconductors, the Lorenz number L has a strong dependency on certain system parameters: dimensionality, strength of interatomic interactions and Fermi level. This law is not valid or the value of the Lorenz\nnumber can be reduced at least in the following cases: manipulating electronic density of states, varying doping density and layer thickness in superlattices and materials with correlated carriers. In thermoelectric materials there are also corrections due to boundary conditions, specifically open circuit vs. closed circuit. \n[11][12]\n[13]\n\nIn 2011, N. Wakeham et al. found that the ratio of the thermal and electrical Hall conductivities in the metallic phase of quasi-one-dimensional lithium molybdenum purple bronze Li0.9Mo6O17 diverges with decreasing temperature, reaching a value five orders of magnitude larger than that found in conventional metals obeying the Wiedemann–Franz law.[14][15] This due to spin-charge separation and it behaving as a Luttinger liquid.[14]\n\nA Berkeley-led study in 2016 by S. Lee et al. also found a large violation of the Wiedemann–Franz law near the insulator-metal transition in VO2 nanobeams. In the metallic phase, the electronic contribution to thermal conductivity was much smaller than what would be expected from the Wiedemann–Franz law. The results can be explained in terms of independent propagation of charge and heat in a strongly correlated system.[16][17]\n\nIn 2020, Galen Craven and Abraham Nitzan derived a Wiedemann–Franz law for molecular systems in which electronic conduction is dominated not by free electron motion as in metals, but instead by electron transfer between molecular sites.[18] The molecular Wiedemann–Franz law is given by\n\nis the Lorenz number for molecules and \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n is the reorganization energy for electron transfer.",
        pageTitle: "Wiedemann–Franz law",
    },
    {
        title: "Wien's displacement law",
        link: "https://en.wikipedia.org/wiki/Wien%27s_displacement_law",
        content:
            "In physics, Wien's displacement law states that the black-body radiation curve for different temperatures will peak at different wavelengths that are inversely proportional to the temperature. The shift of that peak is a direct consequence of the Planck radiation law, which describes the spectral brightness or intensity of black-body radiation as a function of wavelength at any given temperature. However, it had been discovered by German physicist Wilhelm Wien several years before Max Planck developed that more general equation, and describes the entire shift of the spectrum of black-body radiation toward shorter wavelengths as temperature increases.\n\nFormally, the wavelength version of Wien's displacement law states that the spectral radiance of black-body radiation per unit wavelength, peaks at the wavelength \n  \n    \n      \n        \n          λ\n          \n            peak\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{\\text{peak}}}\n  \n given by:\n\n  \n    \n      \n        \n          λ\n          \n            peak\n          \n        \n        =\n        \n          \n            b\n            T\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{\\text{peak}}={\\frac {b}{T}}}\n  \n\nwhere T is the absolute temperature and b is a constant of proportionality called Wien's displacement constant, equal to 2.897771955...×10−3 m⋅K,[1][2] or b ≈ 2898 μm⋅K.\n\nThis is an inverse relationship between wavelength and temperature. So the higher the temperature, the shorter or smaller the wavelength of the thermal radiation. The lower the temperature, the longer or larger the wavelength of the thermal radiation. For visible radiation, hot objects emit bluer light than cool objects. If one is considering the peak of black body emission per unit frequency or per proportional bandwidth, one must use a different proportionality constant. However, the form of the law remains the same: the peak wavelength is inversely proportional to temperature, and the peak frequency is directly proportional to temperature.\n\nThere are other formulations of Wien's displacement law, which are parameterized relative to other quantities. For these alternate formulations, the form of the relationship is similar, but the proportionality constant, b, differs.\n\nWien's displacement law may be referred to as \"Wien's law\", a term which is also used for the Wien approximation.\n\nIn \"Wien's displacement law\", the word displacement refers to how the intensity-wavelength graphs appear shifted (displaced) for different temperatures.\n\nWien's displacement law is relevant to some everyday experiences:\n\nThe law is named for Wilhelm Wien, who derived it in 1893 based on a thermodynamic argument.[7] Wien considered adiabatic expansion of a cavity containing waves of light in thermal equilibrium. Using Doppler's principle, he showed that, under slow expansion or contraction, the energy of light reflecting off the walls changes in exactly the same way as the frequency. A general principle of thermodynamics is that a thermal equilibrium state, when expanded very slowly, stays in thermal equilibrium.\n\nWien himself deduced this law theoretically in 1893, following Boltzmann's thermodynamic reasoning. It had previously been observed, at least semi-quantitatively, by an American astronomer, Langley. This upward shift in \n  \n    \n      \n        \n          ν\n          \n            \n              p\n              e\n              a\n              k\n            \n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\mathrm {peak} }}\n  \n with \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is familiar to everyone—when an iron is heated in a fire, the first visible radiation (at around 900 K) is deep red, the lowest frequency visible light. Further increase in \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n causes the color to change to orange then yellow, and finally blue at very high temperatures (10,000 K or more) for which the peak in radiation intensity has moved beyond the visible into the ultraviolet.[8]\n\nThe adiabatic principle allowed Wien to conclude that for each mode, the adiabatic invariant energy/frequency is only a function of the other adiabatic invariant, the frequency/temperature. From this, he derived the \"strong version\" of Wien's displacement law: the statement that the blackbody spectral radiance is proportional to \n  \n    \n      \n        \n          ν\n          \n            3\n          \n        \n        F\n        (\n        ν\n        \n          /\n        \n        T\n        )\n      \n    \n    {\\displaystyle \\nu ^{3}F(\\nu /T)}\n  \n for some function F of a single variable. A modern variant of Wien's derivation can be found in the textbook by Wannier[9] and in a paper by E. Buckingham[10]\n\nThe consequence is that the shape of the black-body radiation function (which was not yet understood) would shift proportionally in frequency (or inversely proportionally in wavelength) with temperature. When Max Planck later formulated the correct black-body radiation function it did not explicitly include Wien's constant \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n. Rather, the Planck constant \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n was created and introduced into his new formula. From the Planck constant \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n and the Boltzmann constant \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n, Wien's constant \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n can be obtained.\n\nThe results in the tables above summarize results from other sections of this article. Percentiles are percentiles of the Planck blackbody spectrum.[11] Only 25 percent of the energy in the black-body spectrum is associated with wavelengths shorter than the value given by the peak-wavelength version of Wien's law.\n\nNotice that for a given temperature, different parameterizations imply different maximal wavelengths. In particular, the curve of intensity per unit frequency peaks at a different wavelength than the curve of intensity per unit wavelength.[12]\n\nFor example, using \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n = 6,000 K (5,730 °C; 10,340 °F) and parameterization by wavelength, the wavelength for maximal spectral radiance is \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n = 482.962 nm with corresponding frequency \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n = 620.737 THz.  For the same temperature, but parameterizing by frequency, the frequency for maximal spectral radiance is \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n = 352.735 THz with corresponding wavelength \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n = 849.907 nm.\n\nThese functions are radiance density functions, which are probability density functions scaled to give units of radiance. The density function has different shapes for different parameterizations, depending on relative stretching or compression of the abscissa, which measures the change in probability density relative to a linear change in a given parameter. Since wavelength and frequency have a reciprocal relation, they represent significantly non-linear shifts in probability density relative to one another.\n\nThe total radiance is the integral of the distribution over all positive values, and that is invariant for a given temperature under any parameterization.  Additionally, for a given temperature the radiance consisting of all photons between two wavelengths must be the same regardless of which distribution you use.  That is to say, integrating the wavelength distribution from \n  \n    \n      \n        \n          λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{1}}\n  \n to \n  \n    \n      \n        \n          λ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{2}}\n  \n will result in the same value as integrating the frequency distribution between the two frequencies that correspond to \n  \n    \n      \n        \n          λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{1}}\n  \n and \n  \n    \n      \n        \n          λ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{2}}\n  \n, namely from \n  \n    \n      \n        c\n        \n          /\n        \n        \n          λ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle c/\\lambda _{2}}\n  \n to \n  \n    \n      \n        c\n        \n          /\n        \n        \n          λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle c/\\lambda _{1}}\n  \n.[13] However, the distribution shape depends on the parameterization, and for a different parameterization the distribution will typically have a different peak density, as these calculations demonstrate.[12]\n\nThe important point of Wien's law, however, is that any such wavelength marker, including the median wavelength (or, alternatively, the wavelength below which any specified percentage of the emission occurs) is proportional to the reciprocal of temperature.  That is, the shape of the distribution for a given parameterization scales with and translates according to temperature, and can be calculated once for a canonical temperature, then appropriately shifted and scaled to obtain the distribution for another temperature.  This is a consequence of the strong statement of Wien's law.\n\nFor spectral flux considered per unit frequency \n  \n    \n      \n        d\n        ν\n      \n    \n    {\\displaystyle d\\nu }\n  \n (in hertz), Wien's displacement law describes a peak emission at the optical frequency \n  \n    \n      \n        \n          ν\n          \n            peak\n          \n        \n      \n    \n    {\\displaystyle \\nu _{\\text{peak}}}\n  \n given by:[14]\n\n  \n    \n      \n        \n          ν\n          \n            peak\n          \n        \n        =\n        \n          \n            x\n            h\n          \n        \n        k\n        \n        T\n        ≈\n        (\n        5.879\n        ×\n        \n          10\n          \n            10\n          \n        \n         \n        \n          H\n          z\n          \n            /\n          \n          K\n        \n        )\n        ⋅\n        T\n      \n    \n    {\\displaystyle \\nu _{\\text{peak}}={x \\over h}k\\,T\\approx (5.879\\times 10^{10}\\ \\mathrm {Hz/K} )\\cdot T}\n  \n\nor equivalently\n\n  \n    \n      \n        h\n        \n          ν\n          \n            peak\n          \n        \n        =\n        x\n        \n        k\n        \n        T\n        ≈\n        (\n        2.431\n        ×\n        \n          10\n          \n            −\n            4\n          \n        \n         \n        \n          e\n          V\n          \n            /\n          \n          K\n        \n        )\n        ⋅\n        T\n      \n    \n    {\\displaystyle h\\nu _{\\text{peak}}=x\\,k\\,T\\approx (2.431\\times 10^{-4}\\ \\mathrm {eV/K} )\\cdot T}\n  \n\nwhere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n = 2.821439372122078893...[15] is a constant resulting from the maximization equation, k is the Boltzmann constant, h is the Planck constant, and T is the absolute temperature. With the emission now considered per unit frequency, this peak now corresponds to a wavelength about 76% longer than the peak considered per unit wavelength. The relevant math is detailed in the next section.\n\nPlanck's law for the spectrum of black-body radiation predicts the Wien displacement law and may be used to numerically evaluate the constant relating temperature and the peak parameter value for any particular parameterization. Commonly a wavelength parameterization is used and in that case the black body spectral radiance (power per emitting area per solid angle) is:\n\n  \n    \n      \n        \n          u\n          \n            λ\n          \n        \n        (\n        λ\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              λ\n              \n                5\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  h\n                  c\n                  \n                    /\n                  \n                  λ\n                  k\n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle u_{\\lambda }(\\lambda ,T)={2hc^{2} \\over \\lambda ^{5}}{1 \\over e^{hc/\\lambda kT}-1}.}\n\nDifferentiating \n  \n    \n      \n        u\n        (\n        λ\n        ,\n        T\n        )\n      \n    \n    {\\displaystyle u(\\lambda ,T)}\n  \n with respect to \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n and setting the derivative equal to zero gives:\n\n  \n    \n      \n        \n          \n            \n              ∂\n              u\n            \n            \n              ∂\n              λ\n            \n          \n        \n        =\n        2\n        h\n        \n          c\n          \n            2\n          \n        \n        \n          (\n          \n            \n              \n                \n                  h\n                  c\n                \n                \n                  k\n                  T\n                  \n                    λ\n                    \n                      7\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  e\n                  \n                    h\n                    c\n                    \n                      /\n                    \n                    λ\n                    k\n                    T\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        e\n                        \n                          h\n                          c\n                          \n                            /\n                          \n                          λ\n                          k\n                          T\n                        \n                      \n                      −\n                      1\n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n            \n            −\n            \n              \n                1\n                \n                  λ\n                  \n                    6\n                  \n                \n              \n            \n            \n              \n                5\n                \n                  \n                    e\n                    \n                      h\n                      c\n                      \n                        /\n                      \n                      λ\n                      k\n                      T\n                    \n                  \n                  −\n                  1\n                \n              \n            \n          \n          )\n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle {\\partial u \\over \\partial \\lambda }=2hc^{2}\\left({hc \\over kT\\lambda ^{7}}{e^{hc/\\lambda kT} \\over \\left(e^{hc/\\lambda kT}-1\\right)^{2}}-{1 \\over \\lambda ^{6}}{5 \\over e^{hc/\\lambda kT}-1}\\right)=0,}\n  \n\nwhich can be simplified to give:\n\n  \n    \n      \n        \n          \n            \n              h\n              c\n            \n            \n              λ\n              k\n              T\n            \n          \n        \n        \n          \n            \n              e\n              \n                h\n                c\n                \n                  /\n                \n                λ\n                k\n                T\n              \n            \n            \n              \n                e\n                \n                  h\n                  c\n                  \n                    /\n                  \n                  λ\n                  k\n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        −\n        5\n        =\n        0.\n      \n    \n    {\\displaystyle {hc \\over \\lambda kT}{e^{hc/\\lambda kT} \\over e^{hc/\\lambda kT}-1}-5=0.}\n\nBy defining:\n\n  \n    \n      \n        x\n        ≡\n        \n          \n            \n              h\n              c\n            \n            \n              λ\n              k\n              T\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle x\\equiv {hc \\over \\lambda kT},}\n  \n\nthe equation becomes one in the single variable x:\n\n  \n    \n      \n        \n          \n            \n              x\n              \n                e\n                \n                  x\n                \n              \n            \n            \n              \n                e\n                \n                  x\n                \n              \n              −\n              1\n            \n          \n        \n        −\n        5\n        =\n        0.\n      \n    \n    {\\displaystyle {xe^{x} \\over e^{x}-1}-5=0.}\n  \n\nwhich is equivalent to:\n\n  \n    \n      \n        x\n        =\n        5\n        (\n        1\n        −\n        \n          e\n          \n            −\n            x\n          \n        \n        )\n        \n        .\n      \n    \n    {\\displaystyle x=5(1-e^{-x})\\,.}\n\nThis equation is solved by\n\n  \n    \n      \n        x\n        =\n        5\n        +\n        \n          W\n          \n            0\n          \n        \n        (\n        −\n        5\n        \n          e\n          \n            −\n            5\n          \n        \n        )\n      \n    \n    {\\displaystyle x=5+W_{0}(-5e^{-5})}\n  \n\nwhere \n  \n    \n      \n        \n          W\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle W_{0}}\n  \n is the principal branch of the Lambert W function, and gives \n  \n    \n      \n        x\n        =\n      \n    \n    {\\displaystyle x=}\n  \n 4.965114231744276303....[16]  Solving for the wavelength \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n in millimetres, and using kelvins for the temperature yields:[17][2]\n\nAnother common parameterization is by frequency. The derivation yielding peak parameter value is similar, but starts with the form of Planck's law as a function of frequency \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n:\n\n  \n    \n      \n        \n          u\n          \n            ν\n          \n        \n        (\n        ν\n        ,\n        T\n        )\n        =\n        \n          \n            \n              2\n              h\n              \n                ν\n                \n                  3\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n          \n            1\n            \n              \n                e\n                \n                  h\n                  ν\n                  \n                    /\n                  \n                  k\n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle u_{\\nu }(\\nu ,T)={2h\\nu ^{3} \\over c^{2}}{1 \\over e^{h\\nu /kT}-1}.}\n\nThe preceding process using this equation yields:\n\n  \n    \n      \n        −\n        \n          \n            \n              h\n              ν\n            \n            \n              k\n              T\n            \n          \n        \n        \n          \n            \n              e\n              \n                h\n                ν\n                \n                  /\n                \n                k\n                T\n              \n            \n            \n              \n                e\n                \n                  h\n                  ν\n                  \n                    /\n                  \n                  k\n                  T\n                \n              \n              −\n              1\n            \n          \n        \n        +\n        3\n        =\n        0.\n      \n    \n    {\\displaystyle -{h\\nu  \\over kT}{e^{h\\nu /kT} \\over e^{h\\nu /kT}-1}+3=0.}\n  \n\nThe net result is:\n\n  \n    \n      \n        x\n        =\n        3\n        (\n        1\n        −\n        \n          e\n          \n            −\n            x\n          \n        \n        )\n        \n        .\n      \n    \n    {\\displaystyle x=3(1-e^{-x})\\,.}\n  \n\nThis is similarly solved with the Lambert W function:[18]\n\n  \n    \n      \n        x\n        =\n        3\n        +\n        \n          W\n          \n            0\n          \n        \n        (\n        −\n        3\n        \n          e\n          \n            −\n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle x=3+W_{0}(-3e^{-3})}\n  \n\ngiving \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n = 2.821439372122078893....[15]\n\nSolving for \n  \n    \n      \n        ν\n      \n    \n    {\\displaystyle \\nu }\n  \n produces:[14]\n\nUsing the implicit equation \n  \n    \n      \n        x\n        =\n        4\n        (\n        1\n        −\n        \n          e\n          \n            −\n            x\n          \n        \n        )\n      \n    \n    {\\displaystyle x=4(1-e^{-x})}\n  \n yields the peak in the spectral radiance density function expressed in the parameter radiance per proportional bandwidth. (That is, the density of irradiance per frequency bandwidth proportional to the frequency itself, which can be calculated by considering infinitesimal intervals of \n  \n    \n      \n        ln\n        ⁡\n        ν\n      \n    \n    {\\displaystyle \\ln \\nu }\n  \n (or equivalently \n  \n    \n      \n        ln\n        ⁡\n        λ\n      \n    \n    {\\displaystyle \\ln \\lambda }\n  \n) rather of frequency itself.) This is perhaps a more intuitive way of presenting \"wavelength of peak emission\". That yields \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n = 3.920690394872886343....[19]\n\nAnother way of characterizing the radiance distribution is via the mean photon energy[12]\n\n  \n    \n      \n        ⟨\n        \n          E\n          \n            \n              phot\n            \n          \n        \n        ⟩\n        =\n        \n          \n            \n              π\n              \n                4\n              \n            \n            \n              30\n              \n              ζ\n              (\n              3\n              )\n            \n          \n        \n        k\n        \n        T\n        ≈\n        (\n        \n          3.7294\n          ×\n          \n            10\n            \n              −\n              23\n            \n          \n          \n          J\n          \n            /\n          \n          K\n        \n        )\n        ⋅\n        T\n        \n        ,\n      \n    \n    {\\displaystyle \\langle E_{\\textrm {phot}}\\rangle ={\\frac {\\pi ^{4}}{30\\,\\zeta (3)}}k\\,T\\approx (\\mathrm {3.7294\\times 10^{-23}\\,J/K} )\\cdot T\\;,}\n  \n\nwhere \n  \n    \n      \n        ζ\n      \n    \n    {\\displaystyle \\zeta }\n  \n is the Riemann zeta function.\nThe wavelength corresponding to the mean photon energy is given by \n\n  \n    \n      \n        \n          λ\n          \n            ⟨\n            E\n            ⟩\n          \n        \n        ≈\n        (\n        \n          0.532\n          \n          65\n          \n          c\n          m\n          \n            ⋅\n          \n          K\n        \n        )\n        \n          /\n        \n        T\n        \n        .\n      \n    \n    {\\displaystyle \\lambda _{\\langle E\\rangle }\\approx (\\mathrm {0.532\\,65\\,cm{\\cdot }K} )/T\\,.}\n\nMarr and Wilkin (2012) contend that the widespread teaching of Wien's displacement law in introductory courses is undesirable, and it would be better replaced by alternate material. They argue that teaching the law is problematic because:\n\nThey suggest that the average photon energy be presented in place of Wien's displacement law, as being a more physically meaningful indicator of changes that occur with changing temperature. In connection with this, they recommend that the average number of photons per second be discussed in connection with the Stefan–Boltzmann law. They recommend that the Planck spectrum be plotted as a \"spectral energy density per fractional bandwidth distribution,\" using a logarithmic scale for the wavelength or frequency.[12]",
        pageTitle: "Wien's displacement law",
    },
    {
        title: "Wiio's laws",
        link: "https://en.wikipedia.org/wiki/Wiio%27s_laws",
        content:
            'Wiio\'s laws are humoristically formulated observations about how humans communicate.\n\nWiio\'s laws are usually summarized with "Human communications usually fail except by accident", which is the main observation made by Professor Osmo Antero Wiio in 1978.[1][2][3]\n\nThe fundamental Wiio\'s law states that "Communication usually fails, except by accident". The full set of laws is as follows:',
        pageTitle: "Wiio's laws",
    },
    {
        title: "Wike's law of low odd primes",
        link: "https://en.wikipedia.org/wiki/Wike%27s_law_of_low_odd_primes",
        content:
            "Wike's law of low odd primes is a methodological principle to help design experiments in psychology. It is: \"If the number of experimental treatments is a low odd prime number, then the experimental design is unbalanced and partially confounded\" (Wike, 1973, pp. 192–193).\n\nThis law was stated by Edwin Wike in a humorous article in which he also admits that the association of his name with the law is an example of Stigler's law of eponymy.\n\nThe lowest odd prime number is three. Wike illustrates how this yields an unbalanced design with an invented study in which researchers investigated the effects of water beds on sexual satisfaction. The fictitious researchers randomly assigned couples to three groups: those having sex on a conventional bed,  those having sex on a water bed, and those having sex on a water bed having also taken a sea sickness pill. Wike pointed out that any differences in sexual satisfaction among the three groups could be due to the water bed or to the sea sickness pill. It requires a fourth group, couples taking the pill and using a conventional bed, to balance the design and to allow the researchers to attribute any differences in sexual satisfaction among the groups to the sort of bed, to the pill, or to their interaction.",
        pageTitle: "Wike's law of low odd primes",
    },
    {
        title: "Winter's law",
        link: "https://en.wikipedia.org/wiki/Winter%27s_law",
        content:
            "Winter's law, named after Werner Winter, who postulated it in 1978, is a proposed sound law operating on Balto-Slavic short vowels */e/, */o/, */a/ (< Proto-Indo-European (PIE) *h₂e), */i/ and */u/ according to which they lengthen before unaspirated voiced stops, and that syllable gains a rising, acute accent.\n\nWinter's law distinguishes the Balto-Slavic reflexes of PIE */b/, */d/, */g/, */gʷ/ (before which Winter's law operates in closed syllables) and PIE */bʰ/, */dʰ/, */gʰ/, */gʷʰ/ (before which there is no effect of Winter's law). Therefore in relative chronology, Winter's law operated before PIE aspirated stops */bʰ/, */dʰ/, */gʰ/ merged with PIE plain voiced stops */b/, */d/, */g/ in Balto-Slavic.\n\nSecondarily, it distinguishes the reflexes of PIE *h₂e > */a/ and PIE */o/ which otherwise merged to */a/ in Balto-Slavic. Winter's law lengthened old */a/ (< PIE *h₂e) into Balto-Slavic */ā/ (> Lithuanian /o/, Latvian /ā/,  OCS /a/) and old */o/ into Balto-Slavic */ō/ (> Lithuanian and Latvian uo, but OCS /a/). A later Common Slavic innovation merged the reflexes of Balto-Slavic */ā/ and */ō/ into OCS /a/, so Winter's law operated before the common Balto-Slavic change */o/ > */a/.\n\nThe original formulation claimed vowels regularly lengthened in front of PIE voiced stops in all environments. While numerous examples supported this, many counterexamples existed such as OCS stogъ \"stack\" < PBS *stagas < PIE *stógos, OCS voda \"water\" < PBS *wadō < PIE *wodṓr (collective noun formed from PIE *wódr̥). Matasović adjusted Winter's law in 1994 to operate only on closed syllables, which was used in the Lexikon der indogermanischen Verben. Kortlandt, Shintani, Rasmussen, Dybo and Holst vary the blocking mechanism differently.\n\nNot all Balto-Slavic historical linguists accept Winter's law. Patri (2006) found that exceptions to the law create a too heterogeneous and voluminous set of data to allow any phonological generalization into a law.",
        pageTitle: "Winter's law",
    },
    {
        title: "Wirth's law",
        link: "https://en.wikipedia.org/wiki/Wirth%27s_law",
        content:
            'Wirth\'s law is an adage on computer performance which states that software is getting slower more rapidly than hardware is becoming faster.\n\nThe adage is named after Niklaus Wirth, a computer scientist who discussed it in his 1995 article "A Plea for Lean Software".[1][2]\n\nWirth attributed the saying to Martin Reiser, who in the preface to his book on the Oberon System wrote: "The hope is that the progress in hardware will cure all software ills. However, a critical observer may observe that software manages to outgrow hardware in size and sluggishness."[3] Other observers had noted this for some time before; indeed, the trend was becoming obvious as early as 1987.[4]\n\nHe states two contributing factors to the acceptance of ever-growing software as: "rapidly growing hardware performance" and "customers\' ignorance of features that are essential versus nice-to-have".[1] Enhanced user convenience and functionality supposedly justify the increased size of software, but Wirth argues that people are increasingly misinterpreting complexity as sophistication, that "these details are cute but not essential, and they have a hidden cost".[1] As a result, he calls for the creation of "leaner" software and pioneered the development of Oberon, a software system developed between 1986 and 1989 based on nothing but hardware. Its primary goal was to show that software can be developed with a fraction of the memory capacity and processor power usually required, without sacrificing flexibility, functionality, or user convenience.[1]\n\nThe law was restated in 2009 and attributed to Google co-founder Larry Page. It has been referred to as Page\'s law.[5] The first use of that name is attributed to fellow Google co-founder Sergey Brin at the 2009 Google I/O Conference.[6]\n\nOther common forms use the names of the leading hardware and software companies of the 1990s, Intel and Microsoft, or their CEOs, Andy Grove and Bill Gates, for example "What Intel giveth, Microsoft taketh away"[7] and Andy and Bill\'s law: "What Andy giveth, Bill taketh away".[8]\n\nGates\'s law ("The speed of software halves every 18 months"[9]) is an anonymously coined variant on Wirth\'s law, its name referencing Bill Gates,[9] co-founder of Microsoft. It is an observation that the speed of commercial software generally slows by 50% every 18 months, thereby negating all the benefits of Moore\'s law. This could occur for a variety of reasons: feature creep, code cruft, developer laziness, lack of funding, forced updates, forced porting (to a newer OS or to support a new technology) or a management turnover whose design philosophy does not coincide with the previous manager.[10]\n\nMay\'s law, named after David May, is a variant stating: "Software efficiency halves every 18 months, compensating Moore\'s law".[11]',
        pageTitle: "Wirth's law",
    },
    {
        title: "Wolff's law",
        link: "https://en.wikipedia.org/wiki/Wolff%27s_law",
        content:
            "Wolff's law, developed by the German anatomist and surgeon Julius Wolff (1836–1902) in the 19th century, states that bone in a healthy animal will adapt to the loads under which it is placed.[1]  If loading on a particular bone increases, the bone will remodel itself over time to become stronger to resist that sort of loading.[2][3] The internal architecture of the trabeculae undergoes adaptive changes, followed by secondary changes to the external cortical portion of the bone,[4] perhaps becoming thicker as a result. The inverse is true as well: if the loading on a bone decreases, the bone will become less dense and weaker due to the lack of the stimulus required for continued remodeling.[5] This reduction in bone density (osteopenia) is known as stress shielding and can occur as a result of a hip replacement (or other prosthesis).[citation needed]  The normal stress on a bone is shielded from that bone by being placed on a prosthetic implant.\n\nThe remodeling of bone in response to loading is achieved via mechanotransduction, a process through which forces or other mechanical signals are converted to biochemical signals in cellular signaling.[6] Mechanotransduction leading to bone remodeling involves the steps of mechanocoupling, biochemical coupling, signal transmission, and cell response.[7] The specific effects on bone structure depend on the duration, magnitude, and rate of loading, and it has been found that only cyclic loading can induce bone formation.[7] When loaded, fluid flows away from areas of high compressive loading in the bone matrix.[8] Osteocytes are the most abundant cells in bone and are also the most sensitive to such fluid flow caused by mechanical loading.[6] Upon sensing a load, osteocytes regulate bone remodeling by signaling to other cells with signaling molecules or direct contact.[9] Additionally, osteoprogenitor cells, which may differentiate into osteoblasts or osteoclasts, are also mechanosensors and will differentiate depending on the loading condition.[9]\n\nComputational models suggest that mechanical feedback loops can stably regulate bone remodeling by reorienting trabeculae in the direction of the mechanical loads.[10]",
        pageTitle: "Wolff's law",
    },
    {
        title: "Yerkes–Dodson law",
        link: "https://en.wikipedia.org/wiki/Yerkes%E2%80%93Dodson_law",
        content:
            'The Yerkes–Dodson law is an empirical relationship between arousal and performance, originally developed by psychologists Robert M. Yerkes and John Dillingham Dodson in 1908.[1] The law dictates that performance increases with physiological or mental arousal, but only up to a point. When levels of arousal become too high, performance decreases. The process is often illustrated graphically as a bell-shaped curve which increases and then decreases with higher levels of arousal. The original paper (a study of the Japanese house mouse, described as the "dancing mouse") was only referenced ten times over the next half century, yet in four of the citing articles, these findings were described as a psychological "law".[2]\n\nResearchers have found that different tasks require different levels of arousal for optimal performance. For example, difficult or intellectually demanding tasks may require a lower level of arousal (to facilitate concentration), whereas tasks demanding stamina or persistence may be performed better with higher levels of arousal (to increase motivation).\n\nBecause of task differences, the shape of the curve can be highly variable.[3] For simple or well-learned tasks, the relationship is monotonic, and performance improves as arousal increases. For complex, unfamiliar, or difficult tasks, the relationship between arousal and performance reverses after a point, and performance thereafter declines as arousal increases.\n\nThe effect of task difficulty led to the hypothesis that the Yerkes–Dodson Law can be decomposed into two distinct factors as in a bathtub curve. The upward part of the inverted U can be thought of as the energizing effect of arousal. The downward part is caused by negative effects of arousal (or stress) on cognitive processes like attention (e.g., "tunnel vision"), memory, and problem-solving.\n\nThere has been research indicating that the correlation suggested by Yerkes and Dodson exists (such as that of Broadhurst (1959),[4] Duffy (1957),[5] and Anderson et al (1988)[6]), but a cause of the correlation has not yet successfully been established (Anderson, Revelle, & Lynch, 1989).[7]\n\nOther theories and models of arousal do not affirm the Hebb or Yerkes-Dodson curve. The widely supported theory of optimal flow presents a less simplistic understanding of arousal and skill-level match. Reversal theory actively opposes the Yerkes-Dodson law by demonstrating how the psyche operates on the principle bistability rather than homeostasis.\n\nA 2007 review by Lupien at al[8] of the effects of stress hormones (glucocorticoids, GC) and human cognition revealed that memory performance vs. circulating levels of glucocorticoids does manifest an upside-down U-shaped curve, and the authors noted the resemblance to the Yerkes–Dodson curve. For example, long-term potentiation (LTP) (the process of forming long-term memories) is optimal when glucocorticoid levels are mildly elevated, whereas significant decreases of LTP are observed after adrenalectomy (low GC state) or after exogenous glucocorticoid administration (high GC state).\nThis review also revealed that in order for a situation to induce a stress response, it has to be interpreted as one or more of the following:\n\nIt has also been shown that elevated levels of glucocorticoids enhance memory for emotionally arousing events but lead more often than not to poor memory for material unrelated to the source of stress/emotional arousal.[8]',
        pageTitle: "Yerkes–Dodson law",
    },
    {
        title: "Zawinski's law",
        link: "https://en.wikipedia.org/wiki/Zawinski%27s_law",
        content:
            "Jamie Werner Zawinski (born November 3, 1968), commonly known as jwz, is an American computer programmer, blogger, and impresario. He is best known for his role in the creation of Netscape Navigator, Netscape Mail, Lucid Emacs, Mozilla.org, and XScreenSaver. He is also the proprietor of DNA Lounge, a nightclub and live music venue in San Francisco.\n\nZawinski's programming career began at age 16 with Scott Fahlman's Spice Lisp project at Carnegie Mellon University. He then worked at AI startup Expert Technologies, Inc. followed by Robert Wilensky and Peter Norvig's AI research group at UC Berkeley, working on natural language processing.\n\nIn 1990 he began working at Lucid Inc., first working on Lucid Common Lisp, and then on Lucid's Energize C++ IDE. Lucid decided to use GNU Emacs as the text editor for their IDE due to its free license, popularity, and extensibility, and Zawinski led that project. As Zawinski and the other programmers made fundamental changes to GNU Emacs to add new functionality, tensions over how to merge these patches into the main tree eventually led to the fork of the project into GNU Emacs and Lucid Emacs (now XEmacs).[1]\n\nIn 1992 he released the first version of XScreenSaver, a free and open-source collection now containing more than 240[2]\nscreensavers. Initially released for Unix, it now supports macOS, iOS, and Android as well. On Unix systems, it also provides the framework for blanking and locking the screen. He still maintains it, with new releases coming out several times a year.[3]\n\nFollowing Lucid's bankruptcy in 1994, Zawinski was one of the initial employees of Mosaic Communications, later known as Netscape. At Netscape, he developed the Unix release of Netscape Navigator 1.0,[4][5]\nand later, Netscape Mail, the first mail reader (or Usenet reader) to natively support HTML.[6]\n\nZawinski came up with the name \"Mozilla\" (originally the internal code-name of the web browser) during a staff meeting, as a reference to Godzilla and a portmanteau of \"Mosaic killer\".[7][8]\n\nAn easter egg he coded in the Netscape browser became quite well known during the early days of the World Wide Web: typing \"about:jwz\" into the address box would take the user to his home page, and would change the browser's logo animation to a fire-breathing dragon.[9]\n\nThrough his long-time support and advocacy for free software both inside and outside the company, Zawinski is credited with having been the inspiration for Netscape's decision to open-source the source code of the browser in 1998.[10][11]\nHe was a founder of Mozilla.org, personally registering its domain name on the day of Netscape's open source announcement and helping design and run the organization through its first year.[12][13][14]\n\nWhen Netscape was acquired by AOL in 1999, he wrote a bulletin explaining that Mozilla's work would continue with or without Netscape.[15] And a year after the initial source code release, he resigned from Netscape and Mozilla, citing his disappointment that others involved in the project had decided to rewrite the code instead of incrementally improving it.[16][17]\n\nShortly after leaving Mozilla, he announced his purchase of DNA Lounge, a nightclub in San Francisco.[18][19][20][21] Zawinski purchased the nightclub in 1999 for approximately 5 million dollars and it was re-opened in July 2001, a process which he documented extensively in a blog named \"DNA Sequencing\".[22][23]\n\nIn 2016, he explored alternative funding ideas to keep the venue afloat during a downturn in attendance.[22]\n\nIn 2000, Zawinski starred in the 60-minute-long PBS documentary Code Rush, which chronicles the creation of Mozilla.org and the release of the browser source code over the course of 1998.\n\nZawinski features extensively in Josh Quittner's 1998 book Speeding the Net: The Inside Story of Netscape and How It Challenged Microsoft,[24]\nand in Glyn Moody's 2001 book, Rebel Code: Linux and the Open Source Revolution.[11]\nThere is a chapter on Zawinski in Peter Seibel's 2009 book, Coders at Work: Reflections on the Craft of Programming.[25][26]\nAnd in 2001, he was featured in California Dreamin': The Gold Rush, a documentary for German public television.[27]\n[28]\n\nZawinski appears in several video installations at the Computer History Museum's exhibit, Revolution: The First 2000 Years of Computing.[29]\n\nHe was also featured in Sleep Mode: The Art of the Screensaver,[30] a gallery exhibition curated by Rafaël Rozendaal at Rotterdam's Het Nieuwe Instituut in 2017.\n\nZawinski's Law of Software Envelopment, also known as Zawinski's Law, states:\n\nEvery program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can.\n\nSome have interpreted this as commenting on the phenomenon of software bloating with popular features.[31][32]\n\nMy point was not about copycats, it was about platformization. Apps that you \"live in\" all day have pressure to become everything and do everything. An app for editing text becomes an IDE, then an OS. An app for displaying hypertext documents becomes a mail reader, then an OS.\n\nZawinski first attained prominence as a Lisp programmer, but most of his larger projects are written in C. Despite that, he has long been critical of languages lacking memory safety and automatic memory management. He has particularly proselytized against C++. In Peter Seibel's book Coders at Work: Reflections on the Craft of Programming, Zawinski calls C++ an \"abomination... the PDP-11 assembler that thinks it's an object system\".[26][34]\n\nThough he has written and published many utilities in Perl,[35] he is not without his criticisms, characterizing Perl as \"combining all the worst aspects of C and Lisp: a billion different sublanguages in one monolithic executable. It combines the power of C with the readability of PostScript.\"[36]\n\nHe has criticized several language and library deficiencies he encountered while programming in Java, specifically the overhead of certain fundamental classes but especially the marketing and politics behind it that led Sun to conflate the language, the class library, the virtual machine, and the security model all under the same name, \"Java\" – to, he says, the detriment of them all. Despite the positive aspects, ultimately Zawinski returned to programming in C \"since it's still the only way to ship portable programs\".[37]",
        pageTitle: "Jamie Zawinski",
    },
    {
        title: "Zipf's law",
        link: "https://en.wikipedia.org/wiki/Zipf%27s_law",
        content:
            "Zipf's law (/zɪf/; .mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}German pronunciation: [tsɪpf]) is an empirical law stating that when a list of measured values is sorted in decreasing order, the value of the n-th entry is often approximately inversely proportional to n.\n\nThe best known instance of Zipf's law applies to the frequency table of words in a text or corpus of natural language:\n\nw\n            o\n            r\n            d\n             \n            f\n            r\n            e\n            q\n            u\n            e\n            n\n            c\n            y\n          \n        \n         \n        ∝\n         \n        \n          \n            1\n            \n               \n              \n                \n                  w\n                  o\n                  r\n                  d\n                   \n                  r\n                  a\n                  n\n                  k\n                \n              \n               \n            \n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\ {\\mathsf {word\\ frequency}}\\ \\propto \\ {\\frac {1}{\\ {\\mathsf {word\\ rank}}\\ }}~.}\n\nIt is usually found that the most common word occurs approximately twice as often as the next common one, three times as often as the third most common, and so on.  For example, in the Brown Corpus of American English text, the word \"the\" is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's law, the second-place word \"of\" accounts for slightly over 3.5% of words (36,411 occurrences), followed by \"and\" (28,852).[2] It is often used in the following form, called Zipf-Mandelbrot law:\n\nf\n            r\n            e\n            q\n            u\n            e\n            n\n            c\n            y\n          \n        \n         \n        ∝\n         \n        \n          \n            1\n            \n               \n              \n                \n                  (\n                  \n                     \n                    \n                      \n                        r\n                        a\n                        n\n                        k\n                      \n                    \n                    +\n                    b\n                     \n                  \n                  )\n                \n                \n                  a\n                \n              \n               \n            \n          \n        \n         \n      \n    \n    {\\displaystyle \\ {\\mathsf {frequency}}\\ \\propto \\ {\\frac {1}{\\ \\left(\\ {\\mathsf {rank}}+b\\ \\right)^{a}\\ }}\\ }\n  \n where \n  \n    \n      \n         \n        a\n         \n      \n    \n    {\\displaystyle \\ a\\ }\n  \n and \n  \n    \n      \n         \n        b\n         \n      \n    \n    {\\displaystyle \\ b\\ }\n  \n are fitted parameters, with \n  \n    \n      \n         \n        a\n        ≈\n        1\n      \n    \n    {\\displaystyle \\ a\\approx 1}\n  \n, and \n  \n    \n      \n         \n        b\n        ≈\n        2.7\n         \n      \n    \n    {\\displaystyle \\ b\\approx 2.7~}\n  \n.[1]\n\nThis law is named after the American linguist George Kingsley Zipf,[3][4][5] and is still an important concept in quantitative linguistics. It has been found to apply to many other types of data studied in the physical and social sciences.\n\nIn mathematical statistics, the concept has been formalized as the Zipfian distribution: A family of related discrete probability distributions whose rank-frequency distribution is an inverse power law relation. They are related to Benford's law and the Pareto distribution.\n\nSome sets of time-dependent empirical data deviate somewhat from Zipf's law. Such empirical distributions are said to be quasi-Zipfian.\n\nIn 1913, the German physicist Felix Auerbach observed an inverse proportionality between the population sizes of cities, and their ranks when sorted by decreasing order of that variable.[6]\n\nZipf's law had been discovered before Zipf,[a] first by the French stenographer Jean-Baptiste Estoup in 1916,[8][7] and also by G. Dewey in 1923,[9] and by E. Condon in 1928.[10]\n\nThe same relation for frequencies of words in natural language texts was observed by George Zipf in 1932,[4] but he never claimed to have originated it. In fact, Zipf did not like mathematics. In his 1932 publication,[11] the author speaks with disdain about mathematical involvement in linguistics, a.o. ibidem, p. 21:\n\nThe only mathematical expression Zipf used looks like ab2 = constant, which he \"borrowed\" from Alfred J. Lotka's 1926 publication.[12]\n\nThe same relationship was found to occur in many other contexts, and for other variables besides frequency.[1] For example, when corporations are ranked by decreasing size, their sizes are found to be inversely proportional to the rank.[13] The same relation is found for personal incomes (where it is called Pareto principle[14]), number of people watching the same TV channel,[15] notes in music,[16] cells transcriptomes,[17][18] and more.\n\nIn 1992 bioinformatician Wentian Li published a short paper[19] showing that Zipf's law emerges even in randomly generated texts. It included proof that the power law form of Zipf's law was a byproduct of ordering words by rank.\n\nFormally, the Zipf distribution on N elements assigns to the element of rank k (counting from 1) the probability:\n\nf\n        (\n        k\n        ;\n        N\n        )\n         \n        =\n         \n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      1\n                      \n                         \n                        \n                          H\n                          \n                            N\n                          \n                        \n                      \n                    \n                  \n                   \n                  \n                    \n                      1\n                      \n                         \n                        k\n                         \n                      \n                    \n                  \n                   \n                  ,\n                \n                \n                   \n                  \n                    \n                       if \n                    \n                  \n                   \n                  1\n                  ≤\n                  k\n                  ≤\n                  N\n                   \n                  ,\n                \n              \n              \n                \n                  \n\n                  \n                \n              \n              \n                \n                   \n                   \n                  0\n                   \n                   \n                  ,\n                \n                \n                   \n                  \n                    \n                       if \n                    \n                  \n                   \n                  k\n                  <\n                  1\n                   \n                  \n                    \n                       or \n                    \n                  \n                   \n                  N\n                  <\n                  k\n                   \n                  .\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\ f(k;N)~=~{\\begin{cases}{\\frac {1}{\\ H_{N}}}\\ {\\frac {1}{\\ k\\ }}\\ ,&\\ {\\mbox{ if }}\\ 1\\leq k\\leq N~,\\\\{}\\\\~~0~~,&\\ {\\mbox{ if }}\\ k<1\\ {\\mbox{ or }}\\ N<k~.\\end{cases}}}\n  \n where HN is a normalization constant: The Nth harmonic number:\n\nH\n          \n            N\n          \n        \n        ≡\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \n               \n              1\n               \n            \n            k\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle H_{N}\\equiv \\sum _{k=1}^{N}{\\frac {\\ 1\\ }{k}}~.}\n\nThe distribution is sometimes generalized to an inverse power law with exponent s instead of  1 .[20]  Namely,\n\nf\n        (\n        k\n        ;\n        N\n        ,\n        s\n        )\n        =\n        \n          \n            1\n            \n              H\n              \n                N\n                ,\n                s\n              \n            \n          \n        \n        \n        \n          \n            1\n            \n              k\n              \n                s\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(k;N,s)={\\frac {1}{H_{N,s}}}\\,{\\frac {1}{k^{s}}}}\n\nH\n          \n            N\n            ,\n            s\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            1\n            \n              k\n              \n                s\n              \n            \n          \n        \n         \n        .\n      \n    \n    {\\displaystyle H_{N,s}=\\sum _{k=1}^{N}{\\frac {1}{k^{s}}}~.}\n\nThe generalized Zipf distribution can be extended to infinitely many items (N = ∞) only if the exponent s exceeds  1 . In that case, the normalization constant HN,s becomes Riemann's zeta function,\n\nζ\n        (\n        s\n        )\n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            1\n            \n              k\n              \n                s\n              \n            \n          \n        \n        <\n        ∞\n         \n        .\n      \n    \n    {\\displaystyle \\zeta (s)=\\sum _{k=1}^{\\infty }{\\frac {1}{k^{s}}}<\\infty ~.}\n\nThe infinite item case is characterized by the Zeta distribution and is called Lotka's law. If the exponent s is  1  or less, the normalization constant HN,s diverges as N tends to infinity.\n\nEmpirically, a data set can be tested to see whether Zipf's law applies by checking the goodness of fit of an empirical distribution to the hypothesized power law distribution with a Kolmogorov–Smirnov test, and then comparing the (log) likelihood ratio of the power law distribution to alternative distributions like an exponential distribution or lognormal distribution.[21]\n\nZipf's law can be visualized by plotting the item frequency data on a log-log graph, with the axes being the logarithm of rank order, and logarithm of frequency. The data conform to Zipf's law with exponent s to the extent that the plot approximates a linear (more precisely, affine) function with slope −s. For exponent  s = 1  , one can also plot the reciprocal of the frequency (mean interword interval) against rank, or the reciprocal of rank against frequency, and compare the result with the line through the origin with slope  1 .[3]\n\nAlthough Zipf's Law holds for most natural languages, and even certain artificial ones such as Esperanto[22] and Toki Pona,[23] the reason is still not well understood.[24] Recent reviews of generative processes for Zipf's law include Mitzenmacher, \"A Brief History of Generative Models for Power Law and Lognormal Distributions\",[25] and Simkin, \"Re-inventing Willis\".[26]\n\nHowever, it may be partly explained by statistical analysis of randomly generated texts. Wentian Li has shown that in a document in which each character has been chosen randomly from a uniform distribution of all letters (plus a space character), the \"words\" with different lengths follow the macro-trend of Zipf's law (the more probable words are the shortest and have equal probability).[19] In 1959, Vitold Belevitch observed that if any of a large class of well-behaved statistical distributions (not only the normal distribution) is expressed in terms of rank and expanded into a Taylor series, the first-order truncation of the series results in Zipf's law. Further, a second-order truncation of the Taylor series resulted in Mandelbrot's law.[27][28]\n\nThe principle of least effort is another possible explanation:\nZipf himself proposed that neither speakers nor hearers using a given language wants to work any harder than necessary to reach understanding, and the process that results in approximately equal distribution of effort leads to the observed Zipf distribution.[5][29]\n\nA minimal explanation assumes that words are generated by monkeys typing randomly. If language is generated by a single monkey typing randomly, with fixed and nonzero probability of hitting each letter key or white space, then the words (letter strings separated by white spaces) produced by the monkey follows Zipf's law.[30]\n\nAnother possible cause for the Zipf distribution is a preferential attachment process, in which the value x of an item tends to grow at a rate proportional to x (intuitively, \"the rich get richer\" or \"success breeds success\"). Such a growth process results in the Yule–Simon distribution, which has been shown to fit word frequency versus rank in language[31] and population versus city rank[32] better than Zipf's law. It was originally derived to explain population versus rank in species by Yule, and applied to cities by Simon.\n\nA similar explanation is based on atlas models, systems of exchangeable positive-valued diffusion processes with drift and variance parameters that depend only on the rank of the process.  It has been shown mathematically that Zipf's law holds for Atlas models that satisfy certain natural regularity conditions.[33][34]\n\nA generalization of Zipf's law is the Zipf–Mandelbrot law, proposed by Benoit Mandelbrot, whose frequencies are:\n\nf\n        (\n        k\n        ;\n        N\n        ,\n        q\n        ,\n        s\n        )\n        =\n        \n          \n            1\n            \n               \n              C\n               \n            \n          \n        \n         \n        \n          \n            1\n            \n               \n              \n                \n                  (\n                  \n                    k\n                    +\n                    q\n                  \n                  )\n                \n                \n                  s\n                \n              \n            \n          \n        \n         \n        .\n      \n    \n    {\\displaystyle f(k;N,q,s)={\\frac {1}{\\ C\\ }}\\ {\\frac {1}{\\ \\left(k+q\\right)^{s}}}~.}\n  \n[clarification needed]\n\nThe constant C is the Hurwitz zeta function evaluated at s.\n\nZipfian distributions can be obtained from Pareto distributions by an exchange of variables.[20]\n\nThe Zipf distribution is sometimes called the discrete Pareto distribution[35] because it is analogous to the continuous Pareto distribution in the same way that the discrete uniform distribution is analogous to the continuous uniform distribution.\n\nThe tail frequencies of the Yule–Simon distribution are approximately\n\nf\n        (\n        k\n        ;\n        ρ\n        )\n        ≈\n        \n          \n            \n               \n              [\n              \n                \n                  c\n                  o\n                  n\n                  s\n                  t\n                  a\n                  n\n                  t\n                \n              \n              ]\n               \n            \n            \n              k\n              \n                (\n                ρ\n                +\n                1\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(k;\\rho )\\approx {\\frac {\\ [{\\mathsf {constant}}]\\ }{k^{(\\rho +1)}}}}\n  \n for any choice of  ρ > 0 .\n\nIn the parabolic fractal distribution, the logarithm of the frequency is a quadratic polynomial of the logarithm of the rank. This can markedly improve the fit over a simple power-law relationship.[36] Like fractal dimension, it is possible to calculate Zipf dimension, which is a useful parameter in the analysis of texts.[37]\n\nIt has been argued that Benford's law is a special bounded case of Zipf's law,[36] with the connection between these two laws being explained by their both originating from scale invariant functional relations from statistical physics and critical phenomena.[38] The ratios of probabilities in Benford's law are not constant. The leading digits of data satisfying Zipf's law with  s = 1 , satisfy Benford's law.\n\nFollowing Auerbach's 1913 observation, there has been substantial examination of Zipf's law for city sizes.[39] However, more recent empirical[40][41] and theoretical[42] studies have challenged the relevance of Zipf's law for cities.\n\nIn many texts in human languages, word frequencies approximately follow a Zipf distribution with exponent s close to 1; that is, the most common word occurs about n times the n-th most common one.\n\nThe actual rank-frequency plot of a natural language text deviates in some extent from the ideal Zipf distribution, especially at the two ends of the range. The deviations may depend on the language, on the topic of the text, on the author, on whether the text was translated from another language, and on the spelling rules used.[citation needed] Some deviation is inevitable because of sampling error.\n\nAt the low-frequency end, where the rank approaches N, the plot takes a staircase shape, because each word can occur only an integer number of times.\n\nIn some Romance languages, the frequencies of the dozen or so most frequent words deviate significantly from the ideal Zipf distribution, because of those words include articles inflected for grammatical gender and number.[citation needed]\n\nIn many East Asian languages, such as Chinese, Tibetan, and Vietnamese, each morpheme (word or word piece) consists of a single syllable; a word of English being often translated to a compound of two such syllables. The rank-frequency table for those morphemes deviates significantly from the ideal Zipf law, at both ends of the range.[citation needed]\n\nEven in English, the deviations from the ideal Zipf's law become more apparent as one examines large collections of texts. Analysis of a corpus of 30,000 English texts showed that only about 15% of the texts in it have a good fit to Zipf's law.  Slight changes in the definition of Zipf's law can increase this percentage up to close to 50%.[45]\n\nIn these cases, the observed frequency-rank relation can be modeled more accurately as by separate Zipf–Mandelbrot laws distributions for different subsets or subtypes of words. This is the case for the frequency-rank plot of the first 10 million words of the English Wikipedia. In particular, the frequencies of the closed class of function words in English is better described with s lower than 1, while open-ended vocabulary growth with document size and corpus size require s greater than 1 for convergence of the Generalized Harmonic Series.[3]\n\nWhen a text is encrypted in such a way that every occurrence of each distinct plaintext word is always mapped to the same encrypted word (as in the case of simple substitution ciphers, like the Caesar ciphers, or simple codebook ciphers), the frequency-rank distribution is not affected. On the other hand, if separate occurrences of the same word may be mapped to two or more different words (as happens with the Vigenère cipher), the Zipf distribution will typically have a flat part at the high-frequency end.[citation needed]\n\nZipf's law has been used for extraction of parallel fragments of texts out of comparable corpora.[46] Laurance Doyle and others have suggested the application of Zipf's law for detection of alien language in the search for extraterrestrial intelligence.[47][48]\n\nThe frequency-rank word distribution is often characteristic of the author and changes little over time. This feature has been used in the analysis of texts for authorship attribution.[49][50]\n\nThe word-like sign groups of the 15th-century codex Voynich Manuscript have been found to satisfy Zipf's law, suggesting that text is most likely not a hoax but rather written in an obscure language or cipher.[51][52]\n\nRecent analysis of whale vocalization samples shows they contain recurring phonemes whose distribution appears to closely obey Zipf's Law.[53] While this isn't proof that whale communication is a natural language, it is an intriguing discovery.",
        pageTitle: "Zipf's law",
    },
    {
        title: "Zipf–Mandelbrot law",
        link: "https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law",
        content:
            "In probability theory and statistics, the Zipf–Mandelbrot law is a discrete probability distribution. Also known as the Pareto–Zipf law, it is a power-law distribution on ranked data, named after the linguist George Kingsley Zipf, who suggested a simpler distribution called Zipf's law, and the mathematician Benoit Mandelbrot, who subsequently generalized it.\n\nwhere \n  \n    \n      \n        \n          H\n          \n            N\n            ,\n            q\n            ,\n            s\n          \n        \n      \n    \n    {\\displaystyle H_{N,q,s}}\n  \n is given by\n\nwhich may be thought of as a generalization of a harmonic number. In the formula, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n is the rank of the data, and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n and \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n are parameters of the distribution. In the limit as \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n approaches infinity, this becomes the Hurwitz zeta function \n  \n    \n      \n        ζ\n        (\n        s\n        ,\n        q\n        )\n      \n    \n    {\\displaystyle \\zeta (s,q)}\n  \n. For finite \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and \n  \n    \n      \n        q\n        =\n        0\n      \n    \n    {\\displaystyle q=0}\n  \n the Zipf–Mandelbrot law becomes Zipf's law. For infinite \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and \n  \n    \n      \n        q\n        =\n        0\n      \n    \n    {\\displaystyle q=0}\n  \n it becomes a zeta distribution.\n\nThe distribution of words ranked by their frequency in a random text corpus is approximated by a power-law distribution, known as Zipf's law.\n\nIf one plots the frequency rank of words contained in a moderately sized corpus of text data versus the number of occurrences or actual frequencies, one obtains a power-law distribution, with exponent close to one (but see Powers, 1998 and Gelbukh & Sidorov, 2001). Zipf's law implicitly assumes a fixed vocabulary size, but the Harmonic series with s = 1 does not converge, while the Zipf–Mandelbrot generalization with s > 1 does. Furthermore, there is evidence that the closed class of functional words that define a language obeys a Zipf–Mandelbrot distribution with different parameters from the open classes of contentive words that vary by topic, field and register.[1]\n\nIn ecological field studies, the relative abundance distribution (i.e. the graph of the number of species observed as a function of their abundance) is often found to conform to a Zipf–Mandelbrot law.[2]\n\nWithin music, many metrics of measuring \"pleasing\" music conform to Zipf–Mandelbrot distributions.[3]",
        pageTitle: "Zipf–Mandelbrot law",
    },
    {
        title: "List of scientific laws named after people",
        link: "https://en.wikipedia.org/wiki/List_of_scientific_laws_named_after_people",
        content:
            "This is a list of scientific laws named after people (eponymous laws). For other lists of eponyms, see eponym.",
        pageTitle: "List of scientific laws named after people",
    },
    {
        title: "Law-related lists",
        link: "https://en.wikipedia.org/wiki/Category:Law-related_lists",
        content:
            "This category has the following 19 subcategories, out of 19 total.\n\nThe following 71 pages are in this category, out of  71 total. This list may not reflect recent changes.",
        pageTitle: "Category:Law-related lists",
    },
];
